# Matter Certis v2 - ë‹¨ê³„ë³„ ê°œë°œ ê°€ì´ë“œ

## ğŸ—“ï¸ ì „ì²´ ê°œë°œ ì¼ì • (8ì£¼)

### Phase 1: í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ë° í•µì‹¬ ì•„í‚¤í…ì²˜ (2ì£¼)
### Phase 2: ë°±ì—”ë“œ ë„ë©”ì¸ êµ¬í˜„ (2ì£¼)
### Phase 3: í¬ë¡¤ë§ ì—”ì§„ êµ¬í˜„ (2ì£¼)
### Phase 4: í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„ (1.5ì£¼)
### Phase 5: í†µí•© í…ŒìŠ¤íŠ¸ ë° ìµœì í™” (0.5ì£¼)

---

## ğŸ“… Phase 1: í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ë° í•µì‹¬ ì•„í‚¤í…ì²˜ (2ì£¼)

### ğŸ¯ ëª©í‘œ
- Tauri í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
- ê¸°ë³¸ í”„ë¡œì íŠ¸ êµ¬ì¡° êµ¬ì¶•
- í•µì‹¬ ë°ì´í„° ëª¨ë¸ ì •ì˜
- ê¸°ë³¸ Tauri Commands êµ¬í˜„

### ğŸ“‹ ì‘ì—… ëª©ë¡

#### Week 1.1: í”„ë¡œì íŠ¸ ì…‹ì—… (3-4ì¼)

**1ì¼ì°¨: í”„ë¡œì íŠ¸ ì´ˆê¸°í™”**
```bash
# ìƒˆ Tauri í”„ë¡œì íŠ¸ ìƒì„±
npm create tauri-app@latest matter-certis-v2
cd matter-certis-v2

# í”„ë¡œì íŠ¸ í…œí”Œë¦¿ ì„ íƒ
# - Package manager: npm
# - Frontend template: Vanilla
# - TypeScript: Yes
```

**í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±**
```
matter-certis-v2/
â”œâ”€â”€ src-tauri/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.rs
â”‚   â”‚   â”œâ”€â”€ lib.rs
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ application/
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ use_cases/
â”‚   â”‚   â”‚   â””â”€â”€ dto/
â”‚   â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”‚   â”œâ”€â”€ http/
â”‚   â”‚   â”‚   â””â”€â”€ config/
â”‚   â”‚   â””â”€â”€ commands/
â”‚   â”‚       â””â”€â”€ mod.rs
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â””â”€â”€ tauri.conf.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.tsx
â”‚   â”œâ”€â”€ App.tsx
â”‚   â”œâ”€â”€ stores/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ types/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.ts
â””â”€â”€ tsconfig.json
```

**2ì¼ì°¨: Cargo.toml ì„¤ì •**
```toml
[package]
name = "matter-certis-v2"
version = "0.1.0"
edition = "2021"

[dependencies]
tauri = { version = "2.0", features = ["api-all"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tokio = { version = "1.0", features = ["full"] }
reqwest = { version = "0.11", features = ["json", "cookies", "gzip"] }
sqlx = { version = "0.7", features = ["sqlite", "runtime-tokio-rustls", "chrono"] }
scraper = "0.18"
anyhow = "1.0"
thiserror = "1.0"
rayon = "1.7"
futures = "0.3"
config = "0.13"
tracing = "0.1"
tracing-subscriber = "0.3"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.0", features = ["v4", "serde"] }

[build-dependencies]
tauri-build = { version = "2.0", features = [] }
```

**3ì¼ì°¨: TypeScript í™˜ê²½ êµ¬ì„±**
```json
// package.json
{
  "name": "matter-certis-v2",
  "version": "0.1.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview",
    "tauri": "tauri"
  },
  "dependencies": {
    "solid-js": "^1.8.0",
    "@solidjs/router": "^0.10.0",
    "@tauri-apps/api": "^2.0.0",
    "@kobalte/core": "^0.12.0",
    "solid-primitives": "^1.8.0",
    "date-fns": "^2.30.0",
    "nanoid": "^5.0.0"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "typescript": "^5.3.0",
    "vite": "^5.0.0",
    "vite-plugin-solid": "^2.8.0",
    "vite-tsconfig-paths": "^4.2.0",
    "vitest": "^1.0.0"
  }
}
```

**4ì¼ì°¨: ê¸°ë³¸ Vite ë° SolidJS ì„¤ì •**
```typescript
// vite.config.ts
import { defineConfig } from 'vite';
import solid from 'vite-plugin-solid';
import tsconfigPaths from 'vite-tsconfig-paths';

export default defineConfig({
  plugins: [solid(), tsconfigPaths()],
  clearScreen: false,
  server: {
    port: 1420,
    strictPort: true,
  },
  envPrefix: ['VITE_', 'TAURI_'],
  build: {
    target: process.env.TAURI_PLATFORM == 'windows' ? 'chrome105' : 'safari13',
    minify: !process.env.TAURI_DEBUG ? 'esbuild' : false,
    sourcemap: !!process.env.TAURI_DEBUG,
  },
});
```

#### Week 1.2: í•µì‹¬ ë°ì´í„° ëª¨ë¸ êµ¬í˜„ (3-4ì¼)

**5ì¼ì°¨: Rust ë„ë©”ì¸ ì—”í‹°í‹° ì •ì˜**
```rust
// src-tauri/src/domain/entities/mod.rs
pub mod product;
pub mod vendor;
pub mod crawling_session;

pub use product::*;
pub use vendor::*;
pub use crawling_session::*;
```

```rust
// src-tauri/src/domain/entities/product.rs
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};

#[derive(Debug, Clone, Serialize, Deserialize, sqlx::FromRow)]
pub struct Product {
    pub id: String,
    pub name: String,
    pub price: Option<f64>,
    pub currency: String,
    pub description: Option<String>,
    pub image_url: Option<String>,
    pub product_url: String,
    pub vendor_id: String,
    pub category: Option<String>,
    pub in_stock: bool,
    pub collected_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

impl Product {
    pub fn new(
        name: String,
        product_url: String,
        vendor_id: String,
    ) -> Self {
        let now = Utc::now();
        Self {
            id: uuid::Uuid::new_v4().to_string(),
            name,
            price: None,
            currency: "USD".to_string(),
            description: None,
            image_url: None,
            product_url,
            vendor_id,
            category: None,
            in_stock: true,
            collected_at: now,
            updated_at: now,
        }
    }
}
```

**6ì¼ì°¨: TypeScript íƒ€ì… ì •ì˜**
```typescript
// src/types/domain.ts
export interface Product {
  id: string;
  name: string;
  price?: number;
  currency: string;
  description?: string;
  imageUrl?: string;
  productUrl: string;
  vendorId: string;
  category?: string;
  inStock: boolean;
  collectedAt: string;
  updatedAt: string;
}

export interface Vendor {
  id: string;
  name: string;
  baseUrl: string;
  crawlingConfig: CrawlingConfig;
  isActive: boolean;
  lastCrawledAt?: string;
  createdAt: string;
  updatedAt: string;
}

export interface CrawlingConfig {
  maxPages?: number;
  delayBetweenRequests: number;
  maxConcurrentRequests: number;
  selectors: ProductSelectors;
  pagination?: PaginationConfig;
}

export interface ProductSelectors {
  productContainer: string;
  name: string;
  price: string;
  imageUrl?: string;
  productUrl?: string;
  inStock?: string;
}
```

**7ì¼ì°¨: ê¸°ë³¸ Repository ì¸í„°í˜ì´ìŠ¤**
```rust
// src-tauri/src/domain/repositories/mod.rs
pub mod product_repository;
pub mod vendor_repository;
pub mod crawling_session_repository;

pub use product_repository::*;
pub use vendor_repository::*;
pub use crawling_session_repository::*;
```

```rust
// src-tauri/src/domain/repositories/product_repository.rs
use async_trait::async_trait;
use crate::domain::entities::Product;
use anyhow::Result;

#[async_trait]
pub trait ProductRepository: Send + Sync {
    async fn save(&self, product: &Product) -> Result<()>;
    async fn find_by_id(&self, id: &str) -> Result<Option<Product>>;
    async fn find_by_vendor(&self, vendor_id: &str) -> Result<Vec<Product>>;
    async fn update(&self, product: &Product) -> Result<()>;
    async fn delete(&self, id: &str) -> Result<()>;
    async fn find_all(&self) -> Result<Vec<Product>>;
}
```

### ğŸ“‹ Week 1 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Tauri í”„ë¡œì íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ
- [ ] í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„± ì™„ë£Œ
- [ ] Rust ì˜ì¡´ì„± ì„¤ì • ì™„ë£Œ
- [ ] TypeScript í™˜ê²½ êµ¬ì„± ì™„ë£Œ
- [ ] SolidJS ê¸°ë³¸ ì„¤ì • ì™„ë£Œ
- [ ] í•µì‹¬ ë„ë©”ì¸ ì—”í‹°í‹° ì •ì˜ ì™„ë£Œ
- [ ] TypeScript íƒ€ì… ì •ì˜ ì™„ë£Œ
- [ ] Repository ì¸í„°í˜ì´ìŠ¤ ì •ì˜ ì™„ë£Œ

---

## ğŸ“… Phase 2: ë°±ì—”ë“œ ë„ë©”ì¸ êµ¬í˜„ (2ì£¼)

### ğŸ¯ ëª©í‘œ
- SQLite ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
- Repository êµ¬í˜„
- ê¸°ë³¸ Use Cases êµ¬í˜„
- Tauri Commands ê¸°ë³¸ êµ¬ì¡°

### ğŸ“‹ ì‘ì—… ëª©ë¡

#### Week 2.1: ë°ì´í„°ë² ì´ìŠ¤ ë° Infrastructure (3-4ì¼)

**8ì¼ì°¨: SQLite ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •**
```rust
// src-tauri/src/infrastructure/database/mod.rs
pub mod connection;
pub mod migrations;
pub mod repositories;

pub use connection::*;
```

```rust
// src-tauri/src/infrastructure/database/connection.rs
use sqlx::{SqlitePool, sqlite::SqlitePoolOptions};
use anyhow::Result;
use std::path::Path;

pub struct DatabaseConnection {
    pool: SqlitePool,
}

impl DatabaseConnection {
    pub async fn new(database_url: &str) -> Result<Self> {
        // ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
        if let Some(parent) = Path::new(database_url.trim_start_matches("sqlite://")).parent() {
            tokio::fs::create_dir_all(parent).await?;
        }

        let pool = SqlitePoolOptions::new()
            .max_connections(10)
            .connect(database_url)
            .await?;

        Ok(Self { pool })
    }

    pub fn pool(&self) -> &SqlitePool {
        &self.pool
    }

    pub async fn migrate(&self) -> Result<()> {
        sqlx::migrate!("./migrations").run(&self.pool).await?;
        Ok(())
    }
}
```

**9ì¼ì°¨: ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸**
```sql
-- migrations/001_initial.sql
CREATE TABLE IF NOT EXISTS vendors (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    base_url TEXT NOT NULL,
    crawling_config TEXT NOT NULL, -- JSON
    is_active BOOLEAN NOT NULL DEFAULT 1,
    last_crawled_at DATETIME,
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS products (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    price REAL,
    currency TEXT NOT NULL DEFAULT 'USD',
    description TEXT,
    image_url TEXT,
    product_url TEXT NOT NULL,
    vendor_id TEXT NOT NULL,
    category TEXT,
    in_stock BOOLEAN NOT NULL DEFAULT 1,
    collected_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (vendor_id) REFERENCES vendors (id)
);

CREATE TABLE IF NOT EXISTS crawling_sessions (
    id TEXT PRIMARY KEY,
    vendor_id TEXT NOT NULL,
    status TEXT NOT NULL,
    total_pages INTEGER,
    processed_pages INTEGER NOT NULL DEFAULT 0,
    products_found INTEGER NOT NULL DEFAULT 0,
    errors_count INTEGER NOT NULL DEFAULT 0,
    started_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    completed_at DATETIME,
    FOREIGN KEY (vendor_id) REFERENCES vendors (id)
);

CREATE INDEX IF NOT EXISTS idx_products_vendor_id ON products (vendor_id);
CREATE INDEX IF NOT EXISTS idx_products_collected_at ON products (collected_at);
CREATE INDEX IF NOT EXISTS idx_crawling_sessions_vendor_id ON crawling_sessions (vendor_id);
```

**10ì¼ì°¨: Repository êµ¬í˜„**
```rust
// src-tauri/src/infrastructure/database/repositories/product_repository_impl.rs
use async_trait::async_trait;
use sqlx::SqlitePool;
use anyhow::Result;
use crate::domain::{entities::Product, repositories::ProductRepository};

pub struct ProductRepositoryImpl {
    pool: SqlitePool,
}

impl ProductRepositoryImpl {
    pub fn new(pool: SqlitePool) -> Self {
        Self { pool }
    }
}

#[async_trait]
impl ProductRepository for ProductRepositoryImpl {
    async fn save(&self, product: &Product) -> Result<()> {
        sqlx::query!(
            r#"
            INSERT INTO products (
                id, name, price, currency, description, 
                image_url, product_url, vendor_id, category, 
                in_stock, collected_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            "#,
            product.id,
            product.name,
            product.price,
            product.currency,
            product.description,
            product.image_url,
            product.product_url,
            product.vendor_id,
            product.category,
            product.in_stock,
            product.collected_at,
            product.updated_at
        )
        .execute(&self.pool)
        .await?;

        Ok(())
    }

    async fn find_by_id(&self, id: &str) -> Result<Option<Product>> {
        let product = sqlx::query_as!(
            Product,
            "SELECT * FROM products WHERE id = ?",
            id
        )
        .fetch_optional(&self.pool)
        .await?;

        Ok(product)
    }

    async fn find_by_vendor(&self, vendor_id: &str) -> Result<Vec<Product>> {
        let products = sqlx::query_as!(
            Product,
            "SELECT * FROM products WHERE vendor_id = ? ORDER BY collected_at DESC",
            vendor_id
        )
        .fetch_all(&self.pool)
        .await?;

        Ok(products)
    }

    // ... ë‹¤ë¥¸ ë©”ì„œë“œë“¤ êµ¬í˜„
}
```

**11ì¼ì°¨: HTTP í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„**
```rust
// src-tauri/src/infrastructure/http/mod.rs
pub mod client;
pub mod rate_limiter;

pub use client::*;
pub use rate_limiter::*;
```

```rust
// src-tauri/src/infrastructure/http/client.rs
use reqwest::{Client, ClientBuilder, Response};
use anyhow::Result;
use std::time::Duration;
use tokio::time;

pub struct HttpClient {
    client: Client,
    rate_limiter: RateLimiter,
}

impl HttpClient {
    pub fn new(
        timeout: Duration,
        max_requests_per_second: u32,
    ) -> Result<Self> {
        let client = ClientBuilder::new()
            .timeout(timeout)
            .user_agent("MatterCertis/2.0")
            .gzip(true)
            .build()?;

        let rate_limiter = RateLimiter::new(max_requests_per_second);

        Ok(Self {
            client,
            rate_limiter,
        })
    }

    pub async fn get(&self, url: &str) -> Result<Response> {
        self.rate_limiter.wait().await;
        
        let response = self.client
            .get(url)
            .send()
            .await?;

        if !response.status().is_success() {
            anyhow::bail!("HTTP request failed: {}", response.status());
        }

        Ok(response)
    }
}
```

#### Week 2.2: Use Cases ë° Application ê³„ì¸µ (3-4ì¼)

**12ì¼ì°¨: ê¸°ë³¸ Use Cases êµ¬í˜„**
```rust
// src-tauri/src/application/use_cases/mod.rs
pub mod manage_vendors;
pub mod start_crawling;
pub mod get_products;

pub use manage_vendors::*;
pub use start_crawling::*;
pub use get_products::*;
```

```rust
// src-tauri/src/application/use_cases/manage_vendors.rs
use crate::domain::{entities::Vendor, repositories::VendorRepository};
use anyhow::Result;
use std::sync::Arc;

pub struct ManageVendorsUseCase {
    vendor_repository: Arc<dyn VendorRepository>,
}

impl ManageVendorsUseCase {
    pub fn new(vendor_repository: Arc<dyn VendorRepository>) -> Self {
        Self { vendor_repository }
    }

    pub async fn create_vendor(&self, vendor: Vendor) -> Result<()> {
        self.vendor_repository.save(&vendor).await
    }

    pub async fn get_all_vendors(&self) -> Result<Vec<Vendor>> {
        self.vendor_repository.find_all().await
    }

    pub async fn update_vendor(&self, vendor: Vendor) -> Result<()> {
        self.vendor_repository.update(&vendor).await
    }

    pub async fn delete_vendor(&self, id: &str) -> Result<()> {
        self.vendor_repository.delete(id).await
    }
}
```

**13ì¼ì°¨: Tauri Commands ê¸°ë³¸ êµ¬ì¡°**
```rust
// src-tauri/src/commands/mod.rs
pub mod vendor_commands;
pub mod crawling_commands;
pub mod product_commands;

pub use vendor_commands::*;
pub use crawling_commands::*;
pub use product_commands::*;
```

```rust
// src-tauri/src/commands/vendor_commands.rs
use tauri::State;
use crate::application::use_cases::ManageVendorsUseCase;
use crate::domain::entities::Vendor;
use anyhow::Result;

#[tauri::command]
pub async fn create_vendor(
    vendor: Vendor,
    use_case: State<'_, ManageVendorsUseCase>,
) -> Result<(), String> {
    use_case
        .create_vendor(vendor)
        .await
        .map_err(|e| e.to_string())
}

#[tauri::command]
pub async fn get_all_vendors(
    use_case: State<'_, ManageVendorsUseCase>,
) -> Result<Vec<Vendor>, String> {
    use_case
        .get_all_vendors()
        .await
        .map_err(|e| e.to_string())
}
```

**14ì¼ì°¨: ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒíƒœ ê´€ë¦¬ ë° ì˜ì¡´ì„± ì£¼ì…**
```rust
// src-tauri/src/lib.rs
use tauri::Manager;
use std::sync::Arc;

pub mod domain;
pub mod application;
pub mod infrastructure;
pub mod commands;

use infrastructure::database::DatabaseConnection;
use infrastructure::database::repositories::*;
use application::use_cases::*;

pub struct AppState {
    pub database: Arc<DatabaseConnection>,
    pub manage_vendors_use_case: Arc<ManageVendorsUseCase>,
    // ... ë‹¤ë¥¸ use cases
}

impl AppState {
    pub async fn new() -> anyhow::Result<Self> {
        // ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        let database = Arc::new(
            DatabaseConnection::new("sqlite://data/matter_certis.db").await?
        );
        
        // ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
        database.migrate().await?;

        // Repository ìƒì„±
        let vendor_repository = Arc::new(
            VendorRepositoryImpl::new(database.pool().clone())
        );

        // Use Case ìƒì„±
        let manage_vendors_use_case = Arc::new(
            ManageVendorsUseCase::new(vendor_repository)
        );

        Ok(Self {
            database,
            manage_vendors_use_case,
        })
    }
}
```

### ğŸ“‹ Week 2 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] SQLite ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì™„ë£Œ
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] Repository êµ¬í˜„ ì™„ë£Œ
- [ ] HTTP í´ë¼ì´ì–¸íŠ¸ ê¸°ë³¸ êµ¬í˜„
- [ ] ê¸°ë³¸ Use Cases êµ¬í˜„
- [ ] Tauri Commands ê¸°ë³¸ êµ¬ì¡° ì™„ì„±
- [ ] ì˜ì¡´ì„± ì£¼ì… ì„¤ì • ì™„ë£Œ

---

## ğŸ“… Phase 3: í¬ë¡¤ë§ ì—”ì§„ êµ¬í˜„ (2ì£¼)

### ğŸ¯ ëª©í‘œ
- reqwest ê¸°ë°˜ í¬ë¡¤ë§ ì—”ì§„ êµ¬í˜„
- HTML íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ
- ë³‘ë ¬ ì²˜ë¦¬ ë° Rate Limiting
- ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸

### ğŸ“‹ ì‘ì—… ëª©ë¡

#### Week 3.1: í¬ë¡¤ë§ ì—”ì§„ í•µì‹¬ êµ¬í˜„ (3-4ì¼)

**15ì¼ì°¨: í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ë„ë©”ì¸ êµ¬í˜„**
```rust
// src-tauri/src/domain/services/crawling_service.rs
use async_trait::async_trait;
use crate::domain::entities::{CrawlingSession, Product, Vendor};
use anyhow::Result;

#[async_trait]
pub trait CrawlingService: Send + Sync {
    async fn start_crawling(&self, vendor: &Vendor) -> Result<CrawlingSession>;
    async fn pause_crawling(&self, session_id: &str) -> Result<()>;
    async fn resume_crawling(&self, session_id: &str) -> Result<()>;
    async fn stop_crawling(&self, session_id: &str) -> Result<()>;
    async fn get_crawling_progress(&self, session_id: &str) -> Result<CrawlingProgress>;
}

#[derive(Debug, Clone)]
pub struct CrawlingProgress {
    pub session_id: String,
    pub total_pages: Option<u32>,
    pub processed_pages: u32,
    pub products_found: u32,
    pub errors_count: u32,
    pub current_url: Option<String>,
    pub status: CrawlingStatus,
}
```

**16ì¼ì°¨: HTML íŒŒì‹± êµ¬í˜„**
```rust
// src-tauri/src/infrastructure/crawling/mod.rs
pub mod html_parser;
pub mod product_extractor;
pub mod crawler_engine;

pub use html_parser::*;
pub use product_extractor::*;
pub use crawler_engine::*;
```

```rust
// src-tauri/src/infrastructure/crawling/html_parser.rs
use scraper::{Html, Selector};
use anyhow::Result;
use crate::domain::entities::{Product, CrawlingConfig};

pub struct HtmlParser {
    config: CrawlingConfig,
}

impl HtmlParser {
    pub fn new(config: CrawlingConfig) -> Self {
        Self { config }
    }

    pub fn extract_products(&self, html: &str, base_url: &str, vendor_id: &str) -> Result<Vec<Product>> {
        let document = Html::parse_document(html);
        let mut products = Vec::new();

        // ì œí’ˆ ì»¨í…Œì´ë„ˆ ì„ íƒì
        let container_selector = Selector::parse(&self.config.selectors.product_container)
            .map_err(|e| anyhow::anyhow!("Invalid container selector: {}", e))?;

        // ê° ì œí’ˆ ìš”ì†Œì— ëŒ€í•´ ë°ì´í„° ì¶”ì¶œ
        for element in document.select(&container_selector) {
            if let Ok(product) = self.extract_single_product(element, base_url, vendor_id) {
                products.push(product);
            }
        }

        Ok(products)
    }

    fn extract_single_product(
        &self,
        element: scraper::ElementRef,
        base_url: &str,
        vendor_id: &str,
    ) -> Result<Product> {
        // ì œí’ˆëª… ì¶”ì¶œ
        let name_selector = Selector::parse(&self.config.selectors.name)?;
        let name = element
            .select(&name_selector)
            .next()
            .and_then(|e| e.text().next())
            .ok_or_else(|| anyhow::anyhow!("Product name not found"))?
            .trim()
            .to_string();

        // ê°€ê²© ì¶”ì¶œ
        let price_selector = Selector::parse(&self.config.selectors.price)?;
        let price = element
            .select(&price_selector)
            .next()
            .and_then(|e| e.text().next())
            .and_then(|text| self.parse_price(text));

        // ì œí’ˆ URL ì¶”ì¶œ
        let product_url = if let Some(url_selector) = &self.config.selectors.product_url {
            let selector = Selector::parse(url_selector)?;
            element
                .select(&selector)
                .next()
                .and_then(|e| e.value().attr("href"))
                .map(|href| self.resolve_url(base_url, href))
                .unwrap_or_else(|| base_url.to_string())
        } else {
            base_url.to_string()
        };

        // ì´ë¯¸ì§€ URL ì¶”ì¶œ
        let image_url = if let Some(img_selector) = &self.config.selectors.image_url {
            let selector = Selector::parse(img_selector)?;
            element
                .select(&selector)
                .next()
                .and_then(|e| e.value().attr("src").or_else(|| e.value().attr("data-src")))
                .map(|src| self.resolve_url(base_url, src))
        } else {
            None
        };

        // ì¬ê³  ìƒíƒœ í™•ì¸
        let in_stock = if let Some(stock_selector) = &self.config.selectors.in_stock {
            let selector = Selector::parse(stock_selector)?;
            element.select(&selector).next().is_some()
        } else {
            true // ê¸°ë³¸ê°’: ì¬ê³  ìˆìŒ
        };

        let mut product = Product::new(name, product_url, vendor_id.to_string());
        product.price = price;
        product.image_url = image_url;
        product.in_stock = in_stock;

        Ok(product)
    }

    fn parse_price(&self, price_text: &str) -> Option<f64> {
        // ê°€ê²© í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì¶”ì¶œ
        let cleaned = price_text
            .chars()
            .filter(|c| c.is_ascii_digit() || *c == '.' || *c == ',')
            .collect::<String>()
            .replace(',', "");

        cleaned.parse().ok()
    }

    fn resolve_url(&self, base_url: &str, relative_url: &str) -> String {
        if relative_url.starts_with("http") {
            relative_url.to_string()
        } else if relative_url.starts_with("//") {
            format!("https:{}", relative_url)
        } else if relative_url.starts_with('/') {
            let base = url::Url::parse(base_url).unwrap();
            format!("{}://{}{}", base.scheme(), base.host_str().unwrap(), relative_url)
        } else {
            format!("{}/{}", base_url.trim_end_matches('/'), relative_url)
        }
    }
}
```

**17ì¼ì°¨: í¬ë¡¤ë§ ì—”ì§„ êµ¬í˜„**
```rust
// src-tauri/src/infrastructure/crawling/crawler_engine.rs
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex};
use futures::stream::{self, StreamExt};
use anyhow::Result;

use crate::domain::{
    entities::{Vendor, Product, CrawlingSession, CrawlingStatus},
    repositories::{ProductRepository, CrawlingSessionRepository},
    services::CrawlingService,
};
use crate::infrastructure::http::HttpClient;
use super::HtmlParser;

pub struct CrawlerEngine {
    http_client: Arc<HttpClient>,
    product_repository: Arc<dyn ProductRepository>,
    session_repository: Arc<dyn CrawlingSessionRepository>,
    progress_sender: Arc<Mutex<Option<mpsc::UnboundedSender<CrawlingProgress>>>>,
}

impl CrawlerEngine {
    pub fn new(
        http_client: Arc<HttpClient>,
        product_repository: Arc<dyn ProductRepository>,
        session_repository: Arc<dyn CrawlingSessionRepository>,
    ) -> Self {
        Self {
            http_client,
            product_repository,
            session_repository,
            progress_sender: Arc::new(Mutex::new(None)),
        }
    }

    pub async fn crawl_vendor(&self, vendor: &Vendor) -> Result<CrawlingSession> {
        let mut session = CrawlingSession::new(vendor.id.clone());
        session.status = CrawlingStatus::Running;
        
        // ì„¸ì…˜ ì €ì¥
        self.session_repository.save(&session).await?;

        // í˜ì´ì§€ URL ìƒì„±
        let page_urls = self.generate_page_urls(vendor)?;
        session.total_pages = Some(page_urls.len() as u32);
        self.session_repository.update(&session).await?;

        // ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
        let max_concurrent = vendor.crawling_config.max_concurrent_requests;
        let parser = HtmlParser::new(vendor.crawling_config.clone());

        let results = stream::iter(page_urls)
            .map(|url| {
                let http_client = self.http_client.clone();
                let parser = parser.clone();
                let vendor_id = vendor.id.clone();
                
                async move {
                    self.crawl_single_page(&http_client, &parser, &url, &vendor_id).await
                }
            })
            .buffer_unordered(max_concurrent as usize)
            .collect::<Vec<_>>()
            .await;

        // ê²°ê³¼ ì²˜ë¦¬
        let mut total_products = 0;
        let mut errors = 0;

        for result in results {
            match result {
                Ok(products) => {
                    for product in products {
                        if let Err(e) = self.product_repository.save(&product).await {
                            tracing::error!("Failed to save product: {}", e);
                            errors += 1;
                        } else {
                            total_products += 1;
                        }
                    }
                }
                Err(e) => {
                    tracing::error!("Failed to crawl page: {}", e);
                    errors += 1;
                }
            }

            // ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            session.processed_pages += 1;
            session.products_found = total_products;
            session.errors_count = errors;
            self.session_repository.update(&session).await?;

            // ì§„í–‰ ìƒí™© ì´ë²¤íŠ¸ ë°œì†¡
            self.send_progress_update(&session).await;
        }

        // ì„¸ì…˜ ì™„ë£Œ
        session.status = CrawlingStatus::Completed;
        session.completed_at = Some(chrono::Utc::now());
        self.session_repository.update(&session).await?;

        Ok(session)
    }

    async fn crawl_single_page(
        &self,
        http_client: &HttpClient,
        parser: &HtmlParser,
        url: &str,
        vendor_id: &str,
    ) -> Result<Vec<Product>> {
        // HTTP ìš”ì²­
        let response = http_client.get(url).await?;
        let html = response.text().await?;

        // HTML íŒŒì‹± ë° ì œí’ˆ ì¶”ì¶œ
        let products = parser.extract_products(&html, url, vendor_id)?;

        // ë”œë ˆì´ ì ìš© (Rate Limitingì€ HttpClientì—ì„œ ì²˜ë¦¬)
        Ok(products)
    }

    fn generate_page_urls(&self, vendor: &Vendor) -> Result<Vec<String>> {
        let mut urls = vec![vendor.base_url.clone()];

        // í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •ì´ ìˆëŠ” ê²½ìš°
        if let Some(pagination) = &vendor.crawling_config.pagination {
            if let Some(max_pages) = vendor.crawling_config.max_pages {
                for page in 2..=max_pages {
                    let url = pagination.url_pattern
                        .replace("{page}", &page.to_string());
                    urls.push(url);
                }
            }
        }

        Ok(urls)
    }

    async fn send_progress_update(&self, session: &CrawlingSession) {
        let progress = CrawlingProgress {
            session_id: session.id.clone(),
            total_pages: session.total_pages,
            processed_pages: session.processed_pages,
            products_found: session.products_found,
            errors_count: session.errors_count,
            current_url: None,
            status: session.status.clone(),
        };

        if let Ok(sender_guard) = self.progress_sender.try_lock() {
            if let Some(sender) = sender_guard.as_ref() {
                let _ = sender.send(progress);
            }
        }
    }
}
```

**18ì¼ì°¨: Rate Limiter êµ¬í˜„**
```rust
// src-tauri/src/infrastructure/http/rate_limiter.rs
use std::time::{Duration, Instant};
use tokio::time;

pub struct RateLimiter {
    max_requests_per_second: u32,
    last_request_time: std::sync::Mutex<Option<Instant>>,
}

impl RateLimiter {
    pub fn new(max_requests_per_second: u32) -> Self {
        Self {
            max_requests_per_second,
            last_request_time: std::sync::Mutex::new(None),
        }
    }

    pub async fn wait(&self) {
        if self.max_requests_per_second == 0 {
            return;
        }

        let min_interval = Duration::from_millis(1000 / self.max_requests_per_second as u64);
        
        let should_wait = {
            let mut last_time = self.last_request_time.lock().unwrap();
            let now = Instant::now();
            
            let should_wait = if let Some(last) = *last_time {
                let elapsed = now.duration_since(last);
                if elapsed < min_interval {
                    Some(min_interval - elapsed)
                } else {
                    None
                }
            } else {
                None
            };
            
            *last_time = Some(now);
            should_wait
        };

        if let Some(wait_duration) = should_wait {
            time::sleep(wait_duration).await;
        }
    }
}
```

#### Week 3.2: ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ë° ìµœì í™” (3-4ì¼)

**19ì¼ì°¨: Tauri ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ êµ¬í˜„**
```rust
// src-tauri/src/commands/crawling_commands.rs
use tauri::{State, Window};
use tokio::sync::mpsc;
use std::sync::Arc;

use crate::application::use_cases::StartCrawlingUseCase;
use crate::domain::entities::Vendor;
use crate::infrastructure::crawling::CrawlingProgress;

#[tauri::command]
pub async fn start_crawling_session(
    vendor_id: String,
    window: Window,
    use_case: State<'_, StartCrawlingUseCase>,
) -> Result<String, String> {
    // ì§„í–‰ ìƒí™© ì±„ë„ ìƒì„±
    let (tx, mut rx) = mpsc::unbounded_channel::<CrawlingProgress>();
    
    // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ ìƒí™© ì´ë²¤íŠ¸ ì „ì†¡
    let window_clone = window.clone();
    tokio::spawn(async move {
        while let Some(progress) = rx.recv().await {
            let _ = window_clone.emit("crawling-progress", &progress);
        }
    });

    // í¬ë¡¤ë§ ì‹œì‘
    let session = use_case
        .start_crawling(&vendor_id, Some(tx))
        .await
        .map_err(|e| e.to_string())?;

    Ok(session.id)
}

#[tauri::command]
pub async fn pause_crawling_session(
    session_id: String,
    use_case: State<'_, StartCrawlingUseCase>,
) -> Result<(), String> {
    use_case
        .pause_crawling(&session_id)
        .await
        .map_err(|e| e.to_string())
}

#[tauri::command]
pub async fn get_crawling_progress(
    session_id: String,
    use_case: State<'_, StartCrawlingUseCase>,
) -> Result<CrawlingProgress, String> {
    use_case
        .get_crawling_progress(&session_id)
        .await
        .map_err(|e| e.to_string())
}
```

**20ì¼ì°¨: TypeScript ì„œë¹„ìŠ¤ ë ˆì´ì–´**
```typescript
// src/services/crawling-service.ts
import { invoke } from '@tauri-apps/api/tauri';
import { listen, UnlistenFn } from '@tauri-apps/api/event';
import { CrawlingProgress } from '../types/domain';

export class CrawlingService {
  private progressListeners: Set<(progress: CrawlingProgress) => void> = new Set();
  private unlistenFn: UnlistenFn | null = null;

  async startCrawling(vendorId: string): Promise<string> {
    return await invoke('start_crawling_session', { vendorId });
  }

  async pauseCrawling(sessionId: string): Promise<void> {
    return await invoke('pause_crawling_session', { sessionId });
  }

  async resumeCrawling(sessionId: string): Promise<void> {
    return await invoke('resume_crawling_session', { sessionId });
  }

  async getCrawlingProgress(sessionId: string): Promise<CrawlingProgress> {
    return await invoke('get_crawling_progress', { sessionId });
  }

  async subscribeToProgress(callback: (progress: CrawlingProgress) => void): Promise<void> {
    this.progressListeners.add(callback);

    if (!this.unlistenFn) {
      this.unlistenFn = await listen('crawling-progress', (event) => {
        const progress = event.payload as CrawlingProgress;
        this.progressListeners.forEach(listener => listener(progress));
      });
    }
  }

  unsubscribeFromProgress(callback: (progress: CrawlingProgress) => void): void {
    this.progressListeners.delete(callback);

    if (this.progressListeners.size === 0 && this.unlistenFn) {
      this.unlistenFn();
      this.unlistenFn = null;
    }
  }
}

export const crawlingService = new CrawlingService();
```

**21ì¼ì°¨: ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜**
```rust
// src-tauri/src/infrastructure/crawling/error_handler.rs
use std::time::Duration;
use tokio::time;
use anyhow::Result;

pub struct ErrorHandler {
    max_retries: u32,
    base_delay: Duration,
}

impl ErrorHandler {
    pub fn new(max_retries: u32, base_delay: Duration) -> Self {
        Self {
            max_retries,
            base_delay,
        }
    }

    pub async fn retry_with_backoff<F, T, E>(&self, mut operation: F) -> Result<T>
    where
        F: FnMut() -> Result<T, E>,
        E: std::fmt::Display,
    {
        let mut last_error = None;

        for attempt in 0..=self.max_retries {
            match operation() {
                Ok(result) => return Ok(result),
                Err(e) => {
                    tracing::warn!("Attempt {} failed: {}", attempt + 1, e);
                    last_error = Some(e);

                    if attempt < self.max_retries {
                        let delay = self.base_delay * (2_u32.pow(attempt));
                        time::sleep(delay).await;
                    }
                }
            }
        }

        Err(anyhow::anyhow!(
            "Operation failed after {} attempts. Last error: {}",
            self.max_retries + 1,
            last_error.map(|e| e.to_string()).unwrap_or_else(|| "Unknown error".to_string())
        ))
    }
}
```

### ğŸ“‹ Week 3 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ë„ë©”ì¸ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
- [ ] HTML íŒŒì‹± ë° ë°ì´í„° ì¶”ì¶œ êµ¬í˜„
- [ ] reqwest ê¸°ë°˜ í¬ë¡¤ë§ ì—”ì§„ êµ¬í˜„
- [ ] Rate Limiter êµ¬í˜„
- [ ] ë³‘ë ¬ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
- [ ] ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ
- [ ] TypeScript ì„œë¹„ìŠ¤ ë ˆì´ì–´ êµ¬í˜„
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜

---

ì´ì œ Phase 4 (í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„)ì™€ Phase 5 (í†µí•© í…ŒìŠ¤íŠ¸ ë° ìµœì í™”)ì— ëŒ€í•œ ê°€ì´ë“œë¥¼ ê³„ì† ì‘ì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
