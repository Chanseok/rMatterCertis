# Matter Certis ë„ë©”ì¸ ì§€ì‹ - Rust ê°œë°œìë¥¼ ìœ„í•œ í•µì‹¬ ê°€ì´ë“œ

> ì´ ë¬¸ì„œëŠ” ê¸°ì¡´ TypeScript/Electron êµ¬í˜„ì²´ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ ë„ë©”ì¸ ì§€ì‹ì„ Rust ê°œë°œìê°€ Taurië¡œ ì¬êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ì •ë¦¬í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ ê°œìš”](#ë¹„ì¦ˆë‹ˆìŠ¤-ë„ë©”ì¸-ê°œìš”)
2. [í•µì‹¬ ì—”í„°í‹° ëª¨ë¸](#í•µì‹¬-ì—”í„°í‹°-ëª¨ë¸)
3. [í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°](#í¬ë¡¤ë§-ì›Œí¬í”Œë¡œìš°)
4. [ì•„í‚¤í…ì²˜ íŒ¨í„´](#ì•„í‚¤í…ì²˜-íŒ¨í„´)
5. [ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ](#ë°°ì¹˜-ì²˜ë¦¬-ì‹œìŠ¤í…œ)
6. [ì„¤ì • ê´€ë¦¬](#ì„¤ì •-ê´€ë¦¬)
7. [ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ](#ì´ë²¤íŠ¸-ì‹œìŠ¤í…œ)
8. [ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„](#ë°ì´í„°ë² ì´ìŠ¤-ì„¤ê³„)
9. [ì§„í–‰ ìƒí™© ì¶”ì ](#ì§„í–‰-ìƒí™©-ì¶”ì )
10. [ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´](#ì—ëŸ¬-ì²˜ë¦¬-íŒ¨í„´)
11. [Rust êµ¬í˜„ ê³ ë ¤ì‚¬í•­](#rust-êµ¬í˜„-ê³ ë ¤ì‚¬í•­)

---

## ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ ê°œìš”

### í•µì‹¬ ëª©ì 
**CSA-IoT Matter Certification ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¸ì¦ëœ ì œí’ˆ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ**

### ì£¼ìš” ê¸°ëŠ¥
1. **ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘**: ì‚¬ì´íŠ¸ì—ì„œ í˜ì´ì§€ë³„ ì œí’ˆ ëª©ë¡ ì¶”ì¶œ
2. **ì œí’ˆ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘**: ê°œë³„ ì œí’ˆ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
3. **ì¤‘ë³µ ê²€ì¦**: ë¡œì»¬ DBì™€ ë¹„êµí•˜ì—¬ ì‹ ê·œ/ê¸°ì¡´ ì œí’ˆ êµ¬ë¶„
4. **ë°°ì¹˜ ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ ë‹¨ìœ„ ì‘ì—…
5. **ì§„í–‰ ì¶”ì **: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ë° UI ì—…ë°ì´íŠ¸

### ë°ì´í„° íë¦„
```
ì‚¬ì´íŠ¸ í˜ì´ì§€ â†’ ì œí’ˆ ëª©ë¡ ì¶”ì¶œ â†’ ì¤‘ë³µ ê²€ì¦ â†’ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ â†’ ë¡œì»¬ DB ì €ì¥
```

---

## í•µì‹¬ ì—”í„°í‹° ëª¨ë¸

### Product (ê¸°ë³¸ ì œí’ˆ ì •ë³´)
```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Product {
    pub url: String,                    // ì œí’ˆ ìƒì„¸ í˜ì´ì§€ URL (Primary Key)
    pub manufacturer: Option<String>,   // ì œì¡°ì‚¬ëª…
    pub model: Option<String>,          // ëª¨ë¸ëª…
    pub certificate_id: Option<String>, // ì¸ì¦ì„œ ID
    pub page_id: Option<u32>,          // ìˆ˜ì§‘ëœ í˜ì´ì§€ ë²ˆí˜¸
    pub index_in_page: Option<u32>,    // í˜ì´ì§€ ë‚´ ìˆœì„œ
}
```

### MatterProduct (ì™„ì „í•œ ì œí’ˆ ì •ë³´)
```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MatterProduct {
    // Product ê¸°ë³¸ í•„ë“œ
    pub url: String,
    pub page_id: Option<u32>,
    pub index_in_page: Option<u32>,
    pub id: Option<String>,
    pub manufacturer: Option<String>,
    pub model: Option<String>,
    
    // ìƒì„¸ ì •ë³´ í•„ë“œ
    pub device_type: Option<String>,
    pub certificate_id: Option<String>,
    pub certification_date: Option<String>,
    pub software_version: Option<String>,
    pub hardware_version: Option<String>,
    pub vid: Option<String>,              // Vendor ID
    pub pid: Option<String>,              // Product ID
    pub family_sku: Option<String>,
    pub family_variant_sku: Option<String>,
    pub firmware_version: Option<String>,
    pub family_id: Option<String>,
    pub tis_trp_tested: Option<String>,
    pub specification_version: Option<String>,
    pub transport_interface: Option<String>,
    pub primary_device_type_id: Option<String>,
    pub application_categories: Vec<String>, // JSON ë°°ì—´ì„ Vecë¡œ
    pub created_at: Option<String>,
    pub updated_at: Option<String>,
}
```

### CrawlerConfig (í¬ë¡¤ëŸ¬ ì„¤ì •)
```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlerConfig {
    // í•µì‹¬ ì„¤ì •
    pub page_range_limit: u32,           // í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    pub product_list_retry_count: u32,   // ëª©ë¡ ìˆ˜ì§‘ ì¬ì‹œë„ íšŸìˆ˜
    pub product_detail_retry_count: u32, // ìƒì„¸ ì •ë³´ ì¬ì‹œë„ íšŸìˆ˜
    pub products_per_page: u32,          // í˜ì´ì§€ë‹¹ ì œí’ˆ ìˆ˜ (ê¸°ë³¸ 12)
    pub auto_add_to_local_db: bool,      // ìë™ DB ì €ì¥ ì—¬ë¶€
    pub auto_status_check: bool,         // ìë™ ìƒíƒœ ì²´í¬ ì—¬ë¶€
    
    // ë¸Œë¼ìš°ì € ì„¤ì •
    pub headless_browser: Option<bool>,
    pub crawler_type: Option<CrawlerType>, // "reqwest" ë˜ëŠ” "playwright"
    pub user_agent: Option<String>,
    
    // ì„±ëŠ¥ ì„¤ì •
    pub max_concurrent_tasks: Option<u32>,
    pub request_delay: Option<u64>,       // ìš”ì²­ ê°„ ì§€ì—° (ms)
    pub request_timeout: Option<u64>,     // ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ms)
    
    // ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
    pub enable_batch_processing: Option<bool>,
    pub batch_size: Option<u32>,          // ë°°ì¹˜ë‹¹ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 30)
    pub batch_delay_ms: Option<u64>,      // ë°°ì¹˜ ê°„ ì§€ì—° (ê¸°ë³¸ 2000ms)
    pub batch_retry_limit: Option<u32>,   // ë°°ì¹˜ ì¬ì‹œë„ ì œí•œ
    
    // URL ì„¤ì •
    pub base_url: Option<String>,
    pub matter_filter_url: Option<String>, // Matter í•„í„° ì ìš© URL
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum CrawlerType {
    Reqwest,
    Playwright,
}
```

### Vendor (ë²¤ë” ì •ë³´)
```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Vendor {
    pub vendor_id: String,
    pub vendor_number: u32,
    pub vendor_name: String,
    pub company_legal_name: String,
}
```

---

## í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°

### 3ë‹¨ê³„ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤

#### 1ë‹¨ê³„: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ (Product List Collection)
```rust
pub async fn collect_product_list(&self, page_limit: u32) -> Result<Vec<Product>, CrawlingError> {
    // 1. ì‚¬ì´íŠ¸ ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸
    let total_pages = self.get_site_total_pages().await?;
    let pages_to_crawl = std::cmp::min(page_limit, total_pages);
    
    // 2. ë°°ì¹˜ ì²˜ë¦¬ ì—¬ë¶€ ê²°ì •
    if self.config.enable_batch_processing && pages_to_crawl > self.config.batch_size {
        self.collect_in_batches(pages_to_crawl).await
    } else {
        self.collect_sequential(pages_to_crawl).await
    }
}
```

**í•µì‹¬ ë¡œì§:**
- ê° í˜ì´ì§€ì—ì„œ ì œí’ˆ URL, ì œì¡°ì‚¬, ëª¨ë¸ëª…, ì¸ì¦ì„œ ID ì¶”ì¶œ
- í˜ì´ì§€ë‹¹ 12ê°œ ì œí’ˆì´ ê¸°ë³¸ (ì„¤ì • ê°€ëŠ¥)
- ì‹¤íŒ¨í•œ í˜ì´ì§€ëŠ” ì¬ì‹œë„ íì— ì¶”ê°€
- ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ UIì— ì „ì†¡

#### 1.5ë‹¨ê³„: ì¤‘ë³µ ê²€ì¦ (Validation)
```rust
pub async fn validate_products(&self, products: Vec<Product>) -> ValidationResult {
    // 1. ë¡œì»¬ DBì—ì„œ ê¸°ì¡´ ì œí’ˆ URL ì¡°íšŒ
    let existing_urls = self.db.get_existing_product_urls().await?;
    
    // 2. ì œí’ˆ ë¶„ë¥˜
    let mut new_products = Vec::new();
    let mut existing_products = Vec::new();
    let mut duplicate_products = Vec::new();
    
    let mut seen_urls = HashSet::new();
    
    for product in products {
        // 1ë‹¨ê³„ ìˆ˜ì§‘ ê³¼ì •ì—ì„œì˜ ì¤‘ë³µ ê°ì§€
        if seen_urls.contains(&product.url) {
            duplicate_products.push(product);
            continue;
        }
        seen_urls.insert(product.url.clone());
        
        // ë¡œì»¬ DBì™€ì˜ ì¤‘ë³µ í™•ì¸
        if existing_urls.contains(&product.url) {
            existing_products.push(product);
        } else {
            new_products.push(product);
        }
    }
    
    ValidationResult {
        new_products,
        existing_products,
        duplicate_products,
        summary: ValidationSummary {
            total_products: products.len(),
            new_products: new_products.len(),
            // ... ê¸°íƒ€ í†µê³„
        }
    }
}
```

#### 2ë‹¨ê³„: ì œí’ˆ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (Product Detail Collection)
```rust
pub async fn collect_product_details(&self, products: Vec<Product>) -> Result<Vec<MatterProduct>, CrawlingError> {
    // 1. ë™ì‹œì„± í’€ ìƒì„±
    let semaphore = Arc::new(Semaphore::new(self.config.max_concurrent_tasks));
    let mut tasks = Vec::new();
    
    // 2. ê° ì œí’ˆì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ íƒœìŠ¤í¬ ìƒì„±
    for product in products {
        let permit = semaphore.clone().acquire_owned().await?;
        let task = tokio::spawn(async move {
            let _permit = permit; // íƒœìŠ¤í¬ ì™„ë£Œì‹œ ìë™ í•´ì œ
            self.crawl_single_product_detail(product).await
        });
        tasks.push(task);
    }
    
    // 3. ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
    let results = futures::future::join_all(tasks).await;
    
    // 4. ì„±ê³µí•œ ê²°ê³¼ë§Œ ìˆ˜ì§‘
    let mut matter_products = Vec::new();
    for result in results {
        match result? {
            Ok(matter_product) => matter_products.push(matter_product),
            Err(e) => self.log_error(e), // ì‹¤íŒ¨í•œ ì œí’ˆì€ ë¡œê¹…ë§Œ
        }
    }
    
    Ok(matter_products)
}
```

**ìƒì„¸ ì •ë³´ ì¶”ì¶œ ë¡œì§:**
- ì œí’ˆ í˜ì´ì§€ HTML íŒŒì‹±
- Matter ì¸ì¦ ê´€ë ¨ í•„ë“œ ì¶”ì¶œ (VID, PID, ì• í”Œë¦¬ì¼€ì´ì…˜ ì¹´í…Œê³ ë¦¬ ë“±)
- ì œì¡°ì‚¬ ì •ë³´ ë³´ê°• (ë²¤ë” DBì™€ ë§¤ì¹­)
- ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜

---

## ì•„í‚¤í…ì²˜ íŒ¨í„´

### Clean Architecture ê¸°ë°˜ ë ˆì´ì–´ êµ¬ì¡°

```
src/
â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”œâ”€â”€ product.rs
â”‚   â”‚   â”œâ”€â”€ vendor.rs
â”‚   â”‚   â””â”€â”€ crawling_session.rs
â”‚   â”œâ”€â”€ repositories/
â”‚   â”‚   â”œâ”€â”€ product_repository.rs
â”‚   â”‚   â””â”€â”€ vendor_repository.rs
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ crawling_service.rs
â”‚       â””â”€â”€ validation_service.rs
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ use_cases/
â”‚   â”‚   â”œâ”€â”€ start_crawling.rs
â”‚   â”‚   â”œâ”€â”€ collect_product_list.rs
â”‚   â”‚   â””â”€â”€ validate_products.rs
â”‚   â””â”€â”€ dto/
â”‚       â”œâ”€â”€ crawling_request.rs
â”‚       â””â”€â”€ crawling_response.rs
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ sqlite_repository.rs
â”‚   â”‚   â””â”€â”€ migrations/
â”‚   â”œâ”€â”€ http/
â”‚   â”‚   â”œâ”€â”€ reqwest_client.rs
â”‚   â”‚   â””â”€â”€ html_parser.rs
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ config_manager.rs
â””â”€â”€ commands/
    â”œâ”€â”€ crawling_commands.rs
    â””â”€â”€ config_commands.rs
```

### í•µì‹¬ ì„œë¹„ìŠ¤ íŒ¨í„´

#### CrawlingService (ë„ë©”ì¸ ì„œë¹„ìŠ¤)
```rust
pub struct CrawlingService {
    product_repository: Arc<dyn ProductRepository>,
    vendor_repository: Arc<dyn VendorRepository>,
    http_client: Arc<dyn HttpClient>,
    config: CrawlerConfig,
}

impl CrawlingService {
    pub async fn start_crawling(&self, config: CrawlerConfig) -> Result<CrawlingSession, ServiceError> {
        // 1. í¬ë¡¤ë§ ì„¸ì…˜ ì´ˆê¸°í™”
        let session = CrawlingSession::new(config.clone());
        
        // 2. 1ë‹¨ê³„: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘
        let products = self.collect_product_list(config.page_range_limit).await?;
        session.emit_stage_complete(CrawlingStage::ProductList, products.len());
        
        // 3. 1.5ë‹¨ê³„: ì¤‘ë³µ ê²€ì¦
        let validation_result = self.validate_products(products).await?;
        session.emit_stage_complete(CrawlingStage::Validation, validation_result.new_products.len());
        
        // 4. 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        let matter_products = self.collect_product_details(validation_result.new_products).await?;
        session.emit_stage_complete(CrawlingStage::ProductDetail, matter_products.len());
        
        // 5. DB ì €ì¥ (ì„¤ì •ì— ë”°ë¼)
        if config.auto_add_to_local_db {
            self.save_products_to_db(matter_products).await?;
        }
        
        Ok(session)
    }
}
```

#### ProgressTracker (ì§„í–‰ ìƒí™© ì¶”ì )
```rust
#[derive(Debug, Clone)]
pub struct CrawlingProgress {
    pub current: u32,
    pub total: u32,
    pub percentage: f32,
    pub current_step: String,
    pub current_stage: CrawlingStage,
    pub elapsed_time: Duration,
    pub remaining_time: Option<Duration>,
    pub status: CrawlingStatus,
    pub message: Option<String>,
    // ë°°ì¹˜ ì²˜ë¦¬ ì •ë³´
    pub current_batch: Option<u32>,
    pub total_batches: Option<u32>,
    // ì˜¤ë¥˜ ì •ë³´
    pub retry_count: u32,
    pub failed_items: u32,
}

pub struct ProgressTracker {
    progress: Arc<RwLock<CrawlingProgress>>,
    event_sender: EventSender,
}

impl ProgressTracker {
    pub async fn update_progress(&self, update: ProgressUpdate) {
        let mut progress = self.progress.write().await;
        
        // ì§„í–‰ë¥  ê³„ì‚°
        progress.current = update.current;
        progress.total = update.total;
        progress.percentage = if progress.total > 0 {
            (progress.current as f32 / progress.total as f32) * 100.0
        } else {
            0.0
        };
        
        // ë‚¨ì€ ì‹œê°„ ì˜ˆì¸¡
        if progress.current > 0 && progress.elapsed_time.as_secs() > 0 {
            let avg_time_per_item = progress.elapsed_time.as_secs_f32() / progress.current as f32;
            let remaining_items = progress.total - progress.current;
            progress.remaining_time = Some(Duration::from_secs_f32(avg_time_per_item * remaining_items as f32));
        }
        
        // ì´ë²¤íŠ¸ ë°œì†¡
        self.event_sender.send(Event::CrawlingProgress(progress.clone())).await;
    }
}
```

---

## ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ

### ë°°ì¹˜ ì²˜ë¦¬ í•µì‹¬ ë¡œì§

```rust
pub struct BatchProcessor {
    config: CrawlerConfig,
    progress_tracker: Arc<ProgressTracker>,
}

impl BatchProcessor {
    pub async fn process_in_batches(&self, total_pages: u32) -> Result<Vec<Product>, BatchError> {
        let batch_size = self.config.batch_size.unwrap_or(30);
        let total_batches = (total_pages + batch_size - 1) / batch_size;
        
        let mut all_products = Vec::new();
        
        for batch_num in 1..=total_batches {
            // ë°°ì¹˜ ë²”ìœ„ ê³„ì‚°
            let start_page = (batch_num - 1) * batch_size + 1;
            let end_page = std::cmp::min(batch_num * batch_size, total_pages);
            
            // ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ì´ë²¤íŠ¸
            self.progress_tracker.update_batch_start(batch_num, total_batches).await;
            
            // ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤í–‰ (ì¬ì‹œë„ í¬í•¨)
            let batch_products = self.collect_batch_with_retry(start_page, end_page, batch_num).await?;
            
            all_products.extend(batch_products);
            
            // ë°°ì¹˜ ì™„ë£Œ ì´ë²¤íŠ¸
            self.progress_tracker.update_batch_complete(batch_num, all_products.len()).await;
            
            // ë°°ì¹˜ ê°„ ì§€ì—°
            if batch_num < total_batches {
                let delay = Duration::from_millis(self.config.batch_delay_ms.unwrap_or(2000));
                tokio::time::sleep(delay).await;
            }
        }
        
        Ok(all_products)
    }
    
    async fn collect_batch_with_retry(&self, start_page: u32, end_page: u32, batch_num: u32) -> Result<Vec<Product>, BatchError> {
        let retry_limit = self.config.batch_retry_limit.unwrap_or(3);
        
        for attempt in 1..=retry_limit {
            match self.collect_single_batch(start_page, end_page).await {
                Ok(products) => return Ok(products),
                Err(e) if attempt < retry_limit => {
                    // ì¬ì‹œë„ ì „ ì§€ì—°
                    let delay = Duration::from_millis(2000 * attempt as u64);
                    tokio::time::sleep(delay).await;
                    
                    self.progress_tracker.update_batch_retry(batch_num, attempt).await;
                },
                Err(e) => return Err(e),
            }
        }
        
        unreachable!()
    }
}
```

### ë°°ì¹˜ ì²˜ë¦¬ ì¥ì 
1. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
2. **ì˜¤ë¥˜ ê²©ë¦¬**: í•œ ë°°ì¹˜ ì‹¤íŒ¨ê°€ ì „ì²´ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ
3. **ì§„í–‰ ì¶”ì **: ë°°ì¹˜ë³„ ì§„í–‰ ìƒí™©ì„ ì„¸ë°€í•˜ê²Œ ì¶”ì 
4. **ìì› ê´€ë¦¬**: ì‹œìŠ¤í…œ ìì› ì‚¬ìš©ëŸ‰ ì œì–´

---

## ì„¤ì • ê´€ë¦¬

### ì„¤ì • ê´€ë¦¬ì êµ¬í˜„

```rust
pub struct ConfigManager {
    config_path: PathBuf,
    config: Arc<RwLock<CrawlerConfig>>,
}

impl ConfigManager {
    pub fn new(app_handle: &tauri::AppHandle) -> Result<Self, ConfigError> {
        let config_dir = app_handle
            .path_resolver()
            .app_data_dir()
            .ok_or(ConfigError::InvalidPath)?;
        
        let config_path = config_dir.join("crawler-config.json");
        
        // ë””ë ‰í† ë¦¬ ìƒì„±
        if let Some(parent) = config_path.parent() {
            std::fs::create_dir_all(parent)?;
        }
        
        // ì„¤ì • ë¡œë“œ
        let config = Self::load_config(&config_path)?;
        
        Ok(Self {
            config_path,
            config: Arc::new(RwLock::new(config)),
        })
    }
    
    fn load_config(path: &Path) -> Result<CrawlerConfig, ConfigError> {
        if path.exists() {
            let content = std::fs::read_to_string(path)?;
            let mut config: CrawlerConfig = serde_json::from_str(&content)?;
            
            // ê¸°ë³¸ê°’ìœ¼ë¡œ ëˆ„ë½ëœ í•„ë“œ ì±„ìš°ê¸°
            Self::apply_defaults(&mut config);
            
            Ok(config)
        } else {
            Ok(Self::default_config())
        }
    }
    
    pub async fn get_config(&self) -> CrawlerConfig {
        self.config.read().await.clone()
    }
    
    pub async fn update_config(&self, updates: PartialCrawlerConfig) -> Result<CrawlerConfig, ConfigError> {
        let mut config = self.config.write().await;
        
        // ë¶€ë¶„ ì—…ë°ì´íŠ¸ ì ìš©
        Self::merge_config(&mut config, updates);
        
        // ìœ íš¨ì„± ê²€ì¦
        Self::validate_config(&config)?;
        
        // íŒŒì¼ ì €ì¥
        self.save_config(&config).await?;
        
        Ok(config.clone())
    }
    
    fn validate_config(config: &CrawlerConfig) -> Result<(), ConfigError> {
        if config.page_range_limit == 0 {
            return Err(ConfigError::InvalidValue("page_range_limit must be greater than 0".into()));
        }
        
        if config.products_per_page == 0 {
            return Err(ConfigError::InvalidValue("products_per_page must be greater than 0".into()));
        }
        
        // ë°°ì¹˜ ì„¤ì • ê²€ì¦
        if let Some(batch_size) = config.batch_size {
            if batch_size == 0 {
                return Err(ConfigError::InvalidValue("batch_size must be greater than 0".into()));
            }
        }
        
        Ok(())
    }
    
    fn default_config() -> CrawlerConfig {
        CrawlerConfig {
            page_range_limit: 10,
            product_list_retry_count: 9,
            product_detail_retry_count: 9,
            products_per_page: 12,
            auto_add_to_local_db: false,
            auto_status_check: true,
            enable_batch_processing: Some(true),
            batch_size: Some(30),
            batch_delay_ms: Some(2000),
            crawler_type: Some(CrawlerType::Reqwest),
            // ... ê¸°íƒ€ ê¸°ë³¸ê°’
        }
    }
}
```

---

## ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ

### ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜

```rust
#[derive(Debug, Clone, Serialize)]
#[serde(tag = "type")]
pub enum CrawlingEvent {
    ProgressUpdate {
        progress: CrawlingProgress,
    },
    StageComplete {
        stage: CrawlingStage,
        items_processed: u32,
        duration: Duration,
    },
    BatchStart {
        batch_number: u32,
        total_batches: u32,
        pages_in_batch: u32,
    },
    BatchComplete {
        batch_number: u32,
        products_collected: u32,
        failed_pages: u32,
    },
    CrawlingComplete {
        success: bool,
        total_products: u32,
        duration: Duration,
        auto_saved_to_db: bool,
    },
    CrawlingError {
        error: String,
        stage: CrawlingStage,
        recoverable: bool,
    },
    StatusSummary {
        summary: CrawlingStatusSummary,
    },
}

pub struct EventBus {
    sender: broadcast::Sender<CrawlingEvent>,
}

impl EventBus {
    pub fn new() -> Self {
        let (sender, _) = broadcast::channel(1000);
        Self { sender }
    }
    
    pub fn emit(&self, event: CrawlingEvent) {
        if let Err(e) = self.sender.send(event) {
            eprintln!("Failed to emit event: {:?}", e);
        }
    }
    
    pub fn subscribe(&self) -> broadcast::Receiver<CrawlingEvent> {
        self.sender.subscribe()
    }
}

// Tauri ì´ë²¤íŠ¸ ì—°ë™
#[tauri::command]
pub async fn subscribe_to_crawling_events(app_handle: tauri::AppHandle, event_bus: tauri::State<'_, EventBus>) -> Result<(), String> {
    let mut receiver = event_bus.subscribe();
    
    tokio::spawn(async move {
        while let Ok(event) = receiver.recv().await {
            let event_name = match &event {
                CrawlingEvent::ProgressUpdate { .. } => "crawling-progress",
                CrawlingEvent::StageComplete { .. } => "stage-complete",
                CrawlingEvent::CrawlingComplete { .. } => "crawling-complete",
                CrawlingEvent::CrawlingError { .. } => "crawling-error",
                _ => continue,
            };
            
            if let Err(e) = app_handle.emit_all(event_name, &event) {
                eprintln!("Failed to emit event to frontend: {:?}", e);
            }
        }
    });
    
    Ok(())
}
```

---

## ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„

### SQLite í…Œì´ë¸” êµ¬ì¡°

**ğŸ“‹ ìµœì‹  ì—…ë°ì´íŠ¸: 2025-01-15 - ë©”ëª¨ë¦¬ ê¸°ë°˜ ì„¸ì…˜ ê´€ë¦¬ë¡œ ì•„í‚¤í…ì²˜ ìµœì í™”**

```sql
-- Vendor table for CSA-IoT Matter certification database vendors
CREATE TABLE IF NOT EXISTS vendors (
    vendor_id TEXT PRIMARY KEY,              -- UUID-based ID for better distribution
    vendor_number INTEGER NOT NULL UNIQUE,   -- Matter vendor number
    vendor_name TEXT NOT NULL,
    company_legal_name TEXT NOT NULL,
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Product table for basic product information (Stage 1 collection)
CREATE TABLE IF NOT EXISTS products (
    url TEXT PRIMARY KEY,              -- Product detail page URL
    manufacturer TEXT,                 -- Manufacturer name
    model TEXT,                       -- Model name
    certificate_id TEXT,              -- Certificate ID
    page_id INTEGER,                  -- Collected page number
    index_in_page INTEGER,           -- Order within page
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Matter products table for complete Matter certification info (Stage 2 collection)
CREATE TABLE IF NOT EXISTS matter_products (
    url TEXT PRIMARY KEY,                    -- Product detail page URL (FK to products)
    page_id INTEGER,                        -- Collected page number
    index_in_page INTEGER,                 -- Order within page
    id TEXT,                               -- Matter product ID
    manufacturer TEXT,                     -- Manufacturer name
    model TEXT,                           -- Model name
    device_type TEXT,                     -- Device type
    certificate_id TEXT,                  -- Certificate ID
    certification_date TEXT,             -- Certification date
    software_version TEXT,               -- Software version
    hardware_version TEXT,               -- Hardware version
    vid TEXT,                            -- Vendor ID (hex string)
    pid TEXT,                            -- Product ID (hex string)
    family_sku TEXT,                     -- Family SKU
    family_variant_sku TEXT,             -- Family variant SKU
    firmware_version TEXT,               -- Firmware version
    family_id TEXT,                      -- Family ID
    tis_trp_tested TEXT,                 -- TIS/TRP tested
    specification_version TEXT,          -- Specification version
    transport_interface TEXT,            -- Transport interface
    primary_device_type_id TEXT,         -- Primary device type ID
    application_categories TEXT,         -- JSON array as TEXT
    created_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- Crawling results table for final session outcomes only
-- NOTE: Session state is now managed in-memory for better performance
CREATE TABLE IF NOT EXISTS crawling_results (
    session_id TEXT PRIMARY KEY,
    status TEXT NOT NULL,                    -- Completed, Failed, Stopped
    stage TEXT NOT NULL,                     -- products, matter_products, details
    total_pages INTEGER NOT NULL,
    products_found INTEGER NOT NULL,
    errors_count INTEGER NOT NULL,
    started_at DATETIME NOT NULL,
    completed_at DATETIME NOT NULL,
    execution_time_seconds INTEGER NOT NULL,
    config_snapshot TEXT,                    -- JSON configuration used
    error_details TEXT                       -- Detailed error information if failed
);

-- Performance-optimized indexes
CREATE INDEX IF NOT EXISTS idx_products_manufacturer ON products (manufacturer);
CREATE INDEX IF NOT EXISTS idx_products_page_id ON products (page_id);
CREATE INDEX IF NOT EXISTS idx_matter_products_manufacturer ON matter_products (manufacturer);
CREATE INDEX IF NOT EXISTS idx_matter_products_device_type ON matter_products (device_type);
CREATE INDEX IF NOT EXISTS idx_matter_products_vid ON matter_products (vid);
CREATE INDEX IF NOT EXISTS idx_matter_products_certification_date ON matter_products (certification_date);
CREATE INDEX IF NOT EXISTS idx_crawling_results_status ON crawling_results (status);
CREATE INDEX IF NOT EXISTS idx_crawling_results_started_at ON crawling_results (started_at);
CREATE INDEX IF NOT EXISTS idx_vendors_vendor_number ON vendors (vendor_number);
```

### ì•„í‚¤í…ì²˜ ìµœì í™”: ë©”ëª¨ë¦¬ ê¸°ë°˜ ì„¸ì…˜ ê´€ë¦¬

**2025ë…„ 1ì›” ì—…ë°ì´íŠ¸: ì‚°ì—… í‘œì¤€ ì ‘ê·¼ë²• ë„ì…**

í¬ë¡¤ë§ ì„¸ì…˜ ê´€ë¦¬ë¥¼ ê¸°ì¡´ì˜ ë°ì´í„°ë² ì´ìŠ¤ ì¤‘ì‹¬ì—ì„œ ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤:

- **AS-IS**: `crawling_sessions` í…Œì´ë¸”ì— ëª¨ë“  ì„¸ì…˜ ìƒíƒœ ì €ì¥
- **TO-BE**: ë©”ëª¨ë¦¬ì—ì„œ ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬, ìµœì¢… ê²°ê³¼ë§Œ `crawling_results` í…Œì´ë¸”ì— ì €ì¥

ì´ ë³€ê²½ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì´ì ì„ ì–»ì—ˆìŠµë‹ˆë‹¤:
- ğŸš€ **ì„±ëŠ¥ í–¥ìƒ**: ì‹¤ì‹œê°„ ìƒíƒœ ì—…ë°ì´íŠ¸ì‹œ DB I/O ì œê±°
- ğŸ”„ **í™•ì¥ì„± ê°œì„ **: ë‹¤ì¤‘ ì„¸ì…˜ ë™ì‹œ ì²˜ë¦¬ ìµœì í™”
- ğŸ§¹ **ë‹¨ìˆœí™”**: ì„¸ì…˜ ì •ë¦¬ ë¡œì§ ë¶ˆí•„ìš”
- ğŸ“Š **ì•ˆì •ì„±**: ë©”ëª¨ë¦¬ ê¸°ë°˜ ìƒíƒœë¡œ ë½(lock) ê²½í•© ì œê±°
```

### Repository íŒ¨í„´ êµ¬í˜„

```rust
#[async_trait]
pub trait ProductRepository: Send + Sync {
    async fn save_product(&self, product: &Product) -> Result<(), RepositoryError>;
    async fn save_matter_product(&self, product: &MatterProduct) -> Result<(), RepositoryError>;
    async fn get_existing_urls(&self) -> Result<HashSet<String>, RepositoryError>;
    async fn get_products_paginated(&self, page: u32, limit: u32) -> Result<(Vec<MatterProduct>, u32), RepositoryError>;
    async fn search_products(&self, query: &str) -> Result<Vec<MatterProduct>, RepositoryError>;
    async fn get_database_summary(&self) -> Result<DatabaseSummary, RepositoryError>;
}

pub struct SqliteProductRepository {
    pool: Pool<Sqlite>,
}

impl SqliteProductRepository {
    pub async fn new(database_url: &str) -> Result<Self, RepositoryError> {
        let pool = SqlitePoolOptions::new()
            .max_connections(10)
            .connect(database_url)
            .await?;
        
        // ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
        sqlx::migrate!("./migrations").run(&pool).await?;
        
        Ok(Self { pool })
    }
}

#[async_trait]
impl ProductRepository for SqliteProductRepository {
    async fn save_matter_product(&self, product: &MatterProduct) -> Result<(), RepositoryError> {
        let application_categories_json = serde_json::to_string(&product.application_categories)?;
        
        sqlx::query!(
            r#"
            INSERT OR REPLACE INTO product_details (
                url, page_id, index_in_page, manufacturer, model, device_type,
                certification_id, certification_date, software_version, hardware_version,
                vid, pid, family_sku, family_variant_sku, firmware_version,
                family_id, tis_trp_tested, specification_version, transport_interface,
                primary_device_type_id, application_categories, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            "#,
            product.url,
            product.page_id,
            product.index_in_page,
            product.manufacturer,
            product.model,
            product.device_type,
            product.certificate_id,
            product.certification_date,
            product.software_version,
            product.hardware_version,
            product.vid,
            product.pid,
            product.family_sku,
            product.family_variant_sku,
            product.firmware_version,
            product.family_id,
            product.tis_trp_tested,
            product.specification_version,
            product.transport_interface,
            product.primary_device_type_id,
            application_categories_json
        )
        .execute(&self.pool)
        .await?;
        
        Ok(())
    }
    
    async fn get_existing_urls(&self) -> Result<HashSet<String>, RepositoryError> {
        let rows = sqlx::query!("SELECT url FROM products")
            .fetch_all(&self.pool)
            .await?;
        
        Ok(rows.into_iter().map(|row| row.url).collect())
    }
}
```

---

## ì§„í–‰ ìƒí™© ì¶”ì 

### ë©€í‹° ìŠ¤í…Œì´ì§€ ì§„í–‰ ì¶”ì 

```rust
#[derive(Debug, Clone, Serialize)]
pub enum CrawlingStage {
    Idle,
    ProductList,      // 1ë‹¨ê³„: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘
    Validation,       // 1.5ë‹¨ê³„: ì¤‘ë³µ ê²€ì¦
    ProductDetail,    // 2ë‹¨ê³„: ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    DatabaseSave,     // 3ë‹¨ê³„: DB ì €ì¥
    Completed,
    Failed,
}

#[derive(Debug, Clone, Serialize)]
pub enum CrawlingStatus {
    Idle,
    Initializing,
    Running,
    Paused,
    Completed,
    Error,
    Stopped,
}

pub struct StageProgress {
    pub stage: CrawlingStage,
    pub status: CrawlingStatus,
    pub current: u32,
    pub total: u32,
    pub percentage: f32,
    pub start_time: Option<Instant>,
    pub estimated_end_time: Option<Instant>,
    pub retry_count: u32,
    pub failed_items: u32,
}

pub struct CrawlingSession {
    pub session_id: String,
    pub overall_status: CrawlingStatus,
    pub current_stage: CrawlingStage,
    pub stages: HashMap<CrawlingStage, StageProgress>,
    pub start_time: Instant,
    pub total_elapsed: Duration,
    pub config: CrawlerConfig,
}

impl CrawlingSession {
    pub fn new(config: CrawlerConfig) -> Self {
        let session_id = uuid::Uuid::new_v4().to_string();
        let start_time = Instant::now();
        
        // ëª¨ë“  ìŠ¤í…Œì´ì§€ ì´ˆê¸°í™”
        let mut stages = HashMap::new();
        for stage in [CrawlingStage::ProductList, CrawlingStage::Validation, CrawlingStage::ProductDetail] {
            stages.insert(stage.clone(), StageProgress {
                stage: stage.clone(),
                status: CrawlingStatus::Idle,
                current: 0,
                total: 0,
                percentage: 0.0,
                start_time: None,
                estimated_end_time: None,
                retry_count: 0,
                failed_items: 0,
            });
        }
        
        Self {
            session_id,
            overall_status: CrawlingStatus::Initializing,
            current_stage: CrawlingStage::ProductList,
            stages,
            start_time,
            total_elapsed: Duration::ZERO,
            config,
        }
    }
    
    pub fn start_stage(&mut self, stage: CrawlingStage, total_items: u32) {
        self.current_stage = stage.clone();
        
        if let Some(stage_progress) = self.stages.get_mut(&stage) {
            stage_progress.status = CrawlingStatus::Running;
            stage_progress.total = total_items;
            stage_progress.current = 0;
            stage_progress.start_time = Some(Instant::now());
        }
    }
    
    pub fn update_stage_progress(&mut self, stage: CrawlingStage, current: u32) {
        if let Some(stage_progress) = self.stages.get_mut(&stage) {
            stage_progress.current = current;
            stage_progress.percentage = if stage_progress.total > 0 {
                (current as f32 / stage_progress.total as f32) * 100.0
            } else {
                0.0
            };
            
            // ì™„ë£Œ ì‹œê°„ ì˜ˆì¸¡
            if let Some(start_time) = stage_progress.start_time {
                let elapsed = start_time.elapsed();
                if current > 0 {
                    let avg_time_per_item = elapsed.as_secs_f32() / current as f32;
                    let remaining_items = stage_progress.total - current;
                    let estimated_remaining = Duration::from_secs_f32(avg_time_per_item * remaining_items as f32);
                    stage_progress.estimated_end_time = Some(Instant::now() + estimated_remaining);
                }
            }
        }
    }
}
```

---

## ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´

### ê³„ì¸µí™”ëœ ì—ëŸ¬ ì²˜ë¦¬

```rust
#[derive(Debug, thiserror::Error)]
pub enum CrawlingError {
    #[error("HTTP request failed: {0}")]
    Http(#[from] reqwest::Error),
    
    #[error("HTML parsing failed: {0}")]
    HtmlParsing(String),
    
    #[error("Database error: {0}")]
    Database(#[from] sqlx::Error),
    
    #[error("Configuration error: {0}")]
    Config(#[from] ConfigError),
    
    #[error("Network timeout")]
    Timeout,
    
    #[error("Crawling was aborted")]
    Aborted,
    
    #[error("Too many retries: {max_retries}")]
    TooManyRetries { max_retries: u32 },
    
    #[error("Batch processing failed: batch {batch_number}, error: {source}")]
    BatchFailed { batch_number: u32, source: Box<CrawlingError> },
}

#[derive(Debug, thiserror::Error)]
pub enum ConfigError {
    #[error("Invalid configuration value: {0}")]
    InvalidValue(String),
    
    #[error("Configuration file not found: {0}")]
    FileNotFound(String),
    
    #[error("JSON parsing error: {0}")]
    JsonParsing(#[from] serde_json::Error),
    
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
}

// ì—ëŸ¬ ë³µêµ¬ ì „ëµ
pub struct ErrorRecoveryStrategy {
    pub max_retries: u32,
    pub retry_delay: Duration,
    pub backoff_multiplier: f32,
    pub recoverable_errors: HashSet<String>,
}

impl ErrorRecoveryStrategy {
    pub fn should_retry(&self, error: &CrawlingError, current_attempt: u32) -> bool {
        if current_attempt >= self.max_retries {
            return false;
        }
        
        match error {
            CrawlingError::Http(_) => true,
            CrawlingError::Timeout => true,
            CrawlingError::HtmlParsing(_) => false, // íŒŒì‹± ì—ëŸ¬ëŠ” ì¬ì‹œë„ ë¶ˆê°€
            CrawlingError::Database(_) => false,   // DB ì—ëŸ¬ëŠ” ì¬ì‹œë„ ë¶ˆê°€
            _ => false,
        }
    }
    
    pub fn calculate_delay(&self, attempt: u32) -> Duration {
        let multiplier = self.backoff_multiplier.powi(attempt as i32);
        Duration::from_millis((self.retry_delay.as_millis() as f32 * multiplier) as u64)
    }
}

// ì¬ì‹œë„ ë§¤í¬ë¡œ
macro_rules! retry_with_strategy {
    ($strategy:expr, $operation:expr) => {{
        let mut attempt = 0;
        loop {
            match $operation {
                Ok(result) => break Ok(result),
                Err(error) => {
                    attempt += 1;
                    if !$strategy.should_retry(&error, attempt) {
                        break Err(error);
                    }
                    
                    let delay = $strategy.calculate_delay(attempt);
                    tokio::time::sleep(delay).await;
                }
            }
        }
    }};
}
```

---

## Rust êµ¬í˜„ ê³ ë ¤ì‚¬í•­

### 1. ì˜ì¡´ì„± ê´€ë¦¬ (Cargo.toml)

```toml
[dependencies]
# Tauri ê´€ë ¨
tauri = { version = "2.0", features = ["api-all"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"

# ë¹„ë™ê¸° ëŸ°íƒ€ì„
tokio = { version = "1.0", features = ["full"] }

# HTTP í´ë¼ì´ì–¸íŠ¸
reqwest = { version = "0.11", features = ["json", "cookies", "gzip"] }

# HTML íŒŒì‹±
scraper = "0.18"
select = "0.6"

# ë°ì´í„°ë² ì´ìŠ¤
sqlx = { version = "0.7", features = ["sqlite", "runtime-tokio-rustls", "chrono", "migrate"] }

# ë™ì‹œì„±
futures = "0.3"
rayon = "1.7"

# ì—ëŸ¬ ì²˜ë¦¬
anyhow = "1.0"
thiserror = "1.0"

# ì„¤ì • ê´€ë¦¬
config = "0.13"

# ë¡œê¹…
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }

# ì‹œê°„ ì²˜ë¦¬
chrono = { version = "0.4", features = ["serde"] }

# UUID
uuid = { version = "1.0", features = ["v4", "serde"] }

# ë¹„ë™ê¸° íŠ¸ë ˆì´íŠ¸
async-trait = "0.1"
```

### 2. ì„±ëŠ¥ ìµœì í™” í¬ì¸íŠ¸

#### HTTP í´ë¼ì´ì–¸íŠ¸ ìµœì í™”
```rust
pub struct OptimizedHttpClient {
    client: reqwest::Client,
    rate_limiter: Arc<RateLimiter>,
}

impl OptimizedHttpClient {
    pub fn new(config: &CrawlerConfig) -> Self {
        let client = reqwest::Client::builder()
            .timeout(Duration::from_millis(config.request_timeout.unwrap_or(30000)))
            .user_agent(config.user_agent.as_deref().unwrap_or("Matter-Certis-Crawler/2.0"))
            .gzip(true)
            .pool_max_idle_per_host(10)
            .pool_idle_timeout(Duration::from_secs(30))
            .build()
            .expect("Failed to create HTTP client");
        
        let rate_limiter = Arc::new(RateLimiter::new(
            config.request_delay.unwrap_or(1000)
        ));
        
        Self { client, rate_limiter }
    }
    
    pub async fn get_with_rate_limit(&self, url: &str) -> Result<String, reqwest::Error> {
        // ìš”ì²­ ì „ ë ˆì´íŠ¸ ë¦¬ë¯¸í„° ëŒ€ê¸°
        self.rate_limiter.wait().await;
        
        let response = self.client
            .get(url)
            .send()
            .await?;
        
        response.text().await
    }
}
```

#### ë™ì‹œì„± ì œì–´
```rust
pub struct ConcurrencyController {
    semaphore: Arc<Semaphore>,
    active_tasks: Arc<AtomicU32>,
    max_concurrent: u32,
}

impl ConcurrencyController {
    pub fn new(max_concurrent: u32) -> Self {
        Self {
            semaphore: Arc::new(Semaphore::new(max_concurrent as usize)),
            active_tasks: Arc::new(AtomicU32::new(0)),
            max_concurrent,
        }
    }
    
    pub async fn execute<F, T, E>(&self, task: F) -> Result<T, E>
    where
        F: Future<Output = Result<T, E>>,
    {
        let _permit = self.semaphore.acquire().await.unwrap();
        let _guard = TaskGuard::new(&self.active_tasks);
        
        task.await
    }
}

struct TaskGuard<'a> {
    counter: &'a AtomicU32,
}

impl<'a> TaskGuard<'a> {
    fn new(counter: &'a AtomicU32) -> Self {
        counter.fetch_add(1, Ordering::SeqCst);
        Self { counter }
    }
}

impl<'a> Drop for TaskGuard<'a> {
    fn drop(&mut self) {
        self.counter.fetch_sub(1, Ordering::SeqCst);
    }
}
```

### 3. Tauri ëª…ë ¹ì–´ êµ¬í˜„

```rust
#[tauri::command]
pub async fn start_crawling(
    config: CrawlerConfig,
    app_handle: tauri::AppHandle,
    state: tauri::State<'_, AppState>,
) -> Result<bool, String> {
    let crawling_service = &state.crawling_service;
    
    match crawling_service.start_crawling(config).await {
        Ok(_) => Ok(true),
        Err(e) => {
            // ì—ëŸ¬ ì´ë²¤íŠ¸ ë°œì†¡
            app_handle.emit_all("crawling-error", &format!("{}", e))
                .map_err(|e| format!("Failed to emit error event: {}", e))?;
            Err(format!("Crawling failed: {}", e))
        }
    }
}

#[tauri::command]
pub async fn get_crawling_status(
    state: tauri::State<'_, AppState>,
) -> Result<CrawlingProgress, String> {
    state.progress_tracker
        .get_current_progress()
        .await
        .map_err(|e| format!("Failed to get progress: {}", e))
}

#[tauri::command]
pub async fn update_config(
    updates: PartialCrawlerConfig,
    state: tauri::State<'_, AppState>,
) -> Result<CrawlerConfig, String> {
    state.config_manager
        .update_config(updates)
        .await
        .map_err(|e| format!("Failed to update config: {}", e))
}
```

### 4. í…ŒìŠ¤íŠ¸ ì „ëµ

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio_test;
    
    #[tokio::test]
    async fn test_product_collection() {
        let config = CrawlerConfig::default();
        let service = CrawlingService::new_for_test(config);
        
        let products = service.collect_product_list(5).await.unwrap();
        assert!(!products.is_empty());
        assert!(products.len() <= 5 * 12); // í˜ì´ì§€ë‹¹ 12ê°œ ì œí’ˆ
    }
    
    #[tokio::test]
    async fn test_batch_processing() {
        let config = CrawlerConfig {
            enable_batch_processing: Some(true),
            batch_size: Some(2),
            ..Default::default()
        };
        
        let processor = BatchProcessor::new(config);
        let products = processor.process_in_batches(5).await.unwrap();
        
        // ë°°ì¹˜ ì²˜ë¦¬ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í–ˆëŠ”ì§€ í™•ì¸
        assert!(!products.is_empty());
    }
    
    #[test]
    fn test_config_validation() {
        let mut config = CrawlerConfig::default();
        config.page_range_limit = 0;
        
        assert!(ConfigManager::validate_config(&config).is_err());
    }
}
```

---

## ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### TypeScript â†’ Rust ë³€í™˜ ë§¤í•‘

| TypeScript | Rust | ë…¸íŠ¸ |
|------------|------|------|
| `interface Product` | `struct Product` | `#[derive(Serialize, Deserialize)]` ì¶”ê°€ |
| `readonly` í•„ë“œ | ê¸°ë³¸ í•„ë“œ | RustëŠ” ê¸°ë³¸ì ìœ¼ë¡œ immutable |
| `Array<T>` | `Vec<T>` | |
| `Record<K, V>` | `HashMap<K, V>` | |
| `Promise<T>` | `Future<Output = T>` | async/await ì‚¬ìš© |
| `setTimeout` | `tokio::time::sleep` | |
| `EventEmitter` | `broadcast::channel` | |
| SQLite ì¿¼ë¦¬ | `sqlx::query!` ë§¤í¬ë¡œ | ì»´íŒŒì¼ íƒ€ì„ ê²€ì¦ |

### ì£¼ìš” ì•„í‚¤í…ì²˜ ê²°ì •

1. **HTTP í´ë¼ì´ì–¸íŠ¸**: reqwest ì‚¬ìš© (chromiumoxide ëŒ€ì‹ )
2. **ë°ì´í„°ë² ì´ìŠ¤**: SQLx + SQLite (íƒ€ì… ì•ˆì „ì„±)
3. **ë™ì‹œì„±**: Tokio + async/await (ì„±ëŠ¥ ìš°ì„ )
4. **ì„¤ì • ê´€ë¦¬**: serde + JSON (í˜¸í™˜ì„± ìœ ì§€)
5. **ì—ëŸ¬ ì²˜ë¦¬**: thiserror + Result<T, E> (Rust ê´€ë¡€)

### ì„±ëŠ¥ ê°œì„  ëª©í‘œ

- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 70% ê°ì†Œ
- **ì‹œì‘ ì‹œê°„**: 66% ë‹¨ì¶• (3ì´ˆ â†’ 1ì´ˆ)
- **CPU ì‚¬ìš©ë¥ **: 50% ê°ì†Œ
- **ë°°í„°ë¦¬ ìˆ˜ëª…**: 40% í–¥ìƒ

ì´ ë¬¸ì„œëŠ” ê¸°ì¡´ TypeScript êµ¬í˜„ì²´ì˜ í•µì‹¬ ë„ë©”ì¸ ë¡œì§ì„ Rustë¡œ ì¬êµ¬í˜„í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê° ì„¹ì…˜ì˜ ì½”ë“œ ì˜ˆì œëŠ” ì‹¤ì œ êµ¬í˜„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ê²€ì¦ëœ íŒ¨í„´ì„ Rust ìƒíƒœê³„ì— ë§ê²Œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.
