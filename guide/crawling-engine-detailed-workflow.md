# Matter Certis v2 - í¬ë¡¤ë§ ì—”ì§„ ì„¸ë¶€ ì›Œí¬í”Œë¡œìš°

> í¬ë¡¤ë§ ì—”ì§„ì˜ ë‹¨ê³„ë³„ ì„¸ë¶€ ë™ì‘ê³¼ ìµœì í™” ì „ëµì„ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [í¬ë¡¤ë§ ì—”ì§„ ê°œìš”](#í¬ë¡¤ë§-ì—”ì§„-ê°œìš”)
2. [ì„¸ë¶€ ì›Œí¬í”Œë¡œìš°](#ì„¸ë¶€-ì›Œí¬í”Œë¡œìš°)
3. [ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ](#ë³‘ë ¬-ì²˜ë¦¬-ì „ëµ)
4. [ë°ì´í„° ì¶”ì¶œ ë° ê²€ì¦](#ë°ì´í„°-ì¶”ì¶œ-ë°-ê²€ì¦)
5. [ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬](#ì—ëŸ¬-ì²˜ë¦¬-ë°-ë³µêµ¬)
6. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)

## í¬ë¡¤ë§ ì—”ì§„ ê°œìš”

### í˜„ì¬ êµ¬í˜„ ìƒíƒœ
- âœ… **ê¸°ë³¸ í¬ë¡¤ë§ ì¸í”„ë¼**: HTTP í´ë¼ì´ì–¸íŠ¸, ì„¸ì…˜ ê´€ë¦¬, ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
- âœ… **ë°ì´í„° ì¶”ì¶œ ë¡œì§**: HTML íŒŒì‹±, ì œí’ˆ ì •ë³´ ì¶”ì¶œ, Matter ì¸ì¦ ë°ì´í„° íŒŒì‹±
- âœ… **ì €ì¥ì†Œ íŒ¨í„´**: Repository íŒ¨í„´ìœ¼ë¡œ ë°ì´í„° ì €ì¥
- âœ… **ì„¸ì…˜ ê´€ë¦¬**: ì§„í–‰ ìƒí™© ì¶”ì , ì˜¤ë¥˜ ë¡œê¹…, ìƒíƒœ ê´€ë¦¬

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸
```rust
// src-tauri/src/infrastructure/crawler.rs
pub struct WebCrawler {
    http_client: HttpClient,                    // HTTP ìš”ì²­ ì²˜ë¦¬
    session_manager: Arc<SessionManager>,       // ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
    visited_urls: Arc<Mutex<HashSet<String>>>, // ì¤‘ë³µ ë°©ë¬¸ ë°©ì§€
    product_repo: Arc<SqliteProductRepository>, // ì œí’ˆ ë°ì´í„° ì €ì¥
    vendor_repo: Arc<SqliteVendorRepository>,   // ë²¤ë” ë°ì´í„° ì €ì¥
}
```

## ì„¸ë¶€ ì›Œí¬í”Œë¡œìš°

### Phase 1: í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘
```
1. ì„¸ì…˜ ì´ˆê¸°í™”
   â”œâ”€â”€ ì„¸ì…˜ ID ìƒì„± (UUID)
   â”œâ”€â”€ ì´ˆê¸° ì„¤ì • ê²€ì¦ (URL, ë„ë©”ì¸, ì œí•œê°’)
   â”œâ”€â”€ ì„¸ì…˜ ë§¤ë‹ˆì €ì— ë“±ë¡
   â””â”€â”€ ìƒíƒœë¥¼ 'Active'ë¡œ ì„¤ì •

2. ì´ˆê¸° URL ê²€ì¦
   â”œâ”€â”€ robots.txt í™•ì¸
   â”œâ”€â”€ ë„ë©”ì¸ í—ˆìš© ëª©ë¡ ê²€ì‚¬
   â”œâ”€â”€ URL í˜•ì‹ ìœ íš¨ì„± ê²€ì¦
   â””â”€â”€ ì‹œì‘ í˜ì´ì§€ ì ‘ê·¼ ê°€ëŠ¥ì„± í™•ì¸
```

### Phase 2: í˜ì´ì§€ í¬ë¡¤ë§ ë£¨í”„
```
ë©”ì¸ í¬ë¡¤ë§ ë£¨í”„:
while (urls_to_crawl.len() > 0 && pages_crawled < max_pages) {
    current_url = urls_to_crawl.pop()
    
    1. ì¤‘ë³µ ë°©ë¬¸ í™•ì¸
       â”œâ”€â”€ visited_urls ì²´í¬
       â”œâ”€â”€ ì´ë¯¸ ë°©ë¬¸í•œ ê²½ìš° ìŠ¤í‚µ
       â””â”€â”€ ë°©ë¬¸ ê¸°ë¡ì— ì¶”ê°€
    
    2. í˜ì´ì§€ ìš”ì²­ ë° ë‹¤ìš´ë¡œë“œ
       â”œâ”€â”€ HTTP GET ìš”ì²­
       â”œâ”€â”€ ì‘ë‹µ ìƒíƒœ ì½”ë“œ í™•ì¸ (200, 300ë²ˆëŒ€ ì²˜ë¦¬)
       â”œâ”€â”€ Content-Type ê²€ì¦ (HTML ì—¬ë¶€)
       â”œâ”€â”€ í˜ì´ì§€ í¬ê¸° ì œí•œ í™•ì¸
       â””â”€â”€ ì‘ë‹µ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë³€í™˜
    
    3. í˜ì´ì§€ ë‚´ìš© ë¶„ì„
       â”œâ”€â”€ HTML íŒŒì‹± (scraper crate)
       â”œâ”€â”€ ì œëª© ì¶”ì¶œ
       â”œâ”€â”€ ë§í¬ ì¶”ì¶œ ë° í•„í„°ë§
       â””â”€â”€ êµ¬ì¡°í™”ëœ ë°ì´í„° ì‹ë³„
    
    4. ì œí’ˆ ë°ì´í„° ì¶”ì¶œ
       â”œâ”€â”€ Matter ì¸ì¦ í…Œì´ë¸” íƒì§€
       â”œâ”€â”€ ì œí’ˆ ì •ë³´ íŒŒì‹±
       â”œâ”€â”€ ì¸ì¦ ë°ì´í„° ê²€ì¦
       â””â”€â”€ ì¶”ì¶œëœ ë°ì´í„° ì •ê·œí™”
    
    5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
       â”œâ”€â”€ ì¤‘ë³µ ì œí’ˆ ê²€ì‚¬
       â”œâ”€â”€ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
       â”œâ”€â”€ íŠ¸ëœì­ì…˜ ì‹œì‘
       â”œâ”€â”€ ì œí’ˆ/ë²¤ë” ë°ì´í„° ì €ì¥
       â””â”€â”€ íŠ¸ëœì­ì…˜ ì»¤ë°‹
    
    6. ìƒˆë¡œìš´ URL ë°œê²¬
       â”œâ”€â”€ í˜„ì¬ í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ
       â”œâ”€â”€ ì œí’ˆ ê´€ë ¨ URL í•„í„°ë§
       â”œâ”€â”€ ë„ë©”ì¸ í—ˆìš© ëª©ë¡ í™•ì¸
       â”œâ”€â”€ ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€
       â””â”€â”€ ë°©ë¬¸ ì˜ˆì • ëª©ë¡ ì—…ë°ì´íŠ¸
    
    7. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
       â”œâ”€â”€ ì§„í–‰ë¥  ê³„ì‚° (crawled/total)
       â”œâ”€â”€ í˜„ì¬ í˜ì´ì§€ ì •ë³´ ì—…ë°ì´íŠ¸
       â”œâ”€â”€ ì˜¤ë¥˜ ì¹´ìš´íŠ¸ ì¦ê°€ (ì‹¤íŒ¨ ì‹œ)
       â””â”€â”€ í†µê³„ ì •ë³´ ê°±ì‹ 
    
    8. ìš”ì²­ ê°„ ì§€ì—°
       â”œâ”€â”€ ì„¤ì •ëœ delay_ms ëŒ€ê¸°
       â”œâ”€â”€ ì„œë²„ ë¶€í•˜ ê³ ë ¤ ì¡°ì ˆ
       â””â”€â”€ Rate limiting ì¤€ìˆ˜
}
```

### Phase 3: ì„¸ì…˜ ì™„ë£Œ ë° ì •ë¦¬
```
1. ì„¸ì…˜ ìƒíƒœ ìµœì¢… ì—…ë°ì´íŠ¸
   â”œâ”€â”€ ìƒíƒœë¥¼ 'Completed' ë˜ëŠ” 'Failed'ë¡œ ì„¤ì •
   â”œâ”€â”€ ìµœì¢… í†µê³„ ê³„ì‚°
   â”œâ”€â”€ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
   â””â”€â”€ ì˜¤ë¥˜ ìš”ì•½ ìƒì„±

2. ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   â”œâ”€â”€ HTTP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
   â”œâ”€â”€ ë©”ëª¨ë¦¬ í•´ì œ
   â”œâ”€â”€ ì„ì‹œ íŒŒì¼ ì‚­ì œ
   â””â”€â”€ ë¡œê·¸ íŒŒì¼ ì••ì¶•

3. ê²°ê³¼ ìš”ì•½ ìƒì„±
   â”œâ”€â”€ í¬ë¡¤ë§ëœ í˜ì´ì§€ ìˆ˜
   â”œâ”€â”€ ì¶”ì¶œëœ ì œí’ˆ ìˆ˜
   â”œâ”€â”€ ë°œìƒí•œ ì˜¤ë¥˜ ìˆ˜
   â””â”€â”€ ì†Œìš” ì‹œê°„ ë° ì„±ëŠ¥ ì§€í‘œ
```

## ë³‘ë ¬ ì²˜ë¦¬ ì „ëµ

### í˜„ì¬ êµ¬í˜„: ìˆœì°¨ ì²˜ë¦¬
```rust
// í˜„ì¬ëŠ” ë‹¨ì¼ ìŠ¤ë ˆë“œ ìˆœì°¨ ì²˜ë¦¬
for url in urls_to_crawl {
    let page = self.crawl_page(&url).await?;
    self.extract_and_save_products(&page, &session_id, page_count).await?;
}
```

### í–¥í›„ ë³‘ë ¬ ì²˜ë¦¬ ê³„íš
```rust
use tokio::sync::Semaphore;
use futures::stream::{self, StreamExt};

impl WebCrawler {
    pub async fn crawl_parallel(&self, config: CrawlingConfig) -> Result<()> {
        let semaphore = Arc::new(Semaphore::new(config.concurrent_requests as usize));
        let session_id = &config.session_id;
        
        // URL ìŠ¤íŠ¸ë¦¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        stream::iter(urls_to_crawl)
            .map(|url| {
                let crawler = self.clone();
                let permit = semaphore.clone();
                let session_id = session_id.clone();
                
                async move {
                    let _permit = permit.acquire().await?;
                    crawler.process_single_url(url, &session_id).await
                }
            })
            .buffer_unordered(config.concurrent_requests as usize)
            .collect::<Vec<_>>()
            .await;
            
        Ok(())
    }
    
    async fn process_single_url(&self, url: String, session_id: &str) -> Result<()> {
        // 1. ì¤‘ë³µ í™•ì¸ (thread-safe)
        {
            let mut visited = self.visited_urls.lock().await;
            if visited.contains(&url) {
                return Ok(());
            }
            visited.insert(url.clone());
        }
        
        // 2. í˜ì´ì§€ í¬ë¡¤ë§
        let page = self.crawl_page(&url).await?;
        
        // 3. ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
        self.extract_and_save_products(&page, session_id, 0).await?;
        
        // 4. ì„¸ì…˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (thread-safe)
        self.session_manager.increment_progress(session_id).await?;
        
        Ok(())
    }
}
```

## ë°ì´í„° ì¶”ì¶œ ë° ê²€ì¦

### HTML êµ¬ì¡° ë¶„ì„ ë° ì ì‘í˜• íŒŒì‹±
```rust
impl WebCrawler {
    fn extract_matter_products(&self, html: &str, page_id: u32) -> Result<Vec<ExtractedMatterProduct>> {
        let document = Html::parse_document(html);
        let mut products = Vec::new();
        
        // 1. í…Œì´ë¸” ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ ì‹œë„
        if let Some(table_products) = self.extract_from_tables(&document, page_id)? {
            products.extend(table_products);
        }
        
        // 2. JSON-LD êµ¬ì¡°í™” ë°ì´í„° ì¶”ì¶œ ì‹œë„
        if products.is_empty() {
            if let Some(json_products) = self.extract_from_json_ld(&document, page_id)? {
                products.extend(json_products);
            }
        }
        
        // 3. CSS í´ë˜ìŠ¤ ê¸°ë°˜ ì¶”ì¶œ ì‹œë„
        if products.is_empty() {
            if let Some(css_products) = self.extract_from_css_selectors(&document, page_id)? {
                products.extend(css_products);
            }
        }
        
        // 4. í…ìŠ¤íŠ¸ íŒ¨í„´ ë§¤ì¹­ ì¶”ì¶œ (ìµœí›„ ìˆ˜ë‹¨)
        if products.is_empty() {
            if let Some(pattern_products) = self.extract_from_text_patterns(&document, page_id)? {
                products.extend(pattern_products);
            }
        }
        
        // 5. ì¶”ì¶œëœ ë°ì´í„° ê²€ì¦ ë° ì •ê·œí™”
        let validated_products = products.into_iter()
            .filter_map(|p| self.validate_and_normalize_product(p))
            .collect();
            
        Ok(validated_products)
    }
    
    fn validate_and_normalize_product(&self, mut product: ExtractedMatterProduct) -> Option<ExtractedMatterProduct> {
        // í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if product.manufacturer.is_none() && product.model.is_none() && product.certificate_id.is_none() {
            return None;
        }
        
        // ë°ì´í„° ì •ê·œí™”
        if let Some(ref mut manufacturer) = product.manufacturer {
            *manufacturer = manufacturer.trim().to_string();
            if manufacturer.is_empty() {
                product.manufacturer = None;
            }
        }
        
        // ì¸ì¦ ID í˜•ì‹ ê²€ì¦
        if let Some(ref cert_id) = product.certificate_id {
            if !self.is_valid_certificate_id(cert_id) {
                tracing::warn!("Invalid certificate ID format: {}", cert_id);
                product.certificate_id = None;
            }
        }
        
        Some(product)
    }
    
    fn is_valid_certificate_id(&self, cert_id: &str) -> bool {
        // CSA Matter ì¸ì¦ ID íŒ¨í„´ ê²€ì¦
        // ì˜ˆ: CSA-IOT-12345, CERT-MATTER-67890
        let patterns = [
            regex::Regex::new(r"^CSA-[A-Z]+-\d+$").unwrap(),
            regex::Regex::new(r"^CERT-[A-Z]+-\d+$").unwrap(),
            regex::Regex::new(r"^[A-Z0-9]{8,20}$").unwrap(),
        ];
        
        patterns.iter().any(|pattern| pattern.is_match(cert_id))
    }
}
```

### ë°ì´í„° í’ˆì§ˆ ë³´ì¥
```rust
#[derive(Debug, Clone)]
pub struct DataQualityMetrics {
    pub total_extracted: u32,
    pub validated_products: u32,
    pub duplicate_products: u32,
    pub missing_critical_fields: u32,
    pub invalid_formats: u32,
}

impl WebCrawler {
    async fn save_with_quality_check(&self, products: Vec<ExtractedMatterProduct>, session_id: &str) -> Result<DataQualityMetrics> {
        let mut metrics = DataQualityMetrics::default();
        metrics.total_extracted = products.len() as u32;
        
        for product in products {
            // 1. ì¤‘ë³µ í™•ì¸
            if self.is_duplicate_product(&product).await? {
                metrics.duplicate_products += 1;
                continue;
            }
            
            // 2. í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if !self.has_critical_fields(&product) {
                metrics.missing_critical_fields += 1;
                continue;
            }
            
            // 3. í˜•ì‹ ê²€ì¦
            if !self.validate_product_format(&product) {
                metrics.invalid_formats += 1;
                continue;
            }
            
            // 4. ì €ì¥
            self.save_matter_product(&product).await?;
            metrics.validated_products += 1;
        }
        
        // í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ì„¸ì…˜ì— ê¸°ë¡
        self.session_manager.update_quality_metrics(session_id, metrics.clone()).await?;
        
        Ok(metrics)
    }
}
```

## ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬

### ê³„ì¸µë³„ ì—ëŸ¬ ì²˜ë¦¬
```rust
#[derive(Debug, thiserror::Error)]
pub enum CrawlingError {
    #[error("HTTP request failed: {0}")]
    HttpError(#[from] reqwest::Error),
    
    #[error("HTML parsing failed: {0}")]
    ParseError(String),
    
    #[error("Database operation failed: {0}")]
    DatabaseError(#[from] sqlx::Error),
    
    #[error("Session management error: {0}")]
    SessionError(String),
    
    #[error("Configuration error: {0}")]
    ConfigError(String),
    
    #[error("Rate limit exceeded for domain: {0}")]
    RateLimitError(String),
    
    #[error("Maximum retries exceeded for URL: {0}")]
    MaxRetriesError(String),
}

impl WebCrawler {
    async fn crawl_with_retry(&self, url: &str, max_retries: u32) -> Result<CrawledPage, CrawlingError> {
        let mut last_error = None;
        
        for attempt in 1..=max_retries {
            match self.crawl_page(url).await {
                Ok(page) => return Ok(page),
                Err(e) => {
                    last_error = Some(e);
                    
                    // ì¬ì‹œë„ ê°„ê²© (exponential backoff)
                    let delay = std::time::Duration::from_millis(1000 * 2_u64.pow(attempt - 1));
                    tokio::time::sleep(delay).await;
                    
                    tracing::warn!("Retry {}/{} for URL {}: {:?}", attempt, max_retries, url, last_error);
                }
            }
        }
        
        Err(CrawlingError::MaxRetriesError(url.to_string()))
    }
    
    async fn handle_crawling_error(&self, error: &CrawlingError, url: &str, session_id: &str) -> Result<()> {
        match error {
            CrawlingError::HttpError(e) if e.is_timeout() => {
                // íƒ€ì„ì•„ì›ƒ ì—ëŸ¬: URLì„ ì¬ì‹œë„ íì— ì¶”ê°€
                self.session_manager.add_retry_url(session_id, url).await?;
            },
            CrawlingError::RateLimitError(_) => {
                // Rate limit: ì§€ì—° í›„ ì¬ì‹œë„
                tokio::time::sleep(std::time::Duration::from_secs(60)).await;
                self.session_manager.add_retry_url(session_id, url).await?;
            },
            CrawlingError::DatabaseError(_) => {
                // DB ì—ëŸ¬: ë°ì´í„° ì†ì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ ì„ì‹œ íŒŒì¼ì— ì €ì¥
                self.save_to_temp_file(url, &error.to_string()).await?;
            },
            _ => {
                // ì¼ë°˜ ì—ëŸ¬: ë¡œê·¸ë§Œ ê¸°ë¡
                self.session_manager.add_error(session_id, error.to_string()).await?;
            }
        }
        
        Ok(())
    }
}
```

### ì„¸ì…˜ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
```rust
impl WebCrawler {
    pub async fn resume_session(&self, session_id: &str) -> Result<()> {
        // 1. ì„¸ì…˜ ìƒíƒœ ë³µêµ¬
        let session = self.session_manager.get_session(session_id).await?;
        if session.status != SessionStatus::Paused && session.status != SessionStatus::Failed {
            anyhow::bail!("Session cannot be resumed from status: {:?}", session.status);
        }
        
        // 2. ë¯¸ì²˜ë¦¬ URL ëª©ë¡ ë³µêµ¬
        let remaining_urls = self.session_manager.get_remaining_urls(session_id).await?;
        let retry_urls = self.session_manager.get_retry_urls(session_id).await?;
        
        // 3. ë°©ë¬¸í•œ URL ëª©ë¡ ë³µêµ¬
        let visited_urls = self.session_manager.get_visited_urls(session_id).await?;
        {
            let mut visited = self.visited_urls.lock().await;
            visited.extend(visited_urls);
        }
        
        // 4. ì„¸ì…˜ ì¬ì‹œì‘
        self.session_manager.set_status(session_id, SessionStatus::Active).await?;
        
        // 5. í¬ë¡¤ë§ ì¬ê°œ
        let mut all_urls = remaining_urls;
        all_urls.extend(retry_urls);
        
        self.crawl_url_list(all_urls, session_id).await?;
        
        Ok(())
    }
}
```

## ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
```rust
impl WebCrawler {
    fn optimize_memory_usage(&self, html: &str) -> String {
        // 1. ë¶ˆí•„ìš”í•œ HTML íƒœê·¸ ì œê±° (ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ ë“±)
        let cleaned = self.remove_unnecessary_tags(html);
        
        // 2. ì••ì¶•
        if cleaned.len() > 10_000 {
            self.compress_html(&cleaned)
        } else {
            cleaned
        }
    }
    
    async fn manage_url_queue(&self, urls: &mut Vec<String>, session_id: &str) -> Result<()> {
        // URL í í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œì–´)
        const MAX_QUEUE_SIZE: usize = 10_000;
        
        if urls.len() > MAX_QUEUE_SIZE {
            // ìš°ì„ ìˆœìœ„ê°€ ë‚®ì€ URLë“¤ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            let excess_urls = urls.split_off(MAX_QUEUE_SIZE);
            self.session_manager.save_pending_urls(session_id, &excess_urls).await?;
            
            tracing::info!("Saved {} URLs to database to manage memory", excess_urls.len());
        }
        
        Ok(())
    }
}
```

### ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”
```rust
impl WebCrawler {
    async fn batch_save_products(&self, products: Vec<ExtractedMatterProduct>) -> Result<()> {
        const BATCH_SIZE: usize = 100;
        
        for chunk in products.chunks(BATCH_SIZE) {
            let mut transaction = self.product_repo.begin_transaction().await?;
            
            for product in chunk {
                self.product_repo.save_matter_product_in_transaction(&mut transaction, product).await?;
            }
            
            transaction.commit().await?;
        }
        
        Ok(())
    }
    
    async fn optimize_database_performance(&self) -> Result<()> {
        // 1. ì¸ë±ìŠ¤ ìµœì í™”
        self.product_repo.ensure_indexes().await?;
        
        // 2. í†µê³„ ì—…ë°ì´íŠ¸
        self.product_repo.update_statistics().await?;
        
        // 3. í…Œì´ë¸” ë¶„ì„
        self.product_repo.analyze_tables().await?;
        
        Ok(())
    }
}
```

## ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì¶”ì 
```rust
#[derive(Debug, Clone, Serialize)]
pub struct CrawlingProgress {
    pub session_id: String,
    pub pages_crawled: u32,
    pub total_estimated: u32,
    pub products_found: u32,
    pub errors_count: u32,
    pub current_url: String,
    pub stage: CrawlingStage,
    pub elapsed_time: std::time::Duration,
    pub estimated_remaining: std::time::Duration,
    pub crawl_rate: f64, // pages per minute
}

impl WebCrawler {
    async fn update_progress_metrics(&self, session_id: &str) -> Result<()> {
        let session = self.session_manager.get_session(session_id).await?;
        let start_time = session.created_at;
        let elapsed = chrono::Utc::now().signed_duration_since(start_time);
        
        let progress = CrawlingProgress {
            session_id: session_id.to_string(),
            pages_crawled: session.current_page,
            total_estimated: self.estimate_total_pages(session_id).await?,
            products_found: self.count_session_products(session_id).await?,
            errors_count: session.error_count,
            current_url: session.current_url.clone(),
            stage: session.stage,
            elapsed_time: elapsed.to_std().unwrap_or_default(),
            estimated_remaining: self.estimate_remaining_time(session_id).await?,
            crawl_rate: self.calculate_crawl_rate(session_id).await?,
        };
        
        // í”„ë¡ íŠ¸ì—”ë“œë¡œ ì§„í–‰ ìƒí™© ë¸Œë¡œë“œìºìŠ¤íŠ¸
        self.session_manager.broadcast_progress(progress).await?;
        
        Ok(())
    }
}
```

ì´ ì„¸ë¶€ ì›Œí¬í”Œë¡œìš° ë¬¸ì„œëŠ” í¬ë¡¤ë§ ì—”ì§„ì˜ ëª¨ë“  ì£¼ìš” ê¸°ëŠ¥ê³¼ ìµœì í™” ì „ëµì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, í˜„ì¬ êµ¬í˜„ëœ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥í›„ ê°œì„  ë°©í–¥ì„ ì œì‹œí•©ë‹ˆë‹¤.
