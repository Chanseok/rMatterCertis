//! # Crawling Domain Module v2.0
//!
//! Modern Rust 2024 + Clean Architecture
//! - ëª…ì‹œì  ëª¨ë“ˆ êµ¬ì¡° (mod.rs ë¹„ì‚¬ìš©)
//! - ë„ë©”ì¸ ë¡œì§ ì¤‘ì‹¬ ì„¤ê³„
//! - ì˜ì¡´ì„± ì—­ì „ ì›ì¹™ ì¤€ìˆ˜
//! - í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ êµ¬ì¡°
//! - ì¸í”„ë¼ìŠ¤íŠ¸ëŸ­ì²˜ ë¶„ë¦¬

use std::sync::Arc;
use std::time::Duration;
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::sync::RwLock;
use tracing::{info, error};

// Modern Rust 2024 - ëª…ì‹œì  ëª¨ë“ˆ ì„ ì–¸ (mod.rs ë¹„ì‚¬ìš©)
pub mod tasks;
pub mod state;
pub mod queues;
pub mod workers;
pub mod orchestrator;

// Clean re-exports
pub use tasks::{TaskId, CrawlingTask, TaskResult, TaskOutput, TaskProductData};
pub use state::{SharedState, CrawlingStats, CrawlingConfig};
pub use queues::{QueueManager, TaskQueue, QueueError};
pub use workers::{WorkerPool, WorkerError, WorkerPoolStats, WorkerPoolBuilder};
pub use orchestrator::{CrawlingOrchestrator, OrchestratorConfig, OrchestratorError};

/// Clean Architecture - í¬ë¡¤ë§ ì—”ì§„ ë„ë©”ì¸ ì„œë¹„ìŠ¤
#[derive(Clone)]
pub struct CrawlingEngine {
    orchestrator: Arc<CrawlingOrchestrator>,
    shared_state: Arc<SharedState>,
    is_running: Arc<RwLock<bool>>,
    config: CrawlingConfig,
}

/// í¬ë¡¤ë§ ì—”ì§„ ì—ëŸ¬ íƒ€ì…
#[derive(Error, Debug, Clone, Serialize, Deserialize)]
pub enum CrawlingEngineError {
    #[error("Engine initialization failed: {0}")]
    InitializationError(String),
    
    #[error("Engine is already running")]
    AlreadyRunning,
    
    #[error("Engine is not running")]
    NotRunning,
    
    #[error("Configuration error: {0}")]
    ConfigurationError(String),
    
    #[error("Worker pool error: {0}")]
    WorkerPoolError(String),
    
    #[error("Orchestrator error: {0}")]
    OrchestratorError(String),
    
    #[error("Database connection error: {0}")]
    DatabaseError(String),
    
    #[error("Database connection error: {0}")]
    DatabaseConnectionError(String),
    
    #[error("Shutdown timeout exceeded")]
    ShutdownTimeout,
}

/// í¬ë¡¤ë§ ì—”ì§„ íŒ©í† ë¦¬ - ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
pub struct CrawlingEngineFactory;

impl CrawlingEngineFactory {
    /// Clean Architecture ê¸°ë°˜ ì—”ì§„ ìƒì„±
    pub async fn create_engine(
        config: CrawlingConfig,
        dependencies: CrawlingEngineDependencies,
    ) -> Result<CrawlingEngine, CrawlingEngineError> {
        // 1. ê³µìœ  ìƒíƒœ ì´ˆê¸°í™”
        let shared_state = Arc::new(SharedState::new(config.clone()));
        
        // 2. í ë§¤ë‹ˆì € ì´ˆê¸°í™” (ì„¤ì •ê°’ ì‚¬ìš©)
        let queue_manager = Arc::new(QueueManager::new_with_config(
            config.max_queue_size,
            config.backpressure_threshold,
        ));
        
        // 3. ì›Œì»¤ í’€ ìƒì„± - í˜„ì¬ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” Factory íŒ¨í„´ (ì£¼ì„ ì²˜ë¦¬)
        // let worker_pool = WorkerPoolBuilder::new()
        //     .with_max_concurrency(config.max_concurrent_requests)
        //     .with_list_page_fetcher(Arc::new(workers::ListPageFetcher::new_simple()))
        //     .with_list_page_parser(Arc::new(workers::ListPageParser::new_simple()))
        //     .with_product_detail_fetcher(Arc::new(workers::ProductDetailFetcher::new_simple()))
        //     .with_product_detail_parser(Arc::new(workers::ProductDetailParser::new_simple()))
        //     .with_db_saver(Arc::new(workers::DbSaver::new(...)))  // ì‹¤ì œ DB í•„ìš”
        //     .build()
        //     .map_err(|e| CrawlingEngineError::WorkerPoolError(e.to_string()))?;
        
        // Factory íŒ¨í„´ì€ deprecatedë¨. with_config ë˜ëŠ” with_config_and_db ì‚¬ìš© ê¶Œì¥
        Err(CrawlingEngineError::ConfigurationError(
            "Factory pattern is deprecated. Use with_config or with_config_and_db instead.".to_string()
        ))
    }
}

/// ì˜ì¡´ì„± ì£¼ì…ì„ ìœ„í•œ êµ¬ì¡°ì²´
pub struct CrawlingEngineDependencies {
    pub list_page_fetcher: Arc<workers::ListPageFetcher>,
    pub list_page_parser: Arc<workers::ListPageParser>,
    pub product_detail_fetcher: Arc<workers::ProductDetailFetcher>,
    pub product_detail_parser: Arc<workers::ProductDetailParser>,
    pub db_saver: Arc<workers::DbSaver>,
}

impl CrawlingEngine {
    /// í¬ë¡¤ë§ ì‹œì‘ (ë„ë©”ì¸ ë¡œì§)
    pub async fn start(&self) -> Result<(), CrawlingEngineError> {
        let mut is_running = self.is_running.write().await;
        
        if *is_running {
            return Err(CrawlingEngineError::AlreadyRunning);
        }
        
        tracing::info!("Starting crawling engine...");
        
        // ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‹œì‘
        self.orchestrator.start().await
            .map_err(|e| CrawlingEngineError::OrchestratorError(e.to_string()))?;
        
        *is_running = true;
        
        tracing::info!("Crawling engine started successfully");
        Ok(())
    }
    
    /// í¬ë¡¤ë§ ì¤‘ì§€ (ë„ë©”ì¸ ë¡œì§)
    pub async fn stop(&self) -> Result<(), CrawlingEngineError> {
        let mut is_running = self.is_running.write().await;
        
        if !*is_running {
            tracing::warn!("ğŸŸ¡ Crawling engine is already stopped");
            return Ok(()); // ì´ë¯¸ ì¤‘ì§€ëœ ìƒíƒœë©´ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
        }
        
        tracing::info!("ğŸ›‘ Stopping crawling engine...");
        
        // ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì¤‘ì§€
        match self.orchestrator.stop().await {
            Ok(()) => {
                tracing::info!("âœ… Orchestrator stopped successfully");
            }
            Err(e) => {
                tracing::error!("âŒ Error stopping orchestrator: {}", e);
                // ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì¤‘ì§€ ì‹¤íŒ¨í•´ë„ ì—”ì§„ ìƒíƒœëŠ” ì¤‘ì§€ë¡œ ì„¤ì •
            }
        }
        
        *is_running = false;
        
        tracing::info!("âœ… Crawling engine stopped successfully");
        Ok(())
    }
    
    /// ì—”ì§„ ìƒíƒœ í™•ì¸
    pub async fn is_running(&self) -> bool {
        *self.is_running.read().await
    }
    
    /// í¬ë¡¤ë§ í†µê³„ ì¡°íšŒ
    pub async fn get_stats(&self) -> CrawlingStats {
        let stats = self.shared_state.stats.read().await;
        stats.clone()
    }
    
    /// ì„¤ì • ì¡°íšŒ
    pub fn get_config(&self) -> &CrawlingConfig {
        &self.config
    }
    
    /// ì„¤ì • ì—…ë°ì´íŠ¸
    pub async fn update_config(&mut self, new_config: CrawlingConfig) -> Result<(), CrawlingEngineError> {
        if self.is_running().await {
            return Err(CrawlingEngineError::ConfigurationError(
                "Cannot update configuration while engine is running".to_string()
            ));
        }
        
        self.config = new_config.clone();
        // Update shared state configuration as needed
        // For now, we'll just update local config
        Ok(())
    }
    
    /// íƒœìŠ¤í¬ ì¶”ê°€
    pub async fn add_task(&self, task: CrawlingTask) -> Result<(), CrawlingEngineError> {
        if !self.is_running().await {
            return Err(CrawlingEngineError::NotRunning);
        }
        
        self.orchestrator.add_initial_task(task).await
            .map_err(|e| CrawlingEngineError::OrchestratorError(e.to_string()))?;
        
        Ok(())
    }
    
    /// í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘
    pub async fn start_crawling_session(
        &self,
        start_page: u32,
        end_page: u32,
    ) -> Result<(), CrawlingEngineError> {
        if !self.is_running().await {
            return Err(CrawlingEngineError::NotRunning);
        }
        
        tracing::info!("ğŸš€ Starting crawling session: pages {} to {}", start_page, end_page);
        
        // í˜ì´ì§€ ë²”ìœ„ ìœ íš¨ì„± ê²€ì‚¬
        if start_page == 0 || end_page == 0 {
            return Err(CrawlingEngineError::ConfigurationError(
                "Page numbers must be greater than 0".to_string()
            ));
        }
        
        // Matter Certification Products URL íŒ¨í„´ ì‚¬ìš©
        const MATTER_PRODUCTS_URL_PATTERN: &str = "https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver";
        
        // í˜ì´ì§€ ë²”ìœ„ ì²˜ë¦¬: start_pageê°€ end_pageë³´ë‹¤ í´ ìˆ˜ ìˆìŒ (ìµœì‹  í˜ì´ì§€ë¶€í„° í¬ë¡¤ë§)
        let (min_page, max_page) = if start_page <= end_page {
            (start_page, end_page)
        } else {
            (end_page, start_page)
        };
        
        let total_pages = max_page - min_page + 1;
        tracing::info!("ğŸ“‹ Creating {} tasks for pages {} to {}", total_pages, min_page, max_page);
        
        // í˜ì´ì§€ ë²”ìœ„ì— ë”°ë¥¸ íƒœìŠ¤í¬ ìƒì„±
        for page in min_page..=max_page {
            let url = MATTER_PRODUCTS_URL_PATTERN.replace("{}", &page.to_string());
            let task = CrawlingTask::FetchListPage {
                task_id: TaskId::new(),
                page_number: page,
                url: url.clone(),
            };
            
            tracing::info!("âœ… Created task for page {}: {}", page, url);
            self.add_task(task).await?;
        }
        
        tracing::info!("ğŸ¯ All {} tasks created successfully", total_pages);
        Ok(())
    }
    
    /// ì‘ê¸‰ ì¤‘ì§€
    pub async fn emergency_stop(&self) -> Result<(), CrawlingEngineError> {
        tracing::warn!("Emergency stop requested");
        
        // ê°•ì œ ì¤‘ì§€
        *self.is_running.write().await = false;
        
        // ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì¤‘ì§€
        self.orchestrator.stop().await
            .map_err(|e| CrawlingEngineError::OrchestratorError(e.to_string()))?;
        
        tracing::warn!("Emergency stop completed");
        Ok(())
    }
    
    /// ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬
    pub async fn health_check(&self) -> CrawlingEngineHealth {
        let is_running = self.is_running().await;
        let stats = self.get_stats().await;
        
        CrawlingEngineHealth {
            is_running,
            stats,
            orchestrator_health: self.orchestrator.get_stats().await,
            last_check: chrono::Utc::now(),
        }
    }
    
    /// ìƒˆë¡œìš´ í¬ë¡¤ë§ ì—”ì§„ì„ ìƒì„±í•©ë‹ˆë‹¤
    pub async fn new(
        _database_pool: sqlx::Pool<sqlx::Sqlite>, 
        config: CrawlingConfig
    ) -> Result<Self, CrawlingEngineError> {
        // ê°œë°œ ìš©ì´ì„±ì„ ìœ„í•´ Mock ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
        info!("ê°œë°œ ëª¨ë“œ: Mock ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤");
        
        Self::with_config(config).await
    }
    
    /// ì„¤ì •ê³¼ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ ì—”ì§„ì„ ìƒì„±í•©ë‹ˆë‹¤
    pub async fn with_config_and_db(
        config: CrawlingConfig,
        database_pool: sqlx::SqlitePool,
    ) -> Result<Self, CrawlingEngineError> {
        // 1. ê³µìœ  ìƒíƒœ ì´ˆê¸°í™”
        let shared_state = Arc::new(SharedState::new(config.clone()));
        
        // 2. í ë§¤ë‹ˆì € ì´ˆê¸°í™”
        let queue_manager = Arc::new(QueueManager::new_with_config(
            config.max_queue_size,
            config.backpressure_threshold,
        ));
        
        // 3. ì›Œì»¤ í’€ ìƒì„± (ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‚¬ìš©)
        let db_saver = Arc::new(workers::DbSaver::new(
            database_pool,
            100, // batch_size
            Duration::from_secs(30), // flush_interval
        ));
        
        let worker_pool = WorkerPoolBuilder::new()
            .with_max_concurrency(config.max_concurrent_tasks)
            .with_list_page_fetcher(Arc::new(workers::ListPageFetcher::new_simple()))
            .with_list_page_parser(Arc::new(workers::ListPageParser::new_simple()))
            .with_product_detail_fetcher(Arc::new(workers::ProductDetailFetcher::new_simple()))
            .with_product_detail_parser(Arc::new(workers::ProductDetailParser::new_simple()))
            .with_db_saver(db_saver)
            .build()
            .map_err(|e| CrawlingEngineError::WorkerPoolError(e.to_string()))?;
        
        // 4. ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„¤ì •
        let orchestrator_config = OrchestratorConfig {
            max_global_concurrency: config.max_concurrent_tasks,
            scheduler_interval: Duration::from_millis(config.scheduler_interval_ms),
            shutdown_timeout: Duration::from_secs(config.shutdown_timeout_seconds),
            stats_interval: Duration::from_secs(config.stats_interval_seconds),
            auto_retry_enabled: config.auto_retry_enabled,
            max_retries: config.max_retries,
            retry_delay: Duration::from_millis(config.retry_delay_ms),
            backpressure_enabled: config.backpressure_enabled,
            backpressure_threshold: 1000, // Default value
        };
        
        // 5. ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„±
        let orchestrator = Arc::new(CrawlingOrchestrator::new(
            Arc::new(worker_pool),
            queue_manager,
            shared_state.clone(),
            orchestrator_config,
        ));
        
        // 6. ì—”ì§„ ìƒì„±
        let engine = CrawlingEngine {
            orchestrator,
            shared_state,
            is_running: Arc::new(RwLock::new(false)),
            config,
        };
        
        tracing::info!("í¬ë¡¤ë§ ì—”ì§„ ìƒì„± ì™„ë£Œ (ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°)");
        Ok(engine)
    }

    /// ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ ì—”ì§„ì„ ìƒì„±í•©ë‹ˆë‹¤ (ì¸ë©”ëª¨ë¦¬ SQLite DB ì‚¬ìš©)
    pub async fn with_config(config: CrawlingConfig) -> Result<Self, CrawlingEngineError> {
        // ê°œë°œìš© ì¸ë©”ëª¨ë¦¬ SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        let database_pool = sqlx::SqlitePool::connect("sqlite::memory:")
            .await
            .map_err(|e| CrawlingEngineError::DatabaseConnectionError(e.to_string()))?;
        
        // í…Œì´ë¸” ìƒì„± (ê¸°ë³¸ ìŠ¤í‚¤ë§ˆ)
        sqlx::query(r"
            CREATE TABLE IF NOT EXISTS products (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                product_id TEXT NOT NULL UNIQUE,
                name TEXT NOT NULL,
                category TEXT,
                manufacturer TEXT,
                model TEXT,
                certification_number TEXT,
                certification_date TEXT,
                source_url TEXT NOT NULL,
                extracted_at TEXT NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ")
        .execute(&database_pool)
        .await
        .map_err(|e| CrawlingEngineError::DatabaseConnectionError(format!("Failed to create tables: {}", e)))?;
        
        // ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì‚¬ìš©í•˜ì—¬ ì—”ì§„ ìƒì„±
        Self::with_config_and_db(config, database_pool).await
    }
}

/// í¬ë¡¤ë§ ì—”ì§„ í—¬ìŠ¤ ìƒíƒœ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingEngineHealth {
    pub is_running: bool,
    pub stats: CrawlingStats,
    pub orchestrator_health: orchestrator::OrchestratorStats,
    pub last_check: chrono::DateTime<chrono::Utc>,
}

/// í¬ë¡¤ë§ ì—”ì§„ ë©”íŠ¸ë¦­ìŠ¤
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingEngineMetrics {
    pub uptime: Duration,
    pub total_tasks_processed: u64,
    pub success_rate: f64,
    pub average_task_duration: Duration,
    pub current_queue_size: usize,
}

/// í¬ë¡¤ë§ ì—”ì§„ ë¹Œë” íŒ¨í„´
pub struct CrawlingEngineBuilder {
    config: Option<CrawlingConfig>,
    dependencies: Option<CrawlingEngineDependencies>,
}

impl CrawlingEngineBuilder {
    pub fn new() -> Self {
        Self {
            config: None,
            dependencies: None,
        }
    }
    
    pub fn with_config(mut self, config: CrawlingConfig) -> Self {
        self.config = Some(config);
        self
    }
    
    pub fn with_dependencies(mut self, dependencies: CrawlingEngineDependencies) -> Self {
        self.dependencies = Some(dependencies);
        self
    }
    
    pub async fn build(self) -> Result<CrawlingEngine, CrawlingEngineError> {
        let config = self.config.ok_or_else(|| 
            CrawlingEngineError::ConfigurationError("Config not provided".to_string()))?;
        let dependencies = self.dependencies.ok_or_else(|| 
            CrawlingEngineError::ConfigurationError("Dependencies not provided".to_string()))?;
        
        CrawlingEngineFactory::create_engine(config, dependencies).await
    }
}

impl Default for CrawlingEngineBuilder {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_crawling_engine_lifecycle() {
        // Mock dependencies would be injected here
        // This is a placeholder for actual tests
        
        // Test engine creation
        let builder = CrawlingEngineBuilder::new();
        assert!(builder.config.is_none());
        assert!(builder.dependencies.is_none());
    }
    
    #[test]
    fn test_crawling_engine_error_serialization() {
        let error = CrawlingEngineError::AlreadyRunning;
        let serialized = serde_json::to_string(&error).unwrap();
        let deserialized: CrawlingEngineError = serde_json::from_str(&serialized).unwrap();
        
        match deserialized {
            CrawlingEngineError::AlreadyRunning => {},
            _ => panic!("Wrong error type"),
        }
    }
}
