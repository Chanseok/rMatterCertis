//! # Crawling Domain Module v2.0
//!
//! Clean Architecture + Modern Rust 2024 구현
//! - 도메인 로직 중심 설계
//! - 의존성 역전 원칙 준수
//! - 테스트 가능한 구조
//! - 인프라스트럭처 분리

use std::sync::Arc;
use std::time::Duration;
use async_trait::async_trait;
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::sync::RwLock;

// Domain modules
pub mod tasks;
pub mod state;
pub mod queues;
pub mod workers;
pub mod orchestrator;

// Clean re-exports
pub use tasks::{TaskId, CrawlingTask, TaskResult, TaskOutput, TaskProductData};
pub use state::{SharedState, CrawlingStats, CrawlingConfig, CrawlingStatus};
pub use queues::{QueueManager, TaskQueue, QueueManagerError};
pub use workers::{WorkerPool, WorkerError, WorkerPoolStats, WorkerPoolBuilder};
pub use orchestrator::{CrawlingOrchestrator, OrchestratorConfig, OrchestratorError};

/// Clean Architecture - 크롤링 엔진 도메인 서비스
#[derive(Clone)]
pub struct CrawlingEngine {
    orchestrator: Arc<CrawlingOrchestrator>,
    shared_state: Arc<SharedState>,
    is_running: Arc<RwLock<bool>>,
    config: CrawlingConfig,
}

/// 크롤링 엔진 에러 타입
#[derive(Error, Debug, Clone, Serialize, Deserialize)]
pub enum CrawlingEngineError {
    #[error("Engine initialization failed: {0}")]
    InitializationError(String),
    
    #[error("Engine is already running")]
    AlreadyRunning,
    
    #[error("Engine is not running")]
    NotRunning,
    
    #[error("Configuration error: {0}")]
    ConfigurationError(String),
    
    #[error("Worker pool error: {0}")]
    WorkerPoolError(String),
    
    #[error("Orchestrator error: {0}")]
    OrchestratorError(String),
    
    #[error("Database connection error: {0}")]
    DatabaseError(String),
    
    #[error("Shutdown timeout exceeded")]
    ShutdownTimeout,
}

/// 크롤링 엔진 팩토리 - 의존성 주입 패턴
pub struct CrawlingEngineFactory;

impl CrawlingEngineFactory {
    /// Clean Architecture 기반 엔진 생성
    pub async fn create_engine(
        config: CrawlingConfig,
        dependencies: CrawlingEngineDependencies,
    ) -> Result<CrawlingEngine, CrawlingEngineError> {
        // 1. 공유 상태 초기화
        let shared_state = Arc::new(SharedState::new(config.clone()));
        
        // 2. 큐 매니저 초기화
        let queue_manager = Arc::new(QueueManager::new(
            config.max_queue_size,
            config.backpressure_threshold,
        ));
        
        // 3. 워커 풀 생성 (의존성 주입)
        let worker_pool = WorkerPoolBuilder::new()
            .with_max_concurrency(config.max_concurrent_tasks)
            .with_list_page_fetcher(dependencies.list_page_fetcher)
            .with_list_page_parser(dependencies.list_page_parser)
            .with_product_detail_fetcher(dependencies.product_detail_fetcher)
            .with_product_detail_parser(dependencies.product_detail_parser)
            .with_db_saver(dependencies.db_saver)
            .build()
            .map_err(|e| CrawlingEngineError::WorkerPoolError(e.to_string()))?;
        
        // 4. 오케스트레이터 설정
        let orchestrator_config = OrchestratorConfig {
            max_global_concurrency: config.max_concurrent_tasks,
            scheduler_interval: Duration::from_millis(config.scheduler_interval_ms),
            shutdown_timeout: Duration::from_secs(config.shutdown_timeout_seconds),
            stats_interval: Duration::from_secs(config.stats_interval_seconds),
            auto_retry_enabled: config.auto_retry_enabled,
            max_retries: config.max_retries,
            retry_delay: Duration::from_millis(config.retry_delay_ms),
            backpressure_enabled: config.backpressure_enabled,
            backpressure_threshold: config.backpressure_threshold,
        };
        
        // 5. 오케스트레이터 생성
        let orchestrator = Arc::new(CrawlingOrchestrator::new(
            Arc::new(worker_pool),
            queue_manager,
            shared_state.clone(),
            orchestrator_config,
        ));
        
        Ok(CrawlingEngine {
            orchestrator,
            shared_state,
            is_running: Arc::new(RwLock::new(false)),
            config,
        })
    }
}

/// 의존성 주입을 위한 구조체
pub struct CrawlingEngineDependencies {
    pub list_page_fetcher: Arc<workers::ListPageFetcher>,
    pub list_page_parser: Arc<workers::ListPageParser>,
    pub product_detail_fetcher: Arc<workers::ProductDetailFetcher>,
    pub product_detail_parser: Arc<workers::ProductDetailParser>,
    pub db_saver: Arc<workers::DbSaver>,
}

impl CrawlingEngine {
    /// 크롤링 시작 (도메인 로직)
    pub async fn start(&self) -> Result<(), CrawlingEngineError> {
        let mut is_running = self.is_running.write().await;
        
        if *is_running {
            return Err(CrawlingEngineError::AlreadyRunning);
        }
        
        tracing::info!("Starting crawling engine...");
        
        // 오케스트레이터 시작
        self.orchestrator.start().await
            .map_err(|e| CrawlingEngineError::OrchestratorError(e.to_string()))?;
        
        *is_running = true;
        
        tracing::info!("Crawling engine started successfully");
        Ok(())
    }
    
    /// 크롤링 중지 (도메인 로직)
    pub async fn stop(&self) -> Result<(), CrawlingEngineError> {
        let mut is_running = self.is_running.write().await;
        
        if !*is_running {
            return Err(CrawlingEngineError::NotRunning);
        }
        
        tracing::info!("Stopping crawling engine...");
        
        // 오케스트레이터 중지
        self.orchestrator.stop().await
            .map_err(|e| CrawlingEngineError::OrchestratorError(e.to_string()))?;
        
        *is_running = false;
        
        tracing::info!("Crawling engine stopped successfully");
        Ok(())
    }
    
    /// 엔진 상태 확인
    pub async fn is_running(&self) -> bool {
        *self.is_running.read().await
    }
    
    /// 크롤링 통계 조회
    pub async fn get_stats(&self) -> CrawlingStats {
        self.shared_state.get_stats().await
    }
    
    /// 설정 조회
    pub fn get_config(&self) -> &CrawlingConfig {
        &self.config
    }
    
    /// 설정 업데이트
    pub async fn update_config(&mut self, new_config: CrawlingConfig) -> Result<(), CrawlingEngineError> {
        if self.is_running().await {
            return Err(CrawlingEngineError::ConfigurationError(
                "Cannot update configuration while engine is running".to_string()
            ));
        }
        
        self.config = new_config.clone();
        self.shared_state.update_config(new_config).await;
        Ok(())
    }
    
    /// 태스크 추가
    pub async fn add_task(&self, task: CrawlingTask) -> Result<TaskId, CrawlingEngineError> {
        if !self.is_running().await {
            return Err(CrawlingEngineError::NotRunning);
        }
        
        self.orchestrator.add_task(task).await
            .map_err(|e| CrawlingEngineError::OrchestratorError(e.to_string()))
    }
    
    /// 크롤링 세션 시작
    pub async fn start_crawling_session(
        &self,
        start_page: u32,
        end_page: u32,
    ) -> Result<(), CrawlingEngineError> {
        if !self.is_running().await {
            return Err(CrawlingEngineError::NotRunning);
        }
        
        tracing::info!("Starting crawling session: pages {} to {}", start_page, end_page);
        
        // Matter Certification Products URL 패턴 사용
        const MATTER_PRODUCTS_URL_PATTERN: &str = "https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver";
        
        // 페이지 범위에 따른 태스크 생성
        for page in start_page..=end_page {
            let url = MATTER_PRODUCTS_URL_PATTERN.replace("{}", &page.to_string());
            let task = CrawlingTask::FetchListPage {
                task_id: TaskId::new(),
                page_number: page,
                url: url.clone(),
            };
            
            tracing::info!("Created task for page {}: {}", page, url);
            self.add_task(task).await?;
        }
        
        Ok(())
    }
    
    /// 응급 중지
    pub async fn emergency_stop(&self) -> Result<(), CrawlingEngineError> {
        tracing::warn!("Emergency stop requested");
        
        // 강제 중지
        *self.is_running.write().await = false;
        
        // 오케스트레이터 응급 중지
        self.orchestrator.emergency_stop().await
            .map_err(|e| CrawlingEngineError::OrchestratorError(e.to_string()))?;
        
        tracing::warn!("Emergency stop completed");
        Ok(())
    }
    
    /// 시스템 헬스 체크
    pub async fn health_check(&self) -> CrawlingEngineHealth {
        let is_running = self.is_running().await;
        let stats = self.get_stats().await;
        
        CrawlingEngineHealth {
            is_running,
            stats,
            orchestrator_health: self.orchestrator.health_check().await,
            last_check: chrono::Utc::now(),
        }
    }
}

/// 크롤링 엔진 헬스 상태
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingEngineHealth {
    pub is_running: bool,
    pub stats: CrawlingStats,
    pub orchestrator_health: orchestrator::OrchestratorHealth,
    pub last_check: chrono::DateTime<chrono::Utc>,
}

/// 크롤링 엔진 메트릭스
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingEngineMetrics {
    pub uptime: Duration,
    pub total_tasks_processed: u64,
    pub success_rate: f64,
    pub average_task_duration: Duration,
    pub current_queue_size: usize,
}

/// 크롤링 엔진 빌더 패턴
pub struct CrawlingEngineBuilder {
    config: Option<CrawlingConfig>,
    dependencies: Option<CrawlingEngineDependencies>,
}

impl CrawlingEngineBuilder {
    pub fn new() -> Self {
        Self {
            config: None,
            dependencies: None,
        }
    }
    
    pub fn with_config(mut self, config: CrawlingConfig) -> Self {
        self.config = Some(config);
        self
    }
    
    pub fn with_dependencies(mut self, dependencies: CrawlingEngineDependencies) -> Self {
        self.dependencies = Some(dependencies);
        self
    }
    
    pub async fn build(self) -> Result<CrawlingEngine, CrawlingEngineError> {
        let config = self.config.ok_or_else(|| 
            CrawlingEngineError::ConfigurationError("Config not provided".to_string()))?;
        let dependencies = self.dependencies.ok_or_else(|| 
            CrawlingEngineError::ConfigurationError("Dependencies not provided".to_string()))?;
        
        CrawlingEngineFactory::create_engine(config, dependencies).await
    }
}

impl Default for CrawlingEngineBuilder {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_crawling_engine_lifecycle() {
        // Mock dependencies would be injected here
        // This is a placeholder for actual tests
        
        // Test engine creation
        let builder = CrawlingEngineBuilder::new();
        assert!(builder.config.is_none());
        assert!(builder.dependencies.is_none());
    }
    
    #[test]
    fn test_crawling_engine_error_serialization() {
        let error = CrawlingEngineError::AlreadyRunning;
        let serialized = serde_json::to_string(&error).unwrap();
        let deserialized: CrawlingEngineError = serde_json::from_str(&serialized).unwrap();
        
        match deserialized {
            CrawlingEngineError::AlreadyRunning => {},
            _ => panic!("Wrong error type"),
        }
    }
}
