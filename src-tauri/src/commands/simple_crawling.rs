use tauri::State;
use serde::{Deserialize, Serialize};
use ts_rs::TS;
use tracing::info;

use crate::infrastructure::config::ConfigManager;
use crate::infrastructure::crawling_service_impls::CrawlingRangeCalculator;
use crate::infrastructure::integrated_product_repository::IntegratedProductRepository;
use crate::application::AppState;

/// í¬ë¡¤ë§ ì„¸ì…˜ ì •ë³´ (ê°„ì†Œí™”)
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct CrawlingSession {
    pub session_id: String,
    pub started_at: String,
    pub status: String,
}

/// Smart Crawling ì‹œì‘ - ì„¤ì • íŒŒì¼ ê¸°ë°˜ ìë™ ì‹¤í–‰
#[tauri::command]
pub async fn start_smart_crawling(
    app_handle: tauri::AppHandle,
    state: State<'_, AppState>
) -> Result<CrawlingSession, String> {
    // 1. ì„¤ì • íŒŒì¼ ìë™ ë¡œë”©
    let config_manager = ConfigManager::new()
        .map_err(|e| format!("Failed to initialize config manager: {}", e))?;
    
    let config = config_manager.load_config().await
        .map_err(|e| format!("Failed to load config from file: {}", e))?;

    info!("ğŸš€ Starting smart crawling with {} max pages, {}ms delay", 
          config.user.crawling.page_range_limit, config.user.request_delay_ms);

    info!("âœ… Config loaded successfully: max_pages={}, request_delay={}ms", 
          config.user.crawling.page_range_limit, config.user.request_delay_ms);

    // 2. ê³µìœ  ë°ì´í„°ë² ì´ìŠ¤ Pool ì‚¬ìš© (Modern Rust 2024 - Backend-Only CRUD)
    let pool = state.get_database_pool().await?;
    
    let product_repo = IntegratedProductRepository::new(pool);

    // 3. í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚°
    let range_calculator = CrawlingRangeCalculator::new(
        std::sync::Arc::new(product_repo),
        config.clone(),
    );

    // ì„ì‹œë¡œ ì´ í˜ì´ì§€ ìˆ˜ì™€ ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜ë¥¼ í•˜ë“œì½”ë”© (ë‚˜ì¤‘ì— ì‚¬ì´íŠ¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
    let total_pages = 485u32;
    let products_on_last_page = 11u32;

    let range_result = range_calculator.calculate_next_crawling_range(
        total_pages,
        products_on_last_page,
    ).await
    .map_err(|e| format!("Failed to calculate crawling range: {}", e))?;

    match range_result {
        Some((start_page, end_page)) => {
            let session_id = format!("session_{}", chrono::Utc::now().timestamp());
            let started_at = chrono::Utc::now().to_rfc3339();
            
            info!("âœ… Smart crawling session created: {} (pages {} â†’ {})", 
                  session_id, start_page, end_page);

            // ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘ - ServiceBasedBatchCrawlingEngine ì‚¬ìš©
            use crate::commands::crawling_v4::{CrawlingEngineState, StartCrawlingRequest, start_crawling, init_crawling_engine};
            use crate::application::SharedStateCache;
            use tauri::Manager;
            
            // Engine stateë¥¼ ê°€ì ¸ì™€ì„œ í¬ë¡¤ë§ ì‹¤í–‰
            if let Some(engine_state) = app_handle.try_state::<CrawlingEngineState>() {
                if let Some(shared_cache) = app_handle.try_state::<SharedStateCache>() {
                    
                    // ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¨¼ì € ì´ˆê¸°í™”
                    {
                        let engine_guard = engine_state.engine.read().await;
                        if engine_guard.is_none() {
                            drop(engine_guard); // ì½ê¸° ë½ í•´ì œ
                            info!("ğŸ”§ Crawling engine not initialized, initializing now...");
                            
                            // ì—”ì§„ ì´ˆê¸°í™”
                            match init_crawling_engine(app_handle.clone(), engine_state.clone()).await {
                                Ok(response) => {
                                    if response.success {
                                        info!("âœ… Crawling engine initialized successfully");
                                    } else {
                                        return Err(format!("Failed to initialize crawling engine: {}", response.message));
                                    }
                                }
                                Err(e) => {
                                    return Err(format!("Failed to initialize crawling engine: {}", e));
                                }
                            }
                        }
                    }
                    
                    info!("ğŸš€ Starting actual crawling with ServiceBasedBatchCrawlingEngine");
                    
                    let request = StartCrawlingRequest {
                        start_page,
                        end_page,
                        max_products_per_page: Some(12), // í˜ì´ì§€ë‹¹ ìµœëŒ€ ì œí’ˆ ìˆ˜
                        concurrent_requests: Some(3),    // ë™ì‹œ ìš”ì²­ ìˆ˜
                        request_timeout_seconds: Some(30), // ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
                    };
                    
                    // í¬ë¡¤ë§ ì‹œì‘
                    match start_crawling(
                        app_handle.clone(),
                        engine_state,
                        shared_cache,
                        request
                    ).await {
                        Ok(response) => info!("âœ… Crawling started successfully: {}", response.message),
                        Err(e) => {
                            tracing::error!("âŒ Crawling failed to start: {}", e);
                            return Err(format!("Failed to start crawling: {}", e));
                        }
                    }
                    
                    info!("ğŸ¯ Crawling task initiated for session: {}", session_id);
                } else {
                    tracing::warn!("âš ï¸ SharedStateCache not found - falling back to session-only mode");
                }
            } else {
                tracing::warn!("âš ï¸ CrawlingEngineState not found - falling back to session-only mode");
            }
            
            Ok(CrawlingSession {
                session_id,
                started_at,
                status: "started".to_string(),
            })
        }
        None => {
            info!("ğŸ No more pages to crawl");
            Err("ëª¨ë“  í˜ì´ì§€ê°€ ì´ë¯¸ í¬ë¡¤ë§ë˜ì—ˆìŠµë‹ˆë‹¤.".to_string())
        }
    }
}
