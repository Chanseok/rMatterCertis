use tauri::State;
use serde::{Deserialize, Serialize};
use ts_rs::TS;
use tracing::info;
use std::sync::Arc;

use crate::infrastructure::config::ConfigManager;
use crate::application::AppState;
use crate::infrastructure::crawling_service_impls::StatusCheckerImpl;
use crate::infrastructure::{HttpClient, MatterDataExtractor};
use crate::new_architecture::services::crawling_planner::CrawlingPlanner;
use crate::new_architecture::context::SystemConfig;
use crate::domain::services::{StatusChecker, DatabaseAnalyzer};

/// í¬ë¡¤ë§ ì„¸ì…˜ ì •ë³´ (ê°„ì†Œí™”)
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct CrawlingSession {
    pub session_id: String,
    pub started_at: String,
    pub status: String,
}

/// Smart Crawling ì‹œì‘ - ì„¤ì • íŒŒì¼ ê¸°ë°˜ ìë™ ì‹¤í–‰
#[tauri::command]
pub async fn start_smart_crawling(
    app_handle: tauri::AppHandle,
    _state: State<'_, AppState>
) -> Result<CrawlingSession, String> {
    let session_id = format!("session_{}", chrono::Utc::now().timestamp());
    let started_at = chrono::Utc::now().to_rfc3339();
    
    info!("ğŸš€ Starting smart crawling session: {} (ì§€ëŠ¥í˜• ë¶„ì„ ê¸°ë°˜ ììœ¨ ë™ì‘)", session_id);
    info!("ğŸ”§ [NEW ARCHITECTURE] Using config-based CrawlingPlanner instead of hardcoded values");
    
    // ğŸ¯ ì„¤ê³„ ë¬¸ì„œ ì¤€ìˆ˜: íŒŒë¼ë¯¸í„° ì—†ì´ ì„¤ì • íŒŒì¼ë§Œìœ¼ë¡œ ë™ì‘
    // 1. ì„¤ì • íŒŒì¼ ìë™ ë¡œë”© (matter_certis_config.json)
    let config_manager = ConfigManager::new()
        .map_err(|e| format!("Failed to initialize config manager: {}", e))?;
    
    let config = config_manager.load_config().await
        .map_err(|e| format!("Failed to load config from file: {}", e))?;

    info!("âœ… Config loaded from files: max_pages={}, request_delay={}ms", 
          config.user.crawling.page_range_limit, config.user.request_delay_ms);

    // 2. ì§€ëŠ¥í˜• ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    info!("ğŸ§  Initializing intelligent analysis system...");
    
    // HTTP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (íŒŒë¼ë¯¸í„° ì—†ì´)
    let http_client = HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    
    // ë°ì´í„° ì¶”ì¶œê¸° ì´ˆê¸°í™” (Result ë°˜í™˜í•˜ë¯€ë¡œ ? ì—°ì‚°ì ì‚¬ìš©)
    let data_extractor = MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    // StatusChecker ì´ˆê¸°í™” (StatusCheckerì™€ DatabaseAnalyzer ëª¨ë‘ êµ¬í˜„)
    let status_checker_impl = Arc::new(StatusCheckerImpl::new(
        http_client,
        data_extractor,
        config.clone(),
    ));
    
    let status_checker: Arc<dyn StatusChecker> = status_checker_impl.clone();
    let database_analyzer: Arc<dyn DatabaseAnalyzer> = status_checker_impl;
    
    // SystemConfig ìƒì„± (ì„¤ì • íŒŒì¼ ê¸°ë°˜ ì™„ì „ ë™ì‘)
    let system_config = Arc::new(SystemConfig::default()); // í–¥í›„ ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ
    info!("âš™ï¸ [NEW ARCHITECTURE] SystemConfig initialized with intelligent defaults");
    
    // CrawlingPlanner ì´ˆê¸°í™” (ì„¤ì • íŒŒì¼ ê¸°ë°˜)
    let planner = CrawlingPlanner::new(
        status_checker,
        database_analyzer,
        system_config,
    );
    info!("ğŸ§  [NEW ARCHITECTURE] CrawlingPlanner initialized - replacing hardcoded logic");
    
    // 3. ì§€ëŠ¥í˜• ì‹œìŠ¤í…œ ìƒíƒœ ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½
    info!("ğŸ” [NEW ARCHITECTURE] Analyzing system state with intelligent CrawlingPlanner...");
    
    let (site_status, db_analysis) = planner.analyze_system_state().await
        .map_err(|e| format!("System analysis failed: {}", e))?;
    
    let (range_recommendation, processing_strategy) = planner.determine_crawling_strategy(&site_status, &db_analysis).await
        .map_err(|e| format!("Strategy determination failed: {}", e))?;

    info!("âœ… [NEW ARCHITECTURE] Analysis complete - Range: {:?}, Processing: {:?}", range_recommendation, processing_strategy);    // 4. ê³„ì‚°ëœ ë²”ìœ„ë¡œ í¬ë¡¤ë§ ì‹¤í–‰ (ì„¤ì • íŒŒì¼ ê³ ì •ê°’ ëŒ€ì‹  ì§€ëŠ¥í˜• ê³„ì‚° ê²°ê³¼ ì‚¬ìš©)
    // Legacy crawling_v4 path removed. For now, delegate to unified actor-based crawling if range is recommended.
    if let Some((start_page, end_page)) = range_recommendation.to_page_range(site_status.total_pages) {
        info!("ğŸ“Š ì§€ëŠ¥í˜• ë¶„ì„ ê¸°ë°˜ í¬ë¡¤ë§ ë²”ìœ„: {}-{} í˜ì´ì§€ (ì´ {} í˜ì´ì§€ ì¤‘)", start_page, end_page, site_status.total_pages);
        // Fire unified crawling command (actor-based real crawling)
        use crate::commands::unified_crawling::{start_unified_crawling, StartCrawlingRequest};
        let req = StartCrawlingRequest {
            engine_type: "actor".to_string(),
            start_page: Some(start_page),
            end_page: Some(end_page),
            concurrency: None,
        };
        if let Err(e) = start_unified_crawling(app_handle.clone(), req).await {
            return Err(format!("Unified crawling execution failed: {}", e));
        }
    } else {
        info!("ğŸ›‘ ë¶„ì„ ê²°ê³¼: í¬ë¡¤ë§ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (CrawlingRangeRecommendation::None)");
    }
    
    Ok(CrawlingSession {
        session_id,
        started_at,
        status: "started".to_string(),
    })
}
