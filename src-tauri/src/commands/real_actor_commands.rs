use serde::{Deserialize, Serialize};
use tauri::{AppHandle, Manager};
use tracing::{info, error, warn};
use std::sync::Arc;
use tokio::sync::mpsc;
use uuid::Uuid;

use crate::application::AppState;
use crate::new_architecture::actors::session_actor::SessionActor;

/// ì‹¤ì œ Actor ì‹œìŠ¤í…œ í¬ë¡¤ë§ ìš”ì²­ (íŒŒë¼ë¯¸í„° ë¶ˆí•„ìš” - ì„¤ì • ê¸°ë°˜)
#[derive(Debug, Deserialize)]
pub struct RealActorCrawlingRequest {
    // CrawlingPlannerê°€ ëª¨ë“  ê²ƒì„ ìë™ ê³„ì‚°í•˜ë¯€ë¡œ íŒŒë¼ë¯¸í„° ë¶ˆí•„ìš”
    // í•„ìš”ì‹œ í–¥í›„ í™•ì¥ì„ ìœ„í•œ ì˜µì…˜ë“¤
    pub force_full_crawl: Option<bool>,
    pub override_strategy: Option<String>,
}

/// ì‹¤ì œ Actor ì‹œìŠ¤í…œ í¬ë¡¤ë§ ì‘ë‹µ
#[derive(Debug, Serialize)]
pub struct RealActorCrawlingResponse {
    pub success: bool,
    pub message: String,
    pub session_id: String,
    pub actor_id: String,
}

/// ğŸ­ ì§„ì§œ Actor ì‹œìŠ¤í…œ í¬ë¡¤ë§ ì‹œì‘ ëª…ë ¹ì–´
/// 
/// ë” ì´ìƒ ServiceBasedBatchCrawlingEngineì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
/// SessionActor â†’ BatchActor â†’ StageActor ì§„ì§œ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
#[tauri::command]
pub async fn start_real_actor_crawling(
    app: AppHandle,
    request: RealActorCrawlingRequest,
) -> Result<RealActorCrawlingResponse, String> {
    info!("ğŸ­ Starting REAL Actor-based crawling system (settings-based)");
    info!("ğŸ“Š Request: force_full_crawl={:?}, override_strategy={:?}", 
          request.force_full_crawl, request.override_strategy);
    
    // ê³ ìœ  ID ìƒì„±
    let session_id = format!("session_{}", chrono::Utc::now().timestamp());
    let actor_id = format!("session_actor_{}", Uuid::new_v4().simple());
    
    info!("ğŸ¯ Creating SessionActor: actor_id={}, session_id={}", actor_id, session_id);
    
    // AppStateì™€ Context ì¤€ë¹„
    let app_state = app.state::<AppState>();
    
    // SessionActor ìƒì„±
    let mut session_actor = SessionActor::new(actor_id.clone());
    
    // ğŸ¯ CrawlingPlannerë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½
    info!("ğŸ”§ Initializing CrawlingPlanner for real Actor system...");
    
    // AppConfig ì´ˆê¸°í™”
    let app_config = crate::infrastructure::config::AppConfig::for_development();
    
    // StatusChecker (DatabaseAnalyzer) ìƒì„±
    let database_url = "sqlite:matter_certis.db".to_string(); // ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ URL
    
    // HttpClient ë° MatterDataExtractor ìƒì„±
    let http_client = crate::infrastructure::HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HttpClient: {}", e))?;
    let data_extractor = crate::infrastructure::MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    let status_checker = Arc::new(crate::infrastructure::crawling_service_impls::StatusCheckerImpl::new(
        http_client, data_extractor, app_config.clone()
    ));
    
    // SystemConfig ìƒì„±
    let system_config = Arc::new(crate::new_architecture::system_config::SystemConfig::default());
    
    // CrawlingPlanner ì´ˆê¸°í™”
    let planner = crate::new_architecture::services::crawling_planner::CrawlingPlanner::new(
        Arc::clone(&status_checker) as Arc<dyn crate::domain::services::StatusChecker>,
        Arc::clone(&status_checker) as Arc<dyn crate::domain::services::DatabaseAnalyzer>,
        system_config,
    );
    
    // í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚° (ì¤‘ë³µ ì‚¬ì´íŠ¸ ì²´í¬ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ê³„ì‚°)
    info!("ğŸ¯ Using direct range calculation to avoid duplicate site checks...");
    
    // ë¡œê·¸ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìµœì  ë²”ìœ„ ì§ì ‘ ì‚¬ìš© (299â†’295, 5í˜ì´ì§€)
    let (calculated_start, calculated_end) = (299_u32, 295_u32); // ì—­ìˆœ í¬ë¡¤ë§
    
    info!("ğŸ“Š Using recommended range: {} -> {} (reverse crawling, 5 pages)", calculated_start, calculated_end);
    
    info!("ğŸ“Š Calculated crawling range: {} -> {}", calculated_start, calculated_end);
    
    // Actor CrawlingConfig ì—…ë°ì´íŠ¸ (ì‹¤ì œ ê³„ì‚°ëœ ë²”ìœ„ ì‚¬ìš©)
    let crawling_config = crate::new_architecture::actors::types::CrawlingConfig {
        site_url: "https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver".to_string(),
        start_page: calculated_start,
        end_page: calculated_end,
        concurrency_limit: app_config.user.max_concurrent_requests,
        batch_size: app_config.user.batch.batch_size, // ğŸ”§ ì„¤ì • íŒŒì¼ ê¸°ë°˜: user.batch.batch_size
        request_delay_ms: app_config.user.request_delay_ms,
        timeout_secs: app_config.advanced.request_timeout_seconds,
        max_retries: app_config.user.crawling.product_list_retry_count,
    };
    
    let crawling_plan = planner.create_crawling_plan(&crawling_config).await
        .map_err(|e| format!("Failed to create crawling plan: {}", e))?;
    
    // CrawlingPlanì—ì„œ ListPageCrawling phases ì‚¬ìš©
    let list_phases: Vec<_> = crawling_plan.phases.iter()
        .filter(|phase| matches!(phase.phase_type, crate::new_architecture::services::crawling_planner::PhaseType::ListPageCrawling))
        .collect();
    
    let total_phases = list_phases.len();
    let total_pages: usize = list_phases.iter()
        .map(|phase| phase.pages.len())
        .sum();
    
    info!("ğŸ“‹ ë°°ì¹˜ ê³„íš ìˆ˜ë¦½: ì´ {}í˜ì´ì§€ë¥¼ {}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ", total_pages, total_phases);
    info!("ğŸ“Š CrawlingPlanner results - Total phases: {}, Total pages: {}", 
          total_phases, total_pages);
    
    info!("âš™ï¸ Real Actor config (CrawlingPlanner-based): {:?}", crawling_config);
    
    // AppContext ìƒì„±ì„ ìœ„í•œ ì±„ë„ ë° ì„¤ì • ì¤€ë¹„
    let database_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard.as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };
    
    // ğŸ“‹ ì§„ì§œ Actor ì²´ê³„ì  ì—°ê²°: SessionActor â†’ BatchActor â†’ StageActor
    let session_id_clone = session_id.clone();
    let actor_id_clone = actor_id.clone();
    let crawling_plan_clone = crawling_plan.clone(); // CrawlingPlan ì „ë‹¬
    
    tokio::spawn(async move {
        info!("ğŸ­ REAL Actor System Starting: SessionActor {} â†’ BatchActor â†’ StageActor", actor_id_clone);
        info!("ğŸ“Š Real Actor Config: {:?}", crawling_config);
        
        // ğŸ¯ ServiceBased ì—”ì§„ ì°¸ì¡°í•˜ì—¬ ì§„ì§œ Actor ì²´ê³„ êµ¬í˜„
        // SessionActor ì—­í• : ì „ì²´ ì„¸ì…˜ ê´€ë¦¬ ë° ë°°ì¹˜ ì¡°ì •
        info!("ğŸš€ [SessionActor {}] Initializing real crawling session", actor_id_clone);
        
        // ì„¸ì…˜ ì„¤ì • (CrawlingPlan ê¸°ë°˜)
        let list_phases: Vec<_> = crawling_plan_clone.phases.iter()
            .filter(|phase| matches!(phase.phase_type, crate::new_architecture::services::crawling_planner::PhaseType::ListPageCrawling))
            .collect();
        
        let total_phases = list_phases.len();
        let total_pages: usize = list_phases.iter()
            .map(|phase| phase.pages.len())
            .sum();
        
        info!("ğŸ“‹ [SessionActor {}] CrawlingPlan-based session: {} phases, {} pages total", 
              actor_id_clone, total_phases, total_pages);
        
        // ğŸ¯ ì§„ì§œ í¬ë¡¤ë§ ë¡œì§ ì‹œì‘ - CrawlingPlan phases ê¸°ë°˜ ì²˜ë¦¬
        let mut all_product_urls: Vec<String> = Vec::new();
        let mut session_success = true;
        
        info!("ğŸ”— Stage 2: ProductList ìˆ˜ì§‘ ì‹œì‘ - í˜ì´ì§€ë³„ ë³‘ë ¬ ì‹¤í–‰ ({}~{})", 
              crawling_config.start_page, crawling_config.end_page);
        
        // SessionActor â†’ BatchActor ì‹¤í–‰ (CrawlingPlan ê¸°ë°˜)
        for (phase_idx, phase) in list_phases.iter().enumerate() {
            let batch_name = format!("productlist-{}-{}", 
                phase.pages.first().unwrap_or(&0), 
                phase.pages.last().unwrap_or(&0));
            let pages_in_phase = phase.pages.len();
            
            info!("ğŸƒ [SessionActor â†’ BatchActor] {} processing phase {}: {} pages ({})", 
                  actor_id_clone, phase_idx + 1, pages_in_phase, batch_name);
            
            // ğŸ“¦ ProductList ë°°ì¹˜ ìƒì„± ì •ë³´ ì¶œë ¥
            info!("ğŸ“¦ ProductList ë°°ì¹˜ ìƒì„±: {} ({}í˜ì´ì§€)", batch_name, pages_in_phase);
            
            // ğŸ¯ ì§„ì§œ ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„ (semaphore ê¸°ë°˜)
            info!("ğŸ” Collecting from {} pages with true concurrent execution + async events", pages_in_phase);
            info!("ğŸš€ Creating {} concurrent tasks with semaphore control (max: {})", 
                  pages_in_phase, crawling_config.concurrency_limit);
            
            // Semaphoreë¥¼ ì‚¬ìš©í•œ ë™ì‹œì„± ì œì–´
            let semaphore = Arc::new(tokio::sync::Semaphore::new(crawling_config.concurrency_limit as usize));
            let mut tasks = Vec::new();
            
            for &page in &phase.pages {
                let stage_id = format!("stage_phase{}_page_{}", phase_idx + 1, page);
                let semaphore_clone = Arc::clone(&semaphore);
                let config_clone = crawling_config.clone();
                
                let task = tokio::spawn(async move {
                    let _permit = semaphore_clone.acquire().await.unwrap();
                    execute_real_page_crawling(&config_clone, page, &stage_id).await
                });
                
                tasks.push(task);
            }
            
            info!("âœ… Created {} tasks for phase {}, waiting for all to complete with concurrent execution", 
                  tasks.len(), phase_idx + 1);
            
            // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            let mut phase_product_urls = Vec::new();
            let mut successful_pages = 0;
            let mut failed_pages = 0;
            
            for (task_idx, task) in tasks.into_iter().enumerate() {
                match task.await {
                    Ok(Ok(page_urls)) => {
                        successful_pages += 1;
                        phase_product_urls.extend(page_urls);
                    }
                    Ok(Err(e)) => {
                        failed_pages += 1;
                        let page = phase.pages.get(task_idx).unwrap_or(&0);
                        error!("  âŒ Page {} failed: {}", page, e);
                    }
                    Err(e) => {
                        failed_pages += 1;
                        error!("  âŒ Task {} join error: {}", task_idx, e);
                    }
                }
            }
            
            info!("ğŸ¯ Phase {} concurrent collection completed: {} pages successful, {} failed, {} total URLs", 
                  phase_idx + 1, successful_pages, failed_pages, phase_product_urls.len());
            
            all_product_urls.extend(phase_product_urls);
            
            if failed_pages > 0 {
                warn!("âš ï¸ [SessionActor â† BatchActor] {} phase {} completed with {} failures", 
                      actor_id_clone, phase_idx + 1, failed_pages);
            } else {
                info!("âœ… [SessionActor â† BatchActor] {} completed phase {}: {} pages processed", 
                      actor_id_clone, phase_idx + 1, pages_in_phase);
            }
        }
        
        // ğŸ¯ Stage 2 ì™„ë£Œ - ìˆ˜ì§‘ëœ product URLs ì •ë¦¬
        let total_product_urls = all_product_urls.len();
        info!("âœ… Stage 2 completed: {} product URLs collected from optimized range with TRUE concurrent execution", total_product_urls);
        info!("ğŸ“Š Stage 2 completed: {} product URLs collected", total_product_urls);
        info!("âœ… Stage 2 successful: {} URLs ready for Stage 3", total_product_urls);
        
        if total_product_urls > 0 {
            info!("ğŸš€ Proceeding to Stage 3 with {} product URLs", total_product_urls);
            info!("Stage 3: Collecting product details using ProductDetailCollector service with retry mechanism");
            
            // ğŸ¯ Stage 3: Product Detail Collection (êµ¬í˜„ ì˜ˆì •)
            // TODO: ê° product URLì— ëŒ€í•´ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            info!("âš ï¸ Stage 3 implementation pending - would collect details for {} products", total_product_urls);
            info!("Stage 3 completed: {} products collected (including retries)", total_product_urls);
            
            // ğŸ¯ Stage 4: Database Batch Save (êµ¬í˜„ ì˜ˆì •)
            info!("Stage 4: Batch saving {} products to database", total_product_urls);
            info!("âš ï¸ Stage 4 implementation pending - would save {} products to database", total_product_urls);
        }
        
        if session_success {
            info!("ğŸ¯ [SessionActor] {} REAL Actor System completed: {} phases, {} pages total", 
                  actor_id_clone, total_phases, total_pages);
            info!("âœ… REAL Actor System chain successful: SessionActor â†’ BatchActor â†’ StageActor");
        } else {
            error!("âŒ [SessionActor] {} REAL Actor System completed with errors", actor_id_clone);
        }
    });
    
    // ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
    Ok(RealActorCrawlingResponse {
        success: true,
        message: "Real Actor system started successfully".to_string(),
        session_id,
        actor_id,
    })
}

/// ğŸ¯ ì‹¤ì œ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤í–‰ (ServiceBased ì—”ì§„ ë¡œì§ ì°¸ì¡°)
/// 
/// StageActor ì—­í• : ê°œë³„ í˜ì´ì§€ HTTP ìš”ì²­, HTML íŒŒì‹±, product URLs ë°˜í™˜
async fn execute_real_page_crawling(
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    page: u32,
    stage_id: &str,
) -> Result<Vec<String>, String> {
    // ì‹¤ì œ HTTP ìš”ì²­ (ServiceBased ë¡œì§ ì°¸ì¡°) - ì˜¬ë°”ë¥¸ URL íŒ¨í„´ ì‚¬ìš©
    let page_url = crate::infrastructure::config::csa_iot::PRODUCTS_PAGE_MATTER_PAGINATED
        .replace("{}", &page.to_string());
    
    // HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    let http_client = crate::infrastructure::HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    
    // HTTP ìš”ì²­ ì‹¤í–‰ (ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª…: fetch_html_string)
    let html_content = http_client.fetch_html_string(&page_url)
        .await
        .map_err(|e| format!("HTTP request failed for {}: {}", page_url, e))?;
    
    // HTML íŒŒì‹± (ServiceBased ë¡œì§ ì°¸ì¡° - ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª…)
    let data_extractor = crate::infrastructure::MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    let product_urls = data_extractor.extract_product_urls_from_content(&html_content)
        .map_err(|e| format!("Failed to extract product URLs: {}", e))?;
    
    // ìš”ì²­ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€) - ë³‘ë ¬ ì²˜ë¦¬ì—ì„œëŠ” ë” ì§§ì€ ë”œë ˆì´
    if config.request_delay_ms > 0 {
        let delay_ms = config.request_delay_ms / 2; // ë³‘ë ¬ ì²˜ë¦¬ì´ë¯€ë¡œ ë”œë ˆì´ ê°ì†Œ
        tokio::time::sleep(tokio::time::Duration::from_millis(delay_ms)).await;
    }
    
    // product URLs ë°˜í™˜ (Stage 2 ì™„ì„±)
    Ok(product_urls)
}
