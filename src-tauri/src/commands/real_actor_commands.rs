//! ğŸ­ Real Actor System Commands
//! 
//! SessionActor â†’ BatchActor â†’ StageActor ê³„ì¸µ êµ¬ì¡°ë¡œ ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
//! - SessionActor: CrawlingPlanner ê¸°ë°˜ ì „ì²´ ì„¸ì…˜ ê´€ë¦¬
//! - BatchActor: ë³‘ë ¬ ë°°ì¹˜ ì‹¤í–‰ (sequentialì´ ì•„ë‹Œ parallel)
//! - StageActor: Stage 2,3,4ë¥¼ ìˆœì°¨ ì‹¤í–‰ (SessionActorê°€ ì•„ë‹Œ BatchActorê°€ ê´€ë¦¬)
//! - Frontend Events: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ë¸Œë¡œë“œìºìŠ¤íŠ¸

use std::sync::Arc;
use std::time::Instant;
use serde::{Deserialize, Serialize};
use tauri::{command, AppHandle, Manager};
use tracing::{info, warn, error};
use tokio::sync::broadcast;
use uuid::Uuid;
use chrono::Utc;
use futures;

// ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
use crate::new_architecture::actors::{ActorCommand, CrawlingConfig, ActorError};
use crate::infrastructure::{ServiceBasedBatchCrawlingEngine, HttpClient, MatterDataExtractor};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::integrated_product_repository::IntegratedProductRepository;
use crate::infrastructure::database_paths::get_main_database_url;
use crate::domain::product::{Product, ProductDetail};
use crate::application::AppState;

/// ğŸ­ ì‹¤ì œ Actor í¬ë¡¤ë§ ìš”ì²­
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RealActorCrawlingRequest {
    /// ì „ì²´ í¬ë¡¤ë§ ê°•ì œ ì‹¤í–‰
    pub force_full_crawl: Option<bool>,
    /// ì „ëµ ì˜¤ë²„ë¼ì´ë“œ
    pub override_strategy: Option<String>,
}

/// ğŸ¯ Real Actor í¬ë¡¤ë§ ì‹œì‘ (ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸)
#[command]
pub async fn start_real_actor_crawling(
    app: AppHandle,
    request: RealActorCrawlingRequest
) -> Result<String, String> {
    info!("ğŸ­ Starting Real Actor System Crawling");
    info!("ğŸ“‹ Request: force_full_crawl={:?}, override_strategy={:?}", 
          request.force_full_crawl, request.override_strategy);

    let start_time = Instant::now();
    let session_id = Uuid::new_v4().to_string();

    // AppState ê°€ì ¸ì˜¤ê¸°
    let app_state = app.state::<AppState>();
    let app_config = app_state.config.read().await;
    
    // ğŸ”§ ì„¤ì • ê¸°ë°˜ concurrency (í•˜ë“œì½”ë”© 3 ëŒ€ì‹ )
    let max_concurrency = app_config.user.max_concurrent_requests;
    info!("âš™ï¸ Using concurrency from config: {}", max_concurrency);

    // ğŸŒ ê³µìœ  ìì› ìƒì„±
    let http_client = Arc::new(HttpClient::create_from_global_config().map_err(|e| e.to_string())?);
    let data_extractor = Arc::new(MatterDataExtractor::new().map_err(|e| e.to_string())?);

    // ğŸ“¡ Frontend ì´ë²¤íŠ¸ ì±„ë„ ìƒì„±
    let (event_tx, _event_rx) = broadcast::channel::<FrontendEvent>(500);
    
    // ğŸ“Š CrawlingPlannerë¡œ ê³„íš ìƒì„±
    info!("ğŸ”§ Creating basic crawling plan (simplified for Actor testing)");
    
    // ê°„ë‹¨í•œ ë°°ì¹˜ ìƒì„± (CrawlingPlanner ëŒ€ì‹ )
    let batch_configs = vec![
        crate::new_architecture::actors::types::BatchConfig {
            batch_size: 5,
            concurrency_limit: max_concurrency,
            batch_delay_ms: 100,
            retry_on_failure: true,
            start_page: Some(1),
            end_page: Some(5),
        }
    ];

    info!("ğŸ“‹ Created {} batches for testing", batch_configs.len());

    // ğŸ“¢ Session ì‹œì‘ ì´ë²¤íŠ¸ ë°œì†¡
    let _ = event_tx.send(FrontendEvent::SessionStarted {
        session_id: session_id.clone(),
        total_batches: batch_configs.len() as u32,
        timestamp: Utc::now(),
    });

    // ğŸ­ SessionActor ì‹¤í–‰ (ë³‘ë ¬ BatchActor ìŠ¤í°)
    match execute_session_with_parallel_batches(
        session_id.clone(),
        batch_configs,
        max_concurrency,
        app_config.advanced.request_timeout_seconds * 1000,
        http_client,
        data_extractor,
        event_tx.clone(),
        app_config.clone()
    ).await {
        Ok(_) => {
            let duration = start_time.elapsed();
            info!("âœ… Real Actor Crawling completed in {:?}", duration);
            
            // ğŸ“¢ Session ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
            let _ = event_tx.send(FrontendEvent::SessionCompleted {
                session_id: session_id.clone(),
                duration_ms: duration.as_millis() as u64,
                timestamp: Utc::now(),
            });

            Ok(format!("Real Actor crawling completed successfully in {:?}", duration))
        }
        Err(e) => {
            error!("âŒ Real Actor Crawling failed: {:?}", e);
            Err(format!("Actor crawling failed: {:?}", e))
        }
    }
}

/// ğŸ”¥ í•µì‹¬: ìˆœì°¨ Batch ì‹¤í–‰ (SessionActor ì—­í• )
/// 
/// âš ï¸ ì¤‘ìš”: ë°°ì¹˜ëŠ” ë³‘ë ¬ì´ ì•„ë‹Œ ìˆœì°¨ ì‹¤í–‰!
/// - ì´ìœ : ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜ ë° DB ì €ì¥ ì‹¤íŒ¨ ë°©ì§€
/// - Batch1 ì™„ë£Œ â†’ Batch2 ì‹œì‘ â†’ Batch3 ì‹œì‘
/// - ê° ë°°ì¹˜ ë‚´ë¶€ì—ì„œë§Œ HTTP ìš”ì²­ì„ concurrentí•˜ê²Œ ì²˜ë¦¬
async fn execute_session_with_parallel_batches(
    session_id: String,
    batches: Vec<crate::new_architecture::actors::types::BatchConfig>,
    max_concurrency: u32,
    timeout_ms: u64,
    http_client: Arc<HttpClient>,
    data_extractor: Arc<MatterDataExtractor>,
    event_tx: broadcast::Sender<FrontendEvent>,
    app_config: AppConfig
) -> Result<(), ActorError> {
    info!("ğŸ­ SessionActor executing {} batches SEQUENTIALLY (not parallel)", batches.len());

    // ğŸ”„ ë°°ì¹˜ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (ë³‘ë ¬ ì•„ë‹˜!)
    for (batch_index, batch) in batches.iter().enumerate() {
        let batch_id = format!("{}_batch_{}", session_id, batch_index);
        let batch_config = batch.clone();
        let concurrency = max_concurrency;
        let timeout = timeout_ms;

        info!("ğŸ“¦ Starting Batch {}/{}: {}", batch_index + 1, batches.len(), batch_id);

        // ğŸ“¢ Batch ì‹œì‘ ì´ë²¤íŠ¸
        let _ = event_tx.send(FrontendEvent::BatchStarted {
            session_id: session_id.clone(),
            batch_id: batch_id.clone(),
            batch_index: batch_index as u32,
            timestamp: Utc::now(),
        });

        // ğŸ¯ BatchActor ìˆœì°¨ ì‹¤í–‰ (í•œ ë²ˆì— í•˜ë‚˜ì”©)
        let result = execute_batch_actor_complete_pipeline_simulation(
            batch_id.clone(),
            batch_config,
            concurrency,
            timeout,
        ).await;

        // ğŸ“¢ Batch ì™„ë£Œ ì´ë²¤íŠ¸
        let batch_success = result.is_ok();
        let _ = event_tx.send(FrontendEvent::BatchCompleted {
            session_id: session_id.clone(),
            batch_id: batch_id.clone(),
            batch_index: batch_index as u32,
            success: batch_success,
            timestamp: Utc::now(),
        });

        // ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì„¸ì…˜ ì¤‘ë‹¨
        if let Err(e) = result {
            error!("âŒ Batch {} failed, stopping session: {:?}", batch_id, e);
            return Err(e);
        }

        info!("âœ… Batch {} completed successfully", batch_id);
        
        // ë°°ì¹˜ ê°„ ê°„ê²© (ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ìœ„í•´)
        if batch_index < batches.len() - 1 {
            info!("â³ Waiting between batches for system stability...");
            tokio::time::sleep(tokio::time::Duration::from_millis(500)).await;
        }
    }

    info!("âœ… All {} batches completed sequentially", batches.len());
    Ok(())
}

/// ğŸ¯ BatchActor ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤ì œ êµ¬í˜„ (Stage 2â†’3â†’4)
async fn execute_batch_actor_complete_pipeline_simulation(
    batch_id: String,
    batch_config: crate::new_architecture::actors::types::BatchConfig,
    concurrency: u32,
    timeout_ms: u64,
) -> Result<(), ActorError> {
    info!("ğŸ¯ BatchActor {} executing complete Stage 2-3-4 pipeline (REAL IMPLEMENTATION)", batch_id);

    // Stage 2: List Page Collection (ì‹¤ì œ êµ¬í˜„)
    info!("ğŸ” Stage 2: List Page Collection (concurrency: {})", concurrency);
    let stage2_urls = execute_real_stage_2_list_collection(&batch_id, &batch_config, concurrency).await?;
    
    info!("âœ… Stage 2 completed: {} URLs collected ", stage2_urls.len());

    // Stage 3: Product Detail Collection (ì‹¤ì œ êµ¬í˜„)
    info!("ğŸ” Stage 3: Product Detail Collection ");
    let stage3_items = execute_real_stage_3_detail_collection(&batch_id, stage2_urls, concurrency).await?;

    info!("âœ… Stage 3 completed: {} items processed ", stage3_items.len());

    // Stage 4: Database Storage (ì‹¤ì œ êµ¬í˜„)
    info!("ğŸ’¾ Stage 4: Database Storage ");
    execute_real_stage_4_storage(&batch_id, stage3_items).await?;

    info!("âœ… Stage 4 completed: Database storage finished ");
    info!("ğŸ¯ BatchActor {} pipeline completed successfully ", batch_id);

    Ok(())
}

/// Stage 2: ì‹¤ì œ List Page Collection êµ¬í˜„
async fn execute_real_stage_2_list_collection(
    batch_id: &str,
    batch_config: &crate::new_architecture::actors::types::BatchConfig,
    concurrency: u32,
) -> Result<Vec<String>, ActorError> {
    info!("ğŸ” BatchActor {} executing REAL Stage 2", batch_id);

    // ì‹¤ì œ HttpClientì™€ MatterDataExtractor ìƒì„±
    let http_client = Arc::new(crate::infrastructure::simple_http_client::HttpClient::create_from_global_config()?);
    let data_extractor = Arc::new(crate::infrastructure::html_parser::MatterDataExtractor::new()?);
    
    // ì‹¤ì œ í˜ì´ì§€ ë²”ìœ„ (ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
    let start_page = batch_config.start_page.unwrap_or(291);
    let end_page = batch_config.end_page.unwrap_or(287);
    
    info!("ğŸ“„ Collecting pages {} to {} with concurrency {}", start_page, end_page, concurrency);
    
    // ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´
    let semaphore = Arc::new(tokio::sync::Semaphore::new(concurrency as usize));
    let mut tasks = Vec::new();
    
    // í˜ì´ì§€ ë²”ìœ„ ìƒì„± (ì—­ìˆœ - ìµœì‹ ë¶€í„°)
    let pages: Vec<u32> = if start_page > end_page {
        (end_page..=start_page).rev().collect()
    } else {
        (start_page..=end_page).collect()
    };
    
    for page in pages {
        let http_client_clone = Arc::clone(&http_client);
        let data_extractor_clone = Arc::clone(&data_extractor);
        let semaphore_clone = Arc::clone(&semaphore);
        
        let task = tokio::spawn(async move {
            let _permit = semaphore_clone.acquire().await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("Semaphore acquire failed: {}", e))
            })?;
            
            let url = format!("https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver", page);
            info!("ğŸŒ HTTP GET: {}", url);
            
            let response = http_client_clone.fetch_response(&url).await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("HTTP request failed: {}", e))
            })?;
            
            let html_string: String = response.text().await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("Response text failed: {}", e))
            })?;
            
            let doc = scraper::Html::parse_document(&html_string);
            let product_urls = data_extractor_clone.extract_product_urls(&doc, "https://csa-iot.org").map_err(|e| {
                ActorError::CommandProcessingFailed(format!("URL extraction failed: {}", e))
            })?;
            
            info!("ğŸ“„ Page {} completed with {} URLs", page, product_urls.len());
            Ok::<Vec<String>, ActorError>(product_urls)
        });
        
        tasks.push(task);
    }
    
    info!("âœ… Created {} tasks, waiting for all to complete with concurrent execution", tasks.len());
    
    // ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
    let results = futures::future::join_all(tasks).await;
    
    let mut all_urls = Vec::new();
    for result in results {
        match result {
            Ok(Ok(mut urls)) => {
                all_urls.append(&mut urls);
            },
            Ok(Err(e)) => {
                error!("âŒ Page processing failed: {:?}", e);
                return Err(e);
            },
            Err(e) => {
                error!("âŒ Task join failed: {:?}", e);
                return Err(ActorError::CommandProcessingFailed(format!("Task join failed: {}", e)));
            }
        }
    }

    Ok(all_urls)
}

/// Stage 3: ì‹¤ì œ Product Detail Collection êµ¬í˜„
async fn execute_real_stage_3_detail_collection(
    batch_id: &str,
    stage2_urls: Vec<String>,
    concurrency: u32,
) -> Result<Vec<serde_json::Value>, ActorError> {
    info!("ğŸ” BatchActor {} executing REAL Stage 3 detail collection", batch_id);

    if stage2_urls.is_empty() {
        warn!("âš ï¸ No URLs from Stage 2 - skipping Stage 3");
        return Ok(Vec::new());
    }

    // ì‹¤ì œ HttpClientì™€ MatterDataExtractor ìƒì„±
    let http_client = Arc::new(crate::infrastructure::simple_http_client::HttpClient::create_from_global_config()?);
    let data_extractor = Arc::new(crate::infrastructure::html_parser::MatterDataExtractor::new()?);
    
    // ì„¤ì •ì—ì„œ rate limit ê°€ì ¸ì˜¤ê¸° (í•˜ë“œì½”ë”© ì œê±°)
    // HttpClientëŠ” ì´ë¯¸ create_from_global_config()ì—ì„œ ì˜¬ë°”ë¥¸ rate limitìœ¼ë¡œ ì´ˆê¸°í™”ë¨
    info!("ğŸ”„ Using configured rate limit from global config (Stage 3: Product Details)");
    
    // ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´
    let semaphore = Arc::new(tokio::sync::Semaphore::new(concurrency as usize));
    let mut tasks = Vec::new();
    
    for (index, url) in stage2_urls.iter().enumerate() {
        let http_client_clone = Arc::clone(&http_client);
        let data_extractor_clone = Arc::clone(&data_extractor);
        let semaphore_clone = Arc::clone(&semaphore);
        let url_clone = url.clone();
        
        let task = tokio::spawn(async move {
            let _permit = semaphore_clone.acquire().await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("Semaphore acquire failed: {}", e))
            })?;
            
            let task_id = format!("product-{}", url_clone);
            info!("ï¿½ Product task started: {} ({})", url_clone, task_id);
            
            let start_time = std::time::Instant::now();
            
            info!("ğŸŒ HTTP GET (HttpClient): {}", url_clone);
            let response = http_client_clone.fetch_response(&url_clone).await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("HTTP request failed for {}: {}", url_clone, e))
            })?;
            
            let html_string: String = response.text().await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("Response text failed for {}: {}", url_clone, e))
            })?;
            
            let product_data_json = data_extractor_clone.extract_product_data(&html_string).map_err(|e| {
                ActorError::CommandProcessingFailed(format!("Product data extraction failed for {}: {}", url_clone, e))
            })?;
            
            let elapsed = start_time.elapsed();
            let field_count = count_extracted_json_fields(&product_data_json);
            
            info!("âœ… Product task completed: {} ({}) - {} fields extracted in {:.3}ms", 
                url_clone, task_id, field_count, elapsed.as_millis());
            
            Ok::<serde_json::Value, ActorError>(product_data_json)
        });
        
        tasks.push(task);
    }
    
    info!("âœ… Created {} product detail tasks, executing with rate limit", tasks.len());
    
    // ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
    let results = futures::future::join_all(tasks).await;
    
    let mut all_products = Vec::new();
    for (index, result) in results.into_iter().enumerate() {
        match result {
            Ok(Ok(product_data_json)) => {
                all_products.push(product_data_json);
            },
            Ok(Err(e)) => {
                error!("âŒ Product detail processing failed for item {}: {:?}", index, e);
                // ê°œë³„ ì‹¤íŒ¨ëŠ” ì „ì²´ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰
            },
            Err(e) => {
                error!("âŒ Product detail task join failed for item {}: {:?}", index, e);
            }
        }
    }

    Ok(all_products)
}

/// ì¶”ì¶œëœ í•„ë“œ ìˆ˜ ê³„ì‚° (JSONìš©)
fn count_extracted_json_fields(product_json: &serde_json::Value) -> u32 {
    if let Some(obj) = product_json.as_object() {
        obj.len() as u32
    } else {
        0
    }
}

/// Stage 4: ì‹¤ì œ Database Storage êµ¬í˜„
async fn execute_real_stage_4_storage(
    batch_id: &str,
    stage3_items: Vec<serde_json::Value>,
) -> Result<(), ActorError> {
    info!("ğŸ’¾ BatchActor {} executing REAL Stage 4 Database Storage", batch_id);

    if stage3_items.is_empty() {
        warn!("âš ï¸ No items from Stage 3 - skipping Stage 4");
        return Ok(());
    }

    // ğŸ—ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„± (ì¤‘ì•™ì§‘ì¤‘ì‹ ê²½ë¡œ ê´€ë¦¬ ì‚¬ìš©)
    let database_url = get_main_database_url();
    
    let pool = sqlx::SqlitePool::connect(&database_url)
        .await
        .map_err(|e| ActorError::DatabaseError(format!("Database connection failed: {}", e)))?;
    
    let repository = IntegratedProductRepository::new(pool);

    info!("ğŸ“Š Processing {} items for database storage", stage3_items.len());

    for (index, product_json) in stage3_items.iter().enumerate() {
        let url = product_json.get("source_url")
            .and_then(|v| v.as_str())
            .unwrap_or("unknown");
            
        info!("  ğŸ’¾ Processing item {}/{}: {}", 
            index + 1, 
            stage3_items.len(), 
            url
        );

        // ğŸ”„ JSONì„ ProductDetailë¡œ ë³€í™˜
        if let Ok(product_detail) = convert_json_to_product_detail(&product_json) {
            // ğŸª ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            match repository.create_or_update_product_detail(&product_detail).await {
                Ok((was_updated, was_created)) => {
                    if was_created {
                        info!("  âœ… Created new product detail: {}", url);
                    } else if was_updated {
                        info!("  ğŸ”„ Updated existing product detail: {}", url);
                    } else {
                        info!("  â„¹ï¸ No changes needed for: {}", url);
                    }
                }
                Err(e) => {
                    error!("  âŒ Failed to save product detail {}: {}", url, e);
                    // ê°œë³„ ì €ì¥ ì‹¤íŒ¨ëŠ” ê³„ì† ì§„í–‰ (ì „ì²´ ë°°ì¹˜ ì‹¤íŒ¨ ë°©ì§€)
                }
            }
        } else {
            warn!("  âš ï¸ Failed to convert JSON to ProductDetail: {}", url);
        }
        
        // ì €ì¥ ê°„ê²© (DB ë¶€í•˜ ë°©ì§€)
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    }

    info!("âœ… Stage 4 completed: {} items processed for database storage", stage3_items.len());
    Ok(())
}

/// ğŸ”„ JSONì„ ProductDetailë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
fn convert_json_to_product_detail(json: &serde_json::Value) -> Result<ProductDetail, serde_json::Error> {
    // JSON êµ¬ì¡°ë¥¼ ProductDetailë¡œ ë§¤í•‘
    let url = json.get("source_url")
        .and_then(|v| v.as_str())
        .unwrap_or("")
        .to_string();
    
    let device_type = json.get("device_type")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());
    
    let certification_date = json.get("certification_date")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());
    
    let software_version = json.get("software_version")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());
    
    let hardware_version = json.get("hardware_version")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());
    
    let description = json.get("description")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let manufacturer = json.get("manufacturer")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let model = json.get("model")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let certification_id = json.get("certification_id")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    Ok(ProductDetail {
        url,
        page_id: None,
        index_in_page: None,
        id: None,
        manufacturer,
        model,
        device_type,
        certification_id,
        certification_date,
        software_version,
        hardware_version,
        vid: None,
        pid: None,
        family_sku: None,
        family_variant_sku: None,
        firmware_version: None,
        family_id: None,
        tis_trp_tested: None,
        specification_version: None,
        transport_interface: None,
        primary_device_type_id: None,
        application_categories: None,
        description,
        compliance_document_url: None,
        program_type: None,
        created_at: chrono::Utc::now(),
        updated_at: chrono::Utc::now(),
    })
}/// ğŸ“¡ Frontend ì´ë²¤íŠ¸ íƒ€ì…ë“¤
#[derive(Debug, Clone, Serialize)]
#[serde(tag = "type")]
pub enum FrontendEvent {
    SessionStarted {
        session_id: String,
        total_batches: u32,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
    BatchStarted {
        session_id: String,
        batch_id: String,
        batch_index: u32,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
    BatchCompleted {
        session_id: String,
        batch_id: String,
        batch_index: u32,
        success: bool,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
    SessionCompleted {
        session_id: String,
        duration_ms: u64,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
}
