//! ğŸ­ Real Actor System Commands
//!
//! SessionActor â†’ BatchActor â†’ StageActor ê³„ì¸µ êµ¬ì¡°ë¡œ ë³‘ë ¬ í¬ë¡¤ë§ ì‹¤í–‰
//! - SessionActor: CrawlingPlanner ê¸°ë°˜ ì „ì²´ ì„¸ì…˜ ê´€ë¦¬
//! - BatchActor: ë³‘ë ¬ ë°°ì¹˜ ì‹¤í–‰ (sequentialì´ ì•„ë‹Œ parallel)
//! - StageActor: Stage 2,3,4ë¥¼ ìˆœì°¨ ì‹¤í–‰ (SessionActorê°€ ì•„ë‹Œ BatchActorê°€ ê´€ë¦¬)
//! - Frontend Events: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ë¸Œë¡œë“œìºìŠ¤íŠ¸

use chrono::Utc;
use futures;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use std::time::Instant;
use tauri::{AppHandle, Manager, command};
use tokio::sync::broadcast;
use tracing::{error, info, warn};
use uuid::Uuid;

// ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
use crate::infrastructure::{HttpClient, MatterDataExtractor};
use crate::new_architecture::actors::ActorError;
// use crate::infrastructure::service_based_crawling_engine::ServiceBasedBatchCrawlingEngine; // Deprecated
use crate::application::AppState;
use crate::domain::product::ProductDetail;
use crate::domain::services::StatusChecker;
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::integrated_product_repository::IntegratedProductRepository; // StatusChecker trait ì„í¬íŠ¸

/// ğŸ­ ì‹¤ì œ Actor í¬ë¡¤ë§ ìš”ì²­
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RealActorCrawlingRequest {
    /// ì „ì²´ í¬ë¡¤ë§ ê°•ì œ ì‹¤í–‰
    pub force_full_crawl: Option<bool>,
    /// ì „ëµ ì˜¤ë²„ë¼ì´ë“œ
    pub override_strategy: Option<String>,
}

/// ğŸ¯ Legacy Service-Based í¬ë¡¤ë§ ì‹œì‘ (ì°¸ê³ ìš© ë³´ì¡´)
///
/// ì´ ì»¤ë§¨ë“œëŠ” ìˆœìˆ˜ ServiceBasedBatchCrawlingEngineë§Œ ì‚¬ìš©í•˜ëŠ” ë ˆê±°ì‹œ êµ¬í˜„ì…ë‹ˆë‹¤.
/// NOTE: Deprecated â€“ use unified Analysis-Plan-Execute (ExecutionPlan + SessionActor) instead.
#[command]
pub async fn start_legacy_service_based_crawling(
    app: AppHandle,
    request: RealActorCrawlingRequest,
) -> Result<String, String> {
    info!("ğŸ­ Starting Real Actor System Crawling");
    info!(
        "ğŸ“‹ Request: force_full_crawl={:?}, override_strategy={:?}",
        request.force_full_crawl, request.override_strategy
    );

    let start_time = Instant::now();
    let session_id = Uuid::new_v4().to_string();

    // AppState ê°€ì ¸ì˜¤ê¸°
    let app_state = app.state::<AppState>();
    let app_config = app_state.config.read().await;

    // ğŸ”§ ì„¤ì • ê¸°ë°˜ concurrency (í•˜ë“œì½”ë”© 3 ëŒ€ì‹ )
    let max_concurrency = app_config.user.max_concurrent_requests;
    info!("âš™ï¸ Using concurrency from config: {}", max_concurrency);

    // ğŸŒ ê³µìœ  ìì› ìƒì„±
    let http_client = Arc::new(HttpClient::create_from_global_config().map_err(|e| e.to_string())?);
    let data_extractor = Arc::new(MatterDataExtractor::new().map_err(|e| e.to_string())?);

    // ğŸ“¡ Frontend ì´ë²¤íŠ¸ ì±„ë„ ìƒì„±
    let (event_tx, _event_rx) = broadcast::channel::<FrontendEvent>(500);

    // ğŸ“Š ì‹¤ì œ í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚°
    info!("ğŸ”§ Creating intelligent crawling plan using CrawlingPlanner");

    // ğŸ¯ ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ë° ë¶„ì„
    let _http_client_for_engine = HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    let _data_extractor_for_engine = MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    let _product_repo_for_engine = Arc::new(IntegratedProductRepository::new(
        app_state
            .database_pool
            .read()
            .await
            .as_ref()
            .ok_or("Database pool not initialized")?
            .clone(),
    ));

    // LEGACY ENGINE DISABLED: ServiceBasedBatchCrawlingEngine removed
    // let _advanced_engine = ServiceBasedBatchCrawlingEngine::new(...);

    // ğŸ¯ ì‹¤ì œ í¬ë¡¤ë§ í”Œë˜ë„ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ë²”ìœ„ ê³„ì‚°
    let db_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard
            .as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };

    // StatusChecker ìƒì„±
    let status_checker_impl = crate::infrastructure::crawling_service_impls::StatusCheckerImpl::new(
        (*http_client).clone(),
        (*data_extractor).clone(),
        app_config.clone(),
    );
    let status_checker = Arc::new(status_checker_impl);

    // ğŸ” ì‚¬ì´íŠ¸ ë¶„ì„ ìˆ˜í–‰
    let site_status = status_checker
        .check_site_status()
        .await
        .map_err(|e| format!("Failed to check site status: {}", e))?;

    info!(
        "ğŸ“Š Site analysis: {} pages, {} products",
        site_status.total_pages, site_status.estimated_products
    );

    // DatabaseAnalyzer ìƒì„±
    let product_repo = Arc::new(IntegratedProductRepository::new(db_pool));
    let db_analyzer = Arc::new(
        crate::infrastructure::crawling_service_impls::DatabaseAnalyzerImpl::new(
            product_repo.clone(),
        ),
    );

    let crawling_planner =
        crate::new_architecture::services::crawling_planner::CrawlingPlanner::new(
            status_checker.clone(),
            db_analyzer.clone(),
            Arc::new(crate::new_architecture::context::SystemConfig::default()),
        );

    // ğŸ” ë‹¤ìŒ í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚° (ì‹¤ì œ ì‚¬ì´íŠ¸ ì •ë³´ ì‚¬ìš©)
    let (site_status, db_analysis) = crawling_planner
        .analyze_system_state()
        .await
        .map_err(|e| format!("Failed to analyze system state: {}", e))?;

    let (range_recommendation, processing_strategy) = crawling_planner
        .determine_crawling_strategy(&site_status, &db_analysis)
        .map_err(|e| format!("Failed to determine crawling strategy: {}", e))?;

    info!(
        "ğŸ“Š Calculated crawling strategy: {:?}",
        range_recommendation
    );
    info!(
        "âš™ï¸ Processing strategy: batch_size={}, concurrency={}",
        processing_strategy.recommended_batch_size, processing_strategy.recommended_concurrency
    );

    // ï¿½ CrawlingRangeRecommendationì„ ì‹¤ì œ í˜ì´ì§€ ë²”ìœ„ë¡œ ë³€í™˜
    let (start_page, end_page) = match range_recommendation.to_page_range(site_status.total_pages) {
        Some((s, e)) => {
            // ì—­ìˆœ í¬ë¡¤ë§ìœ¼ë¡œ ë³€í™˜ (ìµœì‹  í˜ì´ì§€ë¶€í„°)
            if s > e { (s, e) } else { (e, s) }
        }
        None => {
            // í¬ë¡¤ë§ì´ í•„ìš” ì—†ëŠ” ê²½ìš° ìµœì‹  5í˜ì´ì§€ë§Œ í™•ì¸
            let verification_pages = 5;
            let start = site_status.total_pages;
            let end = if start >= verification_pages {
                start - verification_pages + 1
            } else {
                1
            };
            (start, end)
        }
    };

    info!("ğŸ“Š Final page range: {} to {}", start_page, end_page);

    // ï¿½ğŸ—ï¸ ê³„ì‚°ëœ ë²”ìœ„ë¡œ ë°°ì¹˜ ìƒì„±
    let batch_configs = vec![crate::new_architecture::actors::types::BatchConfig {
        batch_size: (start_page - end_page + 1).min(5), // ìµœëŒ€ 5í˜ì´ì§€ì”©
        concurrency_limit: max_concurrency,
        batch_delay_ms: 100,
        retry_on_failure: true,
        start_page: Some(start_page),
        end_page: Some(end_page),
    }];

    info!("ğŸ“‹ Created {} batches for testing", batch_configs.len());

    // ğŸ“¢ Session ì‹œì‘ ì´ë²¤íŠ¸ ë°œì†¡
    let _ = event_tx.send(FrontendEvent::SessionStarted {
        session_id: session_id.clone(),
        total_batches: batch_configs.len() as u32,
        timestamp: Utc::now(),
    });

    // ğŸ­ SessionActor ì‹¤í–‰ (ë³‘ë ¬ BatchActor ìŠ¤í°)
    match execute_session_with_parallel_batches(
        session_id.clone(),
        batch_configs,
        max_concurrency,
        app_config.advanced.request_timeout_seconds * 1000,
        http_client,
        data_extractor,
        event_tx.clone(),
        app_config.clone(),
    )
    .await
    {
        Ok(_) => {
            let duration = start_time.elapsed();
            info!("âœ… Real Actor Crawling completed in {:?}", duration);

            // ğŸ“¢ Session ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
            let _ = event_tx.send(FrontendEvent::SessionCompleted {
                session_id: session_id.clone(),
                duration_ms: duration.as_millis() as u64,
                timestamp: Utc::now(),
            });

            Ok(format!(
                "Real Actor crawling completed successfully in {:?}",
                duration
            ))
        }
        Err(e) => {
            error!("âŒ Real Actor Crawling failed: {:?}", e);
            Err(format!("Actor crawling failed: {:?}", e))
        }
    }
}

/// ğŸ”¥ í•µì‹¬: ìˆœì°¨ Batch ì‹¤í–‰ (SessionActor ì—­í• )
///
/// âš ï¸ ì¤‘ìš”: ë°°ì¹˜ëŠ” ë³‘ë ¬ì´ ì•„ë‹Œ ìˆœì°¨ ì‹¤í–‰!
/// - ì´ìœ : ë©”ëª¨ë¦¬ ê³¼ë¶€í•˜ ë° DB ì €ì¥ ì‹¤íŒ¨ ë°©ì§€
/// - Batch1 ì™„ë£Œ â†’ Batch2 ì‹œì‘ â†’ Batch3 ì‹œì‘
/// - ê° ë°°ì¹˜ ë‚´ë¶€ì—ì„œë§Œ HTTP ìš”ì²­ì„ concurrentí•˜ê²Œ ì²˜ë¦¬
async fn execute_session_with_parallel_batches(
    session_id: String,
    batches: Vec<crate::new_architecture::actors::types::BatchConfig>,
    max_concurrency: u32,
    timeout_ms: u64,
    _http_client: Arc<HttpClient>,
    _data_extractor: Arc<MatterDataExtractor>,
    event_tx: broadcast::Sender<FrontendEvent>,
    _app_config: AppConfig,
) -> Result<(), ActorError> {
    info!(
        "ğŸ­ SessionActor executing {} batches SEQUENTIALLY (not parallel)",
        batches.len()
    );

    // ğŸ”„ ë°°ì¹˜ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (ë³‘ë ¬ ì•„ë‹˜!)
    for (batch_index, batch) in batches.iter().enumerate() {
        let batch_id = format!("{}_batch_{}", session_id, batch_index);
        let batch_config = batch.clone();
        let concurrency = max_concurrency;
        let timeout = timeout_ms;

        info!(
            "ğŸ“¦ Starting Batch {}/{}: {}",
            batch_index + 1,
            batches.len(),
            batch_id
        );

        // ğŸ“¢ Batch ì‹œì‘ ì´ë²¤íŠ¸
        let _ = event_tx.send(FrontendEvent::BatchStarted {
            session_id: session_id.clone(),
            batch_id: batch_id.clone(),
            batch_index: batch_index as u32,
            timestamp: Utc::now(),
        });

        // ğŸ¯ BatchActor ìˆœì°¨ ì‹¤í–‰ (í•œ ë²ˆì— í•˜ë‚˜ì”©)
        let result = execute_batch_actor_complete_pipeline_simulation(
            batch_id.clone(),
            batch_config,
            concurrency,
            timeout,
        )
        .await;

        // ğŸ“¢ Batch ì™„ë£Œ ì´ë²¤íŠ¸
        let batch_success = result.is_ok();
        let _ = event_tx.send(FrontendEvent::BatchCompleted {
            session_id: session_id.clone(),
            batch_id: batch_id.clone(),
            batch_index: batch_index as u32,
            success: batch_success,
            timestamp: Utc::now(),
        });

        // ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì„¸ì…˜ ì¤‘ë‹¨
        if let Err(e) = result {
            error!("âŒ Batch {} failed, stopping session: {:?}", batch_id, e);
            return Err(e);
        }

        info!("âœ… Batch {} completed successfully", batch_id);

        // ë°°ì¹˜ ê°„ ê°„ê²© (ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ìœ„í•´)
        if batch_index < batches.len() - 1 {
            info!("â³ Waiting between batches for system stability...");
            tokio::time::sleep(tokio::time::Duration::from_millis(500)).await;
        }
    }

    info!("âœ… All {} batches completed sequentially", batches.len());
    Ok(())
}

/// ğŸ¯ BatchActor ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤ì œ êµ¬í˜„ (Stage 2â†’3â†’4)
#[allow(unused_variables)]
async fn execute_batch_actor_complete_pipeline_simulation(
    batch_id: String,
    batch_config: crate::new_architecture::actors::types::BatchConfig,
    concurrency: u32,
    _timeout_ms: u64,
) -> Result<(), ActorError> {
    info!(
        "ğŸ¯ BatchActor {} executing complete Stage 2-3-4 pipeline (REAL IMPLEMENTATION)",
        batch_id
    );

    // Stage 2: List Page Collection (ì‹¤ì œ êµ¬í˜„)
    info!(
        "ğŸ” Stage 2: List Page Collection (concurrency: {})",
        concurrency
    );
    let stage2_urls =
        execute_real_stage_2_list_collection(&batch_id, &batch_config, concurrency).await?;

    info!(
        "âœ… Stage 2 completed: {} URLs collected ",
        stage2_urls.len()
    );

    // Stage 3: Product Detail Collection (ì‹¤ì œ êµ¬í˜„)
    info!("ğŸ” Stage 3: Product Detail Collection ");
    let stage3_items =
        execute_real_stage_3_detail_collection(&batch_id, stage2_urls, concurrency).await?;

    info!(
        "âœ… Stage 3 completed: {} items processed ",
        stage3_items.len()
    );

    // Stage 4: Database Storage (ì‹¤ì œ êµ¬í˜„)
    info!("ğŸ’¾ Stage 4: Database Storage ");
    execute_real_stage_4_storage(&batch_id, stage3_items).await?;

    info!("âœ… Stage 4 completed: Database storage finished ");
    info!(
        "ğŸ¯ BatchActor {} pipeline completed successfully ",
        batch_id
    );

    Ok(())
}

/// Stage 2: ì‹¤ì œ List Page Collection êµ¬í˜„
async fn execute_real_stage_2_list_collection(
    batch_id: &str,
    batch_config: &crate::new_architecture::actors::types::BatchConfig,
    concurrency: u32,
) -> Result<Vec<String>, ActorError> {
    info!("ğŸ” BatchActor {} executing REAL Stage 2", batch_id);

    // ì‹¤ì œ HttpClientì™€ MatterDataExtractor ìƒì„±
    let http_client = Arc::new(
        crate::infrastructure::simple_http_client::HttpClient::create_from_global_config()?,
    );
    let data_extractor = Arc::new(crate::infrastructure::html_parser::MatterDataExtractor::new()?);

    // ì‹¤ì œ í˜ì´ì§€ ë²”ìœ„ (ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’ ì‚¬ìš©)
    let start_page = batch_config.start_page.unwrap_or(291);
    let end_page = batch_config.end_page.unwrap_or(287);

    info!(
        "ğŸ“„ Collecting pages {} to {} with concurrency {}",
        start_page, end_page, concurrency
    );

    // ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´
    let semaphore = Arc::new(tokio::sync::Semaphore::new(concurrency as usize));
    let mut tasks = Vec::new();

    // í˜ì´ì§€ ë²”ìœ„ ìƒì„± (ì—­ìˆœ - ìµœì‹ ë¶€í„°)
    let pages: Vec<u32> = if start_page > end_page {
        (end_page..=start_page).rev().collect()
    } else {
        (start_page..=end_page).collect()
    };

    for page in pages {
        let http_client_clone = Arc::clone(&http_client);
        let data_extractor_clone = Arc::clone(&data_extractor);
        let semaphore_clone = Arc::clone(&semaphore);

        let task = tokio::spawn(async move {
            let _permit = semaphore_clone.acquire().await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("Semaphore acquire failed: {}", e))
            })?;

            let url = format!(
                "https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver",
                page
            );
            info!("ğŸŒ HTTP GET: {}", url);

            let response = http_client_clone.fetch_response(&url).await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("HTTP request failed: {}", e))
            })?;

            let html_string: String = response.text().await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("Response text failed: {}", e))
            })?;

            let doc = scraper::Html::parse_document(&html_string);
            let product_urls = data_extractor_clone
                .extract_product_urls(&doc, "https://csa-iot.org")
                .map_err(|e| {
                    ActorError::CommandProcessingFailed(format!("URL extraction failed: {}", e))
                })?;

            info!(
                "ğŸ“„ Page {} completed with {} URLs",
                page,
                product_urls.len()
            );
            Ok::<Vec<String>, ActorError>(product_urls)
        });

        tasks.push(task);
    }

    info!(
        "âœ… Created {} tasks, waiting for all to complete with concurrent execution",
        tasks.len()
    );

    // ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
    let results = futures::future::join_all(tasks).await;

    let mut all_urls = Vec::new();
    for result in results {
        match result {
            Ok(Ok(mut urls)) => {
                all_urls.append(&mut urls);
            }
            Ok(Err(e)) => {
                error!("âŒ Page processing failed: {:?}", e);
                return Err(e);
            }
            Err(e) => {
                error!("âŒ Task join failed: {:?}", e);
                return Err(ActorError::CommandProcessingFailed(format!(
                    "Task join failed: {}",
                    e
                )));
            }
        }
    }

    Ok(all_urls)
}

/// Stage 3: ì‹¤ì œ Product Detail Collection êµ¬í˜„
async fn execute_real_stage_3_detail_collection(
    batch_id: &str,
    stage2_urls: Vec<String>,
    concurrency: u32,
) -> Result<Vec<serde_json::Value>, ActorError> {
    info!(
        "ğŸ” BatchActor {} executing REAL Stage 3 detail collection",
        batch_id
    );

    if stage2_urls.is_empty() {
        warn!("âš ï¸ No URLs from Stage 2 - skipping Stage 3");
        return Ok(Vec::new());
    }

    // ì‹¤ì œ HttpClientì™€ MatterDataExtractor ìƒì„±
    let http_client = Arc::new(
        crate::infrastructure::simple_http_client::HttpClient::create_from_global_config()?,
    );
    let data_extractor = Arc::new(crate::infrastructure::html_parser::MatterDataExtractor::new()?);

    // ì„¤ì •ì—ì„œ rate limit ê°€ì ¸ì˜¤ê¸° (í•˜ë“œì½”ë”© ì œê±°)
    // HttpClientëŠ” ì´ë¯¸ create_from_global_config()ì—ì„œ ì˜¬ë°”ë¥¸ rate limitìœ¼ë¡œ ì´ˆê¸°í™”ë¨
    info!("ğŸ”„ Using configured rate limit from global config (Stage 3: Product Details)");

    // ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´
    let semaphore = Arc::new(tokio::sync::Semaphore::new(concurrency as usize));
    let mut tasks = Vec::new();

    for (_index, url) in stage2_urls.iter().enumerate() {
        let http_client_clone = Arc::clone(&http_client);
        let data_extractor_clone = Arc::clone(&data_extractor);
        let semaphore_clone = Arc::clone(&semaphore);
        let url_clone = url.clone();

        let task = tokio::spawn(async move {
            let _permit = semaphore_clone.acquire().await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!("Semaphore acquire failed: {}", e))
            })?;

            let task_id = format!("product-{}", url_clone);
            info!("ï¿½ Product task started: {} ({})", url_clone, task_id);

            let start_time = std::time::Instant::now();

            info!("ğŸŒ HTTP GET (HttpClient): {}", url_clone);
            let response = http_client_clone
                .fetch_response(&url_clone)
                .await
                .map_err(|e| {
                    ActorError::CommandProcessingFailed(format!(
                        "HTTP request failed for {}: {}",
                        url_clone, e
                    ))
                })?;

            let html_string: String = response.text().await.map_err(|e| {
                ActorError::CommandProcessingFailed(format!(
                    "Response text failed for {}: {}",
                    url_clone, e
                ))
            })?;

            let product_data_json = data_extractor_clone
                .extract_product_data(&html_string)
                .map_err(|e| {
                    ActorError::CommandProcessingFailed(format!(
                        "Product data extraction failed for {}: {}",
                        url_clone, e
                    ))
                })?;

            let elapsed = start_time.elapsed();
            let field_count = count_extracted_json_fields(&product_data_json);

            info!(
                "âœ… Product task completed: {} ({}) - {} fields extracted in {:.3}ms",
                url_clone,
                task_id,
                field_count,
                elapsed.as_millis()
            );

            Ok::<serde_json::Value, ActorError>(product_data_json)
        });

        tasks.push(task);
    }

    info!(
        "âœ… Created {} product detail tasks, executing with rate limit",
        tasks.len()
    );

    // ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
    let results = futures::future::join_all(tasks).await;

    let mut all_products = Vec::new();
    for (index, result) in results.into_iter().enumerate() {
        match result {
            Ok(Ok(product_data_json)) => {
                all_products.push(product_data_json);
            }
            Ok(Err(e)) => {
                error!(
                    "âŒ Product detail processing failed for item {}: {:?}",
                    index, e
                );
                // ê°œë³„ ì‹¤íŒ¨ëŠ” ì „ì²´ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰
            }
            Err(e) => {
                error!(
                    "âŒ Product detail task join failed for item {}: {:?}",
                    index, e
                );
            }
        }
    }

    Ok(all_products)
}

/// ì¶”ì¶œëœ í•„ë“œ ìˆ˜ ê³„ì‚° (JSONìš©)
fn count_extracted_json_fields(product_json: &serde_json::Value) -> u32 {
    if let Some(obj) = product_json.as_object() {
        obj.len() as u32
    } else {
        0
    }
}

/// Stage 4: ì‹¤ì œ Database Storage êµ¬í˜„
async fn execute_real_stage_4_storage(
    batch_id: &str,
    stage3_items: Vec<serde_json::Value>,
) -> Result<(), ActorError> {
    info!(
        "ğŸ’¾ BatchActor {} executing REAL Stage 4 Database Storage",
        batch_id
    );

    if stage3_items.is_empty() {
        warn!("âš ï¸ No items from Stage 3 - skipping Stage 4");
        return Ok(());
    }

    // ğŸ—ï¸ ë°ì´í„°ë² ì´ìŠ¤ í’€ ì¬ì‚¬ìš© (ì¤‘ì•™ì§‘ì¤‘ì‹ ê²½ë¡œ ê´€ë¦¬ + ê¸€ë¡œë²Œ í’€)
    let pool = crate::infrastructure::database_connection::get_or_init_global_pool()
        .await
        .map_err(|e| ActorError::DatabaseError(format!("Database pool error: {}", e)))?;

    let repository = IntegratedProductRepository::new(pool);

    info!(
        "ğŸ“Š Processing {} items for database storage",
        stage3_items.len()
    );

    for (index, product_json) in stage3_items.iter().enumerate() {
        let url = product_json
            .get("source_url")
            .and_then(|v| v.as_str())
            .unwrap_or("unknown");

        info!(
            "  ğŸ’¾ Processing item {}/{}: {}",
            index + 1,
            stage3_items.len(),
            url
        );

        // ğŸ”„ JSONì„ ProductDetailë¡œ ë³€í™˜
        if let Ok(product_detail) = convert_json_to_product_detail(&product_json) {
            // ğŸª ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            match repository
                .create_or_update_product_detail(&product_detail)
                .await
            {
                Ok((was_updated, was_created)) => {
                    if was_created {
                        info!("  âœ… Created new product detail: {}", url);
                    } else if was_updated {
                        info!("  ğŸ”„ Updated existing product detail: {}", url);
                    } else {
                        info!("  â„¹ï¸ No changes needed for: {}", url);
                    }
                }
                Err(e) => {
                    error!("  âŒ Failed to save product detail {}: {}", url, e);
                    // ê°œë³„ ì €ì¥ ì‹¤íŒ¨ëŠ” ê³„ì† ì§„í–‰ (ì „ì²´ ë°°ì¹˜ ì‹¤íŒ¨ ë°©ì§€)
                }
            }
        } else {
            warn!("  âš ï¸ Failed to convert JSON to ProductDetail: {}", url);
        }

        // ì €ì¥ ê°„ê²© (DB ë¶€í•˜ ë°©ì§€)
        tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    }

    info!(
        "âœ… Stage 4 completed: {} items processed for database storage",
        stage3_items.len()
    );
    Ok(())
}

/// ğŸ”„ JSONì„ ProductDetailë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
fn convert_json_to_product_detail(
    json: &serde_json::Value,
) -> Result<ProductDetail, serde_json::Error> {
    // JSON êµ¬ì¡°ë¥¼ ProductDetailë¡œ ë§¤í•‘
    let url = json
        .get("source_url")
        .and_then(|v| v.as_str())
        .unwrap_or("")
        .to_string();

    let device_type = json
        .get("certification_id")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let certification_date = json
        .get("certification_date")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let software_version = json
        .get("software_version")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let hardware_version = json
        .get("hardware_version")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let description = json
        .get("description")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let manufacturer = json
        .get("manufacturer")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let model = json
        .get("model")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    let certification_id = json
        .get("certification_id")
        .and_then(|v| v.as_str())
        .map(|s| s.to_string());

    Ok(ProductDetail {
        url,
        page_id: None,
        index_in_page: None,
        id: None,
        manufacturer,
        model,
        device_type,
        certificate_id: certification_id,
        certification_date,
        software_version,
        hardware_version,
        vid: None,
        pid: None,
        family_sku: None,
        family_variant_sku: None,
        firmware_version: None,
        family_id: None,
        tis_trp_tested: None,
        specification_version: None,
        transport_interface: None,
        primary_device_type_id: None,
        application_categories: None,
        description,
        compliance_document_url: None,
        program_type: None,
        created_at: chrono::Utc::now(),
        updated_at: chrono::Utc::now(),
    })
}
/// ğŸ“¡ Frontend ì´ë²¤íŠ¸ íƒ€ì…ë“¤
#[derive(Debug, Clone, Serialize)]
#[serde(tag = "type")]
pub enum FrontendEvent {
    SessionStarted {
        session_id: String,
        total_batches: u32,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
    BatchStarted {
        session_id: String,
        batch_id: String,
        batch_index: u32,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
    BatchCompleted {
        session_id: String,
        batch_id: String,
        batch_index: u32,
        success: bool,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
    SessionCompleted {
        session_id: String,
        duration_ms: u64,
        timestamp: chrono::DateTime<chrono::Utc>,
    },
}
