use serde::{Deserialize, Serialize};
use tauri::{AppHandle, Manager};
use tracing::{info, error, warn};
use std::sync::Arc;
use uuid::Uuid;
use chrono::Utc;

use crate::application::AppState;
use crate::new_architecture::actors::session_actor::SessionActor;
use crate::domain::services::ProductDetailCollector;  // trait ì¶”ê°€
use crate::domain::product_url::ProductUrl;  // ProductUrl import ì¶”ê°€

/// ì‹¤ì œ Actor ì‹œìŠ¤í…œ í¬ë¡¤ë§ ìš”ì²­ (íŒŒë¼ë¯¸í„° ë¶ˆí•„ìš” - ì„¤ì • ê¸°ë°˜)
#[derive(Debug, Deserialize)]
pub struct RealActorCrawlingRequest {
    // CrawlingPlannerê°€ ëª¨ë“  ê²ƒì„ ìë™ ê³„ì‚°í•˜ë¯€ë¡œ íŒŒë¼ë¯¸í„° ë¶ˆí•„ìš”
    // í•„ìš”ì‹œ í–¥í›„ í™•ì¥ì„ ìœ„í•œ ì˜µì…˜ë“¤
    pub force_full_crawl: Option<bool>,
    pub override_strategy: Option<String>,
}

/// ì‹¤ì œ Actor ì‹œìŠ¤í…œ í¬ë¡¤ë§ ì‘ë‹µ
#[derive(Debug, Serialize)]
pub struct RealActorCrawlingResponse {
    pub success: bool,
    pub message: String,
    pub session_id: String,
    pub actor_id: String,
}

/// ğŸ­ ì§„ì§œ Actor ì‹œìŠ¤í…œ í¬ë¡¤ë§ ì‹œì‘ ëª…ë ¹ì–´
/// 
/// ë” ì´ìƒ ServiceBasedBatchCrawlingEngineì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
/// SessionActor â†’ BatchActor â†’ StageActor ì§„ì§œ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
#[tauri::command]
pub async fn start_real_actor_crawling(
    app: AppHandle,
    request: RealActorCrawlingRequest,
) -> Result<RealActorCrawlingResponse, String> {
    info!("ğŸ­ Starting REAL Actor-based crawling system (settings-based)");
    info!("ğŸ“Š Request: force_full_crawl={:?}, override_strategy={:?}", 
          request.force_full_crawl, request.override_strategy);
    
    // ê³ ìœ  ID ìƒì„± (ê°„ì†Œí™”)
    let session_id = format!("session_{}", Utc::now().timestamp());
    let actor_id = format!("actor_{}", Uuid::new_v4().simple().to_string().chars().take(8).collect::<String>());
    
    info!("ğŸ¯ Creating SessionActor: actor_id={}, session_id={}", actor_id, session_id);
    
    // AppStateì™€ Context ì¤€ë¹„
    let app_state = app.state::<AppState>();
    
    // SessionActor ìƒì„±
    let mut session_actor = SessionActor::new(actor_id.clone());
    
    // ğŸ¯ ì„¤ì • ê¸°ë°˜ìœ¼ë¡œ í¬ë¡¤ë§ êµ¬ì„±
    info!("ğŸ”§ Initializing Actor system with direct configuration...");
    
    // AppConfig ì´ˆê¸°í™”
    let app_config = crate::infrastructure::config::AppConfig::for_development();
    
    // í¬ë¡¤ë§ ë²”ìœ„ ì§ì ‘ ì„¤ì • (ì¤‘ë³µ ì‚¬ì´íŠ¸ ì²´í¬ ë°©ì§€)
    info!("ğŸ¯ Using direct range calculation to avoid duplicate site checks...");
    
    // ë¡œê·¸ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìµœì  ë²”ìœ„ ì§ì ‘ ì‚¬ìš© (299â†’295, 5í˜ì´ì§€)
    let (calculated_start, calculated_end) = (299_u32, 295_u32); // ì—­ìˆœ í¬ë¡¤ë§
    
    info!("ğŸ“Š Using recommended range: {} -> {} (reverse crawling, 5 pages)", calculated_start, calculated_end);
    
    // Actor CrawlingConfig ì„¤ì • (ì‹¤ì œ ê³„ì‚°ëœ ë²”ìœ„ ì‚¬ìš©)
    let crawling_config = crate::new_architecture::actors::types::CrawlingConfig {
        site_url: "https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver".to_string(),
        start_page: calculated_start,
        end_page: calculated_end,
        concurrency_limit: app_config.user.max_concurrent_requests,
        batch_size: app_config.user.batch.batch_size, // ğŸ”§ ì„¤ì • íŒŒì¼ ê¸°ë°˜: user.batch.batch_size (ì´ì œ 3ìœ¼ë¡œ ì„¤ì •ë¨)
        request_delay_ms: app_config.user.request_delay_ms,
        timeout_secs: app_config.advanced.request_timeout_seconds,
        max_retries: app_config.user.crawling.product_list_retry_count,
    };
    
    // ì§ì ‘ ë°°ì¹˜ ë¶„í•  ê³„ì‚° (CrawlingPlanner ëŒ€ì‹ )
    let page_range: Vec<u32> = if calculated_start > calculated_end {
        // ì—­ìˆœ í¬ë¡¤ë§
        (calculated_end..=calculated_start).rev().collect()
    } else {
        // ì •ìˆœ í¬ë¡¤ë§
        (calculated_start..=calculated_end).collect()
    };
    
    // batch_sizeì— ë”°ë¥¸ ë°°ì¹˜ ë¶„í• 
    let batch_size = crawling_config.batch_size as usize;
    let batches: Vec<Vec<u32>> = page_range.chunks(batch_size).map(|chunk| chunk.to_vec()).collect();
    
    let total_phases = batches.len();
    let total_pages = page_range.len();
    
    info!("ğŸ“‹ ì§ì ‘ ë°°ì¹˜ ê³„íš ìˆ˜ë¦½: ì´ {}í˜ì´ì§€ë¥¼ {}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  (batch_size={})", 
          total_pages, total_phases, batch_size);
    info!("ğŸ“Š Batch details: {:?}", batches);
    
    info!("âš™ï¸ Real Actor config: {:?}", crawling_config);
    
    // AppContext ìƒì„±ì„ ìœ„í•œ ì±„ë„ ë° ì„¤ì • ì¤€ë¹„
    let database_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard.as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };
    
    // ğŸ“‹ ì§„ì§œ Actor ì²´ê³„ì  ì—°ê²°: SessionActor â†’ BatchActor â†’ StageActor
    let session_id_clone = session_id.clone();
    let actor_id_clone = actor_id.clone();
    let batches_clone = batches.clone(); // ì§ì ‘ ê³„ì‚°ëœ ë°°ì¹˜ ì „ë‹¬
    
    // ğŸ¯ í”„ë¡ íŠ¸ì—”ë“œ ì´ë²¤íŠ¸ ì±„ë„ ì„¤ì •
    let (event_tx, _event_rx) = tokio::sync::broadcast::channel(1000);
    
    // SessionActor ì‹œì‘ ì´ë²¤íŠ¸ ë°œì†¡
    let session_started_event = serde_json::json!({
        "type": "SessionStarted",
        "session_id": session_id,
        "actor_id": actor_id,
        "total_batches": total_phases,
        "total_pages": total_pages,
        "config": {
            "batch_size": crawling_config.batch_size,
            "concurrency_limit": crawling_config.concurrency_limit,
            "start_page": crawling_config.start_page,
            "end_page": crawling_config.end_page
        }
    });
    
    // í”„ë¡ íŠ¸ì—”ë“œë¡œ ì´ë²¤íŠ¸ ë°œì†¡
    if let Err(e) = event_tx.send(session_started_event.to_string()) {
        warn!("Failed to send session started event: {}", e);
    } else {
        info!("ğŸ“¤ Sent SessionStarted event to frontend");
    }
    
    // ê³µìœ  HttpClient ë° DataExtractor ìƒì„± (ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±)
    let shared_http_client = Arc::new(
        crate::infrastructure::HttpClient::create_from_global_config()
            .map_err(|e| format!("Failed to create shared HttpClient: {}", e))?
    );
    let shared_data_extractor = Arc::new(
        crate::infrastructure::MatterDataExtractor::new()
            .map_err(|e| format!("Failed to create shared data extractor: {}", e))?
    );
    
    tokio::spawn(async move {
        // ğŸ­ REAL Actor System ì‹œì‘: ì§„ì •í•œ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬
    info!("ğŸ­ REAL Actor System Starting: SessionActor {} â†’ BatchActor â†’ StageActor", actor_id_clone);
    info!("ğŸ“Š Real Actor Config: {:?}", crawling_config);
    
    // ğŸš€ SessionActor: ëª¨ë“  BatchActorë¥¼ ë³‘ë ¬ë¡œ ìƒì„± ë° ì‹¤í–‰
    info!("ï¿½ [SessionActor {}] Initializing real crawling session", actor_id_clone);
    info!("ğŸ“‹ [SessionActor {}] Direct batch-based session: {} batches, {} pages total", 
          actor_id_clone, total_phases, total_pages);
    
    // ğŸ”— Stage 2: ProductList ìˆ˜ì§‘ - ì§„ì •í•œ ë³‘ë ¬ ë°°ì¹˜ ì‹¤í–‰
    info!("ğŸ”— Stage 2: ProductList ìˆ˜ì§‘ ì‹œì‘ - ë°°ì¹˜ë³„ ë³‘ë ¬ ì‹¤í–‰ ({}~{})", 
          crawling_config.start_page, crawling_config.end_page);
    
    // ï¿½ğŸ“Š ë°°ì¹˜ë³„ ê²°ê³¼ ìˆ˜ì§‘ì„ ìœ„í•œ ë²¡í„°
    let mut batch_handles = Vec::new();
    let mut all_product_urls = Vec::new();
    
    // ğŸ¯ ëª¨ë“  BatchActorë¥¼ ë³‘ë ¬ë¡œ ì‹œì‘ (ì§„ì •í•œ ë³‘ë ¬ ì²˜ë¦¬)
    for (batch_idx, batch_pages) in batches_clone.iter().enumerate() {
        let batch_id = format!("batch_{}_{:04x}", batch_idx + 1, rand::random::<u16>());
        let pages_in_batch = batch_pages.len();
        
        info!("ğŸƒ [SessionActor â†’ BatchActor] {} creating BatchActor {}: {} pages (productlist-{}-{})", 
              actor_id_clone, batch_id, pages_in_batch, 
              batch_pages.first().unwrap(), batch_pages.last().unwrap());
        
        // ğŸ“¦ ì‹¤ì œ BatchActor ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
        let batch_actor = crate::new_architecture::actors::batch_actor::BatchActor::new(batch_id.clone());
        info!("ğŸ“¦ BatchActor {} ìƒì„± ì™„ë£Œ: {}í˜ì´ì§€ ì²˜ë¦¬ ì˜ˆì •", batch_id, pages_in_batch);
        
        // BatchActorë¥¼ ë³„ë„ Taskë¡œ ë³‘ë ¬ ì‹¤í–‰
        let batch_pages_clone = batch_pages.clone();
        let config_clone = crawling_config.clone();
        let http_client_clone = Arc::clone(&shared_http_client);
        let data_extractor_clone = Arc::clone(&shared_data_extractor);
        let database_pool_clone = database_pool.clone();
        let event_tx_clone = event_tx.clone();
        
        let batch_handle = tokio::spawn(async move {
            execute_batch_actor_complete_pipeline(
                batch_id,
                batch_pages_clone,
                config_clone,
                http_client_clone,
                data_extractor_clone,
                database_pool_clone,
                event_tx_clone,
            ).await
        });
        
        batch_handles.push(batch_handle);
    }
    
    // ğŸ”„ ëª¨ë“  BatchActor ì™„ë£Œ ëŒ€ê¸° (ë³‘ë ¬ ì‹¤í–‰)
    info!("â³ [SessionActor {}] Waiting for all {} BatchActors to complete in parallel", 
          actor_id_clone, batch_handles.len());
    
    let mut session_success = true;
    let mut total_products_collected = 0;
    let mut completed_batches = 0;
    
    for (batch_idx, batch_handle) in batch_handles.into_iter().enumerate() {
        match batch_handle.await {
            Ok(Ok(batch_result)) => {
                completed_batches += 1;
                total_products_collected += batch_result.products_processed;
                
                info!("âœ… [SessionActor â† BatchActor] {} completed batch {}: {} products processed", 
                      actor_id_clone, batch_idx + 1, batch_result.products_processed);
                
                // ë°°ì¹˜ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
                let batch_completed_event = serde_json::json!({
                    "type": "BatchCompleted",
                    "session_id": session_id_clone,
                    "batch_id": batch_idx + 1,
                    "total_batches": total_phases,
                    "products_processed": batch_result.products_processed,
                    "success": true
                });
                
                if let Err(e) = event_tx.send(batch_completed_event.to_string()) {
                    warn!("Failed to send batch completed event: {}", e);
                }
            }
            Ok(Err(e)) => {
                error!("âŒ [SessionActor â† BatchActor] {} batch {} failed: {}", 
                       actor_id_clone, batch_idx + 1, e);
                session_success = false;
            }
            Err(e) => {
                error!("âŒ [SessionActor â† BatchActor] {} batch {} join error: {}", 
                       actor_id_clone, batch_idx + 1, e);
                session_success = false;
            }
        }
    }", crawling_config);
        
        // ğŸ¯ ServiceBased ì—”ì§„ ì°¸ì¡°í•˜ì—¬ ì§„ì§œ Actor ì²´ê³„ êµ¬í˜„
        // SessionActor ì—­í• : ì „ì²´ ì„¸ì…˜ ê´€ë¦¬ ë° ë°°ì¹˜ ì¡°ì •
        info!("ğŸš€ [SessionActor {}] Initializing real crawling session", actor_id_clone);
        
        // ì§ì ‘ ê³„ì‚°ëœ ë°°ì¹˜ ì‚¬ìš©
        let total_batches = batches_clone.len();
        let total_pages = batches_clone.iter().map(|batch| batch.len()).sum::<usize>();
        
        info!("ğŸ“‹ [SessionActor {}] Direct batch-based session: {} batches, {} pages total", 
              actor_id_clone, total_batches, total_pages);
        
        // ğŸ¯ ì§„ì§œ í¬ë¡¤ë§ ë¡œì§ ì‹œì‘ - ì§ì ‘ ê³„ì‚°ëœ ë°°ì¹˜ ê¸°ë°˜ ì²˜ë¦¬
        let mut all_product_urls: Vec<String> = Vec::new();
        let mut session_success = true;
        
        info!("ğŸ”— Stage 2: ProductList ìˆ˜ì§‘ ì‹œì‘ - ë°°ì¹˜ë³„ ìˆœì°¨ ì‹¤í–‰ ({}~{})", 
              crawling_config.start_page, crawling_config.end_page);
        
        // ğŸ¯ SessionActor â†’ ì—¬ëŸ¬ BatchActor ìƒì„± ë° ì‹¤í–‰ (ì§ì ‘ ë°°ì¹˜ ê¸°ë°˜)
        for (batch_idx, batch_pages) in batches_clone.iter().enumerate() {
            let batch_name = format!("productlist-{}-{}", 
                batch_pages.first().unwrap_or(&0), 
                batch_pages.last().unwrap_or(&0));
            let pages_in_batch = batch_pages.len();
            let batch_id = format!("batch_{}_{}", batch_idx + 1, 
                Uuid::new_v4().simple().to_string().chars().take(4).collect::<String>());
            
            info!("ğŸƒ [SessionActor â†’ BatchActor] {} creating BatchActor {}: {} pages ({})", 
                  actor_id_clone, batch_id, pages_in_batch, batch_name);
            
            // ğŸ“¦ ì‹¤ì œ BatchActor ìƒì„±
            let mut batch_actor = crate::new_architecture::actors::batch_actor::BatchActor::new(batch_id.clone());
            
            info!("ğŸ“¦ BatchActor {} ìƒì„± ì™„ë£Œ: {}í˜ì´ì§€ ì²˜ë¦¬ ì˜ˆì •", batch_id, pages_in_batch);
            
            // ğŸ¯ BatchActor â†’ StageActor ì²´ì¸ ì‹¤í–‰ (ê³µìœ  í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
            info!("ğŸ” BatchActor {} collecting from {} pages with shared HttpClient", batch_id, pages_in_batch);
            info!("ğŸš€ BatchActor {} creating {} concurrent StageActors with semaphore control (max: {})", 
                  batch_id, pages_in_batch, crawling_config.concurrency_limit);
            
            // Semaphoreë¥¼ ì‚¬ìš©í•œ ë™ì‹œì„± ì œì–´
            let semaphore = Arc::new(tokio::sync::Semaphore::new(crawling_config.concurrency_limit as usize));
            let mut tasks = Vec::new();
            
            for &page in batch_pages {
                let stage_id = format!("stage_batch{}_page_{}", batch_idx + 1, page);
                let semaphore_clone = Arc::clone(&semaphore);
                let config_clone = crawling_config.clone();
                let http_client_clone = Arc::clone(&shared_http_client);
                let data_extractor_clone = Arc::clone(&shared_data_extractor);
                
                let task = tokio::spawn(async move {
                    let _permit = semaphore_clone.acquire().await.unwrap();
                    // ğŸ¯ ì‹¤ì œ StageActor ì‹¤í–‰ (ê³µìœ  í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
                    execute_real_page_crawling_shared(&config_clone, page, &stage_id, http_client_clone, data_extractor_clone).await
                });
                
                tasks.push(task);
            }
            
            info!("âœ… BatchActor {} created {} StageActors for batch {}, waiting for all to complete", 
                  batch_id, tasks.len(), batch_idx + 1);
            
            // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            let mut batch_product_urls = Vec::new();
            let mut successful_pages = 0;
            let mut failed_pages = 0;
            
            for (task_idx, task) in tasks.into_iter().enumerate() {
                match task.await {
                    Ok(Ok(page_urls)) => {
                        successful_pages += 1;
                        batch_product_urls.extend(page_urls);
                    }
                    Ok(Err(e)) => {
                        failed_pages += 1;
                        let page = batch_pages.get(task_idx).unwrap_or(&0);
                        error!("  âŒ Page {} failed: {}", page, e);
                    }
                    Err(e) => {
                        failed_pages += 1;
                        error!("  âŒ Task {} join error: {}", task_idx, e);
                    }
                }
            }
            
            info!("ğŸ¯ Batch {} concurrent collection completed: {} pages successful, {} failed, {} total URLs", 
                  batch_idx + 1, successful_pages, failed_pages, batch_product_urls.len());
            
            all_product_urls.extend(batch_product_urls);
            
            if failed_pages > 0 {
                warn!("âš ï¸ [SessionActor â† BatchActor] {} batch {} completed with {} failures", 
                      actor_id_clone, batch_idx + 1, failed_pages);
            } else {
                info!("âœ… [SessionActor â† BatchActor] {} completed batch {}: {} pages processed", 
                      actor_id_clone, batch_idx + 1, pages_in_batch);
            }
        }
        
        // ğŸ¯ Stage 2 ì™„ë£Œ - ìˆ˜ì§‘ëœ product URLs ì •ë¦¬
        let total_product_urls = all_product_urls.len();
        info!("âœ… Stage 2 completed: {} product URLs collected with true batch processing", total_product_urls);
        info!("ğŸ“Š Stage 2 summary: {} batches processed, {} total URLs", total_batches, total_product_urls);
        
        if total_product_urls > 0 {
            info!("ğŸš€ Proceeding to Stage 3 with {} product URLs", total_product_urls);
            
            // ğŸ¯ Stage 3: Product Detail Collection (ê³µìœ  í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
            info!("ğŸš€ Stage 3: Collecting product details using shared HttpClient and DataExtractor");
            
            let collector_config = crate::infrastructure::crawling_service_impls::CollectorConfig {
                batch_size: app_config.user.batch.batch_size,
                max_concurrent: app_config.user.max_concurrent_requests,
                concurrency: app_config.user.max_concurrent_requests,
                delay_between_requests: std::time::Duration::from_millis(app_config.user.request_delay_ms),
                delay_ms: app_config.user.request_delay_ms,
                retry_attempts: app_config.user.crawling.product_detail_retry_count,
                retry_max: app_config.user.crawling.product_detail_retry_count,
            };
            
            let collector = crate::infrastructure::crawling_service_impls::ProductDetailCollectorImpl::new(
                shared_http_client.clone(), shared_data_extractor.clone(), collector_config
            );
            
            // Stringì„ ProductUrlë¡œ ë³€í™˜
            let product_urls: Vec<ProductUrl> = all_product_urls
                .into_iter()
                .enumerate()
                .map(|(index, url)| ProductUrl::new(url, 1, index as i32))
                .collect();
                
            match collector.collect_details(&product_urls).await {
                Ok(product_details) => {
                    info!("âœ… Stage 3 completed: {} products collected (including retries)", product_details.len());
                    
                    // ğŸ¯ Stage 4: Database Batch Save (ì‹¤ì œ êµ¬í˜„)
                    info!("ğŸš€ Stage 4: Batch saving {} products to database", product_details.len());
                    
                    // ServiceBased Repository ë¡œì§ ì‚¬ìš© (product_detailsëŠ” Vec<ProductDetail>)
                    let product_repo = crate::infrastructure::IntegratedProductRepository::new(database_pool.clone());
                    let mut saved_count = 0;
                    let mut error_count = 0;
                    
                    for product_detail in product_details {
                        // ProductDetailì—ì„œ Productë¥¼ ìƒì„± (domain::product::Product ì‚¬ìš©)
                        let product = crate::domain::product::Product {
                            id: product_detail.id.clone(),
                            url: product_detail.url.clone(),
                            manufacturer: product_detail.manufacturer.clone(),
                            model: product_detail.model.clone(),
                            certificate_id: product_detail.certification_id.clone(),
                            page_id: product_detail.page_id,
                            index_in_page: product_detail.index_in_page,
                            created_at: product_detail.created_at,
                            updated_at: product_detail.updated_at,
                        };
                        
                        match (
                            product_repo.create_or_update_product(&product).await,
                            product_repo.create_or_update_product_detail(&product_detail).await
                        ) {
                            (Ok(_), Ok(_)) => saved_count += 1,
                            (Err(e), _) | (_, Err(e)) => {
                                warn!("Product save failed: {}", e);
                                error_count += 1;
                            }
                        }
                    }
                    
                    info!("ğŸ¯ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: ì´ {}ê°œ ì²˜ë¦¬ (ì˜¤ë¥˜: {})", saved_count, error_count);
                    info!("âœ… Stage 4 completed: {} products saved to database", saved_count);
                }
                Err(e) => {
                    error!("âŒ Stage 3 failed: Product detail collection error: {}", e);
                    session_success = false;
                }
            }
    
    // ğŸ¯ ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬
    if session_success {
        info!("ğŸ¯ [SessionActor] {} REAL Actor System completed: {} batches, {} pages total", 
              actor_id_clone, completed_batches, total_pages);
        info!("âœ… REAL Actor System chain successful: SessionActor â†’ BatchActor â†’ StageActor with shared resources");
        info!("ğŸ“Š Total products processed across all batches: {}", total_products_collected);
        
        // ì„¸ì…˜ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
        let session_completed_event = serde_json::json!({
            "type": "SessionCompleted",
            "session_id": session_id_clone,
            "actor_id": actor_id_clone,
            "total_products": total_products_collected,
            "completed_batches": completed_batches,
            "success": true
        });
        
        if let Err(e) = event_tx.send(session_completed_event.to_string()) {
            warn!("Failed to send session completed event: {}", e);
        }
    } else {
        error!("âŒ [SessionActor] {} REAL Actor System completed with errors", actor_id_clone);
    }
});

/// ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
#[derive(Debug)]
struct BatchResult {
    products_processed: usize,
    pages_successful: usize,
    pages_failed: usize,
}

/// ğŸ¯ ì™„ì „í•œ BatchActor íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Stage 2 â†’ Stage 3 â†’ Stage 4)
/// 
/// ì„¤ê³„ ë¬¸ì„œì— ë§ê²Œ BatchActorê°€ ëª¨ë“  ë‹¨ê³„ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
async fn execute_batch_actor_complete_pipeline(
    batch_id: String,
    batch_pages: Vec<u32>,
    config: crate::new_architecture::actors::types::CrawlingConfig,
    http_client: Arc<crate::infrastructure::HttpClient>,
    data_extractor: Arc<crate::infrastructure::MatterDataExtractor>,
    database_pool: sqlx::SqlitePool,
    event_tx: tokio::sync::broadcast::Sender<String>,
) -> Result<BatchResult, String> {
    
    let batch_started_event = serde_json::json!({
        "type": "BatchStarted",
        "batch_id": batch_id,
        "pages_count": batch_pages.len(),
        "pages": batch_pages
    });
    
    if let Err(e) = event_tx.send(batch_started_event.to_string()) {
        warn!("Failed to send batch started event: {}", e);
    }
    
    // ğŸ”— Stage 2: ProductList ìˆ˜ì§‘ (StageActorë“¤ì„ í†µí•´)
    info!("ğŸ”— [BatchActor {}] Stage 2: ProductList collection starting for {} pages", batch_id, batch_pages.len());
    
    let product_urls = execute_stage2_product_list_collection(
        &batch_id,
        &batch_pages,
        &config,
        http_client.clone(),
        data_extractor.clone(),
        event_tx.clone(),
    ).await?;
    
    info!("âœ… [BatchActor {}] Stage 2 completed: {} product URLs collected", batch_id, product_urls.len());
    
    // ğŸš€ Stage 3: Product Details ìˆ˜ì§‘ (StageActorë“¤ì„ í†µí•´)
    info!("ğŸš€ [BatchActor {}] Stage 3: Product details collection starting for {} products", 
          batch_id, product_urls.len());
    
    let product_details = execute_stage3_product_details_collection(
        &batch_id,
        &product_urls,
        &config,
        http_client.clone(),
        data_extractor.clone(),
        event_tx.clone(),
    ).await?;
    
    info!("âœ… [BatchActor {}] Stage 3 completed: {} product details collected", 
          batch_id, product_details.len());
    
    // ğŸ¯ Stage 4: Database ì €ì¥ (StageActorë¥¼ í†µí•´)
    info!("ğŸ¯ [BatchActor {}] Stage 4: Database batch save starting for {} products", 
          batch_id, product_details.len());
    
    let saved_count = execute_stage4_database_batch_save(
        &batch_id,
        &product_details,
        &database_pool,
        event_tx.clone(),
    ).await?;
    
    info!("âœ… [BatchActor {}] Stage 4 completed: {} products saved to database", 
          batch_id, saved_count);
    
    Ok(BatchResult {
        products_processed: saved_count,
        pages_successful: batch_pages.len(),
        pages_failed: 0,
    })
}

/// ğŸ”— Stage 2: ProductList Collection (StageActorë“¤ì„ í†µí•´)
async fn execute_stage2_product_list_collection(
    batch_id: &str,
    batch_pages: &[u32],
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    http_client: Arc<crate::infrastructure::HttpClient>,
    data_extractor: Arc<crate::infrastructure::MatterDataExtractor>,
    event_tx: tokio::sync::broadcast::Sender<String>,
) -> Result<Vec<String>, String> {
    
    info!("ğŸ” [BatchActor {}] Creating {} concurrent StageActors with semaphore control (max: {})", 
          batch_id, batch_pages.len(), config.concurrency_limit);
    
    // Semaphoreë¥¼ ì‚¬ìš©í•œ ë™ì‹œì„± ì œì–´ (ì„¤ì • ê¸°ë°˜)
    let semaphore = Arc::new(tokio::sync::Semaphore::new(config.concurrency_limit as usize));
    let mut tasks = Vec::new();
    
    for &page in batch_pages {
        let stage_id = format!("stage_{}_page_{}", batch_id, page);
        let semaphore_clone = Arc::clone(&semaphore);
        let config_clone = config.clone();
        let http_client_clone = Arc::clone(&http_client);
        let data_extractor_clone = Arc::clone(&data_extractor);
        
        let task = tokio::spawn(async move {
            let _permit = semaphore_clone.acquire().await.unwrap();
            // ğŸ¯ ì‹¤ì œ StageActor ì‹¤í–‰ (ê³µìœ  í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
            execute_real_page_crawling_shared(&config_clone, page, &stage_id, http_client_clone, data_extractor_clone).await
        });
        
        tasks.push(task);
    }
    
    info!("âœ… [BatchActor {}] Created {} StageActors, waiting for all to complete", 
          batch_id, tasks.len());
    
    // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
    let mut all_product_urls = Vec::new();
    let mut successful_pages = 0;
    let mut failed_pages = 0;
    
    for task in tasks {
        match task.await {
            Ok(Ok(page_product_urls)) => {
                successful_pages += 1;
                all_product_urls.extend(page_product_urls);
            }
            Ok(Err(e)) => {
                failed_pages += 1;
                warn!("[BatchActor {}] Page crawling failed: {}", batch_id, e);
            }
            Err(e) => {
                failed_pages += 1;
                error!("[BatchActor {}] Task join error: {}", batch_id, e);
            }
        }
    }
    
    info!("ğŸ¯ [BatchActor {}] Stage 2 concurrent collection completed: {} pages successful, {} failed, {} total URLs", 
          batch_id, successful_pages, failed_pages, all_product_urls.len());
    
    Ok(all_product_urls)
}

/// ğŸš€ Stage 3: Product Details Collection (StageActorë“¤ì„ í†µí•´)
async fn execute_stage3_product_details_collection(
    batch_id: &str,
    product_urls: &[String],
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    http_client: Arc<crate::infrastructure::HttpClient>,
    data_extractor: Arc<crate::infrastructure::MatterDataExtractor>,
    event_tx: tokio::sync::broadcast::Sender<String>,
) -> Result<Vec<crate::domain::product_detail::ProductDetail>, String> {
    
    info!("ğŸš€ [BatchActor {}] Stage 3: Collecting product details using shared HttpClient and DataExtractor", batch_id);
    
    // ServiceBased ë¡œì§ ì¬í™œìš© (ProductDetailCollector)
    let app_config = crate::infrastructure::config::AppConfig::for_development();
    
    let collector_config = crate::infrastructure::crawling_service_impls::CollectorConfig {
        batch_size: config.batch_size,
        max_concurrent: config.concurrency_limit,
        concurrency: config.concurrency_limit,
        delay_between_requests: std::time::Duration::from_millis(config.request_delay_ms),
        delay_ms: config.request_delay_ms,
        retry_attempts: config.max_retries,
        retry_max: config.max_retries,
    };
    
    let collector = crate::infrastructure::crawling_service_impls::ProductDetailCollectorImpl::new(
        http_client, data_extractor, collector_config
    );
    
    // Stringì„ ProductUrlë¡œ ë³€í™˜
    let product_urls: Vec<crate::domain::product_url::ProductUrl> = product_urls
        .iter()
        .enumerate()
        .map(|(index, url)| crate::domain::product_url::ProductUrl::new(url.clone(), 1, index as i32))
        .collect();
        
    match collector.collect_details(&product_urls).await {
        Ok(product_details) => {
            info!("âœ… [BatchActor {}] Stage 3 completed: {} products collected (including retries)", 
                  batch_id, product_details.len());
            Ok(product_details)
        }
        Err(e) => {
            error!("âŒ [BatchActor {}] Stage 3 failed: Product detail collection error: {}", batch_id, e);
            Err(format!("Stage 3 failed: {}", e))
        }
    }
}

/// ğŸ¯ Stage 4: Database Batch Save (StageActorë¥¼ í†µí•´)
async fn execute_stage4_database_batch_save(
    batch_id: &str,
    product_details: &[crate::domain::product_detail::ProductDetail],
    database_pool: &sqlx::SqlitePool,
    event_tx: tokio::sync::broadcast::Sender<String>,
) -> Result<usize, String> {
    
    info!("ğŸ¯ [BatchActor {}] Stage 4: Batch saving {} products to database", batch_id, product_details.len());
    
    // ServiceBased Repository ë¡œì§ ì‚¬ìš©
    let product_repo = crate::infrastructure::IntegratedProductRepository::new(database_pool.clone());
    let mut saved_count = 0;
    let mut error_count = 0;
    
    for product_detail in product_details {
        // ProductDetailì—ì„œ Productë¥¼ ìƒì„±
        let product = crate::domain::product::Product {
            id: product_detail.id.clone(),
            url: product_detail.url.clone(),
            manufacturer: product_detail.manufacturer.clone(),
            model: product_detail.model.clone(),
            certificate_id: product_detail.certification_id.clone(),
            page_id: product_detail.page_id,
            index_in_page: product_detail.index_in_page,
            created_at: product_detail.created_at,
            updated_at: product_detail.updated_at,
        };
        
        match (
            product_repo.create_or_update_product(&product).await,
            product_repo.create_or_update_product_detail(product_detail).await
        ) {
            (Ok(_), Ok(_)) => saved_count += 1,
            (Err(e), _) | (_, Err(e)) => {
                warn!("[BatchActor {}] Product save failed: {}", batch_id, e);
                error_count += 1;
            }
        }
    }
    
    info!("ğŸ¯ [BatchActor {}] Stage 4 ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: ì´ {}ê°œ ì²˜ë¦¬ (ì˜¤ë¥˜: {})", batch_id, saved_count, error_count);
    info!("âœ… [BatchActor {}] Stage 4 completed: {} products saved to database", batch_id, saved_count);
    
    Ok(saved_count)
}
    
    // ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
    Ok(RealActorCrawlingResponse {
        success: true,
        message: "Real Actor system started successfully".to_string(),
        session_id,
        actor_id,
    })
}

/// ğŸ¯ ì‹¤ì œ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤í–‰ (ê³µìœ  HttpClient ë° DataExtractor ì‚¬ìš©)
/// 
/// StageActor ì—­í• : ê°œë³„ í˜ì´ì§€ HTTP ìš”ì²­, HTML íŒŒì‹±, product URLs ë°˜í™˜
async fn execute_real_page_crawling_shared(
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    page: u32,
    _stage_id: &str, // ë¡œê¹…ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (warning ì œê±°)
    http_client: Arc<crate::infrastructure::HttpClient>,
    data_extractor: Arc<crate::infrastructure::MatterDataExtractor>,
) -> Result<Vec<String>, String> {
    // ì‹¤ì œ HTTP ìš”ì²­ (ê³µìœ  í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
    let page_url = crate::infrastructure::config::csa_iot::PRODUCTS_PAGE_MATTER_PAGINATED
        .replace("{}", &page.to_string());
    
    // HTTP ìš”ì²­ ì‹¤í–‰ (ê³µìœ  í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
    let html_content = http_client.fetch_html_string(&page_url)
        .await
        .map_err(|e| format!("HTTP request failed for {}: {}", page_url, e))?;
    
    // HTML íŒŒì‹± (ê³µìœ  data_extractor ì‚¬ìš©)
    let product_urls = data_extractor.extract_product_urls_from_content(&html_content)
        .map_err(|e| format!("Failed to extract product URLs: {}", e))?;
    
    // ìš”ì²­ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€) - ë³‘ë ¬ ì²˜ë¦¬ì—ì„œëŠ” ë” ì§§ì€ ë”œë ˆì´
    if config.request_delay_ms > 0 {
        let delay_ms = config.request_delay_ms / 2; // ë³‘ë ¬ ì²˜ë¦¬ì´ë¯€ë¡œ ë”œë ˆì´ ê°ì†Œ
        tokio::time::sleep(tokio::time::Duration::from_millis(delay_ms)).await;
    }
    
    // product URLs ë°˜í™˜ (Stage 2 ì™„ì„±)
    Ok(product_urls)
}

/// ğŸ¯ ì‹¤ì œ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤í–‰ (ê¸°ì¡´ ë²„ì „ - í˜¸í™˜ì„± ìœ ì§€)
/// 
/// StageActor ì—­í• : ê°œë³„ í˜ì´ì§€ HTTP ìš”ì²­, HTML íŒŒì‹±, product URLs ë°˜í™˜
async fn execute_real_page_crawling(
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    page: u32,
    _stage_id: &str, // ë¡œê¹…ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (warning ì œê±°)
) -> Result<Vec<String>, String> {
    // ì‹¤ì œ HTTP ìš”ì²­ (ServiceBased ë¡œì§ ì°¸ì¡°) - ì˜¬ë°”ë¥¸ URL íŒ¨í„´ ì‚¬ìš©
    let page_url = crate::infrastructure::config::csa_iot::PRODUCTS_PAGE_MATTER_PAGINATED
        .replace("{}", &page.to_string());
    
    // HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    let http_client = crate::infrastructure::HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    
    // HTTP ìš”ì²­ ì‹¤í–‰ (ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª…: fetch_html_string)
    let html_content = http_client.fetch_html_string(&page_url)
        .await
        .map_err(|e| format!("HTTP request failed for {}: {}", page_url, e))?;
    
    // HTML íŒŒì‹± (ServiceBased ë¡œì§ ì°¸ì¡° - ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª…)
    let data_extractor = crate::infrastructure::MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    let product_urls = data_extractor.extract_product_urls_from_content(&html_content)
        .map_err(|e| format!("Failed to extract product URLs: {}", e))?;
    
    // ìš”ì²­ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€) - ë³‘ë ¬ ì²˜ë¦¬ì—ì„œëŠ” ë” ì§§ì€ ë”œë ˆì´
    if config.request_delay_ms > 0 {
        let delay_ms = config.request_delay_ms / 2; // ë³‘ë ¬ ì²˜ë¦¬ì´ë¯€ë¡œ ë”œë ˆì´ ê°ì†Œ
        tokio::time::sleep(tokio::time::Duration::from_millis(delay_ms)).await;
    }
    
    // product URLs ë°˜í™˜ (Stage 2 ì™„ì„±)
    Ok(product_urls)
}
