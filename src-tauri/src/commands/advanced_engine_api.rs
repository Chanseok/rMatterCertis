//! Advanced Crawling Engine í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ Commands (ì‹¤ì œ êµ¬í˜„ ë²„ì „)
//! ts-rsë¡œ ìƒì„±ëœ TypeScript íƒ€ì…ê³¼ ì—°ë™ë˜ëŠ” Tauri ëª…ë ¹ì–´ë“¤

use tauri::{command, AppHandle, State, Emitter};
use tracing::{info, warn, error};
use uuid::Uuid;
use std::sync::Arc;

use crate::types::frontend_api::*;
use crate::commands::crawling_v4::CrawlingEngineState;
use crate::application::shared_state::SharedStateCache;
use crate::infrastructure::{
    AdvancedBatchCrawlingEngine, HttpClient, MatterDataExtractor, 
    IntegratedProductRepository, DatabaseConnection
};
use crate::infrastructure::service_based_crawling_engine::BatchCrawlingConfig;
use crate::application::EventEmitter;

/// Advanced Crawling Engine ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ (ì‹¤ì œ êµ¬í˜„)
#[command]
pub async fn check_advanced_site_status(
    app: AppHandle,
    _state: State<'_, CrawlingEngineState>,
    shared_state: State<'_, SharedStateCache>,
) -> Result<ApiResponse<SiteStatusInfo>, String> {
    info!("ğŸŒ Advanced site status check requested");
    
    // ğŸ”¥ ë…ë¦½ì ì¸ ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ì‹œì‘ ì´ë²¤íŠ¸ ë°œì†¡
    let start_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
        is_standalone: true,
        status: crate::domain::events::SiteCheckStatus::Started,
        message: "ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤...".to_string(),
        timestamp: chrono::Utc::now(),
    };
    
    if let Err(e) = app.emit("site-status-check", &start_event) {
        warn!("Failed to emit site status check start event: {}", e);
    }
    
    // ë¨¼ì € ìºì‹œëœ ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼ í™•ì¸ (5ë¶„ TTL)
    if let Some(cached_analysis) = shared_state.get_valid_site_analysis_async(Some(5)).await {
        info!("ğŸ¯ Using cached site analysis - analyzed: {}, age: {} minutes", 
             cached_analysis.analyzed_at.format("%H:%M:%S"),
             chrono::Utc::now().signed_duration_since(cached_analysis.analyzed_at).num_minutes());
        
        // ğŸ”¥ ìºì‹œ ì‚¬ìš© ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
        let cache_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
            is_standalone: true,
            status: crate::domain::events::SiteCheckStatus::Success,
            message: "ìºì‹œëœ ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤".to_string(),
            timestamp: chrono::Utc::now(),
        };
        
        if let Err(e) = app.emit("site-status-check", &cache_event) {
            warn!("Failed to emit cached site status event: {}", e);
        }
        
        let site_status_info = SiteStatusInfo {
            is_accessible: true,
            response_time_ms: 500, // ê¸°ë³¸ê°’ - ìºì‹œëœ ë°ì´í„°ì´ë¯€ë¡œ
            total_pages: cached_analysis.total_pages,
            products_on_last_page: cached_analysis.products_on_last_page,
            estimated_total_products: cached_analysis.estimated_products,
            health_score: cached_analysis.health_score,
        };
        return Ok(ApiResponse::success(site_status_info));
    }
    
    info!("â° No valid cached site analysis found - performing fresh site check");
    info!("ğŸ”„ Starting real site status check...");
    
    // ğŸ”¥ ì‹¤ì œ ì‚¬ì´íŠ¸ ì²´í¬ ì§„í–‰ ì¤‘ ì´ë²¤íŠ¸ ë°œì†¡
    let progress_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
        is_standalone: true,
        status: crate::domain::events::SiteCheckStatus::InProgress,
        message: "ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ ìƒíƒœë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤...".to_string(),
        timestamp: chrono::Utc::now(),
    };
    
    // ğŸ”¥ ì‹¤ì œ ì‚¬ì´íŠ¸ ì²´í¬ ì§„í–‰ ì¤‘ ì´ë²¤íŠ¸ ë°œì†¡
    let progress_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
        is_standalone: true,
        status: crate::domain::events::SiteCheckStatus::InProgress,
        message: "ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ ìƒíƒœë¥¼ í™•ì¸ ì¤‘ì…ë‹ˆë‹¤...".to_string(),
        timestamp: chrono::Utc::now(),
    };
    
    if let Err(e) = app.emit("site-status-check", &progress_event) {
        warn!("Failed to emit site status progress event: {}", e);
    }
    
    // ìºì‹œê°€ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš°, ì‹¤ì œ í¬ë¡¤ë§ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    let http_client = match HttpClient::create_from_global_config() {
        Ok(client) => client,
        Err(e) => {
            error!("Failed to create HTTP client: {}", e);
            
            // ğŸ”¥ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡
            let failed_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
                is_standalone: true,
                status: crate::domain::events::SiteCheckStatus::Failed,
                message: format!("HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {}", e),
                timestamp: chrono::Utc::now(),
            };
            let _ = app.emit("site-status-check", &failed_event);
            
            return Err(format!("HTTP client creation failed: {}", e));
        }
    };
    
    let data_extractor = match MatterDataExtractor::new() {
        Ok(extractor) => extractor,
        Err(e) => {
            error!("Failed to create data extractor: {}", e);
            return Err(format!("Data extractor creation failed: {}", e));
        }
    };
    
    // ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±
    let database_url = std::env::var("DATABASE_URL")
        .unwrap_or_else(|_| "sqlite:data/matter_certis.db".to_string());
    
    let db_connection = match DatabaseConnection::new(&database_url).await {
        Ok(conn) => conn,
        Err(e) => {
            error!("Database connection failed: {}", e);
            return Err(format!("Database connection error: {}", e));
        }
    };
    
    let product_repo = Arc::new(IntegratedProductRepository::new(db_connection.pool().clone()));
    
    // Advanced í¬ë¡¤ë§ ì—”ì§„ ìƒì„±
    let config = BatchCrawlingConfig {
        start_page: 1,
        end_page: 1, // ìƒíƒœ í™•ì¸ìš©ìœ¼ë¡œ 1í˜ì´ì§€ë§Œ
        batch_size: 10,
        concurrency: 1,
        list_page_concurrency: 1,
        product_detail_concurrency: 1,
        delay_ms: 1000,
        retry_max: 3,
        timeout_ms: 30000,
        cancellation_token: None,
    };
    
    let session_id = format!("status_check_{}", Uuid::new_v4().simple());
    let event_emitter = Arc::new(None::<EventEmitter>);
    
    let engine = AdvancedBatchCrawlingEngine::new(
        http_client,
        data_extractor,
        product_repo,
        event_emitter,
        config,
        session_id,
    );
    
    info!("ğŸš€ Starting real site analysis...");
    
    // ì‹¤ì œ ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸
    match engine.stage0_check_site_status().await {
        Ok(site_status) => {
            info!("âœ… Fresh site status check completed - {} pages found", site_status.total_pages);
            
            // ğŸ”¥ ì„±ê³µ ì´ë²¤íŠ¸ ë°œì†¡
            let success_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
                is_standalone: true,
                status: crate::domain::events::SiteCheckStatus::Success,
                message: format!("ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ: {}ê°œ í˜ì´ì§€ ë°œê²¬", site_status.total_pages),
                timestamp: chrono::Utc::now(),
            };
            
            if let Err(e) = app.emit("site-status-check", &success_event) {
                warn!("Failed to emit site status success event: {}", e);
            }
            
            // ìƒˆë¡œìš´ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            let site_analysis = crate::application::shared_state::SiteAnalysisResult::new(
                site_status.total_pages,
                site_status.products_on_last_page,
                site_status.estimated_products,
                "https://iotready.kr".to_string(), // site_url
                1.0, // health_score
            );
            shared_state.set_site_analysis(site_analysis).await;
            
            let site_status_info = SiteStatusInfo {
                is_accessible: true,
                response_time_ms: 500, // ê¸°ë³¸ê°’
                total_pages: site_status.total_pages,
                products_on_last_page: site_status.products_on_last_page,
                estimated_total_products: site_status.estimated_products,
                health_score: 1.0,
            };
            
            info!("âœ… Fresh site status check completed and cached");
            Ok(ApiResponse::success(site_status_info))
        },
        Err(e) => {
            error!("Site status check failed: {}", e);
            
            // ğŸ”¥ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡
            let failed_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
                is_standalone: true,
                status: crate::domain::events::SiteCheckStatus::Failed,
                message: format!("ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {}", e),
                timestamp: chrono::Utc::now(),
            };
            
            if let Err(emit_err) = app.emit("site-status-check", &failed_event) {
                warn!("Failed to emit site status failed event: {}", emit_err);
            }
            
            Err(format!("Site status check error: {}", e))
        }
    }
}

/// Advanced Crawling Engine ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
#[command]
pub async fn start_advanced_crawling(
    request: StartCrawlingRequest,
    app: AppHandle,
    _state: State<'_, CrawlingEngineState>,
    _shared_state: State<'_, SharedStateCache>,
) -> Result<ApiResponse<CrawlingSession>, String> {
    let session_id = format!("advanced_{}", Uuid::new_v4().simple());
    info!("ğŸš€ Starting real advanced crawling session: {}", session_id);
    
    // ì‹¤ì œ í¬ë¡¤ë§ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    let http_client = match HttpClient::create_from_global_config() {
        Ok(client) => client,
        Err(e) => {
            error!("Failed to create HTTP client: {}", e);
            return Err(format!("HTTP client creation failed: {}", e));
        }
    };
    
    let data_extractor = match MatterDataExtractor::new() {
        Ok(extractor) => extractor,
        Err(e) => {
            error!("Failed to create data extractor: {}", e);
            return Err(format!("Data extractor creation failed: {}", e));
        }
    };
    
    // ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±
    let database_url = std::env::var("DATABASE_URL")
        .unwrap_or_else(|_| "sqlite:data/matter_certis.db".to_string());
    
    let db_connection = match DatabaseConnection::new(&database_url).await {
        Ok(conn) => conn,
        Err(e) => {
            error!("Database connection failed: {}", e);
            return Err(format!("Database connection error: {}", e));
        }
    };
    
    let product_repo = Arc::new(IntegratedProductRepository::new(db_connection.pool().clone()));
    
    // í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì •ì„ BatchCrawlingConfigë¡œ ë³€í™˜
    let config = BatchCrawlingConfig {
        start_page: request.config.start_page,
        end_page: request.config.end_page,
        batch_size: request.config.batch_size,
        concurrency: request.config.concurrency,
        list_page_concurrency: request.config.concurrency,
        product_detail_concurrency: request.config.concurrency,
        delay_ms: request.config.delay_ms,
        retry_max: request.config.retry_max,
        timeout_ms: 30000,
        cancellation_token: None,
    };
    
    // ì„¸ì…˜ ì •ë³´ ìƒì„±
    let session = CrawlingSession {
        session_id: session_id.clone(),
        started_at: chrono::Utc::now(),
        config: request.config,
        status: SessionStatus::Running,
        total_products_processed: 0,
        success_rate: 0.0,
    };
    
    // EventEmitter ì„¤ì • (ì•± í•¸ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì´ë²¤íŠ¸ ë°œì†¡)
    let event_emitter = Arc::new(Some(EventEmitter::new(app.clone())));
    
    // Advanced í¬ë¡¤ë§ ì—”ì§„ ìƒì„±
    let engine = AdvancedBatchCrawlingEngine::new(
        http_client,
        data_extractor,
        product_repo,
        event_emitter,
        config,
        session_id.clone(),
    );
    
    // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
    let session_clone = session.clone();
    let app_clone = app.clone();
    let session_id_for_spawn = session_id.clone();
    
    tokio::spawn(async move {
        info!("ğŸ”„ Starting real advanced crawling execution for session: {}", session_id_for_spawn);
        
        // ì‹¤ì œ í¬ë¡¤ë§ ì—”ì§„ ì‹¤í–‰
        match engine.execute().await {
            Ok(()) => {
                info!("âœ… Real advanced crawling completed successfully for session: {}", session_id_for_spawn);
                
                // ì„±ê³µ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
                if let Err(e) = app_clone.emit("crawling-completed", &session_clone) {
                    warn!("Failed to emit crawling-completed event: {}", e);
                }
            },
            Err(e) => {
                error!("âŒ Real advanced crawling failed for session {}: {}", session_id_for_spawn, e);
                
                // ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡
                let mut failed_session = session_clone;
                failed_session.status = SessionStatus::Failed;
                
                if let Err(emit_err) = app_clone.emit("crawling-failed", &failed_session) {
                    warn!("Failed to emit crawling-failed event: {}", emit_err);
                }
            }
        }
    });
    
    // ì¦‰ì‹œ ì„¸ì…˜ ì •ë³´ ë°˜í™˜
    info!("âœ… Real advanced crawling session started: {}", session_id);
    Ok(ApiResponse::success(session))
}

/// ìµœê·¼ ì œí’ˆ ëª©ë¡ ì¡°íšŒ (ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤)
#[command]
pub async fn get_recent_products(
    page: Option<u32>,
    limit: Option<u32>,
    _state: State<'_, CrawlingEngineState>,
    _shared_state: State<'_, SharedStateCache>,
) -> Result<ApiResponse<ProductPage>, String> {
    let page = page.unwrap_or(1);
    let limit = limit.unwrap_or(20);
    
    info!("ğŸ“‹ Fetching recent products from real database - page: {}, limit: {}", page, limit);
    
    // ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„± (v4ì™€ ë™ì¼í•œ ê²½ë¡œ ì‚¬ìš©)
    let database_url = match crate::commands::crawling_v4::get_database_url_v4() {
        Ok(url) => url,
        Err(e) => {
            error!("Failed to get database URL: {}", e);
            return Err(format!("Database URL error: {}", e));
        }
    };
    
    let db_connection = match DatabaseConnection::new(&database_url).await {
        Ok(conn) => conn,
        Err(e) => {
            error!("Database connection failed: {}", e);
            return Err(format!("Database connection error: {}", e));
        }
    };
    
    let product_repo = IntegratedProductRepository::new(db_connection.pool().clone());
    
    // ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œí’ˆ ëª©ë¡ ì¡°íšŒ
    match product_repo.get_products_paginated(page as i32, limit as i32).await {
        Ok(products) => {
            // Productë¥¼ ProductInfoë¡œ ë³€í™˜
            let product_infos: Vec<ProductInfo> = products.into_iter().map(|product| {
                ProductInfo {
                    id: product.url.clone(), // URLì„ IDë¡œ ì‚¬ìš©
                    url: product.url,
                    name: product.model.unwrap_or_else(|| "Unknown Product".to_string()),
                    company: product.manufacturer.unwrap_or_else(|| "Unknown Company".to_string()),
                    certification_number: product.certificate_id.unwrap_or_else(|| "N/A".to_string()),
                    description: None, // Product êµ¬ì¡°ì²´ì— description í•„ë“œê°€ ì—†ëŠ” ê²½ìš°
                    created_at: product.created_at,
                    updated_at: Some(product.updated_at),
                }
            }).collect();
            
            // ì´ ì œí’ˆ ìˆ˜ ì¡°íšŒ
            let total_items = match product_repo.get_product_count().await {
                Ok(count) => count as u32,
                Err(e) => {
                    warn!("Failed to get total product count: {}", e);
                    0
                }
            };
            
            let total_pages = (total_items + limit - 1) / limit; // ì˜¬ë¦¼ ê³„ì‚°
            
            let product_page = ProductPage {
                products: product_infos,
                current_page: page,
                page_size: limit,
                total_items,
                total_pages,
            };
            
            info!("âœ… Retrieved {} real products from database", product_page.products.len());
            Ok(ApiResponse::success(product_page))
        },
        Err(e) => {
            error!("Failed to fetch products from database: {}", e);
            Err(format!("Database query failed: {}", e))
        }
    }
}

/// ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ (ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤)
#[command]
pub async fn get_database_stats(
    _state: State<'_, CrawlingEngineState>,
    _shared_state: State<'_, SharedStateCache>,
) -> Result<ApiResponse<DatabaseStats>, String> {
    info!("ğŸ“Š Fetching real database statistics");
    
    // ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„± (v4ì™€ ë™ì¼í•œ ê²½ë¡œ ì‚¬ìš©)
    let database_url = match crate::commands::crawling_v4::get_database_url_v4() {
        Ok(url) => url,
        Err(e) => {
            error!("Failed to get database URL: {}", e);
            return Err(format!("Database URL error: {}", e));
        }
    };
    
    let db_connection = match DatabaseConnection::new(&database_url).await {
        Ok(conn) => conn,
        Err(e) => {
            error!("Database connection failed: {}", e);
            return Err(format!("Database connection error: {}", e));
        }
    };
    
    let product_repo = IntegratedProductRepository::new(db_connection.pool().clone());
    
    // ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ
    match product_repo.get_database_statistics().await {
        Ok(db_stats) => {
            // ì´ ì œí’ˆ ìˆ˜ ì¡°íšŒ 
            let total_products = match product_repo.get_product_count().await {
                Ok(count) => count as u32,
                Err(e) => {
                    warn!("Failed to get product count: {}", e);
                    0
                }
            };
            
            // ì˜¤ëŠ˜ ì¶”ê°€ëœ ì œí’ˆ ìˆ˜ ì¡°íšŒ (ìµœê·¼ 24ì‹œê°„ ë‚´)
            let products_added_today = 0; // IntegratedProductRepositoryì— í•´ë‹¹ ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
            
            // ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¡°íšŒ
            let last_updated = match product_repo.get_latest_updated_product().await {
                Ok(Some(product)) => Some(product.updated_at),
                Ok(None) => None,
                Err(e) => {
                    warn!("Failed to get last updated time: {}", e);
                    None
                }
            };
            
            let database_stats = DatabaseStats {
                total_products,
                products_added_today,
                last_updated,
                database_size_bytes: 0, // ê³„ì‚°ì´ ë³µì¡í•˜ë¯€ë¡œ ê¸°ë³¸ê°’
            };
            
            info!("âœ… Retrieved real database statistics: {} products", total_products);
            Ok(ApiResponse::success(database_stats))
        },
        Err(e) => {
            error!("Failed to fetch database statistics: {}", e);
            Err(format!("Database statistics query failed: {}", e))
        }
    }
}
