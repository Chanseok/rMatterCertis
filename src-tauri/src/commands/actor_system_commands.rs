//! Actor System Commands for Tauri Integration
//! 
//! Commands to test and use the Actor system from the UI

use crate::new_architecture::actor_system::SessionActor;
use crate::new_architecture::system_config::SystemConfig;
use crate::new_architecture::channel_types::{AppEvent, BatchConfig};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::service_based_crawling_engine::{ServiceBasedBatchCrawlingEngine, BatchCrawlingConfig};
use crate::infrastructure::simple_http_client::HttpClient;
use crate::infrastructure::html_parser::MatterDataExtractor;
use crate::infrastructure::integrated_product_repository::IntegratedProductRepository;
use crate::application::AppState;
use crate::domain::services::{StatusChecker, DatabaseAnalyzer}; // StatusChecker trait ì¶”ê°€
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tauri::{AppHandle, Manager, Emitter};
use tokio::sync::mpsc;
use tokio::time::Duration;
use tracing::{info, error};
use chrono::Utc;

/// Actor System State managed by Tauri
#[derive(Default)]
pub struct ActorSystemState {
    pub is_running: Arc<tokio::sync::RwLock<bool>>,
}

/// Actor System Response
#[derive(Debug, Serialize, Deserialize)]
pub struct ActorSystemResponse {
    pub success: bool,
    pub message: String,
    pub session_id: Option<String>,
    pub data: Option<serde_json::Value>,
}

/// Crawling Request for Actor System
#[derive(Debug, Serialize, Deserialize)]
pub struct ActorCrawlingRequest {
    pub start_page: u32,
    pub end_page: u32,
    pub concurrency: Option<u32>,
    pub batch_size: Option<u32>,
    pub delay_ms: Option<u64>,
}

/// ğŸ­ NEW ARCHITECTURE: Start Actor-based crawling
#[tauri::command]
pub async fn start_actor_based_crawling(
    app: AppHandle,
    request: ActorCrawlingRequest,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ­ [NEW ARCHITECTURE] Starting REAL Actor-based crawling: {:?}", request);
    
    let batch_size = request.batch_size.unwrap_or(3);
    let total_pages = request.end_page - request.start_page + 1;
    let batch_count = (total_pages + batch_size - 1) / batch_size; // ì˜¬ë¦¼ ê³„ì‚°
    
    info!("âœ… [ACTOR] Creating actual SessionActor for real crawling");
    info!("ğŸ“Š [ACTOR] Pages: {} to {}, Batch size: {}, Expected batches: {}", 
          request.start_page, request.end_page, batch_size, batch_count);
    
    // ğŸš€ ì‹¤ì œ SessionActor ìƒì„± ë° ì‹¤í–‰
    let system_config = Arc::new(SystemConfig::default());
    let (_control_tx, control_rx) = mpsc::channel(100);
    let (event_tx, mut event_rx) = mpsc::channel(500);
    
    // SessionActor ìƒì„±
    let _session_actor = SessionActor::new(
        system_config,
        control_rx,
        event_tx.clone(),
    );
    
    let session_id = format!("actor_session_{}", Utc::now().timestamp());
    info!("ğŸ­ SessionActor created with ID: {}", session_id);
    
    // session_idì™€ request í´ë¡  ìƒì„± (move closureì—ì„œ ì‚¬ìš©í•  ê²ƒ)
    let session_id_for_task = session_id.clone();
    let session_id_for_event = session_id.clone();
    let session_id_for_return = session_id.clone();
    let request_for_task = ActorCrawlingRequest {
        start_page: request.start_page,
        end_page: request.end_page,
        concurrency: request.concurrency,
        batch_size: request.batch_size,
        delay_ms: request.delay_ms,
    };
    let app_handle_for_task = app.clone();
    
    // ğŸ”¥ ì‹¤ì œ í¬ë¡¤ë§ ì—”ì§„ì„ ì‚¬ìš©í•œ SessionActor ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
    let _session_actor_handle = tokio::spawn(async move {
        info!("ğŸš€ SessionActor starting execution with REAL crawling engine...");
        
        // ğŸ¯ ì‹¤ì œ í¬ë¡¤ë§ ì—”ì§„ ì´ˆê¸°í™”
        match initialize_real_crawling_engine(&session_id_for_task, &request_for_task, &app_handle_for_task).await {
            Ok((mut crawling_engine, analysis_info)) => {
                info!("âœ… Real crawling engine initialized successfully");
                
                // ì‹¤ì œ í¬ë¡¤ë§ ì—”ì§„ ì‹¤í–‰
                match crawling_engine.execute().await {
                    Ok(()) => {
                        info!("ğŸ‰ Real crawling completed successfully!");
                    }
                    Err(e) => {
                        error!("âŒ Real crawling failed: {}", e);
                    }
                }
                
                // ë¶„ì„ ì •ë³´ ì €ì¥ (ë‚˜ì¤‘ì— ì‘ë‹µì—ì„œ ì‚¬ìš©)
                // TODO: ë¶„ì„ ì •ë³´ë¥¼ ì„¸ì…˜ì— ì €ì¥í•˜ê±°ë‚˜ ì´ë²¤íŠ¸ë¡œ ì „ë‹¬
            }
            Err(e) => {
                error!("âŒ Failed to initialize real crawling engine: {}", e);
                
                // ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ í´ë°±
                info!("ğŸ”„ Falling back to simulation mode...");
                run_simulation_crawling(&request_for_task, request_for_task.batch_size.unwrap_or(3)).await;
            }
        }
        
        info!("âœ… SessionActor completed execution");
        
        Ok::<(), Box<dyn std::error::Error + Send + Sync>>(())
    });
    
    // ğŸ”¥ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ) - ì‹¤ì œ ì´ë²¤íŠ¸ ë°©ì¶œ
    let event_tx_clone = event_tx.clone();
    let session_id_for_second_spawn = session_id.clone();
    let end_page_for_event = request.end_page;
    let app_handle_for_events = app.clone();
    tokio::spawn(async move {
        // ì‹œì‘ ì´ë²¤íŠ¸ ë°©ì¶œ (AppEvent íƒ€ì…ìœ¼ë¡œ)
        let session_event = AppEvent::SessionStarted {
            session_id: session_id_for_second_spawn.clone(),
            config: BatchConfig {
                target_url: "https://csa-iot.org".to_string(),
                max_pages: Some(end_page_for_event),
            },
        };
        let _ = event_tx_clone.send(session_event).await;
        
        // ì´ë²¤íŠ¸ ìˆ˜ì‹  ì²˜ë¦¬ ë° í”„ë¡ íŠ¸ì—”ë“œë¡œ ë°©ì¶œ
        while let Some(event) = event_rx.recv().await {
            info!("ğŸ“¨ [ACTOR EVENT] Received: {:?}", event);
            
            // í”„ë¡ íŠ¸ì—”ë“œë¡œ ì´ë²¤íŠ¸ ë°©ì¶œ
            if let Err(e) = app_handle_for_events.emit("actor-event", &event) {
                error!("Failed to emit actor event to frontend: {}", e);
            }
        }
    });
    
    // ğŸ”¥ ì‹¤ì œ Actor ì‹œìŠ¤í…œ - ë„ë©”ì¸ ì§€ëŠ¥í˜• ì‹œìŠ¤í…œê³¼ ì—°ê²° ì™„ë£Œ
    info!("ğŸ­ Actor ì‹œìŠ¤í…œ INTELLIGENT MODE: ë„ë©”ì¸ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜");
    info!("ğŸ“Š ìš”ì²­ ë²”ìœ„: {} ~ {} í˜ì´ì§€, ë°°ì¹˜í¬ê¸° {}, ë™ì‹œì„± {}", 
          request.start_page, request.end_page, batch_size, request.concurrency.unwrap_or(8));
    
    let total_pages = if request.start_page >= request.end_page {
        request.start_page - request.end_page + 1
    } else {
        request.end_page - request.start_page + 1
    };
    
    Ok(ActorSystemResponse {
        success: true,
        message: format!("ğŸ§  INTELLIGENT Actor-based crawling started with domain logic compliance"), 
        session_id: Some(session_id_for_return),
        data: Some(serde_json::json!({
            "engine_type": "Actor + Domain Intelligence + ServiceBasedBatchCrawlingEngine",
            "architecture": "SessionActor â†’ Domain Logic â†’ ServiceBasedBatchCrawlingEngine â†’ [StatusChecker, ProductListCollector, ProductDetailCollector]",
            "status": "RUNNING",
            "mode": "INTELLIGENT_CRAWLING",
            "config": {
                "requested_start_page": request.start_page,
                "requested_end_page": request.end_page,
                "batch_size": batch_size,
                "concurrency": request.concurrency.unwrap_or(8),
                "total_pages": total_pages,
                "expected_batches": batch_count,
                "domain_logic_enabled": true
            }
        })),
    })
}

/// Test SessionActor functionality
#[tauri::command]
pub async fn test_session_actor_basic(
    _app: AppHandle,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ§ª Testing SessionActor...");
    
    let system_config = Arc::new(SystemConfig::default());
    let (_control_tx, control_rx) = mpsc::channel(100);
    let (event_tx, _event_rx) = mpsc::channel(500);
    
    let _session_actor = SessionActor::new(
        system_config,
        control_rx,
        event_tx,
    );
    
    info!("âœ… SessionActor created successfully");
    
    Ok(ActorSystemResponse {
        success: true,
        message: "SessionActor test completed successfully".to_string(),
        session_id: Some(format!("test_session_{}", Utc::now().timestamp())),
        data: None,
    })
}

/// Test Actor integration
#[tauri::command]
pub async fn test_actor_integration_basic(
    _app: AppHandle,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ§ª Testing Actor system integration...");
    
    // Integration test - create full system
    let system_config = Arc::new(SystemConfig::default());
    let (_control_tx, control_rx) = mpsc::channel(100);
    let (event_tx, _event_rx) = mpsc::channel(500);
    
    let _session_actor = SessionActor::new(
        system_config,
        control_rx,
        event_tx.clone(),
    );
    
    // Test actor system flow
    tokio::select! {
        _ = tokio::time::sleep(Duration::from_millis(100)) => {
            info!("âœ… Actor integration test completed within timeout");
        }
    }
    
    Ok(ActorSystemResponse {
        success: true,
        message: "Actor integration test completed successfully".to_string(),
        session_id: Some(format!("integration_test_{}", Utc::now().timestamp())),
        data: Some(serde_json::json!({
            "integration_status": "success",
            "components_tested": ["SessionActor", "channels", "configuration"]
        })),
    })
}

/// ì‹¤ì œ í¬ë¡¤ë§ ì—”ì§„ ì´ˆê¸°í™” (ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚° í¬í•¨)
async fn initialize_real_crawling_engine(
    session_id: &str,
    request: &ActorCrawlingRequest,
    app_handle: &AppHandle,
) -> Result<(ServiceBasedBatchCrawlingEngine, serde_json::Value), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ”§ Initializing real crawling engine with intelligent planning for session: {}", session_id);
    
    // ì•± ìƒíƒœì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ í’€ ê°€ì ¸ì˜¤ê¸°
    let app_state = app_handle.state::<AppState>();
    let db_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard.as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };
    
    // IntegratedProductRepository ìƒì„±
    let product_repo = Arc::new(IntegratedProductRepository::new(db_pool));
    
    // HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    let http_client = HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    
    // ë°ì´í„° ì¶”ì¶œê¸° ìƒì„±
    let data_extractor = MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    // ì´ë²¤íŠ¸ ë°©ì¶œê¸° ì„¤ì • (ì„ íƒì )
    let event_emitter = Arc::new(None);
    
    // ğŸ§  CrawlingPlannerë¥¼ í†µí•œ ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚°
    info!("ğŸ§  [ACTOR] Using domain-specific intelligent range calculation...");
    
    // StatusChecker ìƒì„± (ê¸°ì¡´ ë„ë©”ì¸ ë¡œì§ í™œìš©)
    let status_checker = Arc::new(crate::infrastructure::crawling_service_impls::StatusCheckerImpl::new(
        http_client.clone(),
        data_extractor.clone(),
        AppConfig::default(),
    ));
    
    // ì‚¬ì´íŠ¸ ìƒíƒœ ë¶„ì„ (ê¸°ì¡´ ë„ë©”ì¸ ë¡œì§)
    let site_status = status_checker.check_site_status().await
        .map_err(|e| format!("Failed to check site status: {}", e))?;
    
    info!("ğŸŒ [ACTOR] Site analysis: {} pages, {} products on last page", 
          site_status.total_pages, site_status.products_on_last_page);
    
    // ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ (ê¸°ì¡´ ë„ë©”ì¸ ë¡œì§)
    let db_analysis = crate::domain::services::crawling_services::DatabaseAnalysis {
        total_products: 0, // StatusCheckerImplì—ì„œ ì‹¤ì œ DB ì¡°íšŒë¡œ ì±„ì›Œì§
        unique_products: 0,
        duplicate_count: 0,
        last_update: Some(chrono::Utc::now()),
        missing_fields_analysis: crate::domain::services::crawling_services::FieldAnalysis {
            missing_company: 0,
            missing_model: 0,
            missing_matter_version: 0,
            missing_connectivity: 0,
            missing_certification_date: 0,
        },
        data_quality_score: 0.8,
    };
    
    // ì§€ëŠ¥í˜• ë²”ìœ„ ê¶Œì¥ì‚¬í•­ ê³„ì‚° (ê¸°ì¡´ ë„ë©”ì¸ ë¡œì§)
    let range_recommendation = status_checker
        .calculate_crawling_range_recommendation(&site_status, &db_analysis)
        .await
        .map_err(|e| format!("Failed to calculate crawling range recommendation: {}", e))?;
    
    info!("ğŸ“‹ [ACTOR] Domain intelligence recommendation: {:?}", range_recommendation);
    
    // ì§€ëŠ¥í˜• ë²”ìœ„ ê¶Œì¥ì‚¬í•­ì„ ì‹¤ì œ í˜ì´ì§€ ë²”ìœ„ë¡œ ë³€í™˜
    let (calculated_start_page, calculated_end_page) = match range_recommendation.to_page_range(site_status.total_pages) {
        Some((start, end)) => {
            info!("ğŸ¯ [ACTOR] Intelligent range: {} to {} (total: {} pages)", start, end, 
                  if start >= end { start - end + 1 } else { end - start + 1 });
            (start, end)
        },
        None => {
            info!("ğŸ” [ACTOR] No crawling needed, using verification range");
            let verification_pages = 5;
            let start = site_status.total_pages;
            let end = if start >= verification_pages { start - verification_pages + 1 } else { 1 };
            (start, end)
        }
    };
    
    // ì‚¬ìš©ì ìš”ì²­ê³¼ ì§€ëŠ¥í˜• ê¶Œì¥ì‚¬í•­ ë¹„êµ
    let (final_start_page, final_end_page) = if request.start_page != 0 && request.end_page != 0 {
        // ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ë²”ìœ„ë¥¼ ì§€ì •í•œ ê²½ìš°
        info!("ğŸ‘¤ [ACTOR] User specified range: {} to {}", request.start_page, request.end_page);
        info!("ğŸ¤– [ACTOR] Intelligent recommendation: {} to {}", calculated_start_page, calculated_end_page);
        info!("ğŸ§  [ACTOR] Using intelligent recommendation to ensure domain requirements compliance");
        (calculated_start_page, calculated_end_page)
    } else {
        // ì‚¬ìš©ìê°€ ë²”ìœ„ë¥¼ ì§€ì •í•˜ì§€ ì•Šì€ ê²½ìš° ì§€ëŠ¥í˜• ê¶Œì¥ì‚¬í•­ ì‚¬ìš©
        (calculated_start_page, calculated_end_page)
    };
    
    // ê¸°ë³¸ ì²˜ë¦¬ ì „ëµ ì„¤ì • (CrawlingPlanner ì—†ì´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    let recommended_batch_size = request.batch_size.unwrap_or(3);
    let recommended_concurrency = request.concurrency.unwrap_or(8);
    
    // ë°°ì¹˜ í¬ë¡¤ë§ ì„¤ì • ìƒì„± - ğŸ§  ì§€ëŠ¥í˜• ê¶Œì¥ì‚¬í•­ ì ìš©
    let batch_config = BatchCrawlingConfig {
        start_page: final_start_page,
        end_page: final_end_page,
        concurrency: recommended_concurrency,
        batch_size: recommended_batch_size,
        delay_ms: request.delay_ms.unwrap_or(500),
        list_page_concurrency: 3,
        product_detail_concurrency: recommended_concurrency,
        retry_max: 3,
        timeout_ms: 30000, // 30 seconds in milliseconds
        disable_intelligent_range: false, // ğŸ§  ë„ë©”ì¸ ë¡œì§ ì‚¬ìš©í•˜ë¯€ë¡œ false
        cancellation_token: None,
    };
    
    // ì•± ì„¤ì • ë¡œë“œ - ë„ë©”ì¸ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜
    let app_config = AppConfig::default();
    
    info!("ğŸ§  [ACTOR] Using intelligent range: {} to {} (batch_size: {}, concurrency: {})", 
          final_start_page, final_end_page, recommended_batch_size, recommended_concurrency);
    
    // ServiceBasedBatchCrawlingEngine ìƒì„±
    let crawling_engine = ServiceBasedBatchCrawlingEngine::new(
        http_client,
        data_extractor,
        product_repo,
        event_emitter,
        batch_config,
        session_id.to_string(),
        app_config,
    );
    
    // ë¶„ì„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ êµ¬ì„±
    let analysis_info = serde_json::json!({
        "range_recommendation": format!("{:?}", range_recommendation),
        "user_requested": {
            "start_page": request.start_page,
            "end_page": request.end_page
        },
        "intelligent_calculated": {
            "start_page": calculated_start_page,
            "end_page": calculated_end_page
        },
        "final_used": {
            "start_page": final_start_page,
            "end_page": final_end_page
        },
        "site_analysis": {
            "total_pages": site_status.total_pages,
            "products_on_last_page": site_status.products_on_last_page,
            "estimated_products": site_status.estimated_products,
            "is_accessible": site_status.is_accessible
        }
    });
    
    info!("âœ… Real crawling engine initialized successfully with intelligent planning");
    Ok((crawling_engine, analysis_info))
}

/// ì‹œë®¬ë ˆì´ì…˜ í¬ë¡¤ë§ ì‹¤í–‰ (í´ë°±)
async fn run_simulation_crawling(
    request: &ActorCrawlingRequest,
    batch_size: u32,
) {
    info!("ğŸ”„ Running simulation crawling as fallback...");
    
    // í˜ì´ì§€ ë²”ìœ„ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
    let mut current_page = request.start_page;
    let mut batch_number = 1;
    
    while current_page <= request.end_page {
        let batch_end = std::cmp::min(current_page + batch_size - 1, request.end_page);
        info!("ğŸ“¦ Processing Batch {}: pages {} to {}", batch_number, current_page, batch_end);
        
        // ë°°ì¹˜ë³„ í˜ì´ì§€ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        for page in current_page..=batch_end {
            info!("ğŸ” Processing page {} with simulated crawling", page);
            
            // ì‹œë®¬ë ˆì´ì…˜ ì§€ì—° ì‹œê°„
            tokio::time::sleep(Duration::from_millis(request.delay_ms.unwrap_or(800))).await;
            
            info!("âœ… Page {} processed successfully", page);
        }
        
        info!("âœ… Batch {} completed", batch_number);
        current_page = batch_end + 1;
        batch_number += 1;
    }
    
    info!("ğŸ‰ Simulation crawling completed successfully");
}
