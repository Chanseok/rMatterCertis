//! Actor System Commands for Tauri Integration
//! 
//! Commands to test and use the Actor system from the UI

use crate::new_architecture::actors::SessionActor;
use crate::new_architecture::context::SystemConfig;
use crate::new_architecture::channels::types::{AppEvent, BatchConfig};
use crate::new_architecture::actors::types::{CrawlingConfig, ActorCommand};
use crate::new_architecture::actor_event_bridge::{ActorEventBridge, start_actor_event_bridge};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::service_based_crawling_engine::{ServiceBasedBatchCrawlingEngine, BatchCrawlingConfig};
use crate::infrastructure::simple_http_client::HttpClient;
use crate::infrastructure::html_parser::MatterDataExtractor;
use crate::infrastructure::integrated_product_repository::IntegratedProductRepository;
use crate::application::AppState;
use crate::domain::services::{StatusChecker, DatabaseAnalyzer}; // ì‹¤ì œ CrawlingPlannerì—ì„œ ì‚¬ìš©
use crate::infrastructure::config::ConfigManager; // ì„¤ì • ê´€ë¦¬ì ì¶”ê°€
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tauri::{AppHandle, Manager, Emitter};
use tokio::sync::{mpsc, broadcast};
use tokio::time::Duration;
use tracing::{info, error};
use chrono::Utc;

/// Actor System State managed by Tauri
#[derive(Default)]
pub struct ActorSystemState {
    pub is_running: Arc<tokio::sync::RwLock<bool>>,
}

/// Actor System Response
#[derive(Debug, Serialize, Deserialize)]
pub struct ActorSystemResponse {
    pub success: bool,
    pub message: String,
    pub session_id: Option<String>,
    pub data: Option<serde_json::Value>,
}

/// Crawling Request for Actor System
#[derive(Debug, Serialize, Deserialize)]
pub struct ActorCrawlingRequest {
    pub start_page: u32,
    pub end_page: u32,
    pub concurrency: Option<u32>,
    pub batch_size: Option<u32>,
    pub delay_ms: Option<u64>,
}

/// ğŸ­ Actor System í¬ë¡¤ë§ ì‹œì‘ (ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜)
/// 
/// ìˆœìˆ˜ Actor ê¸°ë°˜: SessionActor â†’ BatchActor â†’ StageActor ì•„í‚¤í…ì²˜
/// CrawlingPlanner ê¸°ë°˜ ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚°ê³¼ ActorEventBridge ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ í¬í•¨.
/// âš ï¸ ServiceBasedBatchCrawlingEngineì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ!
#[tauri::command]
pub async fn start_actor_system_crawling(
    app: AppHandle,
    request: ActorCrawlingRequest,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ­ [NEW ARCHITECTURE] Starting REAL Actor-based crawling: {:?}", request);
    
    let batch_size = request.batch_size.unwrap_or(3);
    // ì—­ìˆœ í¬ë¡¤ë§ì„ ê³ ë ¤í•œ total_pages ê³„ì‚°
    let total_pages = if request.start_page >= request.end_page {
        request.start_page - request.end_page + 1
    } else {
        request.end_page - request.start_page + 1
    };
    let batch_count = (total_pages + batch_size - 1) / batch_size; // ì˜¬ë¦¼ ê³„ì‚°
    
    info!("âœ… [ACTOR] Creating actual SessionActor for real crawling");
    info!("ğŸ“Š [ACTOR] Pages: {} to {}, Batch size: {}, Expected batches: {}", 
          request.start_page, request.end_page, batch_size, batch_count);
    
    // ğŸš€ ì‹¤ì œ SessionActor ìƒì„± ë° ì‹¤í–‰
    let system_config = Arc::new(SystemConfig::default());
    let (_control_tx, control_rx) = mpsc::channel::<ActorCommand>(100);
    
    // ğŸŒ‰ Actor ì´ë²¤íŠ¸ ë¸Œë¦¿ì§€ë¥¼ ìœ„í•œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì±„ë„ ìƒì„±
    let (actor_event_tx, actor_event_rx) = broadcast::channel::<AppEvent>(1000);
    
    // ğŸŒ‰ Actor Event Bridge ì‹œì‘ - Actor ì´ë²¤íŠ¸ë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ ìë™ ì „ë‹¬
    let bridge_handle = start_actor_event_bridge(app.clone(), actor_event_rx)
        .await
        .map_err(|e| format!("Failed to start Actor Event Bridge: {}", e))?;
    
    info!("ğŸŒ‰ Actor Event Bridge started successfully");

    // SessionActor ìƒì„±
    let _session_actor = SessionActor::new(
        format!("session_{}", chrono::Utc::now().timestamp())
    );
    
    let session_id = format!("actor_session_{}", Utc::now().timestamp());
    info!("ğŸ­ SessionActor created with ID: {}", session_id);
    
    // session_idì™€ request í´ë¡  ìƒì„± (move closureì—ì„œ ì‚¬ìš©í•  ê²ƒ)
    let session_id_for_task = session_id.clone();
    let session_id_for_event = session_id.clone();
    let session_id_for_return = session_id.clone();
    let request_for_task = ActorCrawlingRequest {
        start_page: request.start_page,
        end_page: request.end_page,
        concurrency: request.concurrency,
        batch_size: request.batch_size,
        delay_ms: request.delay_ms,
    };
    let app_handle_for_task = app.clone();
    
    // actor_event_txë¥¼ ê° spawnì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ clone
    let actor_event_tx_for_spawn1 = actor_event_tx.clone();
    let actor_event_tx_for_spawn2 = actor_event_tx.clone();
    
    // ğŸ”¥ ìˆœìˆ˜ Actor ì‹œìŠ¤í…œ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
    let _session_actor_handle = tokio::spawn(async move {
        info!("ğŸš€ SessionActor starting execution with pure Actor system...");
        
        // ğŸ¯ CrawlingPlannerë¡œ ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚°
        match calculate_intelligent_crawling_range(&session_id_for_task, &request_for_task, &app_handle_for_task).await {
            Ok((final_start_page, final_end_page, analysis_info)) => {
                info!("âœ… Intelligent range calculated: {} to {}", final_start_page, final_end_page);
                
                // ğŸ­ SessionActorê°€ ë²”ìœ„ë¥¼ ì—¬ëŸ¬ BatchActorì—ê²Œ ë°°ë¶„
                match execute_session_actor_with_batches(
                    &session_id_for_task, 
                    final_start_page, 
                    final_end_page,
                    request_for_task.batch_size.unwrap_or(3),
                    actor_event_tx_for_spawn1.clone()
                ).await {
                    Ok(()) => {
                        info!("ğŸ‰ Actor system crawling completed successfully!");
                    }
                    Err(e) => {
                        error!("âŒ Actor system crawling failed: {}", e);
                    }
                }
            }
            Err(e) => {
                error!("âŒ Failed to calculate intelligent range: {}", e);
                
                // ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ í´ë°±
                info!("ğŸ”„ Falling back to simulation mode...");
                run_simulation_crawling(&request_for_task, request_for_task.batch_size.unwrap_or(3)).await;
            }
        }
        
        info!("âœ… SessionActor completed execution");
        
        Ok::<(), Box<dyn std::error::Error + Send + Sync>>(())
    });
    
        // ğŸ”¥ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ) - Actor ì´ë²¤íŠ¸ë¥¼ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì±„ë„ë¡œ ë°œí–‰
    let actor_event_tx_clone = actor_event_tx_for_spawn2.clone();
    let session_id_for_second_spawn = session_id.clone();
    let app_handle_for_events = app.clone();
    tokio::spawn(async move {
        // ğŸ¯ ì‹œì‘ ì´ë²¤íŠ¸ ë°©ì¶œ (Actor ì‹œìŠ¤í…œì˜ AppEvent íƒ€ì…ìœ¼ë¡œ)
        info!("ğŸ“¡ Emitting SessionStarted event through Actor Event Bridge");
        let session_event = AppEvent::SessionStarted {
            session_id: session_id_for_second_spawn.clone(),
            config: CrawlingConfig {
                site_url: format!("https://matter.certis.io/device-list/{}", request.start_page),
                start_page: request.start_page,
                end_page: request.end_page,
                concurrency_limit: 5,
                batch_size: 20,
                request_delay_ms: 1000,
                timeout_secs: 300,
                max_retries: 3,
            },
            timestamp: chrono::Utc::now(),
        };
        
        // Actor Event Bridgeë¥¼ í†µí•´ í”„ë¡ íŠ¸ì—”ë“œë¡œ ìë™ ì „ë‹¬
        if let Err(e) = actor_event_tx_clone.send(session_event) {
            error!("Failed to send Actor event through bridge: {}", e);
        } else {
            info!("âœ… Actor event sent through bridge successfully");
        }
        
        // ì¶”ê°€ ì§„í–‰ ìƒí™© ì´ë²¤íŠ¸ë“¤ (ì‹œë®¬ë ˆì´ì…˜)
        tokio::time::sleep(Duration::from_millis(1000)).await;
        
        let progress_event = AppEvent::Progress {
            session_id: session_id_for_second_spawn.clone(),
            current_step: 1,
            total_steps: request.end_page - request.start_page + 1,
            message: "Starting crawling process...".to_string(),
            percentage: 10.0,
            timestamp: chrono::Utc::now(),
        };
        
        if let Err(e) = actor_event_tx_clone.send(progress_event) {
            error!("Failed to send progress event through bridge: {}", e);
        }
    });
    
    // ğŸ”¥ ì‹¤ì œ Actor ì‹œìŠ¤í…œ - ë„ë©”ì¸ ì§€ëŠ¥í˜• ì‹œìŠ¤í…œê³¼ ì—°ê²° ì™„ë£Œ
    info!("ğŸ­ Actor ì‹œìŠ¤í…œ INTELLIGENT MODE: ë„ë©”ì¸ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜");
    info!("ğŸ“Š ìš”ì²­ ë²”ìœ„: {} ~ {} í˜ì´ì§€, ë°°ì¹˜í¬ê¸° {}, ë™ì‹œì„± {}", 
          request.start_page, request.end_page, batch_size, request.concurrency.unwrap_or(8));
    
    let total_pages = if request.start_page >= request.end_page {
        request.start_page - request.end_page + 1
    } else {
        request.end_page - request.start_page + 1
    };
    
    Ok(ActorSystemResponse {
        success: true,
        message: format!("ğŸ­ Pure Actor-based crawling started with intelligent planning"), 
        session_id: Some(session_id_for_return),
        data: Some(serde_json::json!({
            "engine_type": "Pure Actor System",
            "architecture": "SessionActor â†’ BatchActor â†’ StageActor",
            "status": "RUNNING",
            "mode": "PURE_ACTOR_CRAWLING",
            "config": {
                "requested_start_page": request.start_page,
                "requested_end_page": request.end_page,
                "batch_size": batch_size,
                "concurrency": request.concurrency.unwrap_or(8),
                "total_pages": total_pages,
                "expected_batches": batch_count,
                "domain_logic_enabled": true,
                "service_based_engine": false
            }
        })),
    })
}

/// ğŸ”§ ServiceBasedBatchCrawlingEngine í¬ë¡¤ë§ (ê°€ì§œ í¬ë¡¤ë§ - ì°¸ê³ ìš©)
/// 
/// ê¸°ì¡´ ServiceBasedBatchCrawlingEngineì„ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ë°©ì‹
/// ë„ë©”ì¸ ìš”êµ¬ì‚¬í•­ ì¼ë¶€ êµ¬í˜„, ë‚˜ì¤‘ì— ì œê±° ì˜ˆì •
#[tauri::command]
pub async fn start_service_based_crawling(
    app: AppHandle,
    request: ActorCrawlingRequest,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ”§ [SERVICE-BASED] Starting ServiceBasedBatchCrawlingEngine crawling: {:?}", request);
    
    let session_id = format!("service_session_{}", Utc::now().timestamp());
    
    // ServiceBasedBatchCrawlingEngine ì´ˆê¸°í™” ë° ì‹¤í–‰
    match initialize_service_based_engine(&session_id, &request, &app).await {
        Ok((mut crawling_engine, analysis_info)) => {
            info!("âœ… ServiceBasedBatchCrawlingEngine initialized successfully");
            
            // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
            let _engine_handle = tokio::spawn(async move {
                match crawling_engine.execute().await {
                    Ok(()) => {
                        info!("ğŸ‰ ServiceBasedBatchCrawlingEngine completed successfully!");
                    }
                    Err(e) => {
                        error!("âŒ ServiceBasedBatchCrawlingEngine failed: {}", e);
                    }
                }
            });
            
            Ok(ActorSystemResponse {
                success: true,
                message: "ğŸ”§ ServiceBasedBatchCrawlingEngine started successfully".to_string(),
                session_id: Some(session_id),
                data: Some(serde_json::json!({
                    "engine_type": "ServiceBasedBatchCrawlingEngine",
                    "architecture": "Direct ServiceBasedBatchCrawlingEngine",
                    "status": "RUNNING",
                    "mode": "SERVICE_BASED_CRAWLING",
                    "analysis_info": analysis_info
                })),
            })
        }
        Err(e) => {
            error!("âŒ Failed to initialize ServiceBasedBatchCrawlingEngine: {}", e);
            Err(format!("ServiceBasedBatchCrawlingEngine initialization failed: {}", e))
        }
    }
}

/// ServiceBasedBatchCrawlingEngine ì´ˆê¸°í™” (ì°¸ê³ ìš©)
async fn initialize_service_based_engine(
    session_id: &str,
    request: &ActorCrawlingRequest,
    app_handle: &AppHandle,
) -> Result<(ServiceBasedBatchCrawlingEngine, serde_json::Value), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ”§ Initializing ServiceBasedBatchCrawlingEngine for session: {}", session_id);
    
    // ì•± ìƒíƒœì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ í’€ ê°€ì ¸ì˜¤ê¸°
    let app_state = app_handle.state::<AppState>();
    let db_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard.as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };
    
    // IntegratedProductRepository ìƒì„±
    let product_repo = Arc::new(IntegratedProductRepository::new(db_pool));
    
    // HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    let http_client = HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    
    // ë°ì´í„° ì¶”ì¶œê¸° ìƒì„±
    let data_extractor = MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    // ì´ë²¤íŠ¸ ë°©ì¶œê¸° ì„¤ì • (ì„ íƒì )
    let event_emitter = Arc::new(None);
    
    // ì„¤ì • ë¡œë“œ
    let config_manager = crate::infrastructure::config::ConfigManager::new()
        .map_err(|e| format!("Failed to initialize config manager: {}", e))?;
    let app_config = config_manager.load_config().await
        .map_err(|e| format!("Failed to load config: {}", e))?;
    
    // ê¸°ë³¸ ë°°ì¹˜ í¬ë¡¤ë§ ì„¤ì • ìƒì„± (CrawlingPlanner ì—†ì´)
    let batch_config = BatchCrawlingConfig {
        start_page: request.start_page,
        end_page: request.end_page,
        concurrency: request.concurrency.unwrap_or(8),
        batch_size: request.batch_size.unwrap_or(3),
        delay_ms: request.delay_ms.unwrap_or(app_config.user.request_delay_ms),
        list_page_concurrency: app_config.user.crawling.workers.list_page_max_concurrent as u32,
        product_detail_concurrency: app_config.user.crawling.workers.product_detail_max_concurrent as u32,
        retry_max: app_config.advanced.retry_attempts,
        timeout_ms: (app_config.advanced.request_timeout_seconds * 1000) as u64,
        disable_intelligent_range: true, // CrawlingPlanner ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        cancellation_token: None,
    };
    
    info!("ğŸ”§ [SERVICE-BASED] Configuration applied:");
    info!("   ğŸ“Š Range: {} to {} ({} pages)", 
          batch_config.start_page, batch_config.end_page, 
          if batch_config.start_page >= batch_config.end_page { 
              batch_config.start_page - batch_config.end_page + 1 
          } else { 
              batch_config.end_page - batch_config.start_page + 1 
          });
    info!("   âš™ï¸ Processing: batch_size={}, concurrency={}, delay={}ms", 
          batch_config.batch_size, batch_config.concurrency, batch_config.delay_ms);
    
    // ServiceBasedBatchCrawlingEngine ìƒì„±
    let crawling_engine = ServiceBasedBatchCrawlingEngine::new(
        http_client,
        data_extractor,
        product_repo,
        event_emitter,
        batch_config,
        session_id.to_string(),
        app_config,
    );
    
    // ë¶„ì„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ êµ¬ì„±
    let analysis_info = serde_json::json!({
        "user_requested": {
            "start_page": request.start_page,
            "end_page": request.end_page
        },
        "engine_type": "ServiceBasedBatchCrawlingEngine",
        "intelligent_planning": false
    });
    
    info!("âœ… ServiceBasedBatchCrawlingEngine initialized successfully");
    Ok((crawling_engine, analysis_info))
}

/// Test SessionActor functionality
#[tauri::command]
pub async fn test_session_actor_basic(
    _app: AppHandle,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ§ª Testing SessionActor...");
    
    let system_config = Arc::new(SystemConfig::default());
    let (_control_tx, control_rx) = mpsc::channel::<ActorCommand>(100);
    let (event_tx, _event_rx) = mpsc::channel::<AppEvent>(500);
    
    let _session_actor = SessionActor::new(
        format!("session_{}", chrono::Utc::now().timestamp())
    );
    
    info!("âœ… SessionActor created successfully");
    
    Ok(ActorSystemResponse {
        success: true,
        message: "SessionActor test completed successfully".to_string(),
        session_id: Some(format!("test_session_{}", Utc::now().timestamp())),
        data: None,
    })
}

/// Test Actor integration
#[tauri::command]
pub async fn test_actor_integration_basic(
    _app: AppHandle,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ§ª Testing Actor system integration...");
    
    // Integration test - create full system
    let system_config = Arc::new(SystemConfig::default());
    let (_control_tx, control_rx) = mpsc::channel::<ActorCommand>(100);
    let (event_tx, _event_rx) = mpsc::channel::<AppEvent>(500);
    
    let _session_actor = SessionActor::new(
        format!("session_{}", chrono::Utc::now().timestamp())
    );
    
    // Test actor system flow
    tokio::select! {
        _ = tokio::time::sleep(Duration::from_millis(100)) => {
            info!("âœ… Actor integration test completed within timeout");
        }
    }
    
    Ok(ActorSystemResponse {
        success: true,
        message: "Actor integration test completed successfully".to_string(),
        session_id: Some(format!("integration_test_{}", Utc::now().timestamp())),
        data: Some(serde_json::json!({
            "integration_status": "success",
            "components_tested": ["SessionActor", "channels", "configuration"]
        })),
    })
}

/// CrawlingPlanner ê¸°ë°˜ ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚° (Actor ì‹œìŠ¤í…œìš©)
async fn calculate_intelligent_crawling_range(
    session_id: &str,
    request: &ActorCrawlingRequest,
    app_handle: &AppHandle,
) -> Result<(u32, u32, serde_json::Value), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ§  Calculating intelligent crawling range for Actor system session: {}", session_id);
    
    // ì•± ìƒíƒœì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ í’€ ê°€ì ¸ì˜¤ê¸°
    let app_state = app_handle.state::<AppState>();
    let db_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard.as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };
    
    // IntegratedProductRepository ìƒì„±
    let product_repo = Arc::new(IntegratedProductRepository::new(db_pool));
    
    // HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    let http_client = HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    
    // ë°ì´í„° ì¶”ì¶œê¸° ìƒì„±
    let data_extractor = MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    // ğŸ§  ì‹¤ì œ ì„¤ì • íŒŒì¼ ë¡œë“œ ë° CrawlingPlanner ì‚¬ìš©
    info!("ğŸ§  [ACTOR] Loading configuration and using CrawlingPlanner for intelligent analysis...");
    
    // ì‹¤ì œ ì•± ì„¤ì • ë¡œë“œ (ê¸°ë³¸ê°’ ëŒ€ì‹ )
    let config_manager = crate::infrastructure::config::ConfigManager::new()
        .map_err(|e| format!("Failed to initialize config manager: {}", e))?;
    let app_config = config_manager.load_config().await
        .map_err(|e| format!("Failed to load config: {}", e))?;
    
    info!("ğŸ“‹ [ACTOR] Configuration loaded: page_range_limit={}, batch_size={}, max_concurrent={}", 
          app_config.user.crawling.page_range_limit, 
          app_config.user.batch.batch_size,
          app_config.user.max_concurrent_requests);
    
    // StatusChecker ìƒì„± (ì‹¤ì œ ì„¤ì • ì‚¬ìš©)
    let status_checker_impl = crate::infrastructure::crawling_service_impls::StatusCheckerImpl::new(
        http_client.clone(),
        data_extractor.clone(),
        app_config.clone(),
    );
    let status_checker = Arc::new(status_checker_impl);
    
    // DatabaseAnalyzer ìƒì„± (ì‹¤ì œ DB ë¶„ì„)
    let db_analyzer = Arc::new(crate::infrastructure::crawling_service_impls::DatabaseAnalyzerImpl::new(
        product_repo.clone(),
    ));
    
    // SystemConfigë¡œ ë³€í™˜ (CrawlingPlannerìš©)
    let system_config = Arc::new(crate::new_architecture::context::SystemConfig::default());
    
    // ğŸš€ ì‹¤ì œ CrawlingPlanner ì‚¬ìš©!
    let crawling_planner = crate::new_architecture::services::crawling_planner::CrawlingPlanner::new(
        status_checker.clone(),
        db_analyzer.clone(),
        system_config.clone(),
    );
    
    // ì‹œìŠ¤í…œ ìƒíƒœ ë¶„ì„ (ì§„ì§œ ë„ë©”ì¸ ë¡œì§)
    let (site_status, db_analysis) = crawling_planner.analyze_system_state().await
        .map_err(|e| format!("Failed to analyze system state: {}", e))?;
    
    info!("ğŸŒ [ACTOR] Real site analysis: {} pages, {} products on last page", 
          site_status.total_pages, site_status.products_on_last_page);
    info!("ğŸ’¾ [ACTOR] Real DB analysis: {} total products, {} unique products", 
          db_analysis.total_products, db_analysis.unique_products);
    
    // ğŸ¯ ì‹¤ì œ CrawlingPlannerë¡œ ì§€ëŠ¥í˜• ì „ëµ ê²°ì •
    let (range_recommendation, processing_strategy) = crawling_planner
        .determine_crawling_strategy(&site_status, &db_analysis)
        .await
        .map_err(|e| format!("Failed to determine crawling strategy: {}", e))?;
    
    info!("ğŸ“‹ [ACTOR] CrawlingPlanner recommendation: {:?}", range_recommendation);
    info!("âš™ï¸ [ACTOR] Processing strategy: batch_size={}, concurrency={}", 
          processing_strategy.recommended_batch_size, processing_strategy.recommended_concurrency);
    
    // ì§€ëŠ¥í˜• ë²”ìœ„ ê¶Œì¥ì‚¬í•­ì„ ì‹¤ì œ í˜ì´ì§€ ë²”ìœ„ë¡œ ë³€í™˜
    let (calculated_start_page, calculated_end_page) = match range_recommendation.to_page_range(site_status.total_pages) {
        Some((start, end)) => {
            // ğŸ”„ ì—­ìˆœ í¬ë¡¤ë§ìœ¼ë¡œ ë³€í™˜ (start > end)
            let reverse_start = if start > end { start } else { end };
            let reverse_end = if start > end { end } else { start };
            info!("ğŸ¯ [ACTOR] CrawlingPlanner range: {} to {} (reverse crawling)", reverse_start, reverse_end);
            (reverse_start, reverse_end)
        },
        None => {
            info!("ğŸ” [ACTOR] No crawling needed, using verification range");
            let verification_pages = app_config.user.crawling.page_range_limit.min(5);
            let start = site_status.total_pages;
            let end = if start >= verification_pages { start - verification_pages + 1 } else { 1 };
            (start, end)
        }
    };
    
    // ğŸš¨ ì„¤ì • ê¸°ë°˜ ë²”ìœ„ ì œí•œ ì ìš© (user.crawling.page_range_limit)
    let max_allowed_pages = app_config.user.crawling.page_range_limit;
    let requested_pages = if calculated_start_page >= calculated_end_page {
        calculated_start_page - calculated_end_page + 1
    } else {
        calculated_end_page - calculated_start_page + 1
    };
    
    let (final_start_page, final_end_page) = if requested_pages > max_allowed_pages {
        info!("âš ï¸ [ACTOR] CrawlingPlanner requested {} pages, but config limits to {} pages", 
              requested_pages, max_allowed_pages);
        // ì„¤ì • ì œí•œì— ë§ì¶° ë²”ìœ„ ì¡°ì •
        let limited_start = site_status.total_pages;
        let limited_end = if limited_start >= max_allowed_pages { 
            limited_start - max_allowed_pages + 1 
        } else { 
            1 
        };
        info!("ğŸ”’ [ACTOR] Range limited by config: {} to {} ({} pages)", 
              limited_start, limited_end, max_allowed_pages);
        (limited_start, limited_end)
    } else {
        // ğŸš¨ í”„ë¡ íŠ¸ì—”ë“œì—ì„œëŠ” By Designìœ¼ë¡œ í˜ì´ì§€ ë²”ìœ„ë¥¼ ì§€ì •í•˜ì§€ ì•ŠìŒ
        // ë”°ë¼ì„œ í•­ìƒ CrawlingPlanner ê¶Œì¥ì‚¬í•­ì„ ì‚¬ìš©
        info!("ğŸ§  [ACTOR] Frontend does not specify page ranges by design - using CrawlingPlanner recommendation");
        info!("ğŸ¤– [ACTOR] CrawlingPlanner recommendation: {} to {}", calculated_start_page, calculated_end_page);
        
        // âš ï¸ request.start_pageì™€ request.end_pageëŠ” í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸ ì½”ë“œì—ì„œ ì„¤ì •í•œ ì„ì‹œê°’ì´ë¯€ë¡œ ë¬´ì‹œ
        if request.start_page != 0 && request.end_page != 0 {
            info!("âš ï¸ [ACTOR] Ignoring frontend test values (start_page: {}, end_page: {}) - using intelligent planning", 
                  request.start_page, request.end_page);
        }
        
        // CrawlingPlanner ê¶Œì¥ì‚¬í•­ ì‚¬ìš©
        info!("ğŸ¯ [ACTOR] Using CrawlingPlanner intelligent recommendation for optimal crawling");
        (calculated_start_page, calculated_end_page)
    };
    
    info!("ğŸ§  [ACTOR] Final range calculated:");
    info!("   ğŸ“Š Range: {} to {} ({} pages, config limit: {})", 
          final_start_page, final_end_page, 
          if final_start_page >= final_end_page { final_start_page - final_end_page + 1 } else { final_end_page - final_start_page + 1 },
          app_config.user.crawling.page_range_limit);
    
    // ë¶„ì„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ êµ¬ì„±
    let analysis_info = serde_json::json!({
        "range_recommendation": format!("{:?}", range_recommendation),
        "user_requested": {
            "start_page": request.start_page,
            "end_page": request.end_page
        },
        "intelligent_calculated": {
            "start_page": calculated_start_page,
            "end_page": calculated_end_page
        },
        "final_used": {
            "start_page": final_start_page,
            "end_page": final_end_page
        },
        "site_analysis": {
            "total_pages": site_status.total_pages,
            "products_on_last_page": site_status.products_on_last_page,
            "estimated_products": site_status.estimated_products,
            "is_accessible": site_status.is_accessible
        },
        "processing_strategy": {
            "recommended_batch_size": processing_strategy.recommended_batch_size,
            "recommended_concurrency": processing_strategy.recommended_concurrency
        }
    });
    
    info!("âœ… Intelligent range calculation completed for Actor system");
    Ok((final_start_page, final_end_page, analysis_info))
}

/// ìˆœìˆ˜ Actor ê¸°ë°˜ SessionActor ì‹¤í–‰ (BatchActorë“¤ì„ ê´€ë¦¬)
async fn execute_session_actor_with_batches(
    session_id: &str,
    start_page: u32,
    end_page: u32,
    batch_size: u32,
    actor_event_tx: broadcast::Sender<AppEvent>,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ­ SessionActor {} starting with range {} to {}, batch_size: {}", 
          session_id, start_page, end_page, batch_size);
    
    // í˜ì´ì§€ ë²”ìœ„ë¥¼ BatchActorë“¤ì—ê²Œ ë°°ë¶„
    let pages: Vec<u32> = if start_page > end_page {
        (end_page..=start_page).rev().collect()
    } else {
        (start_page..=end_page).collect()
    };
    
    let total_pages = pages.len();
    let batch_count = (total_pages + batch_size as usize - 1) / batch_size as usize;
    
    info!("ğŸ“Š SessionActor will create {} BatchActors for {} pages", batch_count, total_pages);
    
    // SessionStarted ì´ë²¤íŠ¸ ë°œì†¡
    let session_event = AppEvent::SessionStarted {
        session_id: session_id.to_string(),
        config: CrawlingConfig {
            site_url: "https://csa-iot.org/csa-iot_products/".to_string(),
            start_page,
            end_page,
            concurrency_limit: 5,
            batch_size: batch_size,
            request_delay_ms: 1000,
            timeout_secs: 300,
            max_retries: 3,
        },
        timestamp: chrono::Utc::now(),
    };
    
    if let Err(e) = actor_event_tx.send(session_event) {
        error!("Failed to send SessionStarted event: {}", e);
    }
    
    // BatchActorë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (SessionActorì˜ ì—­í• )
    for (batch_index, page_chunk) in pages.chunks(batch_size as usize).enumerate() {
        let batch_id = format!("{}_batch_{}", session_id, batch_index);
        let batch_start = page_chunk[0];
        let batch_end = page_chunk[page_chunk.len() - 1];
        
        info!("ï¿½ SessionActor creating BatchActor {}: pages {} to {}", 
              batch_id, batch_start, batch_end);
        
        // BatchStarted ì´ë²¤íŠ¸ ë°œì†¡
        let batch_event = AppEvent::BatchStarted {
            session_id: session_id.to_string(),
            batch_id: batch_id.clone(),
            pages_count: page_chunk.len() as u32,
            timestamp: chrono::Utc::now(),
        };
        
        if let Err(e) = actor_event_tx.send(batch_event) {
            error!("Failed to send BatchStarted event: {}", e);
        }
        
        // TODO: ì‹¤ì œ BatchActor êµ¬í˜„ í˜¸ì¶œ
        // í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜
        match execute_batch_actor_simulation(&batch_id, page_chunk, actor_event_tx.clone()).await {
            Ok(()) => {
                info!("âœ… BatchActor {} completed successfully", batch_id);
                
                // BatchCompleted ì´ë²¤íŠ¸ ë°œì†¡
                let batch_completed_event = AppEvent::BatchCompleted {
                    session_id: session_id.to_string(),
                    batch_id: batch_id.clone(),
                    success_count: page_chunk.len() as u32,
                    failed_count: 0,
                    duration: 1000, // TODO: ì‹¤ì œ ì‹œê°„ ê³„ì‚°
                    timestamp: chrono::Utc::now(),
                };
                
                if let Err(e) = actor_event_tx.send(batch_completed_event) {
                    error!("Failed to send BatchCompleted event: {}", e);
                }
            }
            Err(e) => {
                error!("âŒ BatchActor {} failed: {}", batch_id, e);
                
                // BatchFailed ì´ë²¤íŠ¸ ë°œì†¡
                let batch_failed_event = AppEvent::BatchFailed {
                    session_id: session_id.to_string(),
                    batch_id: batch_id.clone(),
                    error: format!("{}", e),
                    final_failure: false,
                    timestamp: chrono::Utc::now(),
                };
                
                if let Err(e) = actor_event_tx.send(batch_failed_event) {
                    error!("Failed to send BatchFailed event: {}", e);
                }
                
                return Err(e);
            }
        }
        
        // ë°°ì¹˜ ê°„ ê°„ê²© (ì‹œìŠ¤í…œ ì•ˆì •ì„±)
        if batch_index < batch_count - 1 {
            tokio::time::sleep(Duration::from_millis(500)).await;
        }
    }
    
    // SessionCompleted ì´ë²¤íŠ¸ ë°œì†¡
    let session_completed_event = AppEvent::SessionCompleted {
        session_id: session_id.to_string(),
        summary: crate::new_architecture::actors::types::SessionSummary {
            session_id: session_id.to_string(),
            total_duration_ms: 5000, // TODO: ì‹¤ì œ ì‹œê°„ ê³„ì‚°
            total_pages_processed: total_pages as u32,
            total_products_processed: total_pages as u32 * 12, // ê·¼ì‚¬ì¹˜
            success_rate: 100.0, // TODO: ì‹¤ì œ ì„±ê³µë¥ 
            avg_page_processing_time: 1000, // TODO: ì‹¤ì œ í‰ê·  ì‹œê°„
            error_summary: vec![], // TODO: ì‹¤ì œ ì—ëŸ¬ ìš”ì•½
            processed_batches: batch_count as u32,
            total_success_count: total_pages as u32,
            final_state: "completed".to_string(),
            timestamp: chrono::Utc::now(),
        },
        timestamp: chrono::Utc::now(),
    };
    
    if let Err(e) = actor_event_tx.send(session_completed_event) {
        error!("Failed to send SessionCompleted event: {}", e);
    }
    
    info!("ğŸ‰ SessionActor {} completed all {} BatchActors successfully", session_id, batch_count);
    Ok(())
}

/// BatchActor ì‹œë®¬ë ˆì´ì…˜ (ë‚˜ì¤‘ì— ì‹¤ì œ êµ¬í˜„ìœ¼ë¡œ êµì²´)
async fn execute_batch_actor_simulation(
    batch_id: &str,
    pages: &[u32],
    actor_event_tx: broadcast::Sender<AppEvent>,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ¯ BatchActor {} simulating processing of {} pages", batch_id, pages.len());
    
    for (index, page) in pages.iter().enumerate() {
        info!("ğŸ” BatchActor {} processing page {}", batch_id, page);
        
        // ì§„í–‰ ìƒí™© ì´ë²¤íŠ¸ ë°œì†¡
        let progress_event = AppEvent::Progress {
            session_id: batch_id.split('_').take(2).collect::<Vec<_>>().join("_"),
            current_step: index as u32 + 1,
            total_steps: pages.len() as u32,
            message: format!("Processing page {}", page),
            percentage: ((index + 1) as f64 / pages.len() as f64) * 100.0,
            timestamp: chrono::Utc::now(),
        };
        
        if let Err(e) = actor_event_tx.send(progress_event) {
            error!("Failed to send Progress event: {}", e);
        }
        
        // ì‹œë®¬ë ˆì´ì…˜ ì²˜ë¦¬ ì‹œê°„
        tokio::time::sleep(Duration::from_millis(200)).await;
        
        info!("âœ… BatchActor {} completed page {}", batch_id, page);
    }
    
    info!("âœ… BatchActor {} completed all {} pages", batch_id, pages.len());
    Ok(())
}

/// ì‹œë®¬ë ˆì´ì…˜ í¬ë¡¤ë§ ì‹¤í–‰ (í´ë°±)
async fn run_simulation_crawling(
    request: &ActorCrawlingRequest,
    batch_size: u32,
) {
    info!("ğŸ”„ Running simulation crawling as fallback...");
    
    // í˜ì´ì§€ ë²”ìœ„ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
    let mut current_page = request.start_page;
    let mut batch_number = 1;
    
    while current_page <= request.end_page {
        let batch_end = std::cmp::min(current_page + batch_size - 1, request.end_page);
        info!("ğŸ“¦ Processing Batch {}: pages {} to {}", batch_number, current_page, batch_end);
        
        // ë°°ì¹˜ë³„ í˜ì´ì§€ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        for page in current_page..=batch_end {
            info!("ğŸ” Processing page {} with simulated crawling", page);
            
            // ì‹œë®¬ë ˆì´ì…˜ ì§€ì—° ì‹œê°„
            tokio::time::sleep(Duration::from_millis(request.delay_ms.unwrap_or(800))).await;
            
            info!("âœ… Page {} processed successfully", page);
        }
        
        info!("âœ… Batch {} completed", batch_number);
        current_page = batch_end + 1;
        batch_number += 1;
    }
    
    info!("ğŸ‰ Simulation crawling completed successfully");
}
