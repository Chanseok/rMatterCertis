//! Actor System Commands for Tauri Integration
//! 
//! Commands to test and use the Actor system from the UI

use crate::new_architecture::actors::SessionActor;
use crate::new_architecture::context::{SystemConfig, AppContext};
use crate::new_architecture::channels::types::AppEvent;
use crate::new_architecture::channels::types::ActorCommand; // ì˜¬ë°”ë¥¸ ActorCommand ì‚¬ìš©
use crate::new_architecture::actors::types::{CrawlingConfig, BatchConfig, ExecutionPlan, PageRange, SessionSummary, CrawlPhase};
use crate::new_architecture::actor_event_bridge::start_actor_event_bridge;
use crate::infrastructure::config::AppConfig;
use crate::domain::services::SiteStatus;
use crate::infrastructure::simple_http_client::HttpClient;
use crate::infrastructure::html_parser::MatterDataExtractor;
use crate::infrastructure::integrated_product_repository::IntegratedProductRepository;
use crate::application::{AppState, shared_state::SharedStateCache};
use crate::domain::services::crawling_services::{SiteStatus as DomainSiteStatus, SiteDataChangeStatus, CrawlingRangeRecommendation};
use tauri::State; // For accessing managed state
 // ì‹¤ì œ CrawlingPlannerì—ì„œ ì‚¬ìš©
use crate::infrastructure::config::ConfigManager; // ì„¤ì • ê´€ë¦¬ì ì¶”ê°€
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tauri::{AppHandle, Manager};
use tokio::sync::{mpsc, broadcast, watch};
use tokio::time::Duration;
use tracing::{info, error, warn};
use chrono::Utc;
use once_cell::sync::OnceCell;

// Global shutdown signal sender for phase runner / session
static PHASE_SHUTDOWN_TX: OnceCell<watch::Sender<bool>> = OnceCell::new();

// (Removed temporary phase runner + single batch helper after unifying execution path)

/// Actor System State managed by Tauri
#[derive(Default)]
pub struct ActorSystemState {
    pub is_running: Arc<tokio::sync::RwLock<bool>>,
}

/// Actor System Response
#[derive(Debug, Serialize, Deserialize)]
pub struct ActorSystemResponse {
    pub success: bool,
    pub message: String,
    pub session_id: Option<String>,
    pub data: Option<serde_json::Value>,
}

/// Crawling Request for Actor System
#[derive(Debug, Serialize, Deserialize)]
pub struct ActorCrawlingRequest {
    pub start_page: u32,
    pub end_page: u32,
    pub concurrency: Option<u32>,
    pub batch_size: Option<u32>,
    pub delay_ms: Option<u64>,
}

/// ğŸ­ Actor System í¬ë¡¤ë§ ì‹œì‘ (ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ - ì›Œí¬í”Œë¡œìš° í†µí•©)
/// 
/// ë¶„ì„-ê³„íš-ì‹¤í–‰ ì›Œí¬í”Œë¡œìš°ë¥¼ ë‹¨ì¼í™”:
/// 1. CrawlingPlannerë¥¼ ë‹¨ í•œ ë²ˆë§Œ í˜¸ì¶œí•˜ì—¬ ExecutionPlan ìƒì„±
/// 2. SessionActorëŠ” ExecutionPlanì„ ë°›ì•„ì„œ ìˆœìˆ˜ ì‹¤í–‰ë§Œ ë‹´ë‹¹
/// 3. UI íŒŒë¼ë¯¸í„° ì˜ì¡´ì„± ì œê±° - ì„¤ì • íŒŒì¼ ê¸°ë°˜ ììœ¨ ìš´ì˜
#[tauri::command]
pub async fn start_actor_system_crawling(
    app: AppHandle,
    _request: ActorCrawlingRequest, // UI íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì‹œ (ì„¤ê³„ ì›ì¹™ì— ë”°ë¼)
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ­ [NEW ARCHITECTURE] Starting unified Analysis-Plan-Execute workflow");
    
    // === Phase 1: ë¶„ì„ ë° ê³„íš (CrawlingPlanner ë‹¨ì¼ í˜¸ì¶œ) ===
    info!("ğŸ§  Phase 1: Creating ExecutionPlan with CrawlingPlanner...");
    
    let (execution_plan, app_config, site_status) = create_execution_plan(&app).await
        .map_err(|e| format!("Failed to create execution plan: {}", e))?;
    
    info!("âœ… ExecutionPlan created: {} batches, {} total pages", 
          execution_plan.crawling_ranges.len(),
          execution_plan.crawling_ranges.iter().map(|r| 
              if r.reverse_order { r.start_page - r.end_page + 1 } 
              else { r.end_page - r.start_page + 1 }
          ).sum::<u32>());
    
    // === Phase 2: ì‹¤í–‰ (SessionActorì— ExecutionPlan ì „ë‹¬) ===
    info!("ğŸ­ Phase 2: Executing with SessionActor...");
    
    // ğŸŒ‰ Actor ì´ë²¤íŠ¸ ë¸Œë¦¿ì§€ë¥¼ ìœ„í•œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì±„ë„ ìƒì„±
    let (actor_event_tx, actor_event_rx) = broadcast::channel::<AppEvent>(1000);
    
    // ğŸŒ‰ Actor Event Bridge ì‹œì‘ - Actor ì´ë²¤íŠ¸ë¥¼ í”„ë¡ íŠ¸ì—”ë“œë¡œ ìë™ ì „ë‹¬
    let _bridge_handle = start_actor_event_bridge(app.clone(), actor_event_rx)
        .await
        .map_err(|e| format!("Failed to start Actor Event Bridge: {}", e))?;
    
    info!("ğŸŒ‰ Actor Event Bridge started successfully");

    // SessionActor ìƒì„±
    let _session_actor = SessionActor::new(execution_plan.session_id.clone());
    
    info!("ğŸ­ SessionActor created with ID: {}", execution_plan.session_id);
    
    // ExecutionPlan ê¸°ë°˜ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
    let execution_plan_for_task_main = execution_plan.clone();
    let execution_plan_for_return = execution_plan.clone();
    let app_config_for_task = app_config.clone();
    let site_status_for_task = site_status.clone();
    let actor_event_tx_for_spawn = actor_event_tx.clone();
    let session_id_for_return = execution_plan.session_id.clone();
    
    let (shutdown_req_tx, shutdown_req_rx) = watch::channel(false);
    let _ = PHASE_SHUTDOWN_TX.set(shutdown_req_tx.clone()); // ignore if already set

    let _session_actor_handle = tokio::spawn(async move {
        info!("ğŸš€ SessionActor executing with predefined ExecutionPlan (phased)...");
        
        // SessionActorëŠ” ë” ì´ìƒ ë¶„ì„/ê³„íší•˜ì§€ ì•Šê³  ìˆœìˆ˜ ì‹¤í–‰ë§Œ
        // Phase sequence definition (extensible)
        let phases = vec![
            CrawlPhase::ListPages,
            // Future: CrawlPhase::ProductDetails,
            // Future: CrawlPhase::DataValidation,
            CrawlPhase::Finalize,
        ];

        let session_id_clone = execution_plan_for_task_main.session_id.clone();
    let total_phase_start = std::time::Instant::now();
        for phase in phases {
            if *shutdown_req_rx.borrow() { 
                let _ = actor_event_tx_for_spawn.send(AppEvent::PhaseAborted { session_id: session_id_clone.clone(), phase: phase.clone(), reason: "shutdown_requested".into(), timestamp: Utc::now() });
                break;
            }
            let phase_started_at = std::time::Instant::now();
            let _ = actor_event_tx_for_spawn.send(AppEvent::PhaseStarted { session_id: session_id_clone.clone(), phase: phase.clone(), timestamp: Utc::now() });
            let phase_res = match phase {
                CrawlPhase::ListPages => execute_session_actor_with_execution_plan(
                    execution_plan_for_task_main.clone(),
                    &app_config_for_task,
                    &site_status_for_task,
                    actor_event_tx_for_spawn.clone(),
                ).await.map(|_| true),
                CrawlPhase::Finalize => { info!("ğŸ§¹ Finalize phase placeholder"); Ok(true) },
                CrawlPhase::ProductDetails | CrawlPhase::DataValidation => { info!("(Phase not yet implemented: {:?})", phase); Ok(true) }
            };
            let duration_ms = phase_started_at.elapsed().as_millis() as u64;
            match phase_res {
                Ok(ok) => {
                    let _ = actor_event_tx_for_spawn.send(AppEvent::PhaseCompleted { session_id: session_id_clone.clone(), phase: phase.clone(), succeeded: ok, duration_ms, timestamp: Utc::now() });
                }
                Err(e) => {
                    error!("Phase {:?} failed: {}", phase, e);
                    let _ = actor_event_tx_for_spawn.send(AppEvent::PhaseAborted { session_id: session_id_clone.clone(), phase: phase.clone(), reason: format!("{}", e), timestamp: Utc::now() });
                    break;
                }
            }
        }
        info!("ğŸ‰ Actor system phase sequence finished in {} ms", total_phase_start.elapsed().as_millis());

    // (PhaseRunner fallback removed: execute_session_actor_with_execution_plan now covers all ranges)
        
        Ok::<(), Box<dyn std::error::Error + Send + Sync>>(())
    });
    
    // ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜ (ë¹„ë™ê¸° ì‹¤í–‰)
    Ok(ActorSystemResponse {
        success: true,
        message: "Actor system crawling started with ExecutionPlan".to_string(),
        session_id: Some(session_id_for_return),
    data: Some(serde_json::to_value(&execution_plan_for_return).map_err(|e| e.to_string())?),
    })
}

/// ìš”ì²­: í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì„¸ì…˜ì— Graceful Shutdown ì‹ í˜¸ ì „ì†¡
#[tauri::command]
pub async fn request_graceful_shutdown(app: AppHandle) -> Result<ActorSystemResponse, String> {
    if let Some(tx) = PHASE_SHUTDOWN_TX.get() {
        if tx.send(true).is_err() { return Err("Failed to send shutdown signal".into()); }
        // Emit ShutdownRequested event via broadcast if bridge exists (best-effort)
        if let Some(state) = app.try_state::<AppState>() { let _ = state; }
        let now = Utc::now();
        // We don't hold a broadcast handle here; Session loop will emit PhaseAborted + SessionCompleted/Failed
        info!("ğŸ›‘ Graceful shutdown requested at {}", now);
        Ok(ActorSystemResponse { success: true, message: "Graceful shutdown signal sent".into(), session_id: None, data: None })
    } else {
        Err("No active session to shutdown".into())
    }
}

// (Removed deprecated ServiceBasedBatchCrawlingEngine command block)

/// Test SessionActor functionality
#[tauri::command]
pub async fn test_session_actor_basic(
    _app: AppHandle,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ§ª Testing SessionActor...");
    
    let _system_config = Arc::new(SystemConfig::default());
    let (_control_tx, _control_rx) = mpsc::channel::<ActorCommand>(100);
    let (_event_tx, _event_rx) = mpsc::channel::<AppEvent>(500);
    
    let _session_actor = SessionActor::new(
        format!("session_{}", chrono::Utc::now().timestamp())
    );
    
    info!("âœ… SessionActor created successfully");
    
    Ok(ActorSystemResponse {
        success: true,
        message: "SessionActor test completed successfully".to_string(),
        session_id: Some(format!("test_session_{}", Utc::now().timestamp())),
        data: None,
    })
}

/// Test Actor integration
#[tauri::command]
pub async fn test_actor_integration_basic(
    _app: AppHandle,
) -> Result<ActorSystemResponse, String> {
    info!("ğŸ§ª Testing Actor system integration...");
    
    // Integration test - create full system
    let _system_config = Arc::new(SystemConfig::default());
    let (_control_tx, _control_rx) = mpsc::channel::<ActorCommand>(100);
    let (_event_tx, _event_rx) = mpsc::channel::<AppEvent>(500);
    
    let _session_actor = SessionActor::new(
        format!("session_{}", chrono::Utc::now().timestamp())
    );
    
    // Test actor system flow
    tokio::select! {
        _ = tokio::time::sleep(Duration::from_millis(100)) => {
            info!("âœ… Actor integration test completed within timeout");
        }
    }
    
    Ok(ActorSystemResponse {
        success: true,
        message: "Actor integration test completed successfully".to_string(),
        session_id: Some(format!("integration_test_{}", Utc::now().timestamp())),
        data: Some(serde_json::json!({
            "integration_status": "success",
            "components_tested": ["SessionActor", "channels", "configuration"]
        })),
    })
}

/// CrawlingPlanner ê¸°ë°˜ ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚° (Actor ì‹œìŠ¤í…œìš©)
#[allow(dead_code)]
async fn calculate_intelligent_crawling_range(
    session_id: &str,
    request: &ActorCrawlingRequest,
    app_handle: &AppHandle,
) -> Result<(u32, u32, serde_json::Value), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ§  Calculating intelligent crawling range for Actor system session: {}", session_id);
    
    // ì•± ìƒíƒœì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ í’€ ê°€ì ¸ì˜¤ê¸°
    let app_state = app_handle.state::<AppState>();
    let db_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard.as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };
    
    // IntegratedProductRepository ìƒì„±
    let product_repo = Arc::new(IntegratedProductRepository::new(db_pool));
    
    // HTTP í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    let http_client = HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    
    // ë°ì´í„° ì¶”ì¶œê¸° ìƒì„±
    let data_extractor = MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    // ğŸ§  ì‹¤ì œ ì„¤ì • íŒŒì¼ ë¡œë“œ ë° CrawlingPlanner ì‚¬ìš©
    info!("ğŸ§  [ACTOR] Loading configuration and using CrawlingPlanner for intelligent analysis...");
    
    // ì‹¤ì œ ì•± ì„¤ì • ë¡œë“œ (ê¸°ë³¸ê°’ ëŒ€ì‹ )
    let config_manager = crate::infrastructure::config::ConfigManager::new()
        .map_err(|e| format!("Failed to initialize config manager: {}", e))?;
    let app_config = config_manager.load_config().await
        .map_err(|e| format!("Failed to load config: {}", e))?;
    
    info!("ğŸ“‹ [ACTOR] Configuration loaded: page_range_limit={}, batch_size={}, max_concurrent={}", 
          app_config.user.crawling.page_range_limit, 
          app_config.user.batch.batch_size,
          app_config.user.max_concurrent_requests);
    
    // StatusChecker ìƒì„± (ì‹¤ì œ ì„¤ì • ì‚¬ìš©)
    let status_checker_impl = crate::infrastructure::crawling_service_impls::StatusCheckerImpl::new(
        http_client.clone(),
        data_extractor.clone(),
        app_config.clone(),
    );
    let status_checker = Arc::new(status_checker_impl);
    
    // DatabaseAnalyzer ìƒì„± (ì‹¤ì œ DB ë¶„ì„)
    let db_analyzer = Arc::new(crate::infrastructure::crawling_service_impls::DatabaseAnalyzerImpl::new(
        product_repo.clone(),
    ));
    
    // SystemConfigë¡œ ë³€í™˜ (CrawlingPlannerìš©)
    let system_config = Arc::new(crate::new_architecture::context::SystemConfig::default());
    
    // ğŸš€ ì‹¤ì œ CrawlingPlanner ì‚¬ìš©!
    let crawling_planner = crate::new_architecture::services::crawling_planner::CrawlingPlanner::new(
        status_checker.clone(),
        db_analyzer.clone(),
        system_config.clone(),
    ).with_repository(product_repo.clone());
    
    // ì‹œìŠ¤í…œ ìƒíƒœ ë¶„ì„ (ì§„ì§œ ë„ë©”ì¸ ë¡œì§)
    let (site_status, db_analysis) = crawling_planner.analyze_system_state().await
        .map_err(|e| format!("Failed to analyze system state: {}", e))?;
    
    info!("ğŸŒ [ACTOR] Real site analysis: {} pages, {} products on last page", 
          site_status.total_pages, site_status.products_on_last_page);
    info!("ğŸ’¾ [ACTOR] Real DB analysis: {} total products, {} unique products", 
          db_analysis.total_products, db_analysis.unique_products);
    
    // ğŸ¯ ì‹¤ì œ CrawlingPlannerë¡œ ì§€ëŠ¥í˜• ì „ëµ ê²°ì •
    let (range_recommendation, processing_strategy) = crawling_planner
        .determine_crawling_strategy(&site_status, &db_analysis)
        .await
        .map_err(|e| format!("Failed to determine crawling strategy: {}", e))?;
    
    info!("ğŸ“‹ [ACTOR] CrawlingPlanner recommendation: {:?}", range_recommendation);
    info!("âš™ï¸ [ACTOR] Processing strategy: batch_size={}, concurrency={}", 
          processing_strategy.recommended_batch_size, processing_strategy.recommended_concurrency);
    
    // ì§€ëŠ¥í˜• ë²”ìœ„ ê¶Œì¥ì‚¬í•­ì„ ì‹¤ì œ í˜ì´ì§€ ë²”ìœ„ë¡œ ë³€í™˜
    let (calculated_start_page, calculated_end_page) = match range_recommendation.to_page_range(site_status.total_pages) {
        Some((start, end)) => {
            // ğŸ”„ ì—­ìˆœ í¬ë¡¤ë§ìœ¼ë¡œ ë³€í™˜ (start > end)
            let reverse_start = if start > end { start } else { end };
            let reverse_end = if start > end { end } else { start };
            info!("ğŸ¯ [ACTOR] CrawlingPlanner range: {} to {} (reverse crawling)", reverse_start, reverse_end);
            (reverse_start, reverse_end)
        },
        None => {
            info!("ğŸ” [ACTOR] No crawling needed, using verification range");
            let verification_pages = app_config.user.crawling.page_range_limit.min(5);
            let start = site_status.total_pages;
            let end = if start >= verification_pages { start - verification_pages + 1 } else { 1 };
            (start, end)
        }
    };
    
    // ğŸš¨ ì„¤ì • ê¸°ë°˜ ë²”ìœ„ ì œí•œ ì ìš© (user.crawling.page_range_limit)
    let max_allowed_pages = app_config.user.crawling.page_range_limit;
    let requested_pages = if calculated_start_page >= calculated_end_page {
        calculated_start_page - calculated_end_page + 1
    } else {
        calculated_end_page - calculated_start_page + 1
    };
    
    let (final_start_page, final_end_page) = if requested_pages > max_allowed_pages {
        info!("âš ï¸ [ACTOR] CrawlingPlanner requested {} pages, but config limits to {} pages", 
              requested_pages, max_allowed_pages);
        // ì„¤ì • ì œí•œì— ë§ì¶° ë²”ìœ„ ì¡°ì •
        let limited_start = site_status.total_pages;
        let limited_end = if limited_start >= max_allowed_pages { 
            limited_start - max_allowed_pages + 1 
        } else { 
            1 
        };
        info!("ğŸ”’ [ACTOR] Range limited by config: {} to {} ({} pages)", 
              limited_start, limited_end, max_allowed_pages);
        (limited_start, limited_end)
    } else {
        // ğŸš¨ í”„ë¡ íŠ¸ì—”ë“œì—ì„œëŠ” By Designìœ¼ë¡œ í˜ì´ì§€ ë²”ìœ„ë¥¼ ì§€ì •í•˜ì§€ ì•ŠìŒ
        // ë”°ë¼ì„œ í•­ìƒ CrawlingPlanner ê¶Œì¥ì‚¬í•­ì„ ì‚¬ìš©
        info!("ğŸ§  [ACTOR] Frontend does not specify page ranges by design - using CrawlingPlanner recommendation");
        info!("ğŸ¤– [ACTOR] CrawlingPlanner recommendation: {} to {}", calculated_start_page, calculated_end_page);
        
        // âš ï¸ request.start_pageì™€ request.end_pageëŠ” í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸ ì½”ë“œì—ì„œ ì„¤ì •í•œ ì„ì‹œê°’ì´ë¯€ë¡œ ë¬´ì‹œ
        if request.start_page != 0 && request.end_page != 0 {
            info!("âš ï¸ [ACTOR] Ignoring frontend test values (start_page: {}, end_page: {}) - using intelligent planning", 
                  request.start_page, request.end_page);
        }
        
        // CrawlingPlanner ê¶Œì¥ì‚¬í•­ ì‚¬ìš©
        info!("ğŸ¯ [ACTOR] Using CrawlingPlanner intelligent recommendation for optimal crawling");
        (calculated_start_page, calculated_end_page)
    };
    
    info!("ğŸ§  [ACTOR] Final range calculated:");
    info!("   ğŸ“Š Range: {} to {} ({} pages, config limit: {})", 
          final_start_page, final_end_page, 
          if final_start_page >= final_end_page { final_start_page - final_end_page + 1 } else { final_end_page - final_start_page + 1 },
          app_config.user.crawling.page_range_limit);
    
    // ë¶„ì„ ì •ë³´ë¥¼ JSONìœ¼ë¡œ êµ¬ì„±
    let analysis_info = serde_json::json!({
        "range_recommendation": format!("{:?}", range_recommendation),
        "user_requested": {
            "start_page": request.start_page,
            "end_page": request.end_page
        },
        "intelligent_calculated": {
            "start_page": calculated_start_page,
            "end_page": calculated_end_page
        },
        "final_used": {
            "start_page": final_start_page,
            "end_page": final_end_page
        },
        "site_analysis": {
            "total_pages": site_status.total_pages,
            "products_on_last_page": site_status.products_on_last_page,
            "estimated_products": site_status.estimated_products,
            "is_accessible": site_status.is_accessible
        },
        "processing_strategy": {
            "recommended_batch_size": processing_strategy.recommended_batch_size,
            "recommended_concurrency": processing_strategy.recommended_concurrency
        }
    });
    
    info!("âœ… Intelligent range calculation completed for Actor system");
    Ok((final_start_page, final_end_page, analysis_info))
}

/// ìˆœìˆ˜ Actor ê¸°ë°˜ SessionActor ì‹¤í–‰ (BatchActorë“¤ì„ ê´€ë¦¬)
async fn execute_session_actor_with_batches(
    session_id: &str,
    start_page: u32,
    end_page: u32,
    batch_size: u32,
    app_config: &AppConfig,
    site_status: &SiteStatus,
    actor_event_tx: broadcast::Sender<AppEvent>,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ­ SessionActor {} starting with range {} to {}, batch_size: {}", 
          session_id, start_page, end_page, batch_size);
    
    // AppContext ìƒì„±ì— í•„ìš”í•œ ì±„ë„ë“¤ ìƒì„±
    let system_config = Arc::new(SystemConfig::default());
    let (control_tx, _control_rx) = mpsc::channel::<ActorCommand>(100);
    let (_cancellation_tx, cancellation_rx) = watch::channel(false);
    
    // AppContext ìƒì„± (ì‹¤ì œë¡œëŠ” IntegratedContext::new í˜¸ì¶œ)
    let context = Arc::new(AppContext::new(
        session_id.to_string(),
        control_tx,
        actor_event_tx.clone(),
        cancellation_rx,
        system_config,
    ));
    
    // í˜ì´ì§€ ë²”ìœ„ë¥¼ BatchActorë“¤ì—ê²Œ ë°°ë¶„
    let pages: Vec<u32> = if start_page > end_page {
        (end_page..=start_page).rev().collect()
    } else {
        (start_page..=end_page).collect()
    };
    
    let total_pages = pages.len();
    let batch_count = (total_pages + batch_size as usize - 1) / batch_size as usize;
    
    info!("ğŸ“Š SessionActor will create {} BatchActors for {} pages", batch_count, total_pages);
    
    // SessionStarted ì´ë²¤íŠ¸ ë°œì†¡ (ì„¤ì • íŒŒì¼ ê¸°ë°˜ ê°’ ì‚¬ìš©)
    let session_event = AppEvent::SessionStarted {
        session_id: session_id.to_string(),
        config: CrawlingConfig {
            site_url: "https://csa-iot.org/csa-iot_products/".to_string(),
            start_page,
            end_page,
            // Align with global max concurrency to avoid mixed values in logs
            concurrency_limit: app_config.user.max_concurrent_requests,
            batch_size: batch_size,
            request_delay_ms: app_config.user.request_delay_ms,
            timeout_secs: app_config.advanced.request_timeout_seconds,
            max_retries: app_config.advanced.retry_attempts,
            strategy: crate::new_architecture::actors::types::CrawlingStrategy::NewestFirst,
        },
        timestamp: chrono::Utc::now(),
    };
    
    if let Err(e) = actor_event_tx.send(session_event) {
        error!("Failed to send SessionStarted event: {}", e);
    }
    
    // BatchActorë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (SessionActorì˜ ì—­í• )
    for (batch_index, page_chunk) in pages.chunks(batch_size as usize).enumerate() {
        let batch_id = format!("{}_batch_{}", session_id, batch_index);
        let batch_start = page_chunk[0];
        let batch_end = page_chunk[page_chunk.len() - 1];
        
        info!("ï¿½ SessionActor creating BatchActor {}: pages {} to {}", 
              batch_id, batch_start, batch_end);
        
        // BatchStarted ì´ë²¤íŠ¸ ë°œì†¡
        let batch_event = AppEvent::BatchStarted {
            session_id: session_id.to_string(),
            batch_id: batch_id.clone(),
            pages_count: page_chunk.len() as u32,
            timestamp: chrono::Utc::now(),
        };
        
        if let Err(e) = actor_event_tx.send(batch_event) {
            error!("Failed to send BatchStarted event: {}", e);
        }
        
        // âœ… ì‹¤ì œ BatchActor êµ¬í˜„ í˜¸ì¶œ 
        info!("ğŸš€ About to call execute_real_batch_actor for batch: {}", batch_id);
    match execute_real_batch_actor(&batch_id, page_chunk, &context, app_config, site_status).await {
            Ok(()) => {
                info!("âœ… BatchActor {} completed successfully", batch_id);
                
                // BatchCompleted ì´ë²¤íŠ¸ ë°œì†¡
                let batch_completed_event = AppEvent::BatchCompleted {
                    session_id: session_id.to_string(),
                    batch_id: batch_id.clone(),
                    success_count: page_chunk.len() as u32,
                    failed_count: 0,
                    duration: 1000, // TODO: ì‹¤ì œ ì‹œê°„ ê³„ì‚°
                    timestamp: chrono::Utc::now(),
                };
                
                if let Err(e) = actor_event_tx.send(batch_completed_event) {
                    error!("Failed to send BatchCompleted event: {}", e);
                }
            }
            Err(e) => {
                error!("âŒ BatchActor {} failed with error: {:?}", batch_id, e);
                error!("âŒ Error details: {}", e);
                
                // BatchFailed ì´ë²¤íŠ¸ ë°œì†¡
                let batch_failed_event = AppEvent::BatchFailed {
                    session_id: session_id.to_string(),
                    batch_id: batch_id.clone(),
                    error: format!("{}", e),
                    final_failure: false,
                    timestamp: chrono::Utc::now(),
                };
                
                if let Err(e) = actor_event_tx.send(batch_failed_event) {
                    error!("Failed to send BatchFailed event: {}", e);
                }
                
                return Err(e);
            }
        }
        
        // ë°°ì¹˜ ê°„ ê°„ê²© (ì‹œìŠ¤í…œ ì•ˆì •ì„±)
        if batch_index < batch_count - 1 {
            tokio::time::sleep(Duration::from_millis(500)).await;
        }
    }
    
    // SessionCompleted ì´ë²¤íŠ¸ ë°œì†¡
    let session_completed_event = AppEvent::SessionCompleted {
        session_id: session_id.to_string(),
        summary: crate::new_architecture::actors::types::SessionSummary {
            session_id: session_id.to_string(),
            total_duration_ms: 5000, // TODO: ì‹¤ì œ ì‹œê°„ ê³„ì‚°
            total_pages_processed: total_pages as u32,
            total_products_processed: total_pages as u32 * 12, // ê·¼ì‚¬ì¹˜
            success_rate: 100.0, // TODO: ì‹¤ì œ ì„±ê³µë¥ 
            avg_page_processing_time: 1000, // TODO: ì‹¤ì œ í‰ê·  ì‹œê°„
            error_summary: vec![], // TODO: ì‹¤ì œ ì—ëŸ¬ ìš”ì•½
            processed_batches: batch_count as u32,
            total_success_count: total_pages as u32,
            duplicates_skipped: 0,
            final_state: "completed".to_string(),
            timestamp: chrono::Utc::now(),
        },
        timestamp: chrono::Utc::now(),
    };
    
    if let Err(e) = actor_event_tx.send(session_completed_event) {
        error!("Failed to send SessionCompleted event: {}", e);
    }
    
    info!("ğŸ‰ SessionActor {} completed all {} BatchActors successfully", session_id, batch_count);
    Ok(())
}

/// ì‹¤ì œ BatchActor ì‹¤í–‰
async fn execute_real_batch_actor(
    batch_id: &str,
    pages: &[u32],
    context: &AppContext,
    app_config: &AppConfig,
    site_status: &SiteStatus,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    use crate::new_architecture::actors::{BatchActor, ActorCommand};
    use crate::new_architecture::actors::traits::Actor;
    use tokio::sync::mpsc;
    
    info!("ğŸ¯ BatchActor {} starting REAL processing of {} pages", batch_id, pages.len());
    info!("ğŸ”§ Creating BatchActor instance with real services...");
    
    // ğŸ”¥ Phase 1: ì‹¤ì œ ì„œë¹„ìŠ¤ë“¤ ìƒì„± ë° ì£¼ì…
    use crate::infrastructure::{HttpClient, MatterDataExtractor};
    // AppConfig type is provided via function parameter; no local import needed
    use crate::infrastructure::IntegratedProductRepository;
    use std::sync::Arc;
    
    // HttpClient ìƒì„±
    let http_client = Arc::new(
        HttpClient::create_from_global_config()
            .map_err(|e| format!("Failed to create HttpClient: {}", e))?
            .with_context_label(&format!("BatchActor:{}", batch_id))
    );
    info!("âœ… HttpClient created (labeled)");
    
    // MatterDataExtractor ìƒì„±  
    let data_extractor = Arc::new(MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create MatterDataExtractor: {}", e))?);
    info!("âœ… MatterDataExtractor created");
    
    // IntegratedProductRepository ìƒì„±
    use crate::infrastructure::DatabaseConnection;
    let database_url = crate::infrastructure::database_paths::get_main_database_url();
    info!("ğŸ”§ Using database URL: {}", database_url);
    let db_connection = DatabaseConnection::new(&database_url).await
        .map_err(|e| format!("Failed to create DatabaseConnection: {}", e))?;
    let product_repo = Arc::new(IntegratedProductRepository::new(db_connection.pool().clone()));
    info!("âœ… IntegratedProductRepository created with centralized database path");
    
    // AppConfig ì‚¬ìš©: ExecutionPlan ê²½ë¡œì—ì„œ ë¡œë“œí•œ ì„¤ì • ì‚¬ìš© (ê°œë°œ ê¸°ë³¸ê°’ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    let app_config = app_config.clone();
    // Clone once more for passing into BatchActor::new_with_services (it takes ownership)
    let app_config_for_actor = app_config.clone();
    info!("âœ… AppConfig provided from ExecutionPlan context");
    
    // AppConfigì—ì„œ ì‹¤ì œ batch_size ë¯¸ë¦¬ ì¶”ì¶œ (app_configì´ moveë˜ê¸° ì „ì—)
    let user_batch_size = app_config.user.batch.batch_size;
    info!("ğŸ“Š Using batch_size from config: {}", user_batch_size);
    
    // BatchActorë¥¼ ì‹¤ì œ ì„œë¹„ìŠ¤ë“¤ê³¼ í•¨ê»˜ ìƒì„±
    let mut batch_actor = BatchActor::new_with_services(
        batch_id.to_string(),
        batch_id.to_string(), // batch_idë„ ê°™ì´ ì „ë‹¬
        http_client,
        data_extractor,
        product_repo,
        app_config_for_actor,
    );
    info!("âœ… BatchActor created successfully with real services");
    
    // BatchActor ì‹¤í–‰ì„ ìœ„í•œ ì±„ë„ ìƒì„±
    info!("ğŸ”§ Creating communication channels...");
    let (command_tx, command_rx) = mpsc::channel::<ActorCommand>(100);
    info!("âœ… Channels created successfully");
    
    // ProcessBatch ëª…ë ¹ ìƒì„±
    info!("ğŸ”§ Creating BatchConfig...");
    
    let batch_config = BatchConfig {
        batch_size: user_batch_size,
        // Use the app-level max concurrency for batch execution to match plan/session
        concurrency_limit: app_config.user.max_concurrent_requests,
        batch_delay_ms: 1000,
        retry_on_failure: true,
        start_page: Some(pages[0]),
        end_page: Some(pages[pages.len() - 1]),
    };
    info!("âœ… BatchConfig created: {:?}", batch_config);
    
    info!("ğŸ”§ Creating ProcessBatch command...");
    let process_batch_cmd = ActorCommand::ProcessBatch {
        batch_id: batch_id.to_string(),
        pages: pages.to_vec(),
        config: batch_config,
        batch_size: user_batch_size,
        concurrency_limit: app_config.user.max_concurrent_requests,
        total_pages: site_status.total_pages,
        products_on_last_page: site_status.products_on_last_page,
    };
    info!("âœ… ProcessBatch command created");
    
    // BatchActor ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘
    info!("ğŸš€ Starting BatchActor task...");
    let context_clone = context.clone();
    let batch_task = tokio::spawn(async move {
        info!("ğŸ“¡ BatchActor.run() starting...");
        let result = batch_actor.run(context_clone, command_rx).await;
        info!("ğŸ“¡ BatchActor.run() completed with result: {:?}", result);
        result
    });
    info!("âœ… BatchActor task spawned");
    
    // ProcessBatch ëª…ë ¹ ì „ì†¡
    info!("ğŸ“¡ Sending ProcessBatch command...");
    command_tx.send(process_batch_cmd).await
        .map_err(|e| format!("Failed to send ProcessBatch command: {}", e))?;
    info!("âœ… ProcessBatch command sent");
    
    // Shutdown ëª…ë ¹ì€ ëª¨ë“  ì‘ì—…ì´ ìì—° ì¢…ë£Œë  ë•Œê¹Œì§€ ì§€ì—° (ë‹¤ìŒ phase/ë°°ì¹˜ ì „í™˜ ë¡œì§ì—ì„œ ê²°ì •)
    info!("â³ Waiting for BatchActor completion (deferred shutdown)...");
    batch_task.await
        .map_err(|e| format!("BatchActor task failed: {}", e))?
        .map_err(|e| format!("BatchActor execution failed: {:?}", e))?;
    
    info!("âœ… BatchActor {} completed REAL processing of {} pages", batch_id, pages.len());
    // TODO: phase/plan ì‹¤í–‰ ì»¨íŠ¸ë¡¤ëŸ¬ì—ì„œ ë‚¨ì€ ë°°ì¹˜/phase ì§„í–‰ í›„ ìµœì¢… Shutdown ë°œì†¡
    Ok(())
}

// (run_single_batch_real removed)

/// CrawlingPlanner ê¸°ë°˜ ExecutionPlan ìƒì„± (ë‹¨ì¼ í˜¸ì¶œ)
/// 
/// ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.
/// ì´ í•¨ìˆ˜ê°€ í˜¸ì¶œëœ í›„ì—ëŠ” ë” ì´ìƒ ë¶„ì„/ê³„íš ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.
async fn create_execution_plan(app: &AppHandle) -> Result<(ExecutionPlan, AppConfig, DomainSiteStatus), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ§  Creating ExecutionPlan with CrawlingPlanner (cache-aware)...");
    
    // 1. ì„¤ì • ë¡œë“œ
    let config_manager = ConfigManager::new()?;
    let app_config = config_manager.load_config().await?;
    
    // 2. ì´ë¯¸ ì´ˆê¸°í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ í’€ ì‚¬ìš© (ìƒˆë¡œ ì—°ê²°í•˜ì§€ ì•ŠìŒ)
    let app_state = app.state::<AppState>();
    let db_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard.as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };
    
    info!("ğŸ“Š Using existing database pool from AppState");
    
    // 3. ì„œë¹„ìŠ¤ ìƒì„± (ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ í’€ ì¬ì‚¬ìš©)
    let product_repo = Arc::new(IntegratedProductRepository::new(db_pool.clone()));
    
    // ğŸ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
    info!("ğŸ” Testing database connection before creating CrawlingPlanner...");
    match product_repo.get_product_count().await {
        Ok(count) => {
            info!("âœ… Database connection successful: {} products found", count);
        }
        Err(e) => {
            error!("âŒ Database connection failed in create_execution_plan: {}", e);
            return Err(format!("Database connection test failed: {}", e).into());
        }
    }
    
    let http_client = HttpClient::create_from_global_config()?.with_context_label("Planner");
    let data_extractor = MatterDataExtractor::new()?;
    
    let status_checker = Arc::new(
        crate::infrastructure::crawling_service_impls::StatusCheckerImpl::with_product_repo(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
            product_repo.clone(),
        )
    );
    
    let database_analyzer = Arc::new(
        crate::infrastructure::crawling_service_impls::DatabaseAnalyzerImpl::new(
            product_repo.clone()
        )
    );
    
    // 4. CrawlingPlanner ìƒì„± ë° ë¶„ì„
    let crawling_planner = crate::new_architecture::services::crawling_planner::CrawlingPlanner::new(
        status_checker,
        database_analyzer,
        Arc::new(SystemConfig::default()),
    ).with_repository(product_repo.clone());
    
    info!("ğŸ¯ Analyzing system state with CrawlingPlanner (attempting cache reuse)...");

    // === Cache: attempt to reuse previously computed site analysis ===
    let shared_cache: Option<State<SharedStateCache>> = app.try_state::<SharedStateCache>();
    let cached_site_status: Option<DomainSiteStatus> = if let Some(cache_state) = shared_cache.as_ref() {
        // TTL 5ë¶„ ê¸°ë³¸
        match cache_state.get_valid_site_analysis_async(Some(5)).await {
            Some(cached) => {
                info!("â™»ï¸ Reusing cached SiteStatus: total_pages={}, last_page_products={} (age<=TTL)", cached.total_pages, cached.products_on_last_page);
                Some(DomainSiteStatus {
                    is_accessible: true,
                    response_time_ms: 0, // Unknown from cache snapshot
                    total_pages: cached.total_pages,
                    estimated_products: cached.estimated_products,
                    products_on_last_page: cached.products_on_last_page,
                    last_check_time: cached.analyzed_at,
                    health_score: cached.health_score,
                    data_change_status: SiteDataChangeStatus::Stable { count: cached.estimated_products },
                    decrease_recommendation: None,
                    crawling_range_recommendation: CrawlingRangeRecommendation::Full, // Conservative default
                })
            }
            None => {
                info!("ğŸ”„ No valid cached SiteStatus (or expired) â€“ performing fresh check");
                None
            }
        }
    } else {
        info!("ğŸ“­ SharedStateCache not available in Tauri state â€“ proceeding without cache");
        None
    };

    // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // (1) ì‚¬ì „ ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¡œ ì „ëµ ê²°ì • íŒíŠ¸ ê³„ì‚°
    let existing_product_count = match product_repo.get_product_count().await {
        Ok(c) => c,
        Err(e) => { warn!("âš ï¸ Failed to get product count for strategy decision: {} -> default NewestFirst", e); 0 }
    };

    // ê¸°ë³¸ ì „ëµì€ NewestFirst. DBì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ ContinueFromDb ì‹œë„
    let mut chosen_strategy = crate::new_architecture::actors::types::CrawlingStrategy::NewestFirst;
    if existing_product_count > 0 {
        chosen_strategy = crate::new_architecture::actors::types::CrawlingStrategy::ContinueFromDb;
        info!("ğŸ§­ Choosing ContinueFromDb strategy (existing products={})", existing_product_count);
    } else {
        info!("ğŸ§­ Choosing NewestFirst strategy (empty DB)");
    }

    // (2) CrawlingConfig ìƒì„± (start_page/end_pageëŠ” 'ê°œìˆ˜' í‘œí˜„: start_page - end_page + 1 = ìš”ì²­ ìˆ˜)
    let crawling_config = CrawlingConfig {
        site_url: "https://csa-iot.org/csa-iot_products/".to_string(),
        start_page: app_config.user.crawling.page_range_limit.max(1), // ìš”ì²­ ê°œìˆ˜ í‘œí˜„
        end_page: 1,
        concurrency_limit: app_config.user.max_concurrent_requests,
        batch_size: app_config.user.batch.batch_size,
        request_delay_ms: 1000,
        timeout_secs: 300,
        max_retries: app_config.user.crawling.workers.max_retries,
        strategy: chosen_strategy.clone(),
    };

    // (3) ì‚¬ì´íŠ¸ ìƒíƒœ ë° ê³„íš ìƒì„± (ì‚¬ì´íŠ¸ ìƒíƒœ 1íšŒ ì¡°íšŒ + DB ë¶„ì„)
    let cache_was_none = cached_site_status.is_none();
    // Attempt DB analysis cache reuse (TTL 3m)
    let cached_db_analysis: Option<crate::domain::services::crawling_services::DatabaseAnalysis> = if let Some(cache_state) = shared_cache.as_ref() {
        cache_state.get_valid_db_analysis_async(Some(3)).await.map(|d| crate::domain::services::crawling_services::DatabaseAnalysis {
            total_products: d.total_products,
            unique_products: d.total_products, // approximation (no uniqueness snapshot in cached struct)
            duplicate_count: 0,
            missing_products_count: 0,
            last_update: Some(d.analyzed_at),
            missing_fields_analysis: crate::domain::services::crawling_services::FieldAnalysis {
                missing_company: 0,
                missing_model: 0,
                missing_matter_version: 0,
                missing_connectivity: 0,
                missing_certification_date: 0,
            },
            data_quality_score: d.quality_score,
        })
    } else { None };
    let db_cache_hit = cached_db_analysis.is_some();
    if db_cache_hit { info!(target: "kpi.execution_plan", "{{\"event\":\"db_analysis_cache_hit\"}}"); }
    let (crawling_plan, site_status, db_analysis_used) = crawling_planner
        .create_crawling_plan_with_caches(&crawling_config, cached_site_status, cached_db_analysis)
        .await?;
    if !db_cache_hit { info!(target: "kpi.execution_plan", "{{\"event\":\"db_analysis_cache_miss\"}}"); }
    // Persist fresh site status & db analysis if newly fetched
    if let Some(cache_state_ref) = shared_cache.as_ref() {
        if cache_was_none {
            use crate::application::shared_state::SiteAnalysisResult;
            let site_analysis = SiteAnalysisResult::new(
                site_status.total_pages,
                site_status.products_on_last_page,
                site_status.estimated_products,
                crawling_config.site_url.clone(),
                site_status.health_score,
            );
            cache_state_ref.set_site_analysis(site_analysis).await;
        }
        if !db_cache_hit {
            use crate::application::shared_state::DbAnalysisResult;
            let db_cached = DbAnalysisResult::new(
                db_analysis_used.total_products,
                None,
                None,
                db_analysis_used.data_quality_score,
            );
            cache_state_ref.set_db_analysis(db_cached).await;
        }
    }
    info!("ğŸ§ª CrawlingPlanner produced plan with {:?} (requested strategy {:?})", crawling_plan.optimization_strategy, chosen_strategy);
    if db_cache_hit { info!(target: "kpi.execution_plan", "{{\"event\":\"db_analysis_used\",\"source\":\"cache\",\"total_products\":{}}}", db_analysis_used.total_products); } else { info!(target: "kpi.execution_plan", "{{\"event\":\"db_analysis_used\",\"source\":\"fresh\",\"total_products\":{}}}", db_analysis_used.total_products); }
    
    info!("ğŸ“‹ CrawlingPlan created: {:?}", crawling_plan);

    // === DB Analysis cache advisory (pre-plan) ===
    if let Some(cache_state) = shared_cache.as_ref() {
        if let Some(db_cached) = cache_state.get_valid_db_analysis_async(Some(3)).await {
            info!("â™»ï¸ Using cached DB analysis advisory: total_products={} (age TTL<=3m)", db_cached.total_products);
        }
    }

    // 5. ExecutionPlan ìƒì„± ì „ hash ì‚°ì¶œ ë° PlanCache ê²€ì‚¬
    let session_id = format!("actor_session_{}", Utc::now().timestamp());
    let plan_id = format!("plan_{}", Utc::now().timestamp());
    
    // CrawlingPlanì—ì„œ ListPageCrawling phasesë¥¼ ìˆ˜ì§‘í•˜ê³ , ìµœì‹ ìˆœ í˜ì´ì§€ë¥¼ ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í• 
    let mut all_pages: Vec<u32> = Vec::new();
    for phase in &crawling_plan.phases {
        if let crate::new_architecture::services::crawling_planner::PhaseType::ListPageCrawling = phase.phase_type {
            // ê° ListPageCrawling phaseì—ëŠ” í•´ë‹¹ ë°°ì¹˜ì˜ í˜ì´ì§€ë“¤ì´ ë‹´ê²¨ìˆìŒ(ìµœì‹ ìˆœ)
            // Phaseì˜ pagesë¥¼ ê·¸ëŒ€ë¡œ append (ì´ë¯¸ ìµœì‹ â†’ê³¼ê±° ìˆœ)
            all_pages.extend(phase.pages.iter());
        }
    }

    // ì„¤ì •ì˜ page_range_limitë¡œ ìƒí•œ ì ìš©
    let page_limit = app_config.user.crawling.page_range_limit.max(1) as usize;
    if all_pages.len() > page_limit {
        all_pages.truncate(page_limit);
    }

    // ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í•  (ì—­ìˆœ ë²”ìœ„ ìœ ì§€)
    let batch_size = app_config.user.batch.batch_size.max(1) as usize;
    let mut crawling_ranges: Vec<PageRange> = Vec::new();
    for chunk in all_pages.chunks(batch_size) {
        if let (Some(&first), Some(&last)) = (chunk.first(), chunk.last()) {
            // chunkëŠ” ìµœì‹ â†’ê³¼ê±° ìˆœì„œì´ë¯€ë¡œ start_page=first, end_page=last, reverse_order=true
            let pages_count = (first.saturating_sub(last)) + 1;
            crawling_ranges.push(PageRange {
                start_page: first,
                end_page: last,
                estimated_products: pages_count * 12, // ëŒ€ëµì¹˜
                reverse_order: true,
            });
        }
    }
    
    if crawling_ranges.is_empty() {
        // ì•ˆì „ í´ë°± (ìµœì‹  1í˜ì´ì§€)
        let last_page = all_pages.first().copied().unwrap_or(1);
        crawling_ranges.push(PageRange {
            start_page: last_page,
            end_page: last_page,
            estimated_products: 12,
            reverse_order: true,
        });
    }
    
    let total_pages: u32 = crawling_ranges.iter().map(|r| {
        if r.reverse_order { r.start_page - r.end_page + 1 } 
        else { r.end_page - r.start_page + 1 }
    }).sum();
    
    // DB page/index ìƒíƒœ ì½ê¸° (ì‹¤íŒ¨ ì‹œ None ìœ ì§€)
    let (db_max_page_id, db_max_index_in_page) = match product_repo.get_max_page_id_and_index().await {
        Ok(v) => v,
        Err(e) => { warn!("âš ï¸ Failed to read max page/index: {}", e); (None, None) }
    };
    info!("ğŸ§¾ DB snapshot: max_page_id={:?} max_index_in_page={:?} total_products_dbMetric={:?}", db_max_page_id, db_max_index_in_page, crawling_plan.db_total_products);

    // ì…ë ¥ ìŠ¤ëƒ…ìƒ· êµ¬ì„± (ì‚¬ì´íŠ¸/DB ìƒíƒœ + í•µì‹¬ ì œí•œê°’)
    let snapshot = crate::new_architecture::actors::types::PlanInputSnapshot {
        total_pages: site_status.total_pages,
        products_on_last_page: site_status.products_on_last_page,
        db_max_page_id,
        db_max_index_in_page,
        db_total_products: crawling_plan.db_total_products.unwrap_or(0) as u64,
        page_range_limit: app_config.user.crawling.page_range_limit,
        batch_size: app_config.user.batch.batch_size,
        concurrency_limit: app_config.user.max_concurrent_requests,
        created_at: Utc::now(),
    };

    // í•´ì‹œ ê³„ì‚° (ìŠ¤ëƒ…ìƒ· + í˜ì´ì§€ë“¤ + ì „ëµ í•µì‹¬ í•„ë“œ ì§ë ¬í™”)
    let hash_input = serde_json::json!({
        "snapshot": &snapshot,
        "ranges": &crawling_ranges,
        "strategy": format!("{:?}", crawling_plan.optimization_strategy),
    });
    let hash_string = serde_json::to_string(&hash_input).unwrap_or_default();
    let plan_hash = blake3::hash(hash_string.as_bytes()).to_hex().to_string();

    if let Some(cache_state) = shared_cache.as_ref() {
        if let Some(hit) = cache_state.get_cached_execution_plan(&plan_hash).await {
            return Ok((hit, app_config, site_status));
        } else {
            info!("ğŸ†• PlanCache miss (hash={}) â€” creating new ExecutionPlan", plan_hash);
        }
    }

    // Partial page reinclusion (if last DB page not fully processed)
    if let (Some(mp), Some(mi)) = (db_max_page_id, db_max_index_in_page) {
        if mi < 11 { // 0-based index; full page means 0..11
            let partial_site_page = site_status.total_pages - mp as u32; // mapping rule
            let already_included = crawling_ranges.iter().any(|r| {
                if r.reverse_order { partial_site_page <= r.start_page && partial_site_page >= r.end_page } else { partial_site_page >= r.start_page && partial_site_page <= r.end_page }
            });
            if !already_included {
                info!("ğŸ” Reinserting partial page {} (db_page_id={}, index_in_page={}) at front of ranges", partial_site_page, mp, mi);
                crawling_ranges.insert(0, PageRange { start_page: partial_site_page, end_page: partial_site_page, estimated_products: 12, reverse_order: true });
            }
        }
    }

    // PlanCache hit í™•ì¸ (hash ê³„ì‚° í›„ ì¡°íšŒ) - hash ëŠ” ì•„ë˜ì—ì„œ ì´ë¯¸ ê³„ì‚°ë¨
    if let Some(cache_state) = app.try_state::<SharedStateCache>() {
        if let Some(cached_plan) = futures::executor::block_on(async { cache_state.get_cached_execution_plan(&plan_hash).await }) {
            info!("â™»ï¸ PlanCache hit: reuse ExecutionPlan hash={}", plan_hash);
            let json_line = format!("{{\"event\":\"plan_cache_hit\",\"hash\":\"{}\"}}", plan_hash);
            info!(target: "kpi.execution_plan", "{}", json_line);
            return Ok((cached_plan, app_config, site_status));
        }
    }

    let ranges_len = crawling_ranges.len();
    let strategy_string = format!("{:?}", crawling_plan.optimization_strategy);
    let execution_plan = ExecutionPlan {
        plan_id,
        session_id,
        crawling_ranges: crawling_ranges,
        batch_size: app_config.user.batch.batch_size,
        concurrency_limit: app_config.user.max_concurrent_requests,
        estimated_duration_secs: crawling_plan.total_estimated_duration_secs,
        created_at: Utc::now(),
        analysis_summary: format!("Strategy: {:?}, Total pages: {}", 
                                strategy_string, total_pages),
    original_strategy: strategy_string.clone(),
        input_snapshot: snapshot,
        plan_hash,
    skip_duplicate_urls: true,
    kpi_meta: Some(crate::new_architecture::actors::types::ExecutionPlanKpi {
        total_ranges: ranges_len,
        total_pages,
        batches: ranges_len,
        strategy: strategy_string,
        created_at: Utc::now(),
    }),
    };
    
    info!("âœ… ExecutionPlan created successfully: {} pages across {} batches (hash={})", 
          total_pages, execution_plan.crawling_ranges.len(), execution_plan.plan_hash);
    if let Some(kpi) = &execution_plan.kpi_meta {
        info!(target: "kpi.execution_plan", "{{\"event\":\"plan_created\",\"hash\":\"{}\",\"total_pages\":{},\"ranges\":{},\"batches\":{},\"strategy\":\"{}\",\"ts\":\"{}\"}}",
            execution_plan.plan_hash, kpi.total_pages, kpi.total_ranges, kpi.batches, kpi.strategy, kpi.created_at);
    }
    if let Some(cache_state) = app.try_state::<SharedStateCache>() { cache_state.cache_execution_plan(execution_plan.clone()).await; }
    
    Ok((execution_plan, app_config, site_status))
}

/// ExecutionPlan ê¸°ë°˜ SessionActor ì‹¤í–‰ (ìˆœìˆ˜ ì‹¤í–‰ ì „ìš©)
/// 
/// SessionActorëŠ” ë” ì´ìƒ ë¶„ì„/ê³„íší•˜ì§€ ì•Šê³  ExecutionPlanì„ ì¶©ì‹¤íˆ ì‹¤í–‰í•©ë‹ˆë‹¤.
async fn execute_session_actor_with_execution_plan(
    execution_plan: ExecutionPlan,
    app_config: &AppConfig,
    site_status: &SiteStatus,
    actor_event_tx: broadcast::Sender<AppEvent>,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    info!("ğŸ­ Executing SessionActor with predefined ExecutionPlan...");
    info!("ğŸ“‹ Plan: {} batches, batch_size: {}, effective_concurrency: {}", 
          execution_plan.crawling_ranges.len(),
          execution_plan.batch_size,
          execution_plan.concurrency_limit);

    // ----- Aggregated metrics (ranges -> batches/pages) -----
    let batch_unit = execution_plan.batch_size.max(1);
    let mut expected_pages: usize = 0;
    let mut expected_batches: usize = 0;
    for r in &execution_plan.crawling_ranges {
        let pages_in_range = if r.reverse_order { r.start_page - r.end_page + 1 } else { r.end_page - r.start_page + 1 } as usize;
        expected_pages += pages_in_range;
        expected_batches += (pages_in_range + batch_unit as usize - 1) / batch_unit as usize;
    }
    info!("ğŸ§® Aggregated metrics => ranges: {}, expected_pages: {}, expected_batches: {}, batch_size: {}", execution_plan.crawling_ranges.len(), expected_pages, expected_batches, batch_unit);
    let mut completed_pages: usize = 0;
    let mut completed_batches: usize = 0;

    // ì‹¤í–‰ ì „ í•´ì‹œ ì¬ê³„ì‚° & ê²€ì¦ (ìƒì„± ì‹œì™€ ë™ì¼í•œ ì§ë ¬í™” ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
    let verify_input = serde_json::json!({
        "snapshot": &execution_plan.input_snapshot,
        "ranges": &execution_plan.crawling_ranges,
        "strategy": &execution_plan.original_strategy,
    });
    if let Ok(serialized) = serde_json::to_string(&verify_input) {
        let current_hash = blake3::hash(serialized.as_bytes()).to_hex().to_string();
        if current_hash != execution_plan.plan_hash {
            tracing::error!("âŒ ExecutionPlan hash mismatch! expected={}, got={}", execution_plan.plan_hash, current_hash);
            return Err("ExecutionPlan integrity check failed".into());
        } else {
            tracing::info!("ğŸ” ExecutionPlan integrity verified (hash={})", current_hash);
        }
    } else {
        tracing::warn!("âš ï¸ Failed to serialize ExecutionPlan for integrity verification â€“ continuing cautiously");
    }
    
    // ì‹œì‘ ì´ë²¤íŠ¸ ë°©ì¶œ (ì„¤ì • íŒŒì¼ ê¸°ë°˜ ê°’ ì‚¬ìš©)
    // ì „ëµ ì¶”ë¡ : ì²« ë°°ì¹˜ê°€ ë§ˆì§€ë§‰ í˜ì´ì§€ë³´ë‹¤ ì‘ì€ í˜ì´ì§€ë¥¼ í¬í•¨í•˜ë©´ ContinueFromDbì˜€ì„ ê°€ëŠ¥ì„± ë†’ìŒ
    let inferred_strategy = if execution_plan.crawling_ranges.len() > 1 {
        // ì—¬ëŸ¬ ë²”ìœ„ê°€ ìˆê³  ì²« start_pageê°€ site_status.total_pages ë³´ë‹¤ ì‘ìœ¼ë©´ ContinueFromDb ì¶”ì •
        let first_start = execution_plan.crawling_ranges.first().map(|r| r.start_page).unwrap_or(1);
        if first_start < site_status.total_pages { crate::new_architecture::actors::types::CrawlingStrategy::ContinueFromDb } else { crate::new_architecture::actors::types::CrawlingStrategy::NewestFirst }
    } else {
        let first_range = execution_plan.crawling_ranges.first();
        if let Some(r) = first_range {
            if r.start_page < site_status.total_pages { crate::new_architecture::actors::types::CrawlingStrategy::ContinueFromDb } else { crate::new_architecture::actors::types::CrawlingStrategy::NewestFirst }
        } else { crate::new_architecture::actors::types::CrawlingStrategy::NewestFirst }
    };

    let session_event = AppEvent::SessionStarted {
        session_id: execution_plan.session_id.clone(),
        config: CrawlingConfig {
            site_url: "https://csa-iot.org/csa-iot_products/".to_string(),
            start_page: execution_plan.crawling_ranges.first().map(|r| r.start_page).unwrap_or(1),
            end_page: execution_plan.crawling_ranges.last().map(|r| r.end_page).unwrap_or(1),
            concurrency_limit: execution_plan.concurrency_limit,
            batch_size: execution_plan.batch_size,
            request_delay_ms: app_config.user.request_delay_ms,
            timeout_secs: app_config.advanced.request_timeout_seconds,
            max_retries: app_config.advanced.retry_attempts,
            strategy: inferred_strategy,
        },
        timestamp: Utc::now(),
    };
    
    if let Err(e) = actor_event_tx.send(session_event) {
        error!("Failed to send SessionStarted event: {}", e);
    }
    
    // ê° ë²”ìœ„ë³„ë¡œ ìˆœì°¨ ì‹¤í–‰
        for (range_idx, page_range) in execution_plan.crawling_ranges.iter().enumerate() {
            let pages_in_range = if page_range.reverse_order { page_range.start_page - page_range.end_page + 1 } else { page_range.end_page - page_range.start_page + 1 } as usize;
            let range_batches = (pages_in_range + batch_unit as usize - 1) / batch_unit as usize;
            info!("ğŸ¯ Range {}/{} start: pages {} to {} ({} pages => {} batches, reverse: {})", 
                    range_idx + 1, execution_plan.crawling_ranges.len(),
                    page_range.start_page, page_range.end_page, pages_in_range, range_batches, page_range.reverse_order);
        
        // ì§„í–‰ ìƒí™© ì´ë²¤íŠ¸ ë°©ì¶œ
        let progress_percentage = ((completed_pages as f64) / (expected_pages as f64).max(1.0)) * 100.0;
        let progress_event = AppEvent::Progress {
            session_id: execution_plan.session_id.clone(),
            current_step: range_idx as u32 + 1,
            total_steps: execution_plan.crawling_ranges.len() as u32,
            message: format!("Processing range {}/{} pages {}->{} (range pages={}, est batches={})", range_idx+1, execution_plan.crawling_ranges.len(), page_range.start_page, page_range.end_page, pages_in_range, range_batches),
            percentage: progress_percentage,
            timestamp: Utc::now(),
        };
        
        if let Err(e) = actor_event_tx.send(progress_event) {
            error!("Failed to send progress event: {}", e);
        }
        
        // BatchActorë¡œ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
    // (tracking variables for diff removed as not yet needed)
    match execute_session_actor_with_batches(
            &execution_plan.session_id,
            page_range.start_page,
            page_range.end_page,
            execution_plan.batch_size,
            app_config,
            site_status,
            actor_event_tx.clone(),
        ).await {
            Ok(()) => {
        // Approximate increments (recompute similar to helper)
        let added_pages = if page_range.reverse_order { page_range.start_page - page_range.end_page + 1 } else { page_range.end_page - page_range.start_page + 1 } as usize;
        let added_batches = (added_pages + batch_unit as usize - 1) / batch_unit as usize;
        completed_pages += added_pages;
        completed_batches += added_batches;
        let pct_batches = (completed_batches as f64 / expected_batches as f64) * 100.0;
        let pct_pages = (completed_pages as f64 / expected_pages as f64) * 100.0;
        info!("âœ… Range {} complete | cumulative: {}/{} batches ({:.1}%), {}/{} pages ({:.1}%)",
              range_idx + 1,
              completed_batches, expected_batches, pct_batches,
              completed_pages, expected_pages, pct_pages);
            }
            Err(e) => {
                error!("âŒ Range {} failed: {}", range_idx + 1, e);
                // ê³„ì† ì§„í–‰ (ë²”ìœ„ë³„ ë…ë¦½ ì‹¤í–‰)
            }
        }
    }
    
    // ì™„ë£Œ ì´ë²¤íŠ¸ ë°©ì¶œ
    let completion_event = AppEvent::SessionCompleted {
        session_id: execution_plan.session_id.clone(),
        summary: SessionSummary {
            session_id: execution_plan.session_id.clone(),
            total_duration_ms: 0, // ì‹¤ì œ ì‹œê°„ì€ ë‚˜ì¤‘ì— ê³„ì‚°
            total_pages_processed: completed_pages as u32,
            total_products_processed: 0, // ì‹¤ì œ ì²˜ë¦¬ í›„ ê³„ì‚°
            success_rate: 100.0,
            avg_page_processing_time: 2000,
            error_summary: vec![],
            processed_batches: completed_batches as u32,
            total_success_count: 0,
            duplicates_skipped: 0,
            final_state: "Completed".to_string(),
            timestamp: Utc::now(),
        },
        timestamp: Utc::now(),
    };
    
    if let Err(e) = actor_event_tx.send(completion_event) {
        error!("Failed to send SessionCompleted event: {}", e);
    }
    
    info!("ğŸ‰ ExecutionPlan fully executed!");
    Ok(())
}

// (Removed unused simulation helpers: execute_batch_actor_simulation, run_simulation_crawling)
