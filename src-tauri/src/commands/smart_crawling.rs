//! Smart crawling commands - uses the range calculation logic from prompts6
//! 
//! This module provides commands for smart crawling that automatically calculates
//! the next pages to crawl based on the current database state and site information.

use crate::application::AppState;
use crate::infrastructure::crawling_service_impls::CrawlingRangeCalculator;
use crate::domain::events::CrawlingProgress;
use crate::infrastructure::config::ConfigManager;
use crate::domain::pagination::CanonicalPageIdCalculator;
use crate::infrastructure::integrated_product_repository::IntegratedProductRepository;
use crate::infrastructure::DatabaseConnection;
use anyhow::Result;
use tauri::State;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tracing::info;

/// Response for crawling range calculation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingRangeResponse {
    pub success: bool,
    pub range: Option<(u32, u32)>,
    pub progress: CrawlingProgressInfo,
    pub site_info: SiteInfo,
    pub local_db_info: LocalDbInfo,
    pub crawling_info: CrawlingInfo,
    pub batch_plan: BatchPlan, // CrawlingPlannerÍ∞Ä Í≥ÑÏÇ∞Ìïú Î∞∞Ïπò Í≥ÑÌöç Ï∂îÍ∞Ä
    pub message: String,
}

/// CrawlingPlannerÍ∞Ä Í≥ÑÏÇ∞Ìïú Î∞∞Ïπò Í≥ÑÌöç Ï†ïÎ≥¥
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchPlan {
    pub batch_size: u32,
    pub total_batches: u32,
    pub concurrency_limit: u32,
    pub batches: Vec<BatchInfo>,
    pub execution_strategy: String, // "concurrent", "sequential", "mixed"
    pub estimated_duration_seconds: u32,
}

/// Í∞úÎ≥Ñ Î∞∞Ïπò Ï†ïÎ≥¥
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchInfo {
    pub batch_id: u32,
    pub pages: Vec<u32>,
    pub estimated_products: u32,
}

/// Site information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SiteInfo {
    pub total_pages: u32,
    pub products_on_last_page: u32,
    pub estimated_total_products: u32,
}

/// Local database information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LocalDbInfo {
    pub total_saved_products: u32,
    pub last_crawled_page: Option<u32>,
    pub last_crawled_page_id: Option<i32>,
    pub coverage_percentage: f64,
}

/// Crawling strategy information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingInfo {
    pub pages_to_crawl: Option<u32>,
    pub estimated_new_products: Option<u32>,
    pub strategy: String, // "full", "partial", "none"
}

/// Crawling progress information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingProgressInfo {
    pub total_products: u32,
    pub saved_products: u32,
    pub progress_percentage: f64,
    pub max_page_id: Option<i32>,
    pub max_index_in_page: Option<i32>,
    pub is_completed: bool,
}

/// Request for calculating crawling range
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingRangeRequest {
    pub total_pages_on_site: u32,
    pub products_on_last_page: u32,
}

/// Create product repository from database connection
async fn create_product_repo() -> Result<IntegratedProductRepository, String> {
    // Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í≤ΩÎ°ú ÏÉùÏÑ± (macOS Í≤ΩÎ°úÏóê ÎßûÍ≤å ÏàòÏ†ï)
    let database_url = {
        let app_data_dir = if cfg!(target_os = "macos") {
            std::env::var("HOME")
                .map(|h| format!("{}/Library/Application Support", h))
                .unwrap_or_else(|_| "./data".to_string())
        } else {
            std::env::var("APPDATA")
                .or_else(|_| std::env::var("HOME").map(|h| format!("{}/.local/share", h)))
                .unwrap_or_else(|_| "./data".to_string())
        };
        let data_dir = format!("{}/matter-certis-v2/database", app_data_dir);
        format!("sqlite:{}/matter_certis.db", data_dir)
    };
    
    info!("Using database at: {}", database_url);
    
    let db_conn = DatabaseConnection::new(&database_url).await
        .map_err(|e| format!("Failed to create database connection: {}", e))?;
    
    let pool = db_conn.pool().clone();
    
    Ok(IntegratedProductRepository::new(pool))
}

/// Calculate the next crawling range based on current DB state
#[tauri::command]
pub async fn calculate_crawling_range(
    _state: State<'_, AppState>,
    request: CrawlingRangeRequest,
) -> Result<CrawlingRangeResponse, String> {
    info!("üéØ Calculating next crawling range with: total_pages={}, products_on_last_page={}", 
          request.total_pages_on_site, request.products_on_last_page);

    // Get configuration
    let config_manager = ConfigManager::new()
        .map_err(|e| format!("Failed to initialize config manager: {}", e))?;
    
    let config = config_manager.load_config().await
        .map_err(|e| format!("Failed to get config: {}", e))?;

    // Create product repository
    let product_repo = create_product_repo().await?;

    // Create range calculator
    let range_calculator = CrawlingRangeCalculator::new(
        Arc::new(product_repo),
        config,
    );

    // Calculate next range
    let result = range_calculator.calculate_next_crawling_range(
        request.total_pages_on_site,
        request.products_on_last_page,
    ).await
    .map_err(|e| format!("Failed to calculate crawling range: {}", e))?;

    // Get progress information
    let progress = range_calculator.analyze_simple_progress(
        request.total_pages_on_site,
        request.products_on_last_page,
    ).await
    .map_err(|e| format!("Failed to analyze progress: {}", e))?;

    // Calculate site information
    let estimated_total_products = ((request.total_pages_on_site - 1) * 12) + request.products_on_last_page;
    let site_info = SiteInfo {
        total_pages: request.total_pages_on_site,
        products_on_last_page: request.products_on_last_page,
        estimated_total_products,
    };

    // Calculate local DB information
    let local_db_info = if progress.current > 0 {
        // DBÏóê Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏúºÎ©¥ Í≥ÑÏÇ∞Í∏∞Î•º ÏÇ¨Ïö©Ìï¥ÏÑú Ïã§Ï†ú ÌéòÏù¥ÏßÄ Î≤àÌò∏Î•º Í≥ÑÏÇ∞
            let calculator = CanonicalPageIdCalculator::new(
            request.total_pages_on_site,
            request.products_on_last_page as usize
        );
        
        let max_page_id = progress.current_batch.unwrap_or(0) as i32;
        let max_index_in_page = 0; // Í∞ÑÎã®Ìûà 0ÏúºÎ°ú Í∞ÄÏ†ï (Ï†ïÌôïÌïú Í∞íÏùÄ DBÏóêÏÑú Í∞ÄÏ†∏ÏôÄÏïº Ìï®)
        
        if let Some((actual_page, _)) = calculator.reverse_calculate(max_page_id, max_index_in_page) {
            LocalDbInfo {
                total_saved_products: progress.current,
                last_crawled_page: Some(actual_page),
                last_crawled_page_id: Some(max_page_id),
                coverage_percentage: progress.percentage,
            }
        } else {
            // Ïó≠Í≥ÑÏÇ∞ Ïã§Ìå® Ïãú Í∏∞Î≥∏Í∞í
            LocalDbInfo {
                total_saved_products: progress.current,
                last_crawled_page: None,
                last_crawled_page_id: None,
                coverage_percentage: progress.percentage,
            }
        }
    } else {
        // DBÍ∞Ä ÎπÑÏñ¥ÏûàÏúºÎ©¥ Î™®Îì† Í∞íÏùÑ NoneÏúºÎ°ú ÏÑ§Ï†ï
        LocalDbInfo {
            total_saved_products: 0,
            last_crawled_page: None,
            last_crawled_page_id: None,
            coverage_percentage: 0.0,
        }
    };

    let response = match result {
        Some((start_page, end_page)) => {
            let total_pages = if start_page >= end_page {
                start_page - end_page + 1
            } else {
                end_page - start_page + 1
            };
            
            let estimated_new_products = total_pages * 12; // ÌèâÍ∑† 12Í∞ú Ï†úÌíà/ÌéòÏù¥ÏßÄ
            
            let crawling_info = CrawlingInfo {
                pages_to_crawl: Some(total_pages),
                estimated_new_products: Some(estimated_new_products),
                strategy: "partial".to_string(),
            };
            
            // CrawlingPlanner Í∏∞Î∞ò Î∞∞Ïπò Í≥ÑÌöç ÏÉùÏÑ±
            info!("üîß Creating batch plan for range: {} to {}", start_page, end_page);
            let batch_plan = create_batch_plan(start_page, end_page).await;
            
            let message = format!("Next crawling range: pages {} to {} (total: {} pages)", 
                                start_page, end_page, total_pages);
            info!("‚úÖ {}", message);
            
            CrawlingRangeResponse {
                success: true,
                range: Some((start_page, end_page)),
                progress: convert_progress(&progress),
                site_info,
                local_db_info,
                crawling_info,
                batch_plan,
                message,
            }
        }
        None => {
            let crawling_info = CrawlingInfo {
                pages_to_crawl: Some(0),
                estimated_new_products: Some(0),
                strategy: "none".to_string(),
            };
            
            // Îπà Î∞∞Ïπò Í≥ÑÌöç
            let batch_plan = BatchPlan {
                batch_size: 0,
                total_batches: 0,
                concurrency_limit: 0,
                batches: vec![],
                execution_strategy: "none".to_string(),
                estimated_duration_seconds: 0,
            };
            
            let message = "All products have been crawled - no more pages to process".to_string();
            info!("üèÅ {}", message);
            
            CrawlingRangeResponse {
                success: true,
                range: None,
                progress: convert_progress(&progress),
                site_info,
                local_db_info,
                crawling_info,
                batch_plan,
                message,
            }
        }
    };

    Ok(response)
}

/// CrawlingPlanner Í∏∞Î∞ò Î∞∞Ïπò Í≥ÑÌöç ÏÉùÏÑ±
async fn create_batch_plan(start_page: u32, end_page: u32) -> BatchPlan {
    info!("üîß Creating batch plan: start_page={}, end_page={}", start_page, end_page);
    
    // ÏÑ§Ï†ïÏóêÏÑú batch_size Í∞ÄÏ†∏Ïò§Í∏∞ (ÎπÑÏ∞®Îã® Î∞©Ïãù; Ïã§Ìå® Ïãú Í∞úÎ∞ú Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©)
    let app_config = match ConfigManager::new() {
        Ok(cm) => match cm.load_config().await {
            Ok(cfg) => cfg,
            Err(e) => {
                tracing::warn!("‚ö†Ô∏è Failed to load AppConfig for batch plan: {}. Falling back to development defaults.", e);
                crate::infrastructure::config::AppConfig::for_development()
            }
        },
        Err(e) => {
            tracing::warn!("‚ö†Ô∏è Failed to initialize ConfigManager for batch plan: {}. Falling back to development defaults.", e);
            crate::infrastructure::config::AppConfig::for_development()
        }
    };
    let batch_size = app_config.user.batch.batch_size;
    let concurrency_limit = app_config.user.max_concurrent_requests; // ExecutionPlan Í≤ΩÎ°úÏôÄ ÏùºÏπò
    
    info!("üìã Batch plan configuration: batch_size={}, concurrency_limit={}", batch_size, concurrency_limit);
    
    // ÌéòÏù¥ÏßÄ Î≤îÏúÑ Í≥ÑÏÇ∞
    let pages: Vec<u32> = if start_page >= end_page {
        (end_page..=start_page).rev().collect() // Ïó≠Ïàú ÌÅ¨Î°§ÎßÅ
    } else {
        (start_page..=end_page).collect()
    };
    
    let total_pages = pages.len() as u32;
    let total_batches = (total_pages + batch_size - 1) / batch_size; // Ïò¨Î¶º Í≥ÑÏÇ∞
    
    info!("üìä Batch plan calculation: total_pages={}, total_batches={}", total_pages, total_batches);
    
    // Î∞∞Ïπò Î∂ÑÌï†
    let mut batches = Vec::new();
    for (batch_id, chunk) in pages.chunks(batch_size as usize).enumerate() {
        let batch_info = BatchInfo {
            batch_id: batch_id as u32,
            pages: chunk.to_vec(),
            estimated_products: chunk.len() as u32 * 12, // ÌèâÍ∑† 12Í∞ú/ÌéòÏù¥ÏßÄ
        };
        info!("üî¢ Batch {}: pages={:?}, estimated_products={}", batch_id + 1, chunk, batch_info.estimated_products);
        batches.push(batch_info);
    }
    
    // ÏòàÏÉÅ Ïã§Ìñâ ÏãúÍ∞Ñ (Í∞Å ÌéòÏù¥ÏßÄÎãπ 2Ï¥à + ÎÑ§Ìä∏ÏõåÌÅ¨ ÏßÄÏó∞)
    let estimated_duration_seconds = total_pages * 2 + (total_batches * batch_size) / concurrency_limit;
    
    info!("‚úÖ Batch plan created successfully: {} batches, estimated duration: {}s", total_batches, estimated_duration_seconds);
    
    BatchPlan {
        batch_size,
        total_batches,
        concurrency_limit,
        batches,
        execution_strategy: "concurrent".to_string(),
        estimated_duration_seconds,
    }
}

/// Get current crawling progress
#[tauri::command]
pub async fn get_crawling_progress(
    _state: State<'_, AppState>,
    total_pages_on_site: u32,
    products_on_last_page: u32,
) -> Result<CrawlingProgressInfo, String> {
    info!("üìä Getting crawling progress information");

    // Get configuration
    let config_manager = ConfigManager::new()
        .map_err(|e| format!("Failed to initialize config manager: {}", e))?;
    
    let config = config_manager.load_config().await
        .map_err(|e| format!("Failed to get config: {}", e))?;

    // Create product repository
    let product_repo = create_product_repo().await?;

    // Create range calculator
    let range_calculator = CrawlingRangeCalculator::new(
        Arc::new(product_repo),
        config,
    );

    // Analyze progress
    let progress = range_calculator.analyze_simple_progress(
        total_pages_on_site,
        products_on_last_page,
    ).await
    .map_err(|e| format!("Failed to analyze progress: {}", e))?;

    Ok(convert_progress(&progress))
}

/// Get database state for range calculation
#[tauri::command]
pub async fn get_database_state_for_range_calculation(
    _state: State<'_, AppState>,
) -> Result<DatabaseStateInfo, String> {
    info!("üìä Getting database state for range calculation");

    // Create product repository
    let product_repo = create_product_repo().await?;

    // Get max pageId and indexInPage
    let (max_page_id, max_index_in_page) = product_repo.get_max_page_id_and_index().await
        .map_err(|e| format!("Failed to get max page ID and index: {}", e))?;

    // Get total product count
    let total_products = product_repo.get_product_count().await
        .map_err(|e| format!("Failed to get product count: {}", e))?;

    let info = DatabaseStateInfo {
        max_page_id,
        max_index_in_page,
        total_products: total_products as u32,
        has_data: max_page_id.is_some() && max_index_in_page.is_some(),
    };

    info!("‚úÖ Database state: max_page_id={:?}, max_index_in_page={:?}, total_products={}", 
          info.max_page_id, info.max_index_in_page, info.total_products);

    Ok(info)
}

/// Database state information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatabaseStateInfo {
    pub max_page_id: Option<i32>,
    pub max_index_in_page: Option<i32>,
    pub total_products: u32,
    pub has_data: bool,
}

/// Demo function to show the prompts6 example calculation
#[tauri::command]
pub async fn demo_prompts6_calculation() -> Result<String, String> {
    info!("üéØ Running prompts6 example calculation demo");

    // Example data from prompts6
    let max_page_id = 10i32;
    let max_index_in_page = 6i32;
    let total_pages_on_site = 481u32;
    let products_on_last_page = 10u32;
    let crawl_page_limit = 10u32;
    let products_per_page = 12u32;

    let mut result = String::new();
    result.push_str("üìä prompts6 Example Calculation Demo\n\n");
    result.push_str(&format!("Input data:\n"));
    result.push_str(&format!("  max_page_id: {}\n", max_page_id));
    result.push_str(&format!("  max_index_in_page: {}\n", max_index_in_page));
    result.push_str(&format!("  total_pages_on_site: {}\n", total_pages_on_site));
    result.push_str(&format!("  products_on_last_page: {}\n", products_on_last_page));
    result.push_str(&format!("  crawl_page_limit: {}\n", crawl_page_limit));
    result.push_str(&format!("  products_per_page: {}\n\n", products_per_page));

    // Step 1: Calculate last saved index
    let last_saved_index = (max_page_id as u32 * products_per_page) + max_index_in_page as u32;
    result.push_str(&format!("Step 1: lastSavedIndex = ({} * {}) + {} = {}\n", 
                           max_page_id, products_per_page, max_index_in_page, last_saved_index));

    // Step 2: Calculate next product index
    let next_product_index = last_saved_index + 1;
    result.push_str(&format!("Step 2: nextProductIndex = {} + 1 = {}\n", 
                           last_saved_index, next_product_index));

    // Step 3: Calculate total products
    let total_products = ((total_pages_on_site - 1) * products_per_page) + products_on_last_page;
    result.push_str(&format!("Step 3: totalProducts = (({} - 1) * {}) + {} = {}\n", 
                           total_pages_on_site, products_per_page, products_on_last_page, total_products));

    // Step 4: Convert to forward index
    let forward_index = (total_products - 1) - next_product_index;
    result.push_str(&format!("Step 4: forwardIndex = ({} - 1) - {} = {}\n", 
                           total_products, next_product_index, forward_index));

    // Step 5: Calculate target page number
    let target_page_number = (forward_index / products_per_page) + 1;
    result.push_str(&format!("Step 5: targetPageNumber = ({} / {}) + 1 = {}\n", 
                           forward_index, products_per_page, target_page_number));

    // Step 6: Apply crawl page limit
    let start_page = target_page_number;
    let end_page = if start_page >= crawl_page_limit {
        start_page - crawl_page_limit + 1
    } else {
        1
    };
    result.push_str(&format!("Step 6: startPage = {}, endPage = {} - {} + 1 = {}\n", 
                           start_page, start_page, crawl_page_limit, end_page));

    result.push_str(&format!("\n‚úÖ Final result: crawl pages {} to {}\n", start_page, end_page));
    result.push_str(&format!("üéØ This matches the prompts6 specification exactly!\n"));

    Ok(result)
}

/// Convert internal progress to API response format
fn convert_progress(progress: &CrawlingProgress) -> CrawlingProgressInfo {
    CrawlingProgressInfo {
        total_products: progress.total,
        saved_products: progress.current,
        progress_percentage: progress.percentage,
        max_page_id: progress.current_batch.map(|b| b as i32),
        max_index_in_page: progress.total_batches.map(|b| b as i32),
        is_completed: progress.status == crate::domain::events::CrawlingStatus::Completed,
    }
}
