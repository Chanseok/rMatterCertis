//! í†µí•© í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € - PHASE2_CRAWLING_ENHANCEMENT_PLAN êµ¬í˜„
//! 
//! ì´ ëª¨ë“ˆì€ .local/crawling_explanation.mdì— ì •ì˜ëœ CrawlerManager ì—­í• ì„ ìˆ˜í–‰í•˜ë©°,
//! 3ê°œì˜ í¬ë¡¤ë§ ì—”ì§„ì„ í†µí•© ê´€ë¦¬í•˜ê³  ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤.

use std::sync::Arc;
use std::collections::HashMap;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};
use anyhow::{Result, anyhow};
use tracing::{info, warn, error, debug};
use chrono::{DateTime, Utc};

use crate::domain::session_manager::SessionManager;
use crate::domain::events::{CrawlingProgress, CrawlingStage, CrawlingStatus};
use crate::application::EventEmitter;
use crate::infrastructure::{
    BatchCrawlingEngine, 
    ServiceBasedBatchCrawlingEngine, 
    AdvancedBatchCrawlingEngine
};

/// ë°°ì¹˜ í”„ë¡œì„¸ì„œ íŠ¸ë ˆì´íŠ¸ - 3ê°œ ì—”ì§„ì„ í†µí•©í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤
#[async_trait::async_trait]
pub trait BatchProcessor: Send + Sync {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult>;
    async fn get_progress(&self) -> CrawlingProgress;
    async fn pause(&self) -> Result<()>;
    async fn resume(&self) -> Result<()>;
    async fn stop(&self) -> Result<()>;
}

/// í¬ë¡¤ë§ ì„¤ì • í†µí•© êµ¬ì¡°ì²´
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CrawlingConfig {
    pub start_page: u32,
    pub end_page: u32,
    pub concurrency: u32,
    pub delay_ms: u64,
    pub batch_size: u32,
    pub retry_max: u32,
    pub timeout_ms: u64,
    pub engine_type: CrawlingEngineType,
}

/// í¬ë¡¤ë§ ì—”ì§„ íƒ€ì…
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub enum CrawlingEngineType {
    Basic,      // BatchCrawlingEngine
    Service,    // ServiceBasedBatchCrawlingEngine  
    Advanced,   // AdvancedBatchCrawlingEngine
}

/// ì‘ì—… ê²°ê³¼
#[derive(Debug, Clone)]
pub struct TaskResult {
    pub session_id: String,
    pub items_processed: u32,
    pub items_success: u32,
    pub items_failed: u32,
    pub duration: Duration,
    pub final_status: CrawlingStatus,
}

/// ì¬ì‹œë„ ê´€ë¦¬ì
pub struct RetryManager {
    max_retries: u32,
    retry_queue: Arc<Mutex<std::collections::VecDeque<RetryItem>>>,
    failure_classifier: Arc<dyn FailureClassifier>,
}

/// ì¬ì‹œë„ ì•„ì´í…œ
#[derive(Debug, Clone)]
pub struct RetryItem {
    pub item_id: String,
    pub stage: CrawlingStage,
    pub attempt_count: u32,
    pub last_error: String,
    pub next_retry_time: DateTime<Utc>,
    pub exponential_backoff: Duration,
}

/// ì‹¤íŒ¨ ë¶„ë¥˜ê¸° íŠ¸ë ˆì´íŠ¸
#[async_trait::async_trait]
pub trait FailureClassifier: Send + Sync {
    async fn classify_error(&self, error: &str, stage: CrawlingStage) -> ErrorClassification;
    async fn calculate_backoff(&self, attempt_count: u32) -> Duration;
}

/// ì—ëŸ¬ ë¶„ë¥˜
#[derive(Debug, Clone)]
pub enum ErrorClassification {
    Recoverable { retry_after: Duration },
    NonRecoverable { reason: String },
    RateLimited { retry_after: Duration },
    NetworkError { retry_after: Duration },
}

/// ì„±ëŠ¥ ëª¨ë‹ˆí„°
pub struct PerformanceMonitor {
    session_metrics: Arc<RwLock<HashMap<String, SessionMetrics>>>,
    global_metrics: Arc<RwLock<GlobalMetrics>>,
}

#[derive(Debug, Clone)]
pub struct SessionMetrics {
    pub start_time: Instant,
    pub items_processed: u32,
    pub average_response_time: Duration,
    pub error_rate: f64,
    pub current_concurrency: u32,
}

#[derive(Debug, Clone)]
pub struct GlobalMetrics {
    pub total_sessions: u32,
    pub active_sessions: u32,
    pub average_session_duration: Duration,
    pub total_items_processed: u64,
    pub overall_success_rate: f64,
}

/// í†µí•© í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €
#[derive(Clone)]
pub struct CrawlerManager {
    // í•µì‹¬ ì»´í¬ë„ŒíŠ¸
    session_manager: Arc<SessionManager>,
    retry_manager: Arc<RetryManager>,
    performance_monitor: Arc<PerformanceMonitor>,
    event_emitter: Arc<RwLock<Option<EventEmitter>>>,
    
    // í¬ë¡¤ë§ ì—”ì§„ë“¤
    basic_engine: Arc<BatchCrawlingEngine>,
    service_engine: Arc<ServiceBasedBatchCrawlingEngine>,
    advanced_engine: Arc<AdvancedBatchCrawlingEngine>,
    
    // í™œì„± ì„¸ì…˜ë“¤
    active_processors: Arc<RwLock<HashMap<String, Arc<dyn BatchProcessor>>>>,
}

impl CrawlerManager {
    /// ìƒˆ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ìƒì„±
    pub fn new(
        session_manager: Arc<SessionManager>,
        basic_engine: Arc<BatchCrawlingEngine>,
        service_engine: Arc<ServiceBasedBatchCrawlingEngine>,
        advanced_engine: Arc<AdvancedBatchCrawlingEngine>,
    ) -> Self {
        let retry_manager = Arc::new(RetryManager::new(3)); // ê¸°ë³¸ 3íšŒ ì¬ì‹œë„
        let performance_monitor = Arc::new(PerformanceMonitor::new());
        
        Self {
            session_manager,
            retry_manager,
            performance_monitor,
            event_emitter: Arc::new(RwLock::new(None)),
            basic_engine,
            service_engine,
            advanced_engine,
            active_processors: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// ì´ë²¤íŠ¸ ì—ë¯¸í„° ì„¤ì •
    pub async fn set_event_emitter(&self, emitter: EventEmitter) {
        let mut event_emitter = self.event_emitter.write().await;
        *event_emitter = Some(emitter);
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘ - .local/crawling_explanation.mdì˜ CrawlerManager::startBatchCrawling êµ¬í˜„
    pub async fn start_batch_crawling(&self, config: CrawlingConfig) -> Result<String> {
        info!("ğŸš€ Starting batch crawling with config: {:?}", config);
        
        // 1. ì„¸ì…˜ ìƒì„±
        let session_id = self.session_manager.create_session().await;
        info!("ğŸ“ Created crawling session: {}", session_id);
        
        // 2. ì—”ì§„ íƒ€ì…ì— ë”°ë¥¸ ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì„ íƒ
        let processor = self.create_batch_processor(&config).await?;
        
        // 3. í™œì„± í”„ë¡œì„¸ì„œì— ë“±ë¡
        {
            let mut active = self.active_processors.write().await;
            active.insert(session_id.clone(), processor.clone());
        }
        
        // 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_monitor.start_session_tracking(&session_id).await;
        
        // 5. ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
        let manager_clone = Arc::new(self.clone());
        let session_id_clone = session_id.clone();
        let config_clone = config.clone();
        
        tokio::spawn(async move {
            match manager_clone.execute_batch_crawling(session_id_clone.clone(), config_clone).await {
                Ok(result) => {
                    info!("âœ… Batch crawling completed successfully: {:?}", result);
                    manager_clone.handle_batch_success(&session_id_clone, result).await;
                }
                Err(error) => {
                    error!("âŒ Batch crawling failed: {}", error);
                    manager_clone.handle_batch_failure(&session_id_clone, error).await;
                }
            }
        });
        
        Ok(session_id)
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì¤‘ì§€
    pub async fn stop_batch_crawling(&self, session_id: &str) -> Result<()> {
        info!("ğŸ›‘ Stopping batch crawling for session: {}", session_id);
        
        // 1. í™œì„± í”„ë¡œì„¸ì„œì—ì„œ ì°¾ê¸°
        let processor = {
            let active = self.active_processors.read().await;
            active.get(session_id).cloned()
        };
        
        if let Some(processor) = processor {
            // 2. í”„ë¡œì„¸ì„œ ì¤‘ì§€
            processor.stop().await?;
            
            // 3. í™œì„± í”„ë¡œì„¸ì„œì—ì„œ ì œê±°
            {
                let mut active = self.active_processors.write().await;
                active.remove(session_id);
            }
            
            // 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.performance_monitor.stop_session_tracking(session_id).await;
            
            info!("âœ… Batch crawling stopped for session: {}", session_id);
        } else {
            warn!("âš ï¸ Session not found or already stopped: {}", session_id);
        }
        
        Ok(())
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì¼ì‹œì •ì§€
    pub async fn pause_batch_crawling(&self, session_id: &str) -> Result<()> {
        info!("â¸ï¸ Pausing batch crawling for session: {}", session_id);
        
        let processor = {
            let active = self.active_processors.read().await;
            active.get(session_id).cloned()
        };
        
        if let Some(processor) = processor {
            processor.pause().await?;
            info!("âœ… Batch crawling paused for session: {}", session_id);
        } else {
            return Err(anyhow!("Session not found: {}", session_id));
        }
        
        Ok(())
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì¬ê°œ
    pub async fn resume_batch_crawling(&self, session_id: &str) -> Result<()> {
        info!("â–¶ï¸ Resuming batch crawling for session: {}", session_id);
        
        let processor = {
            let active = self.active_processors.read().await;
            active.get(session_id).cloned()
        };
        
        if let Some(processor) = processor {
            processor.resume().await?;
            info!("âœ… Batch crawling resumed for session: {}", session_id);
        } else {
            return Err(anyhow!("Session not found: {}", session_id));
        }
        
        Ok(())
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ì¡°íšŒ
    pub async fn get_batch_progress(&self, session_id: &str) -> Result<CrawlingProgress> {
        let processor = {
            let active = self.active_processors.read().await;
            active.get(session_id).cloned()
        };
        
        if let Some(processor) = processor {
            Ok(processor.get_progress().await)
        } else {
            Err(anyhow!("Session not found: {}", session_id))
        }
    }
    
    /// ë°°ì¹˜ í”„ë¡œì„¸ì„œ ìƒì„±
    async fn create_batch_processor(&self, config: &CrawlingConfig) -> Result<Arc<dyn BatchProcessor>> {
        match config.engine_type {
            CrawlingEngineType::Basic => {
                Ok(Arc::new(BasicBatchProcessor::new(self.basic_engine.clone())))
            }
            CrawlingEngineType::Service => {
                Ok(Arc::new(ServiceBatchProcessor::new(self.service_engine.clone())))
            }
            CrawlingEngineType::Advanced => {
                Ok(Arc::new(AdvancedBatchProcessor::new(self.advanced_engine.clone())))
            }
        }
    }
    
    /// ì‹¤ì œ ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
    async fn execute_batch_crawling(&self, session_id: String, config: CrawlingConfig) -> Result<TaskResult> {
        let start_time = Instant::now();
        
        // ë°°ì¹˜ í”„ë¡œì„¸ì„œ ê°€ì ¸ì˜¤ê¸°
        let active_processors = self.active_processors.read().await;
        let processor = active_processors.get(&session_id)
            .ok_or_else(|| anyhow!("Processor not found for session: {}", session_id))?;
        
        // í¬ë¡¤ë§ ì‹¤í–‰
        let result = processor.execute_task(config.clone()).await?;
        
        Ok(TaskResult {
            session_id,
            items_processed: result.items_processed,
            items_success: result.items_success,
            items_failed: result.items_failed,
            duration: start_time.elapsed(),
            final_status: CrawlingStatus::Completed,
        })
    }
    
    /// ë°°ì¹˜ ì„±ê³µ ì²˜ë¦¬
    async fn handle_batch_success(&self, session_id: &str, result: TaskResult) {
        info!("ğŸ‰ Batch crawling success for session {}: {:?}", session_id, result);
        
        // ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.session_manager.update_session_status(session_id, CrawlingStatus::Completed).await;
        
        // ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
        self.performance_monitor.record_success(session_id, &result).await;
        
        // ì´ë²¤íŠ¸ ë°œì†¡
        self.emit_batch_complete(session_id, result).await;
    }
    
    /// ë°°ì¹˜ ì‹¤íŒ¨ ì²˜ë¦¬
    async fn handle_batch_failure(&self, session_id: &str, error: anyhow::Error) {
        warn!("ğŸ’¥ Batch crawling failed for session {}: {}", session_id, error);
        
        // ì¬ì‹œë„ ê°€ëŠ¥í•œì§€ í™•ì¸
        if self.should_retry(&error).await {
            info!("ğŸ”„ Scheduling retry for session: {}", session_id);
            self.schedule_retry(session_id, error.to_string()).await;
        } else {
            // ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬
            self.session_manager.update_session_status(session_id, CrawlingStatus::Error).await;
            self.emit_batch_failed(session_id, error.to_string()).await;
        }
    }
    
    /// ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •
    async fn should_retry(&self, _error: &anyhow::Error) -> bool {
        // TODO: ì—ëŸ¬ íƒ€ì…ì— ë”°ë¥¸ ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
        false
    }
    
    /// ì¬ì‹œë„ ì˜ˆì•½
    async fn schedule_retry(&self, session_id: &str, error: String) {
        // TODO: RetryManagerë¥¼ í†µí•œ ì¬ì‹œë„ ìŠ¤ì¼€ì¤„ë§
    }
    
    /// ìƒíƒœ ë³€ê²½ ì´ë²¤íŠ¸ ë°œì†¡
    async fn emit_status_change(&self, session_id: &str, status: CrawlingStatus) {
        if let Some(emitter) = self.event_emitter.read().await.as_ref() {
            let progress = CrawlingProgress {
                current: 0,
                total: 100,
                percentage: 0.0,
                current_stage: CrawlingStage::Idle,
                status,
                new_items: 0,
                updated_items: 0,
                errors: 0,
                timestamp: Utc::now().to_rfc3339(),
                current_step: format!("Session {} status changed", session_id),
                message: "".to_string(),
                elapsed_time: 0,
            };
            
            if let Err(e) = emitter.emit_progress(progress).await {
                error!("Failed to emit status change: {}", e);
            }
        }
    }
    
    /// ë°°ì¹˜ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
    async fn emit_batch_complete(&self, session_id: &str, result: TaskResult) {
        if let Some(emitter) = self.event_emitter.read().await.as_ref() {
            let progress = CrawlingProgress {
                current: result.items_processed,
                total: result.items_processed,
                percentage: 100.0,
                current_stage: CrawlingStage::Completed,
                status: CrawlingStatus::Completed,
                new_items: result.items_success,
                updated_items: 0,
                errors: result.items_failed,
                timestamp: Utc::now().to_rfc3339(),
                current_step: format!("Batch crawling completed for session {}", session_id),
                message: format!("Processed {} items in {:?}", result.items_processed, result.duration),
                elapsed_time: result.duration.as_secs(),
            };
            
            if let Err(e) = emitter.emit_progress(progress).await {
                error!("Failed to emit batch complete: {}", e);
            }
        }
    }
    
    /// ë°°ì¹˜ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡
    async fn emit_batch_failed(&self, session_id: &str, error: String) {
        if let Some(emitter) = self.event_emitter.read().await.as_ref() {
            let progress = CrawlingProgress {
                current: 0,
                total: 0,
                percentage: 0.0,
                current_stage: CrawlingStage::Error,
                status: CrawlingStatus::Error,
                new_items: 0,
                updated_items: 0,
                errors: 1,
                timestamp: Utc::now().to_rfc3339(),
                current_step: format!("Batch crawling failed for session {}", session_id),
                message: error,
                elapsed_time: 0,
            };
            
            if let Err(e) = emitter.emit_progress(progress).await {
                error!("Failed to emit batch failed: {}", e);
            }
        }
    }
}

// Clone êµ¬í˜„ (Arc ê¸°ë°˜ì´ë¯€ë¡œ ì•ˆì „)
impl Clone for CrawlerManager {
    fn clone(&self) -> Self {
        Self {
            session_manager: self.session_manager.clone(),
            retry_manager: self.retry_manager.clone(),
            performance_monitor: self.performance_monitor.clone(),
            event_emitter: self.event_emitter.clone(),
            basic_engine: self.basic_engine.clone(),
            service_engine: self.service_engine.clone(),
            advanced_engine: self.advanced_engine.clone(),
            active_processors: self.active_processors.clone(),
        }
    }
}

// ============================================================================
// BatchProcessor êµ¬í˜„ì²´ë“¤
// ============================================================================

impl BasicBatchProcessor {
    pub fn new(engine: Arc<BatchCrawlingEngine>) -> Self {
        Self { engine }
    }
}

#[async_trait::async_trait]
impl BatchProcessor for BasicBatchProcessor {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult> {
        info!("ğŸ”§ Executing task with BasicBatchProcessor");
        
        // BatchCrawlingEngineì˜ ì„¤ì • íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        let batch_config = crate::infrastructure::crawling_engine::BatchCrawlingConfig {
            start_page: config.start_page,
            end_page: config.end_page,
            concurrency: config.concurrency,
            delay_ms: config.delay_ms,
            batch_size: config.batch_size,
            retry_max: config.retry_max,
            timeout_ms: config.timeout_ms,
        };
        
        // ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ (TODO: ì‹¤ì œ ë©”ì„œë“œ í˜¸ì¶œë¡œ êµì²´)
        let start_time = Instant::now();
        
        // ì„ì‹œ ê²°ê³¼ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” engineì„ ì‚¬ìš©)
        Ok(TaskResult {
            session_id: "basic_session".to_string(),
            items_processed: config.end_page - config.start_page + 1,
            items_success: config.end_page - config.start_page + 1,
            items_failed: 0,
            duration: start_time.elapsed(),
            final_status: CrawlingStatus::Completed,
        })
    }
    
    async fn get_progress(&self) -> CrawlingProgress {
        CrawlingProgress {
            current: 0,
            total: 100,
            percentage: 0.0,
            current_stage: CrawlingStage::Processing,
            status: CrawlingStatus::Running,
            new_items: 0,
            updated_items: 0,
            errors: 0,
            timestamp: Utc::now().to_rfc3339(),
            current_step: "BasicBatchProcessor running".to_string(),
            message: "".to_string(),
            elapsed_time: 0,
        }
    }
    
    async fn pause(&self) -> Result<()> {
        info!("â¸ï¸ BasicBatchProcessor paused");
        Ok(())
    }
    
    async fn resume(&self) -> Result<()> {
        info!("â–¶ï¸ BasicBatchProcessor resumed");
        Ok(())
    }
    
    async fn stop(&self) -> Result<()> {
        info!("â¹ï¸ BasicBatchProcessor stopped");
        Ok(())
    }
}

impl ServiceBatchProcessor {
    pub fn new(engine: Arc<ServiceBasedBatchCrawlingEngine>) -> Self {
        Self { engine }
    }
}

#[async_trait::async_trait]
impl BatchProcessor for ServiceBatchProcessor {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult> {
        info!("ğŸ”§ Executing task with ServiceBatchProcessor");
        
        let start_time = Instant::now();
        
        // ì„ì‹œ ê²°ê³¼ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” engineì„ ì‚¬ìš©)
        Ok(TaskResult {
            session_id: "service_session".to_string(),
            items_processed: config.end_page - config.start_page + 1,
            items_success: config.end_page - config.start_page + 1,
            items_failed: 0,
            duration: start_time.elapsed(),
            final_status: CrawlingStatus::Completed,
        })
    }
    
    async fn get_progress(&self) -> CrawlingProgress {
        CrawlingProgress {
            current: 0,
            total: 100,
            percentage: 0.0,
            current_stage: CrawlingStage::Processing,
            status: CrawlingStatus::Running,
            new_items: 0,
            updated_items: 0,
            errors: 0,
            timestamp: Utc::now().to_rfc3339(),
            current_step: "ServiceBatchProcessor running".to_string(),
            message: "".to_string(),
            elapsed_time: 0,
        }
    }
    
    async fn pause(&self) -> Result<()> {
        info!("â¸ï¸ ServiceBatchProcessor paused");
        Ok(())
    }
    
    async fn resume(&self) -> Result<()> {
        info!("â–¶ï¸ ServiceBatchProcessor resumed");
        Ok(())
    }
    
    async fn stop(&self) -> Result<()> {
        info!("â¹ï¸ ServiceBatchProcessor stopped");
        Ok(())
    }
}

impl AdvancedBatchProcessor {
    pub fn new(engine: Arc<AdvancedBatchCrawlingEngine>) -> Self {
        Self { engine }
    }
}

#[async_trait::async_trait]
impl BatchProcessor for AdvancedBatchProcessor {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult> {
        info!("ğŸ”§ Executing task with AdvancedBatchProcessor");
        
        let start_time = Instant::now();
        
        // ì„ì‹œ ê²°ê³¼ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” engineì„ ì‚¬ìš©)
        Ok(TaskResult {
            session_id: "advanced_session".to_string(),
            items_processed: config.end_page - config.start_page + 1,
            items_success: config.end_page - config.start_page + 1,
            items_failed: 0,
            duration: start_time.elapsed(),
            final_status: CrawlingStatus::Completed,
        })
    }
    
    async fn get_progress(&self) -> CrawlingProgress {
        CrawlingProgress {
            current: 0,
            total: 100,
            percentage: 0.0,
            current_stage: CrawlingStage::Processing,
            status: CrawlingStatus::Running,
            new_items: 0,
            updated_items: 0,
            errors: 0,
            timestamp: Utc::now().to_rfc3339(),
            current_step: "AdvancedBatchProcessor running".to_string(),
            message: "".to_string(),
            elapsed_time: 0,
        }
    }
    
    async fn pause(&self) -> Result<()> {
        info!("â¸ï¸ AdvancedBatchProcessor paused");
        Ok(())
    }
    
    async fn resume(&self) -> Result<()> {
        info!("â–¶ï¸ AdvancedBatchProcessor resumed");
        Ok(())
    }
    
    async fn stop(&self) -> Result<()> {
        info!("â¹ï¸ AdvancedBatchProcessor stopped");
        Ok(())
    }
}

// ì¬ì‹œë„ ê´€ë¦¬ì êµ¬í˜„
impl RetryManager {
    /// ìƒˆë¡œìš´ ì¬ì‹œë„ ê´€ë¦¬ì ìƒì„±
    pub fn new(max_retries: u32) -> Self {
        Self {
            max_retries,
            retry_queue: Arc::new(Mutex::new(std::collections::VecDeque::new())),
            failure_classifier: Arc::new(DefaultFailureClassifier::new()),
        }
    }
    
    /// ì‹¤íŒ¨í•œ í•­ëª©ì„ ì¬ì‹œë„ íì— ì¶”ê°€
    pub async fn add_retry_item(&self, item_id: String, stage: CrawlingStage, error: String) -> Result<()> {
        let mut queue = self.retry_queue.lock().await;
        
        // ê¸°ì¡´ í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸
        let existing_item = queue.iter_mut().find(|item| item.item_id == item_id);
        
        if let Some(item) = existing_item {
            // ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸
            item.attempt_count += 1;
            item.last_error = error.clone();
            item.exponential_backoff = self.failure_classifier.calculate_backoff(item.attempt_count).await;
            item.next_retry_time = Utc::now() + chrono::Duration::from_std(item.exponential_backoff)?;
            
            if item.attempt_count >= self.max_retries {
                warn!("âŒ Item {} exceeded max retries ({}), removing from queue", item_id, self.max_retries);
                queue.retain(|i| i.item_id != item_id);
                return Ok(());
            }
        } else {
            // ìƒˆ í•­ëª© ì¶”ê°€
            let classification = self.failure_classifier.classify_error(&error, stage).await;
            
            match classification {
                ErrorClassification::NonRecoverable { reason } => {
                    warn!("âŒ Non-recoverable error for {}: {}", item_id, reason);
                    return Ok(());
                }
                ErrorClassification::Recoverable { retry_after } |
                ErrorClassification::RateLimited { retry_after } |
                ErrorClassification::NetworkError { retry_after } => {
                    let retry_item = RetryItem {
                        item_id,
                        stage,
                        attempt_count: 1,
                        last_error: error,
                        next_retry_time: Utc::now() + chrono::Duration::from_std(retry_after)?,
                        exponential_backoff: retry_after,
                    };
                    
                    queue.push_back(retry_item);
                    info!("ğŸ”„ Added item to retry queue, total items: {}", queue.len());
                }
            }
        }
        
        Ok(())
    }
    
    /// ì¬ì‹œë„ ê°€ëŠ¥í•œ í•­ëª©ë“¤ì„ ê°€ì ¸ì˜´
    pub async fn get_ready_items(&self) -> Vec<RetryItem> {
        let mut queue = self.retry_queue.lock().await;
        let now = Utc::now();
        
        let ready_items: Vec<RetryItem> = queue
            .iter()
            .filter(|item| item.next_retry_time <= now)
            .cloned()
            .collect();
        
        // ì¤€ë¹„ëœ í•­ëª©ë“¤ì„ íì—ì„œ ì œê±°
        queue.retain(|item| item.next_retry_time > now);
        
        if !ready_items.is_empty() {
            info!("ğŸ”„ Retrieved {} items ready for retry", ready_items.len());
        }
        
        ready_items
    }
    
    /// ì¬ì‹œë„ í ìƒíƒœ ì¡°íšŒ
    pub async fn get_queue_status(&self) -> (usize, usize) {
        let queue = self.retry_queue.lock().await;
        let total_items = queue.len();
        let ready_items = queue
            .iter()
            .filter(|item| item.next_retry_time <= Utc::now())
            .count();
        
        (total_items, ready_items)
    }
}

/// ê¸°ë³¸ ì‹¤íŒ¨ ë¶„ë¥˜ê¸° êµ¬í˜„
pub struct DefaultFailureClassifier;

impl DefaultFailureClassifier {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait::async_trait]
impl FailureClassifier for DefaultFailureClassifier {
    async fn classify_error(&self, error: &str, stage: CrawlingStage) -> ErrorClassification {
        let error_lower = error.to_lowercase();
        
        // HTTP ìƒíƒœ ì½”ë“œ ë¶„ì„
        if error_lower.contains("429") || error_lower.contains("rate limit") {
            return ErrorClassification::RateLimited { 
                retry_after: Duration::from_secs(60) // 1ë¶„ ëŒ€ê¸°
            };
        }
        
        if error_lower.contains("timeout") || error_lower.contains("connection") {
            return ErrorClassification::NetworkError { 
                retry_after: Duration::from_secs(30) // 30ì´ˆ ëŒ€ê¸°
            };
        }
        
        if error_lower.contains("500") || error_lower.contains("502") || error_lower.contains("503") {
            return ErrorClassification::Recoverable { 
                retry_after: Duration::from_secs(45) // 45ì´ˆ ëŒ€ê¸°
            };
        }
        
        // 404, 400 ë“±ì€ ì¬ì‹œë„í•´ë„ ì˜ë¯¸ ì—†ìŒ
        if error_lower.contains("404") || error_lower.contains("400") || error_lower.contains("403") {
            return ErrorClassification::NonRecoverable { 
                reason: "Client error - retry will not help".to_string()
            };
        }
        
        // íŒŒì‹± ë‹¨ê³„ë³„ ì—ëŸ¬ ë¶„ë¥˜
        match stage {
            CrawlingStage::ParseItemDetails => {
                if error_lower.contains("parse") || error_lower.contains("format") {
                    ErrorClassification::NonRecoverable { 
                        reason: "Parsing error - data format issue".to_string()
                    }
                } else {
                    ErrorClassification::Recoverable { 
                        retry_after: Duration::from_secs(20)
                    }
                }
            }
            CrawlingStage::DatabaseSave => {
                ErrorClassification::Recoverable { 
                    retry_after: Duration::from_secs(10) // DB ì—ëŸ¬ëŠ” ë¹ ë¥´ê²Œ ì¬ì‹œë„
                }
            }
            _ => {
                ErrorClassification::Recoverable { 
                    retry_after: Duration::from_secs(30)
                }
            }
        }
    }
    
    async fn calculate_backoff(&self, attempt_count: u32) -> Duration {
        // ì§€ìˆ˜ ë°±ì˜¤í”„: 2^attempt_count ì´ˆ, ìµœëŒ€ 5ë¶„
        let base_seconds = 2_u64.pow(attempt_count.min(8)); // ìµœëŒ€ 256ì´ˆ
        let max_seconds = 300; // 5ë¶„
        
        Duration::from_secs(base_seconds.min(max_seconds))
    }
}

impl PerformanceMonitor {
    /// ìƒˆë¡œìš´ ì„±ëŠ¥ ëª¨ë‹ˆí„° ìƒì„±
    pub fn new() -> Self {
        Self {
            session_metrics: Arc::new(RwLock::new(HashMap::new())),
            global_metrics: Arc::new(RwLock::new(GlobalMetrics {
                total_sessions: 0,
                active_sessions: 0,
                average_session_duration: Duration::from_secs(0),
                total_items_processed: 0,
                overall_success_rate: 0.0,
            })),
        }
    }
    
    /// ì„¸ì…˜ ì¶”ì  ì‹œì‘
    pub async fn start_session_tracking(&self, session_id: &str) {
        let mut sessions = self.session_metrics.write().await;
        let mut global = self.global_metrics.write().await;
        
        sessions.insert(session_id.to_string(), SessionMetrics {
            start_time: Instant::now(),
            items_processed: 0,
            average_response_time: Duration::from_secs(0),
            error_rate: 0.0,
            current_concurrency: 1,
        });
        
        global.total_sessions += 1;
        global.active_sessions += 1;
        
        info!("ğŸ“Š Started performance tracking for session: {}", session_id);
    }
    
    /// ì„¸ì…˜ ì¶”ì  ì¢…ë£Œ
    pub async fn end_session_tracking(&self, session_id: &str) {
        let mut sessions = self.session_metrics.write().await;
        let mut global = self.global_metrics.write().await;
        
        if sessions.remove(session_id).is_some() {
            global.active_sessions = global.active_sessions.saturating_sub(1);
            info!("ğŸ“Š Ended performance tracking for session: {}", session_id);
        }
    }
    
    /// ì„¸ì…˜ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    pub async fn update_session_metrics(&self, session_id: &str, items_processed: u32, response_time: Duration, error_rate: f64) {
        let mut sessions = self.session_metrics.write().await;
        
        if let Some(metrics) = sessions.get_mut(session_id) {
            metrics.items_processed = items_processed;
            metrics.average_response_time = response_time;
            metrics.error_rate = error_rate;
        }
    }
    
    /// ê¸€ë¡œë²Œ ë©”íŠ¸ë¦­ ì¡°íšŒ
    pub async fn get_global_metrics(&self) -> GlobalMetrics {
        self.global_metrics.read().await.clone()
    }
    
    /// ì„¸ì…˜ ë©”íŠ¸ë¦­ ì¡°íšŒ
    pub async fn get_session_metrics(&self, session_id: &str) -> Option<SessionMetrics> {
        let sessions = self.session_metrics.read().await;
        sessions.get(session_id).cloned()
    }
}
