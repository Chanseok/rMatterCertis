//! í†µí•© í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € - PHASE2_CRAWLING_ENHANCEMENT_PLAN êµ¬í˜„
//! 
//! ì´ ëª¨ë“ˆì€ .local/crawling_explanation.mdì— ì •ì˜ëœ CrawlerManager ì—­í• ì„ ìˆ˜í–‰í•˜ë©°,
//! 3ê°œì˜ í¬ë¡¤ë§ ì—”ì§„ì„ í†µí•© ê´€ë¦¬í•˜ê³  ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤.

use std::sync::Arc;
use std::collections::HashMap;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};
use anyhow::{Result, anyhow};
use tracing::{info, warn, error, debug};
use chrono::{DateTime, Utc};
use tokio_util::sync::CancellationToken;

use crate::domain::session_manager::SessionManager;
use crate::domain::events::{CrawlingProgress, CrawlingStage, CrawlingStatus};
use crate::application::EventEmitter;
use crate::infrastructure::{
    BatchCrawlingEngine,
    AdvancedBatchCrawlingEngine,
    BatchCrawlingConfig,
    HttpClient,
    MatterDataExtractor,
    IntegratedProductRepository,
};
use crate::new_architecture::{
    actor_system::{SessionActor, ActorError},
    system_config::SystemConfig,
    channels::types::{ActorCommand, AppEvent, BatchConfig},
    integrated_context::{IntegratedContext, IntegratedContextFactory},
};
use tauri::AppHandle;

/// ë°°ì¹˜ í”„ë¡œì„¸ì„œ íŠ¸ë ˆì´íŠ¸ - 3ê°œ ì—”ì§„ì„ í†µí•©í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤
#[async_trait::async_trait]
pub trait BatchProcessor: Send + Sync {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult>;
    async fn get_progress(&self) -> CrawlingProgress;
    async fn pause(&self) -> Result<()>;
    async fn resume(&self) -> Result<()>;
    async fn stop(&self) -> Result<()>;
}

/// í¬ë¡¤ë§ ì„¤ì • í†µí•© êµ¬ì¡°ì²´
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CrawlingConfig {
    pub start_page: u32,
    pub end_page: u32,
    pub concurrency: u32,
    pub delay_ms: u64,
    pub batch_size: u32,
    pub retry_max: u32,
    pub timeout_ms: u64,
    pub engine_type: CrawlingEngineType,
    #[serde(skip)]
    pub cancellation_token: Option<tokio_util::sync::CancellationToken>,
}

/// í¬ë¡¤ë§ ì—”ì§„ íƒ€ì…
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub enum CrawlingEngineType {
    Basic,      // BatchCrawlingEngine
    // Service variant removed (legacy ServiceBasedBatchCrawlingEngine deprecated)
    Advanced,   // AdvancedBatchCrawlingEngine
    Actor,      // ğŸ¯ NEW: ActorBatchProcessor (SessionActor â†’ BatchActor â†’ StageActor)
}

/// ì‘ì—… ê²°ê³¼
#[derive(Debug, Clone)]
pub struct TaskResult {
    pub session_id: String,
    pub items_processed: u32,
    pub items_success: u32,
    pub items_failed: u32,
    pub duration: Duration,
    pub final_status: CrawlingStatus,
}

/// ì¬ì‹œë„ ê´€ë¦¬ì
pub struct RetryManager {
    max_retries: u32,
    retry_queue: Arc<Mutex<std::collections::VecDeque<RetryItem>>>,
    failure_classifier: Arc<dyn FailureClassifier>,
}

/// ì¬ì‹œë„ ì•„ì´í…œ
#[derive(Debug, Clone)]
pub struct RetryItem {
    pub item_id: String,
    pub stage: CrawlingStage,
    pub attempt_count: u32,
    pub last_error: String,
    pub next_retry_time: DateTime<Utc>,
    pub exponential_backoff: Duration,
}

/// ì‹¤íŒ¨ ë¶„ë¥˜ê¸° íŠ¸ë ˆì´íŠ¸
#[async_trait::async_trait]
pub trait FailureClassifier: Send + Sync {
    async fn classify_error(&self, error: &str, stage: CrawlingStage) -> ErrorClassification;
    async fn calculate_backoff(&self, attempt_count: u32) -> Duration;
}

/// ì—ëŸ¬ ë¶„ë¥˜
#[derive(Debug, Clone)]
pub enum ErrorClassification {
    Recoverable { retry_after: Duration },
    NonRecoverable { reason: String },
    RateLimited { retry_after: Duration },
    NetworkError { retry_after: Duration },
}

/// ì„±ëŠ¥ ëª¨ë‹ˆí„°
pub struct PerformanceMonitor {
    session_metrics: Arc<RwLock<HashMap<String, SessionMetrics>>>,
    global_metrics: Arc<RwLock<GlobalMetrics>>,
}

#[derive(Debug, Clone)]
pub struct SessionMetrics {
    pub start_time: Instant,
    pub items_processed: u32,
    pub average_response_time: Duration,
    pub error_rate: f64,
    pub current_concurrency: u32,
}

#[derive(Debug, Clone)]
pub struct GlobalMetrics {
    pub total_sessions: u32,
    pub active_sessions: u32,
    pub average_session_duration: Duration,
    pub total_items_processed: u64,
    pub overall_success_rate: f64,
}

/// í†µí•© í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €
#[derive(Clone)]
pub struct CrawlerManager {
    // í•µì‹¬ ì»´í¬ë„ŒíŠ¸
    session_manager: Arc<SessionManager>,
    retry_manager: Arc<RetryManager>,
    performance_monitor: Arc<PerformanceMonitor>,
    event_emitter: Arc<RwLock<Option<EventEmitter>>>,
    
    // í¬ë¡¤ë§ ì—”ì§„ë“¤
    basic_engine: Arc<BatchCrawlingEngine>,
    service_engine: Arc<ServiceBasedBatchCrawlingEngine>,
    advanced_engine: Arc<AdvancedBatchCrawlingEngine>,
    
    // ê³µí†µ ì»´í¬ë„ŒíŠ¸ë“¤ (ServiceBatchProcessorì—ì„œ ì‚¬ìš©)
    http_client: HttpClient,
    data_extractor: MatterDataExtractor,
    product_repo: Arc<IntegratedProductRepository>,
    app_handle: Option<AppHandle>,
    
    // í™œì„± ì„¸ì…˜ë“¤
    active_processors: Arc<RwLock<HashMap<String, Arc<dyn BatchProcessor>>>>,
}

impl CrawlerManager {
    /// ìƒˆ í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ìƒì„±
    pub fn new(
        session_manager: Arc<SessionManager>,
        basic_engine: Arc<BatchCrawlingEngine>,
        service_engine: Arc<ServiceBasedBatchCrawlingEngine>,
        advanced_engine: Arc<AdvancedBatchCrawlingEngine>,
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        product_repo: Arc<IntegratedProductRepository>,
        app_handle: Option<AppHandle>,
    ) -> Self {
        let retry_manager = Arc::new(RetryManager::new(3)); // ê¸°ë³¸ 3íšŒ ì¬ì‹œë„
        let performance_monitor = Arc::new(PerformanceMonitor::new());
        
        Self {
            session_manager,
            retry_manager,
            performance_monitor,
            event_emitter: Arc::new(RwLock::new(None)),
            basic_engine,
            service_engine,
            advanced_engine,
            http_client,
            data_extractor,
            product_repo,
            app_handle,
            active_processors: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// ì´ë²¤íŠ¸ ì—ë¯¸í„° ì„¤ì •
    pub async fn set_event_emitter(&self, emitter: EventEmitter) {
        let mut event_emitter = self.event_emitter.write().await;
        *event_emitter = Some(emitter);
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘ - .local/crawling_explanation.mdì˜ CrawlerManager::startBatchCrawling êµ¬í˜„
    pub async fn start_batch_crawling(&self, config: CrawlingConfig) -> Result<String> {
        info!("ğŸš€ Starting batch crawling with config: {:?}", config);
        
        // 1. ì„¸ì…˜ ìƒì„±
        let session_id = self.session_manager.create_session().await;
        info!("ğŸ“ Created crawling session: {}", session_id);
        
        // 2. ì—”ì§„ íƒ€ì…ì— ë”°ë¥¸ ë°°ì¹˜ í”„ë¡œì„¸ì„œ ì„ íƒ
        let processor = self.create_batch_processor(&config).await?;
        
        // 3. í™œì„± í”„ë¡œì„¸ì„œì— ë“±ë¡
        {
            let mut active = self.active_processors.write().await;
            active.insert(session_id.clone(), processor.clone());
        }
        
        // 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.performance_monitor.start_session_tracking(&session_id).await;
        
        // 5. ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
        let manager_clone = Arc::new(self.clone());
        let session_id_clone = session_id.clone();
        let config_clone = config.clone();
        
        tokio::spawn(async move {
            match manager_clone.execute_batch_crawling(session_id_clone.clone(), config_clone).await {
                Ok(result) => {
                    info!("âœ… Batch crawling completed successfully: {:?}", result);
                    manager_clone.handle_batch_success(&session_id_clone, result).await;
                }
                Err(error) => {
                    error!("âŒ Batch crawling failed: {}", error);
                    manager_clone.handle_batch_failure(&session_id_clone, error).await;
                }
            }
        });
        
        Ok(session_id)
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì¤‘ì§€
    pub async fn stop_batch_crawling(&self, session_id: &str) -> Result<()> {
        info!("ğŸ›‘ Stopping batch crawling for session: {}", session_id);
        
        // 1. ì„¸ì…˜ ë§¤ë‹ˆì €ì—ì„œ ì„¸ì…˜ì„ ì¤‘ì§€ ìƒíƒœë¡œ ë§ˆí‚¹ (cancel token ì—­í• )
        if let Err(e) = self.session_manager.stop_session(session_id).await {
            warn!("Failed to mark session as stopped: {}", e);
        }
        
        // 2. í™œì„± í”„ë¡œì„¸ì„œì—ì„œ ì°¾ê¸°
        let processor = {
            let active = self.active_processors.read().await;
            active.get(session_id).cloned()
        };
        
        if let Some(processor) = processor {
            // 3. í”„ë¡œì„¸ì„œ ì¤‘ì§€
            processor.stop().await?;
            
            // 4. í™œì„± í”„ë¡œì„¸ì„œì—ì„œ ì œê±°
            {
                let mut active = self.active_processors.write().await;
                active.remove(session_id);
            }
            
            // 5. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.performance_monitor.stop_session_tracking(session_id).await;
            
            info!("âœ… Batch crawling stopped for session: {}", session_id);
        } else {
            warn!("âš ï¸ Session not found or already stopped: {}", session_id);
        }
        
        Ok(())
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì¼ì‹œì •ì§€
    pub async fn pause_batch_crawling(&self, session_id: &str) -> Result<()> {
        info!("â¸ï¸ Pausing batch crawling for session: {}", session_id);
        
        // 1. ì„¸ì…˜ ë§¤ë‹ˆì €ì—ì„œ ì„¸ì…˜ì„ ì¼ì‹œì •ì§€ ìƒíƒœë¡œ ë§ˆí‚¹
        if let Err(e) = self.session_manager.pause_session(session_id).await {
            warn!("Failed to mark session as paused: {}", e);
        }
        
        let processor = {
            let active = self.active_processors.read().await;
            active.get(session_id).cloned()
        };
        
        if let Some(processor) = processor {
            processor.pause().await?;
            info!("âœ… Batch crawling paused for session: {}", session_id);
        } else {
            return Err(anyhow!("Session not found: {}", session_id));
        }
        
        Ok(())
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì¬ê°œ
    pub async fn resume_batch_crawling(&self, session_id: &str) -> Result<()> {
        info!("â–¶ï¸ Resuming batch crawling for session: {}", session_id);
        
        // 1. ì„¸ì…˜ ë§¤ë‹ˆì €ì—ì„œ ì„¸ì…˜ì„ ì‹¤í–‰ ìƒíƒœë¡œ ë³€ê²½
        if let Err(e) = self.session_manager.resume_session(session_id).await {
            warn!("Failed to mark session as resumed: {}", e);
        }
        
        let processor = {
            let active = self.active_processors.read().await;
            active.get(session_id).cloned()
        };
        
        if let Some(processor) = processor {
            processor.resume().await?;
            info!("âœ… Batch crawling resumed for session: {}", session_id);
        } else {
            return Err(anyhow!("Session not found: {}", session_id));
        }
        
        Ok(())
    }
    
    /// ë°°ì¹˜ í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ì¡°íšŒ
    pub async fn get_batch_progress(&self, session_id: &str) -> Result<CrawlingProgress> {
        let processor = {
            let active = self.active_processors.read().await;
            active.get(session_id).cloned()
        };
        
        if let Some(processor) = processor {
            Ok(processor.get_progress().await)
        } else {
            Err(anyhow!("Session not found: {}", session_id))
        }
    }
    
    /// ë°°ì¹˜ í”„ë¡œì„¸ì„œ ìƒì„±
    async fn create_batch_processor(&self, config: &CrawlingConfig) -> Result<Arc<dyn BatchProcessor>> {
        match config.engine_type {
            CrawlingEngineType::Basic => {
                Ok(Arc::new(BasicBatchProcessor::new(self.basic_engine.clone())))
            }
            CrawlingEngineType::Service => {
                Ok(Arc::new(ServiceBatchProcessor::new(
                    self.service_engine.clone(),
                    self.http_client.clone(),
                    self.data_extractor.clone(),
                    self.product_repo.clone(),
                    self.app_handle.clone(),
                )))
            }
            CrawlingEngineType::Advanced => {
                Ok(Arc::new(AdvancedBatchProcessor::new(self.advanced_engine.clone())))
            }
            CrawlingEngineType::Actor => {
                info!("ğŸ­ [NEW ARCHITECTURE] Creating ActorBatchProcessor");
                Ok(Arc::new(ActorBatchProcessor::new(
                    self.http_client.clone(),
                    self.data_extractor.clone(),
                    self.product_repo.clone(),
                    self.event_emitter.clone(),
                    self.app_handle.clone(),
                )))
            }
        }
    }
    
    /// ì‹¤ì œ ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
    async fn execute_batch_crawling(&self, session_id: String, config: CrawlingConfig) -> Result<TaskResult> {
        let start_time = Instant::now();
        
        // ë°°ì¹˜ í”„ë¡œì„¸ì„œ ê°€ì ¸ì˜¤ê¸°
        let active_processors = self.active_processors.read().await;
        let processor = active_processors.get(&session_id)
            .ok_or_else(|| anyhow!("Processor not found for session: {}", session_id))?;
        
        // í¬ë¡¤ë§ ì‹¤í–‰
        let result = processor.execute_task(config.clone()).await?;
        
        Ok(TaskResult {
            session_id,
            items_processed: result.items_processed,
            items_success: result.items_success,
            items_failed: result.items_failed,
            duration: start_time.elapsed(),
            final_status: CrawlingStatus::Completed,
        })
    }
    
    /// ë°°ì¹˜ ì„±ê³µ ì²˜ë¦¬
    async fn handle_batch_success(&self, session_id: &str, result: TaskResult) {
        info!("ğŸ‰ Batch crawling success for session {}: {:?}", session_id, result);
        
        // ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.session_manager.update_session_status(session_id, CrawlingStatus::Completed).await;
        
        // ì„±ëŠ¥ ì§€í‘œ ì—…ë°ì´íŠ¸
        self.performance_monitor.record_success(session_id, &result).await;
        
        // ì´ë²¤íŠ¸ ë°œì†¡
        self.emit_batch_complete(session_id, result).await;
    }
    
    /// ë°°ì¹˜ ì‹¤íŒ¨ ì²˜ë¦¬
    async fn handle_batch_failure(&self, session_id: &str, error: anyhow::Error) {
        warn!("ğŸ’¥ Batch crawling failed for session {}: {}", session_id, error);
        
        // ì¬ì‹œë„ ê°€ëŠ¥í•œì§€ í™•ì¸
        if self.should_retry(&error).await {
            info!("ğŸ”„ Scheduling retry for session: {}", session_id);
            self.schedule_retry(session_id, error.to_string()).await;
        } else {
            // ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬
            self.session_manager.update_session_status(session_id, CrawlingStatus::Error).await;
            self.emit_batch_failed(session_id, error.to_string()).await;
        }
    }
    
    /// ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •
    async fn should_retry(&self, _error: &anyhow::Error) -> bool {
        // TODO: ì—ëŸ¬ íƒ€ì…ì— ë”°ë¥¸ ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
        false
    }
    
    /// ì¬ì‹œë„ ì˜ˆì•½
    async fn schedule_retry(&self, session_id: &str, error: String) {
        // TODO: RetryManagerë¥¼ í†µí•œ ì¬ì‹œë„ ìŠ¤ì¼€ì¤„ë§
    }
    
    /// ìƒíƒœ ë³€ê²½ ì´ë²¤íŠ¸ ë°œì†¡
    async fn emit_status_change(&self, session_id: &str, status: CrawlingStatus) {
        if let Some(emitter) = self.event_emitter.read().await.as_ref() {
            let progress = CrawlingProgress {
                current: 0,
                total: 100,
                percentage: 0.0,
                current_stage: CrawlingStage::Idle,
                status,
                new_items: 0,
                updated_items: 0,
                errors: 0,
                timestamp: Utc::now().to_rfc3339(),
                current_step: format!("Session {} status changed", session_id),
                message: "".to_string(),
                elapsed_time: 0,
            };
            
            if let Err(e) = emitter.emit_progress(progress).await {
                error!("Failed to emit status change: {}", e);
            }
        }
    }
    
    /// ë°°ì¹˜ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
    async fn emit_batch_complete(&self, session_id: &str, result: TaskResult) {
        if let Some(emitter) = self.event_emitter.read().await.as_ref() {
            let progress = CrawlingProgress {
                current: result.items_processed,
                total: result.items_processed,
                percentage: 100.0,
                current_stage: CrawlingStage::Completed,
                status: CrawlingStatus::Completed,
                new_items: result.items_success,
                updated_items: 0,
                errors: result.items_failed,
                timestamp: Utc::now().to_rfc3339(),
                current_step: format!("Batch crawling completed for session {}", session_id),
                message: format!("Processed {} items in {:?}", result.items_processed, result.duration),
                elapsed_time: result.duration.as_secs(),
            };
            
            if let Err(e) = emitter.emit_progress(progress).await {
                error!("Failed to emit batch complete: {}", e);
            }
        }
    }
    
    /// ë°°ì¹˜ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡
    async fn emit_batch_failed(&self, session_id: &str, error: String) {
        if let Some(emitter) = self.event_emitter.read().await.as_ref() {
            let progress = CrawlingProgress {
                current: 0,
                total: 0,
                percentage: 0.0,
                current_stage: CrawlingStage::Error,
                status: CrawlingStatus::Error,
                new_items: 0,
                updated_items: 0,
                errors: 1,
                timestamp: Utc::now().to_rfc3339(),
                current_step: format!("Batch crawling failed for session {}", session_id),
                message: error,
                elapsed_time: 0,
            };
            
            if let Err(e) = emitter.emit_progress(progress).await {
                error!("Failed to emit batch failed: {}", e);
            }
        }
    }
}

// Clone êµ¬í˜„ (Arc ê¸°ë°˜ì´ë¯€ë¡œ ì•ˆì „)
impl Clone for CrawlerManager {
    fn clone(&self) -> Self {
        Self {
            session_manager: self.session_manager.clone(),
            retry_manager: self.retry_manager.clone(),
            performance_monitor: self.performance_monitor.clone(),
            event_emitter: self.event_emitter.clone(),
            basic_engine: self.basic_engine.clone(),
            service_engine: self.service_engine.clone(),
            advanced_engine: self.advanced_engine.clone(),
            http_client: self.http_client.clone(),
            data_extractor: self.data_extractor.clone(),
            product_repo: self.product_repo.clone(),
            app_handle: self.app_handle.clone(),
            active_processors: self.active_processors.clone(),
        }
    }
}

// ============================================================================
// BatchProcessor êµ¬í˜„ì²´ë“¤
// ============================================================================

pub struct BasicBatchProcessor {
    engine: Arc<BatchCrawlingEngine>,
}

pub struct ServiceBatchProcessor {
    // ê¸°ì¡´ ì—”ì§„ì˜ ì»´í¬ë„ŒíŠ¸ë“¤ì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ ì—”ì§„ ì°¸ì¡° ìœ ì§€
    base_engine: Arc<ServiceBasedBatchCrawlingEngine>,
    http_client: HttpClient,
    data_extractor: MatterDataExtractor,
    product_repo: Arc<IntegratedProductRepository>,
    app_handle: Option<AppHandle>,
}

pub struct AdvancedBatchProcessor {
    engine: Arc<AdvancedBatchCrawlingEngine>,
}

impl BasicBatchProcessor {
    pub fn new(engine: Arc<BatchCrawlingEngine>) -> Self {
        Self { engine }
    }
}

#[async_trait::async_trait]
impl BatchProcessor for BasicBatchProcessor {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult> {
        info!("ğŸ”§ Executing task with BasicBatchProcessor");
        
        // BatchCrawlingEngineì˜ ì„¤ì • íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        let batch_config = crate::infrastructure::crawling_engine::BatchCrawlingConfig {
            start_page: config.start_page,
            end_page: config.end_page,
            concurrency: config.concurrency,
            delay_ms: config.delay_ms,
            batch_size: config.batch_size,
            retry_max: config.retry_max,
            timeout_ms: config.timeout_ms,
        };
        
        // ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ (TODO: ì‹¤ì œ ë©”ì„œë“œ í˜¸ì¶œë¡œ êµì²´)
        let start_time = Instant::now();
        
        // ì„ì‹œ ê²°ê³¼ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” engineì„ ì‚¬ìš©)
        Ok(TaskResult {
            session_id: "basic_session".to_string(),
            items_processed: config.end_page - config.start_page + 1,
            items_success: config.end_page - config.start_page + 1,
            items_failed: 0,
            duration: start_time.elapsed(),
            final_status: CrawlingStatus::Completed,
        })
    }
    
    async fn get_progress(&self) -> CrawlingProgress {
        CrawlingProgress {
            current: 0,
            total: 100,
            percentage: 0.0,
            current_stage: CrawlingStage::Processing,
            status: CrawlingStatus::Running,
            new_items: 0,
            updated_items: 0,
            errors: 0,
            timestamp: Utc::now().to_rfc3339(),
            current_step: "BasicBatchProcessor running".to_string(),
            message: "".to_string(),
            elapsed_time: 0,
        }
    }
    
    async fn pause(&self) -> Result<()> {
        info!("â¸ï¸ BasicBatchProcessor paused");
        Ok(())
    }
    
    async fn resume(&self) -> Result<()> {
        info!("â–¶ï¸ BasicBatchProcessor resumed");
        Ok(())
    }
    
    async fn stop(&self) -> Result<()> {
        info!("â¹ï¸ BasicBatchProcessor stopped");
        Ok(())
    }
}

/// ğŸ¯ NEW ARCHITECTURE: Actor ê¸°ë°˜ ë°°ì¹˜ í”„ë¡œì„¸ì„œ
/// guide/re-arch-plan-final2.mdì˜ Actor ëª¨ë¸ ê¸°ë°˜ êµ¬í˜„
pub struct ActorBatchProcessor {
    system_config: Arc<SystemConfig>,
    context_factory: IntegratedContextFactory,
    http_client: HttpClient,
    data_extractor: MatterDataExtractor,
    product_repo: Arc<IntegratedProductRepository>,
    event_emitter: Arc<RwLock<Option<EventEmitter>>>,
    app_handle: Option<AppHandle>,
}

impl ActorBatchProcessor {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        product_repo: Arc<IntegratedProductRepository>,
        event_emitter: Arc<RwLock<Option<EventEmitter>>>,
        app_handle: Option<AppHandle>,
    ) -> Self {
        let system_config = Arc::new(SystemConfig::default());
        let context_factory = IntegratedContextFactory::new(system_config.clone());
        
        Self {
            system_config,
            context_factory,
            http_client,
            data_extractor,
            product_repo,
            event_emitter,
            app_handle,
        }
    }
}

#[async_trait::async_trait]
impl BatchProcessor for ActorBatchProcessor {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult> {
        info!("ğŸ­ [NEW ARCHITECTURE] Executing task with ActorBatchProcessor");
        
        let start_time = Instant::now();
        let session_id = format!("actor_session_{}", chrono::Utc::now().timestamp());
        
        // 1. í†µí•© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        let (session_context, channels) = self.context_factory
            .create_session_context(session_id.clone())
            .map_err(|e| anyhow!("Failed to create session context: {}", e))?;
        
        info!("âœ… [ACTOR] Session context created: {}", session_id);
        
        // 2. Actor ì‹œìŠ¤í…œ ì„¤ì •ì— ë”°ë¥¸ ë°°ì¹˜ ì„¤ì • ë³€í™˜
        let batch_config = BatchConfig {
            target_url: "https://csa-iot.org/csa-iot_products/".to_string(),
            max_pages: Some(config.end_page),
        };
        
        // 3. CrawlingPlanner ìƒì„± (ì§€ëŠ¥í˜• ë°°ì¹˜ ë¶„í• ì„ ìœ„í•´)
        let config_for_planner = crate::new_architecture::config::SystemConfig::default();
        let crawling_planner = Arc::new(
            crate::new_architecture::services::crawling_planner::CrawlingPlanner::new(
                Arc::new(config_for_planner)
            )
        );
        
        info!("ğŸ§  [ACTOR] CrawlingPlanner created for intelligent batch planning");
        
        // 4. SessionActor ìƒì„± ë° CrawlingPlanner ì£¼ì…
        let session_actor = SessionActor::new(
            self.system_config.clone(),
            channels.control_rx,
            session_context.event_tx.clone(),
        ).with_planner(crawling_planner);
        
        info!("ğŸ­ [ACTOR] SessionActor created with CrawlingPlanner, starting execution");
        
        // 4. SessionActor ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
        let session_actor_handle = tokio::spawn(async move {
            info!("ğŸš€ [ACTOR] SessionActor background task started");
            match session_actor.run().await {
                Ok(_) => {
                    info!("âœ… [ACTOR] SessionActor completed successfully");
                }
                Err(e) => {
                    error!("âŒ [ACTOR] SessionActor failed: {}", e);
                }
            }
        });
        
        // 5. SessionActorê°€ ì¤€ë¹„ë  ì‹œê°„ì„ ì¤Œ
        tokio::time::sleep(Duration::from_millis(10)).await;
        info!("â³ [ACTOR] Waiting for SessionActor to be ready...");
        
        // 6. ë°°ì¹˜ ì²˜ë¦¬ ëª…ë ¹ ì „ì†¡
        let pages: Vec<u32> = (config.start_page..=config.end_page).collect();
        info!("ğŸ“‹ [ACTOR] Preparing command: pages {:?}, batch_size {}", pages, config.batch_size);
        
        let command = ActorCommand::ProcessBatch {
            pages: pages.clone(),
            config: batch_config,
            batch_size: config.batch_size,
            concurrency_limit: config.concurrency,
        };
        
        info!("ğŸ“¤ [ACTOR] Sending ProcessBatch command with {} pages", pages.len());
        
        // ì»¨í…ìŠ¤íŠ¸ë¥¼ í†µí•´ ëª…ë ¹ ì „ì†¡
        session_context.send_control_command(command).await
            .map_err(|e| anyhow!("Failed to send command to SessionActor: {}", e))?;
        
        info!("âœ… [ACTOR] Batch processing command sent successfully");
        
        // 6. ê²°ê³¼ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ í¬í•¨)
        let timeout_duration = Duration::from_millis(config.timeout_ms);
        let result = match tokio::time::timeout(timeout_duration, session_actor_handle).await {
            Ok(Ok(_)) => {
                info!("âœ… [ACTOR] Actor system completed within timeout");
                TaskResult {
                    session_id,
                    items_processed: config.end_page - config.start_page + 1,
                    items_success: config.end_page - config.start_page + 1,
                    items_failed: 0,
                    duration: start_time.elapsed(),
                    final_status: CrawlingStatus::Completed,
                }
            }
            Ok(Err(e)) => {
                error!("âŒ [ACTOR] Actor system failed: {}", e);
                TaskResult {
                    session_id,
                    items_processed: 0,
                    items_success: 0,
                    items_failed: config.end_page - config.start_page + 1,
                    duration: start_time.elapsed(),
                    final_status: CrawlingStatus::Error,
                }
            }
            Err(_) => {
                warn!("â° [ACTOR] Actor system timeout after {}ms", config.timeout_ms);
                TaskResult {
                    session_id,
                    items_processed: 0,
                    items_success: 0,
                    items_failed: config.end_page - config.start_page + 1,
                    duration: start_time.elapsed(),
                    final_status: CrawlingStatus::Timeout,
                }
            }
        };
        
        Ok(result)
    }
    
    async fn get_progress(&self) -> CrawlingProgress {
        CrawlingProgress {
            current: 0,
            total: 100,
            percentage: 0.0,
            current_stage: CrawlingStage::Processing,
            status: CrawlingStatus::Running,
            new_items: 0,
            updated_items: 0,
            errors: 0,
            timestamp: Utc::now().to_rfc3339(),
            current_step: "ActorBatchProcessor running".to_string(),
            message: "Using Actor system (SessionActor â†’ BatchActor â†’ StageActor)".to_string(),
            elapsed_time: 0,
        }
    }
    
    async fn pause(&self) -> Result<()> {
        info!("â¸ï¸ ActorBatchProcessor paused");
        // TODO: Actor ì‹œìŠ¤í…œì— ì¼ì‹œì •ì§€ ëª…ë ¹ ì „ì†¡
        Ok(())
    }
    
    async fn resume(&self) -> Result<()> {
        info!("â–¶ï¸ ActorBatchProcessor resumed");
        // TODO: Actor ì‹œìŠ¤í…œì— ì¬ê°œ ëª…ë ¹ ì „ì†¡
        Ok(())
    }
    
    async fn stop(&self) -> Result<()> {
        info!("â¹ï¸ ActorBatchProcessor stopped");
        // TODO: Actor ì‹œìŠ¤í…œì— ì¤‘ì§€ ëª…ë ¹ ì „ì†¡
        Ok(())
    }
}

impl ServiceBatchProcessor {
    pub fn new(
        base_engine: Arc<ServiceBasedBatchCrawlingEngine>,
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        product_repo: Arc<IntegratedProductRepository>,
        app_handle: Option<AppHandle>,
    ) -> Self {
        Self { 
            base_engine,
            http_client,
            data_extractor,
            product_repo,
            app_handle,
        }
    }
}

#[async_trait::async_trait]
impl BatchProcessor for ServiceBatchProcessor {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult> {
        info!("ğŸ”§ Executing task with ServiceBatchProcessor");
        
        let start_time = Instant::now();
        
        // CrawlingConfigë¥¼ BatchCrawlingConfigë¡œ ë³€í™˜
        let batch_config = BatchCrawlingConfig {
            start_page: config.start_page,
            end_page: config.end_page,
            concurrency: config.concurrency,
            delay_ms: config.delay_ms,
            batch_size: config.batch_size,
            retry_max: config.retry_max,
            timeout_ms: config.timeout_ms,
            cancellation_token: config.cancellation_token.clone(), // ğŸ”¥ ì¤‘ìš”: cancellation_token ì „ë‹¬
        };
        
        // ìƒˆë¡œìš´ ServiceBasedBatchCrawlingEngine ìƒì„± (cancellation_token í¬í•¨)
        // ì‚¬ìš©ì ì„¤ì •ì„ ë¡œë“œí•˜ì—¬ ì‚¬ìš©ìê°€ ì„¤ì •í•œ page_range_limitì„ ì¡´ì¤‘
        let mut app_config = match crate::infrastructure::config::ConfigManager::new() {
            Ok(config_manager) => {
                match config_manager.load_config().await {
                    Ok(config) => {
                        info!("âœ… Loaded user configuration with page_range_limit: {}", config.user.crawling.page_range_limit);
                        config
                    },
                    Err(e) => {
                        warn!("âš ï¸ Failed to load user config: {}", e);
                        crate::infrastructure::config::AppConfig::default()
                    }
                }
            },
            Err(e) => {
                warn!("âš ï¸ Failed to create config manager: {}", e);
                crate::infrastructure::config::AppConfig::default()
            }
        };
        
        // ì§€ëŠ¥í˜• ëª¨ë“œê°€ í™œì„±í™”ë˜ê³  override_config_limitì´ trueì¸ ê²½ìš°ì—ë§Œ
        // ì„¤ì •ê°’ì„ ì¡°ì •í•  ìˆ˜ ìˆìŒ (ì‚¬ìš©ì ëª…ì‹œì  í—ˆìš© í•˜ì—ì„œë§Œ)
        if app_config.user.crawling.intelligent_mode.enabled 
           && app_config.user.crawling.intelligent_mode.override_config_limit {
            let requested_range = config.end_page - config.start_page + 1;
            let max_allowed = app_config.user.crawling.intelligent_mode.max_range_limit;
            
            if requested_range > max_allowed {
                warn!("ğŸš¨ Requested range {} exceeds intelligent mode limit {}, adjusting to {}", 
                      requested_range, max_allowed, max_allowed);
                app_config.user.crawling.page_range_limit = max_allowed;
            } else {
                // ì‚¬ìš©ìê°€ ìš”ì²­í•œ ë²”ìœ„ê°€ í—ˆìš© ë²”ìœ„ ë‚´ë¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                app_config.user.crawling.page_range_limit = requested_range;
                info!("âœ… Using requested range {} (within intelligent mode limits)", requested_range);
            }
        } else {
            // ì§€ëŠ¥í˜• ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì—ˆê±°ë‚˜ overrideê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°
            // ì‚¬ìš©ì ì„¤ì •ê°’ì„ ê·¸ëŒ€ë¡œ ìœ ì§€
            info!("â„¹ï¸ Using user-configured page_range_limit: {} (intelligent mode override: {})", 
                  app_config.user.crawling.page_range_limit,
                  app_config.user.crawling.intelligent_mode.override_config_limit);
        }
        
        let mut engine = ServiceBasedBatchCrawlingEngine::new(
            self.http_client.clone(),
            self.data_extractor.clone(),
            self.product_repo.clone(),
            self.event_emitter.clone(),
            batch_config,
            format!("service_session_{}", chrono::Utc::now().timestamp()),
            app_config,
        );
        
        info!("ğŸ›‘ Created ServiceBasedBatchCrawlingEngine with cancellation_token: {}", 
              config.cancellation_token.is_some());
        
        // SystemStateBroadcaster ì„¤ì • (Live Production Line UIìš©)
        if let Some(app_handle) = self.app_handle.clone() {
            let broadcaster = crate::infrastructure::system_broadcaster::SystemStateBroadcaster::new(
                app_handle,
            );
            engine.set_broadcaster(broadcaster);
            info!("ğŸ“¡ SystemStateBroadcaster configured for Live Production Line UI");
        }
        
        // ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬
        match engine.execute().await {
            Ok(()) => {
                info!("âœ… ServiceBatchProcessor completed successfully");
                Ok(TaskResult {
                    session_id: "service_session".to_string(),
                    items_processed: config.end_page - config.start_page + 1,
                    items_success: config.end_page - config.start_page + 1,
                    items_failed: 0,
                    duration: start_time.elapsed(),
                    final_status: CrawlingStatus::Completed,
                })
            }
            Err(e) => {
                warn!("âŒ ServiceBatchProcessor failed: {}", e);
                Ok(TaskResult {
                    session_id: "service_session".to_string(),
                    items_processed: 0,
                    items_success: 0,
                    items_failed: config.end_page - config.start_page + 1,
                    duration: start_time.elapsed(),
                    final_status: CrawlingStatus::Error,
                })
            }
        }
    }
    
    async fn get_progress(&self) -> CrawlingProgress {
        CrawlingProgress {
            current: 0,
            total: 100,
            percentage: 0.0,
            current_stage: CrawlingStage::Processing,
            status: CrawlingStatus::Running,
            new_items: 0,
            updated_items: 0,
            errors: 0,
            timestamp: Utc::now().to_rfc3339(),
            current_step: "ServiceBatchProcessor running".to_string(),
            message: "".to_string(),
            elapsed_time: 0,
        }
    }
    
    async fn pause(&self) -> Result<()> {
        info!("â¸ï¸ ServiceBatchProcessor paused");
        Ok(())
    }
    
    async fn resume(&self) -> Result<()> {
        info!("â–¶ï¸ ServiceBatchProcessor resumed");
        Ok(())
    }
    
    async fn stop(&self) -> Result<()> {
        info!("â¹ï¸ ServiceBatchProcessor stopped");
        Ok(())
    }
}

impl AdvancedBatchProcessor {
    pub fn new(engine: Arc<AdvancedBatchCrawlingEngine>) -> Self {
        Self { engine }
    }
}

#[async_trait::async_trait]
impl BatchProcessor for AdvancedBatchProcessor {
    async fn execute_task(&self, config: CrawlingConfig) -> Result<TaskResult> {
        info!("ğŸ”§ Executing task with AdvancedBatchProcessor");
        
        let start_time = Instant::now();
        
        // ì„ì‹œ ê²°ê³¼ ë°˜í™˜ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” engineì„ ì‚¬ìš©)
        Ok(TaskResult {
            session_id: "advanced_session".to_string(),
            items_processed: config.end_page - config.start_page + 1,
            items_success: config.end_page - config.start_page + 1,
            items_failed: 0,
            duration: start_time.elapsed(),
            final_status: CrawlingStatus::Completed,
        })
    }
    
    async fn get_progress(&self) -> CrawlingProgress {
        CrawlingProgress {
            current: 0,
            total: 100,
            percentage: 0.0,
            current_stage: CrawlingStage::Processing,
            status: CrawlingStatus::Running,
            new_items: 0,
            updated_items: 0,
            errors: 0,
            timestamp: Utc::now().to_rfc3339(),
            current_step: "AdvancedBatchProcessor running".to_string(),
            message: "".to_string(),
            elapsed_time: 0,
        }
    }
    
    async fn pause(&self) -> Result<()> {
        info!("â¸ï¸ AdvancedBatchProcessor paused");
        Ok(())
    }
    
    async fn resume(&self) -> Result<()> {
        info!("â–¶ï¸ AdvancedBatchProcessor resumed");
        Ok(())
    }
    
    async fn stop(&self) -> Result<()> {
        info!("â¹ï¸ AdvancedBatchProcessor stopped");
        Ok(())
    }
}

// ì¬ì‹œë„ ê´€ë¦¬ì êµ¬í˜„
impl RetryManager {
    /// ìƒˆë¡œìš´ ì¬ì‹œë„ ê´€ë¦¬ì ìƒì„±
    pub fn new(max_retries: u32) -> Self {
        Self {
            max_retries,
            retry_queue: Arc::new(Mutex::new(std::collections::VecDeque::new())),
            failure_classifier: Arc::new(DefaultFailureClassifier::new()),
        }
    }
    
    /// ì‹¤íŒ¨í•œ í•­ëª©ì„ ì¬ì‹œë„ íì— ì¶”ê°€
    pub async fn add_retry_item(&self, item_id: String, stage: CrawlingStage, error: String) -> Result<()> {
        let mut queue = self.retry_queue.lock().await;
        
        // ê¸°ì¡´ í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸
        let existing_item = queue.iter_mut().find(|item| item.item_id == item_id);
        
        if let Some(item) = existing_item {
            // ê¸°ì¡´ í•­ëª© ì—…ë°ì´íŠ¸
            item.attempt_count += 1;
            item.last_error = error.clone();
            item.exponential_backoff = self.failure_classifier.calculate_backoff(item.attempt_count).await;
            item.next_retry_time = Utc::now() + chrono::Duration::from_std(item.exponential_backoff)?;
            
            if item.attempt_count >= self.max_retries {
                warn!("âŒ Item {} exceeded max retries ({}), removing from queue", item_id, self.max_retries);
                queue.retain(|i| i.item_id != item_id);
                return Ok(());
            }
        } else {
            // ìƒˆ í•­ëª© ì¶”ê°€
            let classification = self.failure_classifier.classify_error(&error, stage).await;
            
            match classification {
                ErrorClassification::NonRecoverable { reason } => {
                    warn!("âŒ Non-recoverable error for {}: {}", item_id, reason);
                    return Ok(());
                }
                ErrorClassification::Recoverable { retry_after } |
                ErrorClassification::RateLimited { retry_after } |
                ErrorClassification::NetworkError { retry_after } => {
                    let retry_item = RetryItem {
                        item_id,
                        stage,
                        attempt_count: 1,
                        last_error: error,
                        next_retry_time: Utc::now() + chrono::Duration::from_std(retry_after)?,
                        exponential_backoff: retry_after,
                    };
                    
                    queue.push_back(retry_item);
                    info!("ğŸ”„ Added item to retry queue, total items: {}", queue.len());
                }
            }
        }
        
        Ok(())
    }
    
    /// ì¬ì‹œë„ ê°€ëŠ¥í•œ í•­ëª©ë“¤ì„ ê°€ì ¸ì˜´
    pub async fn get_ready_items(&self) -> Vec<RetryItem> {
        let mut queue = self.retry_queue.lock().await;
        let now = Utc::now();
        
        let ready_items: Vec<RetryItem> = queue
            .iter()
            .filter(|item| item.next_retry_time <= now)
            .cloned()
            .collect();
        
        // ì¤€ë¹„ëœ í•­ëª©ë“¤ì„ íì—ì„œ ì œê±°
        queue.retain(|item| item.next_retry_time > now);
        
        if !ready_items.is_empty() {
            info!("ğŸ”„ Retrieved {} items ready for retry", ready_items.len());
        }
        
        ready_items
    }
    
    /// ì¬ì‹œë„ í ìƒíƒœ ì¡°íšŒ
    pub async fn get_queue_status(&self) -> (usize, usize) {
        let queue = self.retry_queue.lock().await;
        let total_items = queue.len();
        let ready_items = queue
            .iter()
            .filter(|item| item.next_retry_time <= Utc::now())
            .count();
        
        (total_items, ready_items)
    }
}

/// ê¸°ë³¸ ì‹¤íŒ¨ ë¶„ë¥˜ê¸° êµ¬í˜„
pub struct DefaultFailureClassifier;

impl DefaultFailureClassifier {
    pub fn new() -> Self {
        Self
    }
}

#[async_trait::async_trait]
impl FailureClassifier for DefaultFailureClassifier {
    async fn classify_error(&self, error: &str, stage: CrawlingStage) -> ErrorClassification {
        let error_lower = error.to_lowercase();
        
        // HTTP ìƒíƒœ ì½”ë“œ ë¶„ì„
        if error_lower.contains("429") || error_lower.contains("rate limit") {
            return ErrorClassification::RateLimited { 
                retry_after: Duration::from_secs(60) // 1ë¶„ ëŒ€ê¸°
            };
        }
        
        if error_lower.contains("timeout") || error_lower.contains("connection") {
            return ErrorClassification::NetworkError { 
                retry_after: Duration::from_secs(30) // 30ì´ˆ ëŒ€ê¸°
            };
        }
        
        if error_lower.contains("500") || error_lower.contains("502") || error_lower.contains("503") {
            return ErrorClassification::Recoverable { 
                retry_after: Duration::from_secs(45) // 45ì´ˆ ëŒ€ê¸°
            };
        }
        
        // 404, 400 ë“±ì€ ì¬ì‹œë„í•´ë„ ì˜ë¯¸ ì—†ìŒ
        if error_lower.contains("404") || error_lower.contains("400") || error_lower.contains("403") {
            return ErrorClassification::NonRecoverable { 
                reason: "Client error - retry will not help".to_string()
            };
        }
        
        // íŒŒì‹± ë‹¨ê³„ë³„ ì—ëŸ¬ ë¶„ë¥˜
        match stage {
            CrawlingStage::ParseItemDetails => {
                if error_lower.contains("parse") || error_lower.contains("format") {
                    ErrorClassification::NonRecoverable { 
                        reason: "Parsing error - data format issue".to_string()
                    }
                } else {
                    ErrorClassification::Recoverable { 
                        retry_after: Duration::from_secs(20)
                    }
                }
            }
            CrawlingStage::DatabaseSave => {
                ErrorClassification::Recoverable { 
                    retry_after: Duration::from_secs(10) // DB ì—ëŸ¬ëŠ” ë¹ ë¥´ê²Œ ì¬ì‹œë„
                }
            }
            _ => {
                ErrorClassification::Recoverable { 
                    retry_after: Duration::from_secs(30)
                }
            }
        }
    }
    
    async fn calculate_backoff(&self, attempt_count: u32) -> Duration {
        // ì§€ìˆ˜ ë°±ì˜¤í”„: 2^attempt_count ì´ˆ, ìµœëŒ€ 5ë¶„
        let base_seconds = 2_u64.pow(attempt_count.min(8)); // ìµœëŒ€ 256ì´ˆ
        let max_seconds = 300; // 5ë¶„
        
        Duration::from_secs(base_seconds.min(max_seconds))
    }
}

impl PerformanceMonitor {
    /// ìƒˆë¡œìš´ ì„±ëŠ¥ ëª¨ë‹ˆí„° ìƒì„±
    pub fn new() -> Self {
        Self {
            session_metrics: Arc::new(RwLock::new(HashMap::new())),
            global_metrics: Arc::new(RwLock::new(GlobalMetrics {
                total_sessions: 0,
                active_sessions: 0,
                average_session_duration: Duration::from_secs(0),
                total_items_processed: 0,
                overall_success_rate: 0.0,
            })),
        }
    }
    
    /// ì„¸ì…˜ ì¶”ì  ì‹œì‘
    pub async fn start_session_tracking(&self, session_id: &str) {
        let mut sessions = self.session_metrics.write().await;
        let mut global = self.global_metrics.write().await;
        
        sessions.insert(session_id.to_string(), SessionMetrics {
            start_time: Instant::now(),
            items_processed: 0,
            average_response_time: Duration::from_secs(0),
            error_rate: 0.0,
            current_concurrency: 1,
        });
        
        global.total_sessions += 1;
        global.active_sessions += 1;
        
        info!("ğŸ“Š Started performance tracking for session: {}", session_id);
    }
    
    /// ì„¸ì…˜ ì¶”ì  ì¢…ë£Œ
    pub async fn end_session_tracking(&self, session_id: &str) {
        let mut sessions = self.session_metrics.write().await;
        let mut global = self.global_metrics.write().await;
        
        if sessions.remove(session_id).is_some() {
            global.active_sessions = global.active_sessions.saturating_sub(1);
            info!("ğŸ“Š Ended performance tracking for session: {}", session_id);
        }
    }
    
    /// ì„¸ì…˜ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    pub async fn update_session_metrics(&self, session_id: &str, items_processed: u32, response_time: Duration, error_rate: f64) {
        let mut sessions = self.session_metrics.write().await;
        
        if let Some(metrics) = sessions.get_mut(session_id) {
            metrics.items_processed = items_processed;
            metrics.average_response_time = response_time;
            metrics.error_rate = error_rate;
        }
    }
    
    /// ê¸€ë¡œë²Œ ë©”íŠ¸ë¦­ ì¡°íšŒ
    pub async fn get_global_metrics(&self) -> GlobalMetrics {
        self.global_metrics.read().await.clone()
    }
    
    /// ì„¸ì…˜ ë©”íŠ¸ë¦­ ì¡°íšŒ
    pub async fn get_session_metrics(&self, session_id: &str) -> Option<SessionMetrics> {
        let sessions = self.session_metrics.read().await;
        sessions.get(session_id).cloned()
    }
}
