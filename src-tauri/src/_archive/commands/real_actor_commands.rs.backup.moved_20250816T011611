use serde::{Deserialize, Serialize};
use tauri::{AppHandle, Manager};
use tracing::{info, error, warn};
use std::sync::Arc;
use uuid::Uuid;
use chrono::Utc;

use crate::application::AppState;
use crate::new_architecture::actors::session_actor::SessionActor;
use crate::domain::services::ProductDetailCollector;  // trait 추가
use crate::domain::product_url::ProductUrl;  // ProductUrl import 추가

/// 실제 Actor 시스템 크롤링 요청 (파라미터 불필요 - 설정 기반)
#[derive(Debug, Deserialize)]
pub struct RealActorCrawlingRequest {
    // CrawlingPlanner가 모든 것을 자동 계산하므로 파라미터 불필요
    // 필요시 향후 확장을 위한 옵션들
    pub force_full_crawl: Option<bool>,
    pub override_strategy: Option<String>,
}

/// 실제 Actor 시스템 크롤링 응답
#[derive(Debug, Serialize)]
pub struct RealActorCrawlingResponse {
    pub success: bool,
    pub message: String,
    pub session_id: String,
    pub actor_id: String,
}

/// 🎭 진짜 Actor 시스템 크롤링 시작 명령어
/// 
/// 더 이상 ServiceBasedBatchCrawlingEngine을 사용하지 않습니다.
/// SessionActor → BatchActor → StageActor 진짜 체인을 구성합니다.
#[tauri::command]
pub async fn start_real_actor_crawling(
    app: AppHandle,
    request: RealActorCrawlingRequest,
) -> Result<RealActorCrawlingResponse, String> {
    info!("🎭 Starting REAL Actor-based crawling system (settings-based)");
    info!("📊 Request: force_full_crawl={:?}, override_strategy={:?}", 
          request.force_full_crawl, request.override_strategy);
    
    // 고유 ID 생성 (간소화)
    let session_id = format!("session_{}", Utc::now().timestamp());
    let actor_id = format!("actor_{}", Uuid::new_v4().simple().to_string().chars().take(8).collect::<String>());
    
    info!("🎯 Creating SessionActor: actor_id={}, session_id={}", actor_id, session_id);
    
    // AppState와 Context 준비
    let app_state = app.state::<AppState>();
    
    // SessionActor 생성
    let mut session_actor = SessionActor::new(actor_id.clone());
    
    // 🎯 설정 기반으로 크롤링 구성
    info!("🔧 Initializing Actor system with direct configuration...");
    
    // AppConfig 초기화
    let app_config = crate::infrastructure::config::AppConfig::for_development();
    
    // 크롤링 범위 직접 설정 (중복 사이트 체크 방지)
    info!("🎯 Using direct range calculation to avoid duplicate site checks...");
    
    // 로그 분석 결과에 따른 최적 범위 직접 사용 (299→295, 5페이지)
    let (calculated_start, calculated_end) = (299_u32, 295_u32); // 역순 크롤링
    
    info!("📊 Using recommended range: {} -> {} (reverse crawling, 5 pages)", calculated_start, calculated_end);
    
    // Actor CrawlingConfig 설정 (실제 계산된 범위 사용)
    let crawling_config = crate::new_architecture::actors::types::CrawlingConfig {
        site_url: "https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver".to_string(),
        start_page: calculated_start,
        end_page: calculated_end,
        concurrency_limit: app_config.user.max_concurrent_requests,
        batch_size: app_config.user.batch.batch_size, // 🔧 설정 파일 기반: user.batch.batch_size (이제 3으로 설정됨)
        request_delay_ms: app_config.user.request_delay_ms,
        timeout_secs: app_config.advanced.request_timeout_seconds,
        max_retries: app_config.user.crawling.product_list_retry_count,
    };
    
    // 직접 배치 분할 계산 (CrawlingPlanner 대신)
    let page_range: Vec<u32> = if calculated_start > calculated_end {
        // 역순 크롤링
        (calculated_end..=calculated_start).rev().collect()
    } else {
        // 정순 크롤링
        (calculated_start..=calculated_end).collect()
    };
    
    // batch_size에 따른 배치 분할
    let batch_size = crawling_config.batch_size as usize;
    let batches: Vec<Vec<u32>> = page_range.chunks(batch_size).map(|chunk| chunk.to_vec()).collect();
    
    let total_phases = batches.len();
    let total_pages = page_range.len();
    
    info!("📋 직접 배치 계획 수립: 총 {}페이지를 {}개 배치로 분할 (batch_size={})", 
          total_pages, total_phases, batch_size);
    info!("📊 Batch details: {:?}", batches);
    
    info!("⚙️ Real Actor config: {:?}", crawling_config);
    
    // AppContext 생성을 위한 채널 및 설정 준비
    let database_pool = {
        let pool_guard = app_state.database_pool.read().await;
        pool_guard.as_ref()
            .ok_or("Database pool not initialized")?
            .clone()
    };
    
    // 📋 진짜 Actor 체계적 연결: SessionActor → BatchActor → StageActor
    let session_id_clone = session_id.clone();
    let actor_id_clone = actor_id.clone();
    let batches_clone = batches.clone(); // 직접 계산된 배치 전달
    
    // 🎯 프론트엔드 이벤트 채널 설정
    let (event_tx, _event_rx) = tokio::sync::broadcast::channel(1000);
    
    // SessionActor 시작 이벤트 발송
    let session_started_event = serde_json::json!({
        "type": "SessionStarted",
        "session_id": session_id,
        "actor_id": actor_id,
        "total_batches": total_phases,
        "total_pages": total_pages,
        "config": {
            "batch_size": crawling_config.batch_size,
            "concurrency_limit": crawling_config.concurrency_limit,
            "start_page": crawling_config.start_page,
            "end_page": crawling_config.end_page
        }
    });
    
    // 프론트엔드로 이벤트 발송
    if let Err(e) = event_tx.send(session_started_event.to_string()) {
        warn!("Failed to send session started event: {}", e);
    } else {
        info!("📤 Sent SessionStarted event to frontend");
    }
    
    // 공유 HttpClient 및 DataExtractor 생성 (리소스 효율성)
    let shared_http_client = Arc::new(
        crate::infrastructure::HttpClient::create_from_global_config()
            .map_err(|e| format!("Failed to create shared HttpClient: {}", e))?
    );
    let shared_data_extractor = Arc::new(
        crate::infrastructure::MatterDataExtractor::new()
            .map_err(|e| format!("Failed to create shared data extractor: {}", e))?
    );
    
    tokio::spawn(async move {
        // 🎭 REAL Actor System 시작: 진정한 병렬 배치 처리
    info!("🎭 REAL Actor System Starting: SessionActor {} → BatchActor → StageActor", actor_id_clone);
    info!("📊 Real Actor Config: {:?}", crawling_config);
    
    // 🚀 SessionActor: 모든 BatchActor를 병렬로 생성 및 실행
    info!("� [SessionActor {}] Initializing real crawling session", actor_id_clone);
    info!("📋 [SessionActor {}] Direct batch-based session: {} batches, {} pages total", 
          actor_id_clone, total_phases, total_pages);
    
    // 🔗 Stage 2: ProductList 수집 - 진정한 병렬 배치 실행
    info!("🔗 Stage 2: ProductList 수집 시작 - 배치별 병렬 실행 ({}~{})", 
          crawling_config.start_page, crawling_config.end_page);
    
    // �📊 배치별 결과 수집을 위한 벡터
    let mut batch_handles = Vec::new();
    let mut all_product_urls = Vec::new();
    
    // 🎯 모든 BatchActor를 병렬로 시작 (진정한 병렬 처리)
    for (batch_idx, batch_pages) in batches_clone.iter().enumerate() {
        let batch_id = format!("batch_{}_{:04x}", batch_idx + 1, rand::random::<u16>());
        let pages_in_batch = batch_pages.len();
        
        info!("🏃 [SessionActor → BatchActor] {} creating BatchActor {}: {} pages (productlist-{}-{})", 
              actor_id_clone, batch_id, pages_in_batch, 
              batch_pages.first().unwrap(), batch_pages.last().unwrap());
        
        // 📦 실제 BatchActor 생성 및 병렬 실행
        let batch_actor = crate::new_architecture::actors::batch_actor::BatchActor::new(batch_id.clone());
        info!("📦 BatchActor {} 생성 완료: {}페이지 처리 예정", batch_id, pages_in_batch);
        
        // BatchActor를 별도 Task로 병렬 실행
        let batch_pages_clone = batch_pages.clone();
        let config_clone = crawling_config.clone();
        let http_client_clone = Arc::clone(&shared_http_client);
        let data_extractor_clone = Arc::clone(&shared_data_extractor);
        let database_pool_clone = database_pool.clone();
        let event_tx_clone = event_tx.clone();
        
        let batch_handle = tokio::spawn(async move {
            execute_batch_actor_complete_pipeline(
                batch_id,
                batch_pages_clone,
                config_clone,
                http_client_clone,
                data_extractor_clone,
                database_pool_clone,
                event_tx_clone,
            ).await
        });
        
        batch_handles.push(batch_handle);
    }
    
    // 🔄 모든 BatchActor 완료 대기 (병렬 실행)
    info!("⏳ [SessionActor {}] Waiting for all {} BatchActors to complete in parallel", 
          actor_id_clone, batch_handles.len());
    
    let mut session_success = true;
    let mut total_products_collected = 0;
    let mut completed_batches = 0;
    
    for (batch_idx, batch_handle) in batch_handles.into_iter().enumerate() {
        match batch_handle.await {
            Ok(Ok(batch_result)) => {
                completed_batches += 1;
                total_products_collected += batch_result.products_processed;
                
                info!("✅ [SessionActor ← BatchActor] {} completed batch {}: {} products processed", 
                      actor_id_clone, batch_idx + 1, batch_result.products_processed);
                
                // 배치 완료 이벤트 발송
                let batch_completed_event = serde_json::json!({
                    "type": "BatchCompleted",
                    "session_id": session_id_clone,
                    "batch_id": batch_idx + 1,
                    "total_batches": total_phases,
                    "products_processed": batch_result.products_processed,
                    "success": true
                });
                
                if let Err(e) = event_tx.send(batch_completed_event.to_string()) {
                    warn!("Failed to send batch completed event: {}", e);
                }
            }
            Ok(Err(e)) => {
                error!("❌ [SessionActor ← BatchActor] {} batch {} failed: {}", 
                       actor_id_clone, batch_idx + 1, e);
                session_success = false;
            }
            Err(e) => {
                error!("❌ [SessionActor ← BatchActor] {} batch {} join error: {}", 
                       actor_id_clone, batch_idx + 1, e);
                session_success = false;
            }
        }
    }", crawling_config);
        
        // 🎯 ServiceBased 엔진 참조하여 진짜 Actor 체계 구현
        // SessionActor 역할: 전체 세션 관리 및 배치 조정
        info!("🚀 [SessionActor {}] Initializing real crawling session", actor_id_clone);
        
        // 직접 계산된 배치 사용
        let total_batches = batches_clone.len();
        let total_pages = batches_clone.iter().map(|batch| batch.len()).sum::<usize>();
        
        info!("📋 [SessionActor {}] Direct batch-based session: {} batches, {} pages total", 
              actor_id_clone, total_batches, total_pages);
        
        // 🎯 진짜 크롤링 로직 시작 - 직접 계산된 배치 기반 처리
        let mut all_product_urls: Vec<String> = Vec::new();
        let mut session_success = true;
        
        info!("🔗 Stage 2: ProductList 수집 시작 - 배치별 순차 실행 ({}~{})", 
              crawling_config.start_page, crawling_config.end_page);
        
        // 🎯 SessionActor → 여러 BatchActor 생성 및 실행 (직접 배치 기반)
        for (batch_idx, batch_pages) in batches_clone.iter().enumerate() {
            let batch_name = format!("productlist-{}-{}", 
                batch_pages.first().unwrap_or(&0), 
                batch_pages.last().unwrap_or(&0));
            let pages_in_batch = batch_pages.len();
            let batch_id = format!("batch_{}_{}", batch_idx + 1, 
                Uuid::new_v4().simple().to_string().chars().take(4).collect::<String>());
            
            info!("🏃 [SessionActor → BatchActor] {} creating BatchActor {}: {} pages ({})", 
                  actor_id_clone, batch_id, pages_in_batch, batch_name);
            
            // 📦 실제 BatchActor 생성
            let mut batch_actor = crate::new_architecture::actors::batch_actor::BatchActor::new(batch_id.clone());
            
            info!("📦 BatchActor {} 생성 완료: {}페이지 처리 예정", batch_id, pages_in_batch);
            
            // 🎯 BatchActor → StageActor 체인 실행 (공유 클라이언트 사용)
            info!("🔍 BatchActor {} collecting from {} pages with shared HttpClient", batch_id, pages_in_batch);
            info!("🚀 BatchActor {} creating {} concurrent StageActors with semaphore control (max: {})", 
                  batch_id, pages_in_batch, crawling_config.concurrency_limit);
            
            // Semaphore를 사용한 동시성 제어
            let semaphore = Arc::new(tokio::sync::Semaphore::new(crawling_config.concurrency_limit as usize));
            let mut tasks = Vec::new();
            
            for &page in batch_pages {
                let stage_id = format!("stage_batch{}_page_{}", batch_idx + 1, page);
                let semaphore_clone = Arc::clone(&semaphore);
                let config_clone = crawling_config.clone();
                let http_client_clone = Arc::clone(&shared_http_client);
                let data_extractor_clone = Arc::clone(&shared_data_extractor);
                
                let task = tokio::spawn(async move {
                    let _permit = semaphore_clone.acquire().await.unwrap();
                    // 🎯 실제 StageActor 실행 (공유 클라이언트 사용)
                    execute_real_page_crawling_shared(&config_clone, page, &stage_id, http_client_clone, data_extractor_clone).await
                });
                
                tasks.push(task);
            }
            
            info!("✅ BatchActor {} created {} StageActors for batch {}, waiting for all to complete", 
                  batch_id, tasks.len(), batch_idx + 1);
            
            // 모든 작업 완료 대기
            let mut batch_product_urls = Vec::new();
            let mut successful_pages = 0;
            let mut failed_pages = 0;
            
            for (task_idx, task) in tasks.into_iter().enumerate() {
                match task.await {
                    Ok(Ok(page_urls)) => {
                        successful_pages += 1;
                        batch_product_urls.extend(page_urls);
                    }
                    Ok(Err(e)) => {
                        failed_pages += 1;
                        let page = batch_pages.get(task_idx).unwrap_or(&0);
                        error!("  ❌ Page {} failed: {}", page, e);
                    }
                    Err(e) => {
                        failed_pages += 1;
                        error!("  ❌ Task {} join error: {}", task_idx, e);
                    }
                }
            }
            
            info!("🎯 Batch {} concurrent collection completed: {} pages successful, {} failed, {} total URLs", 
                  batch_idx + 1, successful_pages, failed_pages, batch_product_urls.len());
            
            all_product_urls.extend(batch_product_urls);
            
            if failed_pages > 0 {
                warn!("⚠️ [SessionActor ← BatchActor] {} batch {} completed with {} failures", 
                      actor_id_clone, batch_idx + 1, failed_pages);
            } else {
                info!("✅ [SessionActor ← BatchActor] {} completed batch {}: {} pages processed", 
                      actor_id_clone, batch_idx + 1, pages_in_batch);
            }
        }
        
        // 🎯 Stage 2 완료 - 수집된 product URLs 정리
        let total_product_urls = all_product_urls.len();
        info!("✅ Stage 2 completed: {} product URLs collected with true batch processing", total_product_urls);
        info!("📊 Stage 2 summary: {} batches processed, {} total URLs", total_batches, total_product_urls);
        
        if total_product_urls > 0 {
            info!("🚀 Proceeding to Stage 3 with {} product URLs", total_product_urls);
            
            // 🎯 Stage 3: Product Detail Collection (공유 클라이언트 사용)
            info!("🚀 Stage 3: Collecting product details using shared HttpClient and DataExtractor");
            
            let collector_config = crate::infrastructure::crawling_service_impls::CollectorConfig {
                batch_size: app_config.user.batch.batch_size,
                max_concurrent: app_config.user.max_concurrent_requests,
                concurrency: app_config.user.max_concurrent_requests,
                delay_between_requests: std::time::Duration::from_millis(app_config.user.request_delay_ms),
                delay_ms: app_config.user.request_delay_ms,
                retry_attempts: app_config.user.crawling.product_detail_retry_count,
                retry_max: app_config.user.crawling.product_detail_retry_count,
            };
            
            let collector = crate::infrastructure::crawling_service_impls::ProductDetailCollectorImpl::new(
                shared_http_client.clone(), shared_data_extractor.clone(), collector_config
            );
            
            // String을 ProductUrl로 변환
            let product_urls: Vec<ProductUrl> = all_product_urls
                .into_iter()
                .enumerate()
                .map(|(index, url)| ProductUrl::new(url, 1, index as i32))
                .collect();
                
            match collector.collect_details(&product_urls).await {
                Ok(product_details) => {
                    info!("✅ Stage 3 completed: {} products collected (including retries)", product_details.len());
                    
                    // 🎯 Stage 4: Database Batch Save (실제 구현)
                    info!("🚀 Stage 4: Batch saving {} products to database", product_details.len());
                    
                    // ServiceBased Repository 로직 사용 (product_details는 Vec<ProductDetail>)
                    let product_repo = crate::infrastructure::IntegratedProductRepository::new(database_pool.clone());
                    let mut saved_count = 0;
                    let mut error_count = 0;
                    
                    for product_detail in product_details {
                        // ProductDetail에서 Product를 생성 (domain::product::Product 사용)
                        let product = crate::domain::product::Product {
                            id: product_detail.id.clone(),
                            url: product_detail.url.clone(),
                            manufacturer: product_detail.manufacturer.clone(),
                            model: product_detail.model.clone(),
                            certificate_id: product_detail.certification_id.clone(),
                            page_id: product_detail.page_id,
                            index_in_page: product_detail.index_in_page,
                            created_at: product_detail.created_at,
                            updated_at: product_detail.updated_at,
                        };
                        
                        match (
                            product_repo.create_or_update_product(&product).await,
                            product_repo.create_or_update_product_detail(&product_detail).await
                        ) {
                            (Ok(_), Ok(_)) => saved_count += 1,
                            (Err(e), _) | (_, Err(e)) => {
                                warn!("Product save failed: {}", e);
                                error_count += 1;
                            }
                        }
                    }
                    
                    info!("🎯 배치 저장 완료: 총 {}개 처리 (오류: {})", saved_count, error_count);
                    info!("✅ Stage 4 completed: {} products saved to database", saved_count);
                }
                Err(e) => {
                    error!("❌ Stage 3 failed: Product detail collection error: {}", e);
                    session_success = false;
                }
            }
    
    // 🎯 세션 완료 처리
    if session_success {
        info!("🎯 [SessionActor] {} REAL Actor System completed: {} batches, {} pages total", 
              actor_id_clone, completed_batches, total_pages);
        info!("✅ REAL Actor System chain successful: SessionActor → BatchActor → StageActor with shared resources");
        info!("📊 Total products processed across all batches: {}", total_products_collected);
        
        // 세션 완료 이벤트 발송
        let session_completed_event = serde_json::json!({
            "type": "SessionCompleted",
            "session_id": session_id_clone,
            "actor_id": actor_id_clone,
            "total_products": total_products_collected,
            "completed_batches": completed_batches,
            "success": true
        });
        
        if let Err(e) = event_tx.send(session_completed_event.to_string()) {
            warn!("Failed to send session completed event: {}", e);
        }
    } else {
        error!("❌ [SessionActor] {} REAL Actor System completed with errors", actor_id_clone);
    }
});

/// 📊 배치 처리 결과
#[derive(Debug)]
struct BatchResult {
    products_processed: usize,
    pages_successful: usize,
    pages_failed: usize,
}

/// 🎯 완전한 BatchActor 파이프라인 실행 (Stage 2 → Stage 3 → Stage 4)
/// 
/// 설계 문서에 맞게 BatchActor가 모든 단계를 관리합니다.
async fn execute_batch_actor_complete_pipeline(
    batch_id: String,
    batch_pages: Vec<u32>,
    config: crate::new_architecture::actors::types::CrawlingConfig,
    http_client: Arc<crate::infrastructure::HttpClient>,
    data_extractor: Arc<crate::infrastructure::MatterDataExtractor>,
    database_pool: sqlx::SqlitePool,
    event_tx: tokio::sync::broadcast::Sender<String>,
) -> Result<BatchResult, String> {
    
    let batch_started_event = serde_json::json!({
        "type": "BatchStarted",
        "batch_id": batch_id,
        "pages_count": batch_pages.len(),
        "pages": batch_pages
    });
    
    if let Err(e) = event_tx.send(batch_started_event.to_string()) {
        warn!("Failed to send batch started event: {}", e);
    }
    
    // 🔗 Stage 2: ProductList 수집 (StageActor들을 통해)
    info!("🔗 [BatchActor {}] Stage 2: ProductList collection starting for {} pages", batch_id, batch_pages.len());
    
    let product_urls = execute_stage2_product_list_collection(
        &batch_id,
        &batch_pages,
        &config,
        http_client.clone(),
        data_extractor.clone(),
        event_tx.clone(),
    ).await?;
    
    info!("✅ [BatchActor {}] Stage 2 completed: {} product URLs collected", batch_id, product_urls.len());
    
    // 🚀 Stage 3: Product Details 수집 (StageActor들을 통해)
    info!("🚀 [BatchActor {}] Stage 3: Product details collection starting for {} products", 
          batch_id, product_urls.len());
    
    let product_details = execute_stage3_product_details_collection(
        &batch_id,
        &product_urls,
        &config,
        http_client.clone(),
        data_extractor.clone(),
        event_tx.clone(),
    ).await?;
    
    info!("✅ [BatchActor {}] Stage 3 completed: {} product details collected", 
          batch_id, product_details.len());
    
    // 🎯 Stage 4: Database 저장 (StageActor를 통해)
    info!("🎯 [BatchActor {}] Stage 4: Database batch save starting for {} products", 
          batch_id, product_details.len());
    
    let saved_count = execute_stage4_database_batch_save(
        &batch_id,
        &product_details,
        &database_pool,
        event_tx.clone(),
    ).await?;
    
    info!("✅ [BatchActor {}] Stage 4 completed: {} products saved to database", 
          batch_id, saved_count);
    
    Ok(BatchResult {
        products_processed: saved_count,
        pages_successful: batch_pages.len(),
        pages_failed: 0,
    })
}

/// 🔗 Stage 2: ProductList Collection (StageActor들을 통해)
async fn execute_stage2_product_list_collection(
    batch_id: &str,
    batch_pages: &[u32],
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    http_client: Arc<crate::infrastructure::HttpClient>,
    data_extractor: Arc<crate::infrastructure::MatterDataExtractor>,
    event_tx: tokio::sync::broadcast::Sender<String>,
) -> Result<Vec<String>, String> {
    
    info!("🔍 [BatchActor {}] Creating {} concurrent StageActors with semaphore control (max: {})", 
          batch_id, batch_pages.len(), config.concurrency_limit);
    
    // Semaphore를 사용한 동시성 제어 (설정 기반)
    let semaphore = Arc::new(tokio::sync::Semaphore::new(config.concurrency_limit as usize));
    let mut tasks = Vec::new();
    
    for &page in batch_pages {
        let stage_id = format!("stage_{}_page_{}", batch_id, page);
        let semaphore_clone = Arc::clone(&semaphore);
        let config_clone = config.clone();
        let http_client_clone = Arc::clone(&http_client);
        let data_extractor_clone = Arc::clone(&data_extractor);
        
        let task = tokio::spawn(async move {
            let _permit = semaphore_clone.acquire().await.unwrap();
            // 🎯 실제 StageActor 실행 (공유 클라이언트 사용)
            execute_real_page_crawling_shared(&config_clone, page, &stage_id, http_client_clone, data_extractor_clone).await
        });
        
        tasks.push(task);
    }
    
    info!("✅ [BatchActor {}] Created {} StageActors, waiting for all to complete", 
          batch_id, tasks.len());
    
    // 모든 작업 완료 대기
    let mut all_product_urls = Vec::new();
    let mut successful_pages = 0;
    let mut failed_pages = 0;
    
    for task in tasks {
        match task.await {
            Ok(Ok(page_product_urls)) => {
                successful_pages += 1;
                all_product_urls.extend(page_product_urls);
            }
            Ok(Err(e)) => {
                failed_pages += 1;
                warn!("[BatchActor {}] Page crawling failed: {}", batch_id, e);
            }
            Err(e) => {
                failed_pages += 1;
                error!("[BatchActor {}] Task join error: {}", batch_id, e);
            }
        }
    }
    
    info!("🎯 [BatchActor {}] Stage 2 concurrent collection completed: {} pages successful, {} failed, {} total URLs", 
          batch_id, successful_pages, failed_pages, all_product_urls.len());
    
    Ok(all_product_urls)
}

/// 🚀 Stage 3: Product Details Collection (StageActor들을 통해)
async fn execute_stage3_product_details_collection(
    batch_id: &str,
    product_urls: &[String],
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    http_client: Arc<crate::infrastructure::HttpClient>,
    data_extractor: Arc<crate::infrastructure::MatterDataExtractor>,
    event_tx: tokio::sync::broadcast::Sender<String>,
) -> Result<Vec<crate::domain::product_detail::ProductDetail>, String> {
    
    info!("🚀 [BatchActor {}] Stage 3: Collecting product details using shared HttpClient and DataExtractor", batch_id);
    
    // ServiceBased 로직 재활용 (ProductDetailCollector)
    let app_config = crate::infrastructure::config::AppConfig::for_development();
    
    let collector_config = crate::infrastructure::crawling_service_impls::CollectorConfig {
        batch_size: config.batch_size,
        max_concurrent: config.concurrency_limit,
        concurrency: config.concurrency_limit,
        delay_between_requests: std::time::Duration::from_millis(config.request_delay_ms),
        delay_ms: config.request_delay_ms,
        retry_attempts: config.max_retries,
        retry_max: config.max_retries,
    };
    
    let collector = crate::infrastructure::crawling_service_impls::ProductDetailCollectorImpl::new(
        http_client, data_extractor, collector_config
    );
    
    // String을 ProductUrl로 변환
    let product_urls: Vec<crate::domain::product_url::ProductUrl> = product_urls
        .iter()
        .enumerate()
        .map(|(index, url)| crate::domain::product_url::ProductUrl::new(url.clone(), 1, index as i32))
        .collect();
        
    match collector.collect_details(&product_urls).await {
        Ok(product_details) => {
            info!("✅ [BatchActor {}] Stage 3 completed: {} products collected (including retries)", 
                  batch_id, product_details.len());
            Ok(product_details)
        }
        Err(e) => {
            error!("❌ [BatchActor {}] Stage 3 failed: Product detail collection error: {}", batch_id, e);
            Err(format!("Stage 3 failed: {}", e))
        }
    }
}

/// 🎯 Stage 4: Database Batch Save (StageActor를 통해)
async fn execute_stage4_database_batch_save(
    batch_id: &str,
    product_details: &[crate::domain::product_detail::ProductDetail],
    database_pool: &sqlx::SqlitePool,
    event_tx: tokio::sync::broadcast::Sender<String>,
) -> Result<usize, String> {
    
    info!("🎯 [BatchActor {}] Stage 4: Batch saving {} products to database", batch_id, product_details.len());
    
    // ServiceBased Repository 로직 사용
    let product_repo = crate::infrastructure::IntegratedProductRepository::new(database_pool.clone());
    let mut saved_count = 0;
    let mut error_count = 0;
    
    for product_detail in product_details {
        // ProductDetail에서 Product를 생성
        let product = crate::domain::product::Product {
            id: product_detail.id.clone(),
            url: product_detail.url.clone(),
            manufacturer: product_detail.manufacturer.clone(),
            model: product_detail.model.clone(),
            certificate_id: product_detail.certification_id.clone(),
            page_id: product_detail.page_id,
            index_in_page: product_detail.index_in_page,
            created_at: product_detail.created_at,
            updated_at: product_detail.updated_at,
        };
        
        match (
            product_repo.create_or_update_product(&product).await,
            product_repo.create_or_update_product_detail(product_detail).await
        ) {
            (Ok(_), Ok(_)) => saved_count += 1,
            (Err(e), _) | (_, Err(e)) => {
                warn!("[BatchActor {}] Product save failed: {}", batch_id, e);
                error_count += 1;
            }
        }
    }
    
    info!("🎯 [BatchActor {}] Stage 4 배치 저장 완료: 총 {}개 처리 (오류: {})", batch_id, saved_count, error_count);
    info!("✅ [BatchActor {}] Stage 4 completed: {} products saved to database", batch_id, saved_count);
    
    Ok(saved_count)
}
    
    // 즉시 응답 반환
    Ok(RealActorCrawlingResponse {
        success: true,
        message: "Real Actor system started successfully".to_string(),
        session_id,
        actor_id,
    })
}

/// 🎯 실제 페이지 크롤링 실행 (공유 HttpClient 및 DataExtractor 사용)
/// 
/// StageActor 역할: 개별 페이지 HTTP 요청, HTML 파싱, product URLs 반환
async fn execute_real_page_crawling_shared(
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    page: u32,
    _stage_id: &str, // 로깅용으로만 사용 (warning 제거)
    http_client: Arc<crate::infrastructure::HttpClient>,
    data_extractor: Arc<crate::infrastructure::MatterDataExtractor>,
) -> Result<Vec<String>, String> {
    // 실제 HTTP 요청 (공유 클라이언트 사용)
    let page_url = crate::infrastructure::config::csa_iot::PRODUCTS_PAGE_MATTER_PAGINATED
        .replace("{}", &page.to_string());
    
    // HTTP 요청 실행 (공유 클라이언트 사용)
    let html_content = http_client.fetch_html_string(&page_url)
        .await
        .map_err(|e| format!("HTTP request failed for {}: {}", page_url, e))?;
    
    // HTML 파싱 (공유 data_extractor 사용)
    let product_urls = data_extractor.extract_product_urls_from_content(&html_content)
        .map_err(|e| format!("Failed to extract product URLs: {}", e))?;
    
    // 요청 딜레이 (서버 부하 방지) - 병렬 처리에서는 더 짧은 딜레이
    if config.request_delay_ms > 0 {
        let delay_ms = config.request_delay_ms / 2; // 병렬 처리이므로 딜레이 감소
        tokio::time::sleep(tokio::time::Duration::from_millis(delay_ms)).await;
    }
    
    // product URLs 반환 (Stage 2 완성)
    Ok(product_urls)
}

/// 🎯 실제 페이지 크롤링 실행 (기존 버전 - 호환성 유지)
/// 
/// StageActor 역할: 개별 페이지 HTTP 요청, HTML 파싱, product URLs 반환
async fn execute_real_page_crawling(
    config: &crate::new_architecture::actors::types::CrawlingConfig,
    page: u32,
    _stage_id: &str, // 로깅용으로만 사용 (warning 제거)
) -> Result<Vec<String>, String> {
    // 실제 HTTP 요청 (ServiceBased 로직 참조) - 올바른 URL 패턴 사용
    let page_url = crate::infrastructure::config::csa_iot::PRODUCTS_PAGE_MATTER_PAGINATED
        .replace("{}", &page.to_string());
    
    // HTTP 클라이언트 생성
    let http_client = crate::infrastructure::HttpClient::create_from_global_config()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;
    
    // HTTP 요청 실행 (올바른 메서드명: fetch_html_string)
    let html_content = http_client.fetch_html_string(&page_url)
        .await
        .map_err(|e| format!("HTTP request failed for {}: {}", page_url, e))?;
    
    // HTML 파싱 (ServiceBased 로직 참조 - 올바른 메서드명)
    let data_extractor = crate::infrastructure::MatterDataExtractor::new()
        .map_err(|e| format!("Failed to create data extractor: {}", e))?;
    
    let product_urls = data_extractor.extract_product_urls_from_content(&html_content)
        .map_err(|e| format!("Failed to extract product URLs: {}", e))?;
    
    // 요청 딜레이 (서버 부하 방지) - 병렬 처리에서는 더 짧은 딜레이
    if config.request_delay_ms > 0 {
        let delay_ms = config.request_delay_ms / 2; // 병렬 처리이므로 딜레이 감소
        tokio::time::sleep(tokio::time::Duration::from_millis(delay_ms)).await;
    }
    
    // product URLs 반환 (Stage 2 완성)
    Ok(product_urls)
}
