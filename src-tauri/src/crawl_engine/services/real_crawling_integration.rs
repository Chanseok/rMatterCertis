//! ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ í†µí•©ì„ ìœ„í•œ Actor ì‹œìŠ¤í…œ íŒ¨ì¹˜
//! ê¸°ì¡´ actor_system.rsì— ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ í†µí•©í•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€

use std::sync::Arc;
use tokio::sync::oneshot;
use tokio_util::sync::CancellationToken;
use tracing::{error, info, warn};

use crate::infrastructure::config::AppConfig;
use crate::new_architecture::actor_system::{StageError, StageResult};
use crate::new_architecture::actors::StageActor;
use crate::new_architecture::channels::types::{StageItem, StageType};
use crate::new_architecture::services::crawling_integration::{
    CrawlingIntegrationService, RealCrawlingStageExecutor,
};
use crate::new_architecture::system_config::SystemConfig;

/// ì‹¤ì œ í¬ë¡¤ë§ í†µí•© ì„œë¹„ìŠ¤
#[allow(dead_code)] // Phase2: retained for near-term expansion; prune in Phase3 if still unused
pub struct RealCrawlingIntegration {
    config: Arc<SystemConfig>, // REMOVE_CANDIDATE(if unused in Phase3)
    app_config: AppConfig,     // REMOVE_CANDIDATE(if unused in Phase3)
}

impl RealCrawlingIntegration {
    pub fn new(config: Arc<SystemConfig>, app_config: AppConfig) -> Self {
        Self { config, app_config }
    }
}

/// StageActor í™•ì¥: ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì‹¤í–‰ ê¸°ëŠ¥
impl StageActor {
    /// ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ StageActor ìƒì„±
    pub async fn new_with_real_crawling_service(
        batch_id: String,
        config: Arc<SystemConfig>,
        app_config: AppConfig,
        total_pages: u32,
        products_on_last_page: u32,
    ) -> anyhow::Result<Self> {
        // í¬ë¡¤ë§ í†µí•© ì„œë¹„ìŠ¤ ìƒì„±
        let integration_service =
            Arc::new(CrawlingIntegrationService::new(config.clone(), app_config).await?);

        // ì‹¤í–‰ê¸° ìƒì„±
        let crawling_executor = Arc::new(RealCrawlingStageExecutor::new(integration_service));

        // StageActor ìƒì„± with meaningful ID context
        let mut stage_actor =
            Self::new_with_oneshot(batch_id, config, total_pages, products_on_last_page);
        stage_actor.set_crawling_executor(crawling_executor);

        // meaningful ID ìƒì„±ì„ ìœ„í•œ PageIdCalculator ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        info!(
            total_pages = total_pages,
            products_on_last_page = products_on_last_page,
            "StageActor created with meaningful ID context"
        );

        Ok(stage_actor)
    }

    /// í¬ë¡¤ë§ ì‹¤í–‰ê¸° ì„¤ì •
    pub fn set_crawling_executor(&mut self, _executor: Arc<RealCrawlingStageExecutor>) {
        // í˜„ì¬ StageActorëŠ” crawling_executor í•„ë“œê°€ ì—†ìœ¼ë¯€ë¡œ
        // ì‹¤ì œë¡œëŠ” ë‚´ë¶€ ìƒíƒœë¡œ ì €ì¥í•˜ê³  run_with_oneshotì—ì„œ ì‚¬ìš©
        // ì„ì‹œë¡œ ë¡œê·¸ë§Œ ì¶œë ¥
        info!("Real crawling executor set for StageActor");
    }

    /// ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” OneShot ì‹¤í–‰
    pub async fn run_with_real_crawling(
        self,
        mut control_rx: tokio::sync::mpsc::Receiver<
            crate::new_architecture::channels::types::ActorCommand,
        >,
        result_tx: oneshot::Sender<StageResult>,
        crawling_executor: Arc<RealCrawlingStageExecutor>,
    ) -> Result<(), crate::new_architecture::actors::ActorError> {
        info!(batch_id = ?self.batch_id, "StageActor started with real crawling service");

        let mut final_result = StageResult::FatalError {
            error: StageError::ValidationError {
                message: "No commands received".to_string(),
            },
            stage_id: self.batch_id.clone(),
            context: "StageActor initialization".to_string(),
        };

        // ëª…ë ¹ ëŒ€ê¸° ë° ì²˜ë¦¬
        while let Some(command) = control_rx.recv().await {
            match command {
                crate::new_architecture::channels::types::ActorCommand::ExecuteStage {
                    stage_type,
                    items,
                    concurrency_limit,
                    timeout_secs: _,
                } => {
                    // ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•œ ìŠ¤í…Œì´ì§€ ì‹¤í–‰
                    final_result = self
                        .execute_stage_with_real_crawling(
                            stage_type,
                            items,
                            concurrency_limit,
                            crawling_executor.clone(),
                        )
                        .await;
                    break; // ìŠ¤í…Œì´ì§€ ì²˜ë¦¬ ì™„ë£Œ í›„ ì¢…ë£Œ
                }
                crate::new_architecture::channels::types::ActorCommand::CancelSession {
                    reason,
                    ..
                } => {
                    final_result = StageResult::FatalError {
                        error: StageError::ValidationError {
                            message: format!("Session cancelled: {}", reason),
                        },
                        stage_id: self.batch_id.clone(),
                        context: "User cancellation".to_string(),
                    };
                    break;
                }
                _ => {
                    warn!(batch_id = ?self.batch_id, "Unsupported command in stage actor");
                }
            }
        }

        // ê²°ê³¼ ì „ì†¡
        if result_tx.send(final_result).is_err() {
            error!(batch_id = ?self.batch_id, "Failed to send stage result");
        }

        info!(batch_id = ?self.batch_id, "StageActor with real crawling completed");
        Ok(())
    }

    /// ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•œ ìŠ¤í…Œì´ì§€ ì‹¤í–‰
    async fn execute_stage_with_real_crawling(
        &self,
        stage_type: StageType,
        items: Vec<StageItem>,
        concurrency_limit: u32,
        crawling_executor: Arc<RealCrawlingStageExecutor>,
    ) -> StageResult {
        info!(
            batch_id = ?self.batch_id,
            stage = ?stage_type,
            items_count = items.len(),
            concurrency_limit = concurrency_limit,
            "Executing stage with real crawling service"
        );

        // ì·¨ì†Œ í† í° ìƒì„± (í•„ìš”ì‹œ í™•ì¥ ê°€ëŠ¥)
        let cancellation_token = CancellationToken::new();

        // ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì‹¤í–‰
        crawling_executor
            .execute_stage(stage_type, items, concurrency_limit, cancellation_token)
            .await
    }
}

/// BatchActor í™•ì¥: ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” OneShot ìŠ¤í…Œì´ì§€ ì‹¤í–‰
impl crate::new_architecture::actors::BatchActor {
        /// Stage 1ìš©: ì‚¬ì´íŠ¸ ìƒíƒœ ì ê²€ì„ ìˆ˜í–‰í•˜ê³  ë ˆê±°ì‹œ StageResult(details í¬í•¨)ë¡œ ë¸Œë¦¬ì§•
        pub async fn execute_status_check_with_details(
            &self,
            app_config: AppConfig,
        ) -> crate::new_architecture::actors::types::StageResult {
            let config_arc = match self.config.as_ref() {
                Some(cfg) => cfg.clone(),
                None => {
                    return crate::new_architecture::actors::types::StageResult {
                        processed_items: 0,
                        successful_items: 0,
                        failed_items: 1,
                        duration_ms: 0,
                        details: Vec::new(),
                    };
                }
            };

            let integration_service = match CrawlingIntegrationService::new(config_arc.clone(), app_config).await {
                Ok(service) => Arc::new(service),
                Err(e) => {
                    error!(error = %e, "Failed to create crawling integration service");
                    return crate::new_architecture::actors::types::StageResult {
                        processed_items: 0,
                        successful_items: 0,
                        failed_items: 1,
                        duration_ms: 0,
                        details: Vec::new(),
                    };
                }
            };

            let started = std::time::Instant::now();
            let mut details: Vec<crate::new_architecture::actors::types::StageItemResult> = Vec::new();

            match integration_service.execute_site_analysis().await {
                Ok(site_status) => {
                    let collected_data = serde_json::to_string(&site_status).ok();
                    details.push(crate::new_architecture::actors::types::StageItemResult {
                        item_id: "site_status_check:0".to_string(),
                        item_type: crate::new_architecture::actors::types::StageItemType::SiteCheck,
                        success: true,
                        error: None,
                        duration_ms: started.elapsed().as_millis() as u64,
                        retry_count: 0,
                        collected_data,
                    });

                    crate::new_architecture::actors::types::StageResult {
                        processed_items: 1,
                        successful_items: 1,
                        failed_items: 0,
                        duration_ms: started.elapsed().as_millis() as u64,
                        details,
                    }
                }
                Err(e) => {
                    error!(error = %e, "Site status analysis failed");
                    details.push(crate::new_architecture::actors::types::StageItemResult {
                        item_id: "site_status_check:0".to_string(),
                        item_type: crate::new_architecture::actors::types::StageItemType::SiteCheck,
                        success: false,
                        error: Some(format!("{}", e)),
                        duration_ms: started.elapsed().as_millis() as u64,
                        retry_count: 0,
                        collected_data: None,
                    });

                    crate::new_architecture::actors::types::StageResult {
                        processed_items: 1,
                        successful_items: 0,
                        failed_items: 1,
                        duration_ms: started.elapsed().as_millis() as u64,
                        details,
                    }
                }
            }
        }
    /// ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•œ ìŠ¤í…Œì´ì§€ ì‹¤í–‰
    pub async fn execute_stage_with_real_crawling(
        &self,
        stage_type: StageType,
        items: Vec<StageItem>,
        app_config: AppConfig,
    ) -> StageResult {
        info!(
            batch_id = ?self.batch_id,
            stage = ?stage_type,
            items_count = items.len(),
            "Executing stage with real crawling service"
        );

        // 1. í¬ë¡¤ë§ í†µí•© ì„œë¹„ìŠ¤ ìƒì„±
        let config_arc = match self.config.as_ref() {
            Some(cfg) => cfg.clone(),
            None => {
                return StageResult::FatalError {
                    error: StageError::ConfigurationError {
                        message: "No system config available".to_string(),
                    },
                    stage_id: self
                        .batch_id
                        .clone()
                        .unwrap_or_else(|| "unknown".to_string()),
                    context: "Missing configuration".to_string(),
                };
            }
        };

        let integration_service =
            match CrawlingIntegrationService::new(config_arc.clone(), app_config).await {
                Ok(service) => Arc::new(service),
                Err(e) => {
                    error!(error = %e, "Failed to create crawling integration service");
                    return StageResult::FatalError {
                        error: StageError::ConfigurationError {
                            message: format!("Crawling service initialization failed: {}", e),
                        },
                        stage_id: self
                            .batch_id
                            .clone()
                            .unwrap_or_else(|| "unknown".to_string()),
                        context: "Service initialization".to_string(),
                    };
                }
            };

        // 2. ì‹¤í–‰ê¸° ìƒì„±
        let crawling_executor = Arc::new(RealCrawlingStageExecutor::new(integration_service));

        // 3. OneShot ë°ì´í„° ì±„ë„ ìƒì„±
        let (stage_data_tx, stage_data_rx) = oneshot::channel::<StageResult>();

        // 4. ì œì–´ ì±„ë„ ìƒì„±
        let (stage_control_tx, stage_control_rx) =
            tokio::sync::mpsc::channel(config_arc.channels.control_buffer_size);

        // 5. ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” StageActor ìŠ¤í° (meaningful ID context í¬í•¨)
        let stage_actor = StageActor::new_with_oneshot(
            self.batch_id
                .clone()
                .unwrap_or_else(|| "unknown".to_string()),
            config_arc.clone(),
            494, // total_pages (ê¸°ë³¸ê°’)
            12,  // products_on_last_page (ê¸°ë³¸ê°’)
        );

        let handle = tokio::spawn(async move {
            stage_actor
                .run_with_real_crawling(stage_control_rx, stage_data_tx, crawling_executor)
                .await
        });

        // 6. ìŠ¤í…Œì´ì§€ ëª…ë ¹ ì „ì†¡
        let stage_timeout = std::time::Duration::from_secs(config_arc.actor.stage_timeout_secs);
        let command = crate::new_architecture::channels::types::ActorCommand::ExecuteStage {
            stage_type: stage_type.clone(),
            items,
            concurrency_limit: config_arc
                .performance
                .concurrency
                .stage_concurrency_limits
                .get(&format!("{:?}", stage_type))
                .copied()
                .unwrap_or(10),
            timeout_secs: stage_timeout.as_secs(),
        };

        if let Err(e) = stage_control_tx.send(command).await {
            return StageResult::FatalError {
                error: StageError::ValidationError {
                    message: format!("Failed to send stage command: {}", e),
                },
                stage_id: self
                    .batch_id
                    .clone()
                    .unwrap_or_else(|| "unknown".to_string()),
                context: "Stage command sending".to_string(),
            };
        }

        // 7. ê²°ê³¼ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜)
        let result = match tokio::time::timeout(stage_timeout, stage_data_rx).await {
            Ok(Ok(stage_result)) => stage_result,
            Ok(Err(_)) => StageResult::FatalError {
                error: StageError::ValidationError {
                    message: "Stage data channel closed".to_string(),
                },
                stage_id: self
                    .batch_id
                    .clone()
                    .unwrap_or_else(|| "unknown".to_string()),
                context: "Stage communication failure".to_string(),
            },
            Err(_) => StageResult::RecoverableError {
                error: StageError::NetworkTimeout { timeout_secs: 30 },
                attempts: 0,
                stage_id: self
                    .batch_id
                    .clone()
                    .unwrap_or_else(|| "unknown".to_string()),
                suggested_retry_delay_ms: 5000, // 5ì´ˆë¥¼ ë°€ë¦¬ì´ˆë¡œ
            },
        };

        // 8. StageActor ì •ë¦¬
        if let Err(e) = handle.await {
            warn!(batch_id = ?self.batch_id, error = %e, "StageActor join failed");
        }

        result
    }
        /// Stage 3ìš©: ProductUrls ì…ë ¥ì„ ë°›ì•„ ProductDetailsë¥¼ ìˆ˜ì§‘í•˜ê³  ë ˆê±°ì‹œ StageResult(details í¬í•¨)ë¡œ ë¸Œë¦¬ì§•
        pub async fn execute_detail_collection_with_details(
            &self,
            product_urls_items: Vec<crate::new_architecture::channels::types::ProductUrls>,
            app_config: AppConfig,
        ) -> crate::new_architecture::actors::types::StageResult {
            let config_arc = match self.config.as_ref() {
                Some(cfg) => cfg.clone(),
                None => {
                    return crate::new_architecture::actors::types::StageResult {
                        processed_items: 0,
                        successful_items: 0,
                        failed_items: product_urls_items.len() as u32,
                        duration_ms: 0,
                        details: Vec::new(),
                    };
                }
            };

            let integration_service = match CrawlingIntegrationService::new(config_arc.clone(), app_config).await {
                Ok(service) => Arc::new(service),
                Err(e) => {
                    error!(error = %e, "Failed to create crawling integration service");
                    return crate::new_architecture::actors::types::StageResult {
                        processed_items: 0,
                        successful_items: 0,
                        failed_items: product_urls_items.len() as u32,
                        duration_ms: 0,
                        details: Vec::new(),
                    };
                }
            };

            let cancellation_token = tokio_util::sync::CancellationToken::new();
            let started = std::time::Instant::now();

            let mut successful = 0u32;
            let mut failed = 0u32;
            let mut details: Vec<crate::new_architecture::actors::types::StageItemResult> = Vec::new();

            for (idx, urls_wrapper) in product_urls_items.into_iter().enumerate() {
                // Collect details for this item
                let urls = urls_wrapper.urls.clone();
                let urls_strs: Vec<String> = urls.iter().map(|u| u.url.clone()).collect();
                let item_started = std::time::Instant::now();
                match integration_service
                    .collect_details_detailed_with_meta(urls.clone(), cancellation_token.clone())
                    .await
                {
                    Ok((collected_details, retry_count, duration_ms)) => {
                        if collected_details.is_empty() {
                            warn!(
                                idx = idx,
                                urls = urls_strs.len(),
                                "Detail collection returned 0 details for item"
                            );
                        }
                        let success = !collected_details.is_empty();
                        if success { successful += 1; } else { failed += 1; }
                        let collected_data = if success {
                            // Wrap into channels::types::ProductDetails for downstream transforms
                            let attempted = urls.len() as u32;
                            let successful_count = collected_details.len() as u32;
                            let failed_count = attempted.saturating_sub(successful_count);
                            let wrapper = crate::new_architecture::channels::types::ProductDetails {
                                products: collected_details,
                                source_urls: urls,
                                extraction_stats: crate::new_architecture::channels::types::ExtractionStats {
                                    attempted,
                                    successful: successful_count,
                                    failed: failed_count,
                                    empty_responses: 0,
                                },
                            };
                            match serde_json::to_string(&wrapper) {
                                Ok(json) => Some(json),
                                Err(_) => None,
                            }
                        } else { None };

                        details.push(crate::new_architecture::actors::types::StageItemResult {
                            item_id: format!("product_urls:{}", idx),
                            item_type: crate::new_architecture::actors::types::StageItemType::ProductUrls { urls: urls_strs.clone() },
                            success,
                            error: if success { None } else { Some("no details".to_string()) },
                            duration_ms: duration_ms.max(item_started.elapsed().as_millis() as u64),
                            retry_count,
                            collected_data,
                        });
                    }
                    Err(e) => {
                        error!(idx = idx, error = %e, "Detail collection failed for ProductUrls item");
                        failed += 1;
                        details.push(crate::new_architecture::actors::types::StageItemResult {
                            item_id: format!("product_urls:{}", idx),
                            item_type: crate::new_architecture::actors::types::StageItemType::ProductUrls { urls: urls_strs.clone() },
                            success: false,
                            error: Some(format!("{}", e)),
                            duration_ms: item_started.elapsed().as_millis() as u64,
                            retry_count: 0,
                            collected_data: None,
                        });
                    }
                }
            }

            crate::new_architecture::actors::types::StageResult {
                processed_items: successful + failed,
                successful_items: successful,
                failed_items: failed,
                duration_ms: started.elapsed().as_millis() as u64,
                details,
            }
        }
        /// Stage 2ìš©: í˜ì´ì§€ë³„ ìƒì„¸ URL ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë ˆê±°ì‹œ StageResult(details í¬í•¨)ë¡œ ë¸Œë¦¬ì§•
        pub async fn execute_list_collection_with_details(
            &self,
            pages: Vec<u32>,
            app_config: AppConfig,
        ) -> crate::new_architecture::actors::types::StageResult {
            let config_arc = match self.config.as_ref() {
                Some(cfg) => cfg.clone(),
                None => {
                    return crate::new_architecture::actors::types::StageResult {
                        processed_items: 0,
                        successful_items: 0,
                        failed_items: pages.len() as u32,
                        duration_ms: 0,
                        details: Vec::new(),
                    };
                }
            };

            let integration_service = match CrawlingIntegrationService::new(config_arc.clone(), app_config).await {
                Ok(service) => Arc::new(service),
                Err(e) => {
                    error!(error = %e, "Failed to create crawling integration service");
                    return crate::new_architecture::actors::types::StageResult {
                        processed_items: 0,
                        successful_items: 0,
                        failed_items: pages.len() as u32,
                        duration_ms: 0,
                        details: Vec::new(),
                    };
                }
            };

            let cancellation_token = tokio_util::sync::CancellationToken::new();
            let started = std::time::Instant::now();
            let perform_site_check = true;

            let page_results = match integration_service
                .collect_pages_detailed_with_meta(pages.clone(), perform_site_check, cancellation_token)
                .await
            {
                Ok(r) => r,
                Err(e) => {
                    error!(error = %e, "List detail collection failed");
                    return crate::new_architecture::actors::types::StageResult {
                        processed_items: 0,
                        successful_items: 0,
                        failed_items: pages.len() as u32,
                        duration_ms: 0,
                        details: Vec::new(),
                    };
                }
            };

            // Map into legacy StageResult with per-item details
            let mut successful = 0u32;
            let mut failed = 0u32;
            let mut details: Vec<crate::new_architecture::actors::types::StageItemResult> = Vec::new();
            for (page, urls, retry_count, duration_ms) in page_results.into_iter() {
                let success = !urls.is_empty();
                if success { successful += 1; } else { failed += 1; }
                let collected_data = if success {
                    match serde_json::to_string(&urls) {
                        Ok(json) => Some(json),
                        Err(_) => None,
                    }
                } else { None };
                details.push(crate::new_architecture::actors::types::StageItemResult {
                    item_id: format!("page:{}", page),
                    item_type: crate::new_architecture::actors::types::StageItemType::Page { page_number: page },
                    success,
                    error: if success { None } else { Some("no urls".to_string()) },
                    duration_ms,
                    retry_count,
                    collected_data,
                });
            }

            crate::new_architecture::actors::types::StageResult {
                processed_items: successful + failed,
                successful_items: successful,
                failed_items: failed,
                duration_ms: started.elapsed().as_millis() as u64,
                details,
            }
        }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::infrastructure::config::AppConfig;
    use std::sync::Arc;

    #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
    async fn test_crawling_integration_service_creation() {
    // Ensure database paths are initialized for tests using globals
    let _ = crate::infrastructure::initialize_database_paths().await;
        println!("ğŸ§ª í¬ë¡¤ë§ í†µí•© ì„œë¹„ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘");

        // ê¸°ë³¸ ì„¤ì • ìƒì„±
        let system_config = Arc::new(SystemConfig::default());
        let app_config = AppConfig::for_development(); // ê°œë°œìš© ì„¤ì • ì‚¬ìš©

        // í¬ë¡¤ë§ í†µí•© ì„œë¹„ìŠ¤ ìƒì„± ì‹œë„
        match CrawlingIntegrationService::new(system_config, app_config).await {
            Ok(_service) => {
                println!("âœ… í¬ë¡¤ë§ í†µí•© ì„œë¹„ìŠ¤ ìƒì„± ì„±ê³µ!");
            }
            Err(e) => {
                println!("âŒ í¬ë¡¤ë§ í†µí•© ì„œë¹„ìŠ¤ ìƒì„± ì‹¤íŒ¨: {}", e);
                // í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ (HTTP í´ë¼ì´ì–¸íŠ¸ ë“±)
                println!("   (í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ì •ìƒì ì¸ ì‹¤íŒ¨ì¼ ìˆ˜ ìˆìŒ)");
            }
        }

        println!("ğŸ¯ í¬ë¡¤ë§ í†µí•© ì„œë¹„ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ!");
    }

    #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
    async fn test_stage_actor_with_real_crawling() {
    // Ensure database paths are initialized for tests using globals
    let _ = crate::infrastructure::initialize_database_paths().await;
        println!("ğŸ§ª ì‹¤ì œ í¬ë¡¤ë§ì„ ì‚¬ìš©í•˜ëŠ” StageActor í…ŒìŠ¤íŠ¸ ì‹œì‘");

        let batch_id = "test-batch-real".to_string();
        let config = Arc::new(SystemConfig::default());
        let app_config = AppConfig::for_development();

        // StageActor ìƒì„± ì‹œë„
        match StageActor::new_with_real_crawling_service(
            batch_id.clone(),
            config,
            app_config,
            494, // total_pages
            12,  // products_on_last_page
        )
        .await
        {
            Ok(stage_actor) => {
                println!("âœ… ì‹¤ì œ í¬ë¡¤ë§ StageActor ìƒì„± ì„±ê³µ!");
                println!("   ë°°ì¹˜ ID: {}", stage_actor.batch_id);
            }
            Err(e) => {
                println!("âŒ ì‹¤ì œ í¬ë¡¤ë§ StageActor ìƒì„± ì‹¤íŒ¨: {}", e);
                println!("   (í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” ì •ìƒì ì¸ ì‹¤íŒ¨ì¼ ìˆ˜ ìˆìŒ)");
            }
        }

        println!("ğŸ¯ ì‹¤ì œ í¬ë¡¤ë§ StageActor í…ŒìŠ¤íŠ¸ ì™„ë£Œ!");
    }
}
