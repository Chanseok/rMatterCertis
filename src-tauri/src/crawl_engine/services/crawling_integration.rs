//! ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ì™€ OneShot Actor ì‹œìŠ¤í…œ í†µí•©
//! Modern Rust 2024 ì¤€ìˆ˜: ê¸°ì¡´ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ OneShot Actor íŒ¨í„´ìœ¼ë¡œ ì—°ë™

#![warn(clippy::all, clippy::pedantic, clippy::nursery)]
#![deny(clippy::unwrap_used, clippy::expect_used, clippy::panic)]

use std::sync::Arc;
use std::time::{Duration, Instant};

use anyhow::Result;
use tokio_util::sync::CancellationToken;
use tracing::{debug, error, info, warn};

use crate::crawl_engine::actor_system::{StageError, StageResult};
use crate::crawl_engine::channels::types::{StageItem, StageType};
use crate::crawl_engine::config::SystemConfig;
use crate::domain::product::ProductDetail;
use crate::domain::product_url::ProductUrl;
use crate::domain::services::crawling_services::{
    CrawlingRangeRecommendation, DatabaseAnalyzer, FieldAnalysis, ProductDetailCollector,
    ProductListCollector, SiteStatus, StatusChecker,
};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::crawling_service_impls::{
    CollectorConfig, ProductListCollectorImpl, StatusCheckerImpl,
};
use crate::infrastructure::{HttpClient, IntegratedProductRepository, MatterDataExtractor};

/// ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ì™€ OneShot Actor ì‹œìŠ¤í…œì„ ì—°ê²°í•˜ëŠ” í†µí•© ì„œë¹„ìŠ¤
#[allow(dead_code)] // Phase2: allow unused fields temporarily â€“ evaluate in Phase3
pub struct CrawlingIntegrationService {
    status_checker: Arc<dyn StatusChecker>,
    list_collector: Arc<dyn ProductListCollector>,
    detail_collector: Arc<dyn ProductDetailCollector>,
    database_analyzer: Arc<dyn DatabaseAnalyzer>, // REMOVE_CANDIDATE(if still unused)
    product_repository: Arc<IntegratedProductRepository>,
    config: Arc<SystemConfig>, // REMOVE_CANDIDATE(if still unused)
    app_config: AppConfig,     // REMOVE_CANDIDATE(if still unused)
}

impl CrawlingIntegrationService {
    /// ê¸°ì¡´ ì¸í”„ë¼ë¥¼ ì‚¬ìš©í•˜ì—¬ í†µí•© ì„œë¹„ìŠ¤ ìƒì„±
    pub async fn new(config: Arc<SystemConfig>, app_config: AppConfig) -> Result<Self> {
        // ê¸°ì¡´ ì¸í”„ë¼ ì„œë¹„ìŠ¤ë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ íŒ¨í„´ ì¬ì‚¬ìš©)
        let http_client = Arc::new(tokio::sync::Mutex::new(
            HttpClient::create_from_global_config()?,
        ));

        let data_extractor = Arc::new(MatterDataExtractor::new()?);

        // DB í’€ ì¬ì‚¬ìš© (ê¸€ë¡œë²Œ í’€)
        let db_pool = crate::infrastructure::database_connection::get_or_init_global_pool()
            .await
            .map_err(|e| anyhow::anyhow!("Failed to obtain database pool: {}", e))?;

        // ProductRepository ì´ˆê¸°í™” (DB ì—°ê²° í¬í•¨)
        let product_repository = Arc::new(IntegratedProductRepository::new(db_pool));

        // StatusChecker ìƒì„± (ProductRepository í¬í•¨)
        let http_client_for_checker = http_client.lock().await.clone();
        let data_extractor_for_checker = (*data_extractor).clone();
        let status_checker_impl = Arc::new(StatusCheckerImpl::with_product_repo(
            http_client_for_checker,
            data_extractor_for_checker,
            app_config.clone(),
            product_repository.clone(),
        ));
        let status_checker: Arc<dyn StatusChecker> = status_checker_impl.clone();

        // DatabaseAnalyzerëŠ” StatusCheckerImpl ì¬ì‚¬ìš©
        let database_analyzer: Arc<dyn DatabaseAnalyzer> = status_checker_impl.clone();

        // ProductListCollector ìƒì„±
        let collector_config = CollectorConfig {
            batch_size: app_config.user.batch.batch_size,
            max_concurrent: app_config.user.max_concurrent_requests,
            concurrency: app_config.user.max_concurrent_requests,
            delay_between_requests: Duration::from_millis(app_config.user.request_delay_ms),
            delay_ms: app_config.user.request_delay_ms,
            retry_attempts: 3,
            retry_max: 3,
        };

        let list_collector: Arc<dyn ProductListCollector> =
            Arc::new(ProductListCollectorImpl::new(
                Arc::new(HttpClient::create_from_global_config()?), // ğŸ”¥ Mutex ì œê±°
                data_extractor.clone(),
                collector_config.clone(),
                status_checker_impl.clone(),
            ));

        // ProductDetailCollector: ì‹¤ì œ ìƒì„¸ ìˆ˜ì§‘ ì „ìš© êµ¬í˜„ ì‚¬ìš©
        // ìƒì„¸ ë‹¨ê³„ëŠ” ë¦¬ìŠ¤íŠ¸ ë‹¨ê³„ì™€ ë‹¤ë¥¸ ë™ì‹œì„± í•œë„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³„ë„ CollectorConfig êµ¬ì„±
        let detail_config = CollectorConfig {
            batch_size: app_config.user.batch.batch_size,
            max_concurrent: app_config
                .user
                .crawling
                .workers
                .product_detail_max_concurrent as u32,
            concurrency: app_config
                .user
                .crawling
                .workers
                .product_detail_max_concurrent as u32,
            delay_between_requests: Duration::from_millis(app_config.user.request_delay_ms),
            delay_ms: app_config.user.request_delay_ms,
            retry_attempts: 3,
            retry_max: 3,
        };
        let detail_collector: Arc<dyn ProductDetailCollector> = Arc::new(
            crate::infrastructure::crawling_service_impls::ProductDetailCollectorImpl::new(
                Arc::new(HttpClient::create_from_global_config()?),
                data_extractor.clone(),
                detail_config,
            ),
        );

        Ok(Self {
            status_checker,
            list_collector,
            detail_collector,
            database_analyzer,
            product_repository,
            config,
            app_config,
        })
    }

    /// ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ë‹¨ê³„ ì‹¤í–‰ (OneShot ê²°ê³¼ ë°˜í™˜)
    pub async fn execute_list_collection_stage(
        &self,
        pages: Vec<u32>,
        concurrency_limit: u32,
        cancellation_token: CancellationToken,
    ) -> StageResult {
        self.execute_list_collection_stage_internal(
            pages,
            concurrency_limit,
            cancellation_token,
            true,
        )
        .await
    }

    /// ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì—†ì´ ì§ì ‘ í˜ì´ì§€ ìˆ˜ì§‘ (ì¤‘ë³µ ë°©ì§€ìš©)
    pub async fn execute_list_collection_stage_no_site_check(
        &self,
        pages: Vec<u32>,
        concurrency_limit: u32,
        cancellation_token: CancellationToken,
    ) -> StageResult {
        self.execute_list_collection_stage_internal(
            pages,
            concurrency_limit,
            cancellation_token,
            false,
        )
        .await
    }

    /// ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ êµ¬í˜„ (ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì„ íƒì )
    async fn execute_list_collection_stage_internal(
        &self,
        pages: Vec<u32>,
        concurrency_limit: u32,
        cancellation_token: CancellationToken,
        perform_site_check: bool,
    ) -> StageResult {
        let start_time = Instant::now();

        info!(
            pages_count = pages.len(),
            concurrency_limit = concurrency_limit,
            "Starting real list collection stage"
        );

        // ì„¤ì •ì—ì„œ ë°°ì¹˜ í¬ê¸° ë¡œë“œ
        let batch_size = self.config.performance.batch_sizes.initial_size.min(50) as usize;
        let mut all_collected_urls = Vec::new();
        let mut successful_pages = Vec::new();
        let mut failed_pages = Vec::new();

        // í˜ì´ì§€ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for chunk in pages.chunks(batch_size) {
            // ì·¨ì†Œ í™•ì¸
            if cancellation_token.is_cancelled() {
                return StageResult::FatalError {
                    error: StageError::ValidationError {
                        message: "Collection cancelled by user".to_string(),
                    },
                    stage_id: "list-collection".to_string(),
                    context: "User cancellation".to_string(),
                };
            }

            match self
                .collect_page_batch_with_retry(
                    chunk,
                    cancellation_token.clone(),
                    perform_site_check,
                )
                .await
            {
                Ok(batch_result) => {
                    for (page, urls) in batch_result {
                        if urls.is_empty() {
                            failed_pages.push(page);
                        } else {
                            all_collected_urls.extend(urls);
                            successful_pages.push(page);
                        }
                    }
                }
                Err(e) => {
                    error!(error = %e, "Batch collection failed");
                    failed_pages.extend(chunk.iter());
                }
            }
        }

        let elapsed = start_time.elapsed();
        let total_pages = pages.len() as u32;
        let successful_count = successful_pages.len() as u32;
        let failed_count = failed_pages.len() as u32;

        // ê²°ê³¼ ë¶„ë¥˜
        if successful_count == 0 {
            StageResult::FatalError {
                error: StageError::NetworkError {
                    message: format!("All {} pages failed to collect", total_pages),
                },
                stage_id: "list-collection".to_string(),
                context: "Complete collection failure".to_string(),
            }
        } else if failed_count == 0 {
            StageResult::Success {
                processed_items: total_pages,
                duration_ms: elapsed.as_millis() as u64,
            }
        } else {
            StageResult::Failure {
                error: StageError::ProcessingError {
                    message: format!(
                        "Partial failure: {} successes, {} failures",
                        successful_count, failed_count
                    ),
                },
                partial_results: successful_count,
            }
        }
    }

    /// í˜ì´ì§€ë³„ ìƒì„¸ URL ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ë°˜í™˜ (per-page detailed results)
    ///
    /// ë°˜í™˜ê°’: Vec<(page_number, Vec<ProductUrl>)>
    pub async fn collect_pages_detailed(
        &self,
        pages: Vec<u32>,
        perform_site_check: bool,
        cancellation_token: CancellationToken,
    ) -> anyhow::Result<Vec<(u32, Vec<ProductUrl>)>> {
        // ë™ì¼í•œ ë‚´ë¶€ ë°°ì¹˜ ìˆ˜ì§‘ ë¡œì§ ì¬ì‚¬ìš©
        let result = self
            .collect_page_batch_with_retry(&pages, cancellation_token, perform_site_check)
            .await?;
        Ok(result)
    }

    /// í˜ì´ì§€ë³„ ìƒì„¸ URL ìˆ˜ì§‘ ê²°ê³¼(+ë©”íƒ€: retry_count, duration_ms)ë¥¼ ë°˜í™˜
    ///
    /// ë°˜í™˜ê°’: Vec<(page_number, Vec<ProductUrl>, retry_count, duration_ms)>
    pub async fn collect_pages_detailed_with_meta(
        &self,
        pages: Vec<u32>,
        perform_site_check: bool,
        cancellation_token: CancellationToken,
    ) -> anyhow::Result<Vec<(u32, Vec<ProductUrl>, u32, u64)>> {
        let mut results: Vec<(u32, Vec<ProductUrl>, u32, u64)> = Vec::new();

        for &page in &pages {
            if cancellation_token.is_cancelled() {
                break;
            }

            match self
                .collect_single_page_with_retry_with_meta(page, 3, perform_site_check)
                .await
            {
                Ok((urls, retry_count, duration_ms)) => {
                    results.push((page, urls, retry_count, duration_ms));
                }
                Err(e) => {
                    warn!(page = page, error = %e, "Failed to collect page after retries");
                    // ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ í˜•ì‹ì„ ìœ ì§€í•˜ë˜ ë¹ˆ URLê³¼ ì‹œë„ íšŸìˆ˜/ì†Œìš”ì‹œê°„ 0ìœ¼ë¡œ ì±„ì›€
                    results.push((page, Vec::new(), 3, 0));
                }
            }
        }

        Ok(results)
    }

    /// ì‹¤ì œ ìƒì„¸ ìˆ˜ì§‘ ë‹¨ê³„ ì‹¤í–‰ (OneShot ê²°ê³¼ ë°˜í™˜)
    pub async fn execute_detail_collection_stage(
        &self,
        product_urls: Vec<ProductUrl>,
        concurrency_limit: u32,
        cancellation_token: CancellationToken,
    ) -> StageResult {
        let start_time = Instant::now();

        info!(
            urls_count = product_urls.len(),
            concurrency_limit = concurrency_limit,
            "Starting real detail collection stage"
        );

        // ì„¤ì •ì—ì„œ ë°°ì¹˜ í¬ê¸° ë¡œë“œ
        let batch_size = self.config.performance.batch_sizes.initial_size.min(20) as usize;
        let mut all_collected_details = Vec::new();
        let mut successful_urls = Vec::new();
        let mut failed_urls = Vec::new();

        // URLì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for chunk in product_urls.chunks(batch_size) {
            // ì·¨ì†Œ í™•ì¸
            if cancellation_token.is_cancelled() {
                return StageResult::FatalError {
                    error: StageError::ValidationError {
                        message: "Detail collection cancelled by user".to_string(),
                    },
                    stage_id: "detail-collection".to_string(),
                    context: "User cancellation".to_string(),
                };
            }

            match self
                .collect_detail_batch_with_retry(chunk, cancellation_token.clone())
                .await
            {
                Ok(batch_details) => {
                    for detail in batch_details {
                        all_collected_details.push(detail.clone());
                        successful_urls.push(detail.url.clone());
                    }
                }
                Err(e) => {
                    error!(error = %e, "Detail batch collection failed");
                    failed_urls.extend(chunk.iter().map(|url| url.url.clone()));
                }
            }
        }

        let elapsed = start_time.elapsed();
        let total_urls = product_urls.len() as u32;
        let successful_count = successful_urls.len() as u32;
        let failed_count = failed_urls.len() as u32;

        // ê²°ê³¼ ë¶„ë¥˜
        if successful_count == 0 {
            StageResult::FatalError {
                error: StageError::NetworkError {
                    message: format!("All {} product details failed to collect", total_urls),
                },
                stage_id: "detail-collection".to_string(),
                context: "Complete detail collection failure".to_string(),
            }
        } else if failed_count == 0 {
            StageResult::Success {
                processed_items: total_urls,
                duration_ms: elapsed.as_millis() as u64,
            }
        } else {
            StageResult::Failure {
                error: StageError::ProcessingError {
                    message: format!("Partial failure in detail collection"),
                },
                partial_results: successful_count,
            }
        }
    }

    /// ìƒì„¸ ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ë„ë©”ì¸ ê°ì²´ë¡œ ì§ì ‘ ë°˜í™˜ (per-item detailed bridging ìš©)
    pub async fn collect_details_detailed(
        &self,
        urls: Vec<ProductUrl>,
        cancellation_token: CancellationToken,
    ) -> anyhow::Result<Vec<ProductDetail>> {
        // ë‚´ë¶€ ë°°ì¹˜ ìˆ˜ì§‘ ë¡œì§ ì¬ì‚¬ìš©
        self.collect_detail_batch_with_retry(&urls, cancellation_token)
            .await
            .map_err(|e| anyhow::anyhow!("{}", e))
    }

    /// ìƒì„¸ ìˆ˜ì§‘ ê²°ê³¼(+ë©”íƒ€: retry_count, duration_ms)ë¥¼ ë°˜í™˜ (per-item detailed bridging ìš©)
    pub async fn collect_details_detailed_with_meta(
        &self,
        urls: Vec<ProductUrl>,
        cancellation_token: CancellationToken,
    ) -> anyhow::Result<(Vec<ProductDetail>, u32, u64)> {
        self.collect_detail_batch_with_retry_with_meta(&urls, cancellation_token, 2)
            .await
            .map_err(|e| anyhow::anyhow!("{}", e))
    }

    /// ì‚¬ì´íŠ¸ ìƒíƒœ ë¶„ì„ ì‹¤í–‰
    pub async fn execute_site_analysis(&self) -> Result<SiteStatus> {
        info!("Starting real site status analysis");

        self.status_checker.check_site_status().await
    }

    /// í¬ë¡¤ë§ ë²”ìœ„ ê¶Œì¥ì‚¬í•­ ê³„ì‚°
    pub async fn calculate_crawling_recommendation(&self) -> Result<CrawlingRangeRecommendation> {
        info!("Calculating real crawling range recommendation with actual DB data");

        let site_status = self.status_checker.check_site_status().await?;

        // ì‹¤ì œ DB ìƒíƒœ í™•ì¸
        let db_stats = self.product_repository.get_database_statistics().await?;
        info!(
            total_products = db_stats.total_products,
            active_products = db_stats.active_products,
            "Real DB stats retrieved"
        );

        // DB ë¶„ì„ì„ ìœ„í•œ ë¶„ì„ ê²°ê³¼ ìƒì„± (ì‹¤ì œ DB ë°ì´í„° ê¸°ë°˜)
        let db_analysis = crate::domain::services::crawling_services::DatabaseAnalysis {
            total_products: db_stats.total_products as u32,
            unique_products: db_stats.active_products as u32,
            duplicate_count: (db_stats.total_products - db_stats.active_products) as u32,
            missing_products_count: 0,
            last_update: None,
            missing_fields_analysis: FieldAnalysis {
                missing_company: 0,
                missing_model: 0,
                missing_matter_version: 0,
                missing_connectivity: 0,
                missing_certification_date: 0,
            },
            data_quality_score: 0.8,
        };

        self.status_checker
            .calculate_crawling_range_recommendation(&site_status, &db_analysis)
            .await
    }

    /// ë°°ì¹˜ í˜ì´ì§€ ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)
    async fn collect_page_batch_with_retry(
        &self,
        pages: &[u32],
        cancellation_token: CancellationToken,
        perform_site_check: bool,
    ) -> Result<Vec<(u32, Vec<ProductUrl>)>> {
        let mut results = Vec::new();

        // ê°œë³„ í˜ì´ì§€ ìˆ˜ì§‘
        for &page in pages {
            if cancellation_token.is_cancelled() {
                break;
            }

            match self
                .collect_single_page_with_retry(page, 3, perform_site_check)
                .await
            {
                Ok(urls) => {
                    results.push((page, urls));
                }
                Err(e) => {
                    warn!(page = page, error = %e, "Failed to collect page after retries");
                    results.push((page, Vec::new()));
                }
            }
        }

        Ok(results)
    }

    /// ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)
    async fn collect_single_page_with_retry(
        &self,
        page: u32,
        max_retries: u32,
        perform_site_check: bool,
    ) -> Result<Vec<ProductUrl>> {
        // ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ (ì„ íƒì )
        let site_status = if perform_site_check {
            info!(page = page, "ğŸ” Performing site status check for page");
            self.status_checker.check_site_status().await?
        } else {
            info!(
                page = page,
                "âš¡ Skipping site status check - using cached site info"
            );
            // ê¸°ë³¸ê°’ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ìºì‹œëœ ê°’ì„ ì‚¬ìš©í•´ì•¼ í•¨)
            SiteStatus {
                total_pages: 495,
                products_on_last_page: 6,
                is_accessible: true,
                estimated_products: 5934,
                response_time_ms: 500,
                last_check_time: chrono::Utc::now(),
                health_score: 1.0,
                data_change_status:
                    crate::domain::services::crawling_services::SiteDataChangeStatus::Stable {
                        count: 5934,
                    },
                decrease_recommendation: None,
                crawling_range_recommendation: CrawlingRangeRecommendation::Partial(5),
            }
        };
        let mut last_error = None;

        for attempt in 0..=max_retries {
            match self
                .list_collector
                .collect_single_page(
                    page,
                    site_status.total_pages,
                    site_status.products_on_last_page,
                )
                .await
            {
                Ok(urls) => {
                    if attempt > 0 {
                        info!(
                            page = page,
                            attempt = attempt,
                            "Page collection succeeded after retry"
                        );
                    }
                    return Ok(urls);
                }
                Err(e) => {
                    last_error = Some(e);
                    if attempt < max_retries {
                        let delay = Duration::from_millis(1000 * (2_u64.pow(attempt)));
                        debug!(
                            page = page,
                            attempt = attempt,
                            delay_ms = delay.as_millis(),
                            "Retrying page collection"
                        );
                        tokio::time::sleep(delay).await;
                    }
                }
            }
        }

        Err(last_error.unwrap_or_else(|| anyhow::anyhow!("Unknown error")))
    }

    /// ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘ (ì¬ì‹œë„ ë° ë©”íƒ€ë°ì´í„° í¬í•¨)
    async fn collect_single_page_with_retry_with_meta(
        &self,
        page: u32,
        max_retries: u32,
        perform_site_check: bool,
    ) -> Result<(Vec<ProductUrl>, u32, u64)> {
        let site_status = if perform_site_check {
            info!(page = page, "Performing site status check for page (meta)");
            self.status_checker.check_site_status().await?
        } else {
            info!(
                page = page,
                "Skipping site status check - using cached site info (meta)"
            );
            SiteStatus {
                total_pages: 495,
                products_on_last_page: 6,
                is_accessible: true,
                estimated_products: 5934,
                response_time_ms: 500,
                last_check_time: chrono::Utc::now(),
                health_score: 1.0,
                data_change_status:
                    crate::domain::services::crawling_services::SiteDataChangeStatus::Stable {
                        count: 5934,
                    },
                decrease_recommendation: None,
                crawling_range_recommendation: CrawlingRangeRecommendation::Partial(5),
            }
        };

        let mut last_error = None;
        let started = std::time::Instant::now();

        for attempt in 0..=max_retries {
            match self
                .list_collector
                .collect_single_page(
                    page,
                    site_status.total_pages,
                    site_status.products_on_last_page,
                )
                .await
            {
                Ok(urls) => {
                    if attempt > 0 {
                        info!(
                            page = page,
                            attempt = attempt,
                            "Page collection succeeded after retry (meta)"
                        );
                    }
                    let duration_ms = started.elapsed().as_millis() as u64;
                    return Ok((urls, attempt, duration_ms));
                }
                Err(e) => {
                    last_error = Some(e);
                    if attempt < max_retries {
                        let delay = Duration::from_millis(1000 * (2_u64.pow(attempt)));
                        debug!(
                            page = page,
                            attempt = attempt,
                            delay_ms = delay.as_millis(),
                            "Retrying page collection (meta)"
                        );
                        tokio::time::sleep(delay).await;
                    }
                }
            }
        }

        Err(last_error.unwrap_or_else(|| anyhow::anyhow!("Unknown error")))
    }

    /// ë°°ì¹˜ ìƒì„¸ ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)
    async fn collect_detail_batch_with_retry(
        &self,
        urls: &[ProductUrl],
        cancellation_token: CancellationToken,
    ) -> Result<Vec<ProductDetail>> {
        // ì·¨ì†Œ í† í°ê³¼ í•¨ê»˜ ì‹¤ì œ ìƒì„¸ ìˆ˜ì§‘ í˜¸ì¶œ
        info!(
            urls_count = urls.len(),
            "[Integration] collect_detail_batch_with_retry starting"
        );
        self.detail_collector
            .collect_details_with_cancellation(urls, cancellation_token)
            .await
    }

    /// ë°°ì¹˜ ìƒì„¸ ìˆ˜ì§‘ (ì¬ì‹œë„ ë° ë©”íƒ€ í¬í•¨)
    async fn collect_detail_batch_with_retry_with_meta(
        &self,
        urls: &[ProductUrl],
        cancellation_token: CancellationToken,
        max_retries: u32,
    ) -> Result<(Vec<ProductDetail>, u32, u64)> {
        let started = std::time::Instant::now();
        let mut last_error: Option<anyhow::Error> = None;
        info!(
            urls_count = urls.len(),
            max_retries = max_retries,
            "[Integration] collect_detail_batch_with_retry_with_meta starting"
        );
        for attempt in 0..=max_retries {
            match self
                .detail_collector
                .collect_details_with_cancellation(urls, cancellation_token.clone())
                .await
            {
                Ok(details) => {
                    debug!(
                        attempt = attempt,
                        details_count = details.len(),
                        "[Integration] detail collection succeeded"
                    );
                    let duration_ms = started.elapsed().as_millis() as u64;
                    return Ok((details, attempt, duration_ms));
                }
                Err(e) => {
                    last_error = Some(e);
                    if attempt < max_retries {
                        let delay = Duration::from_millis(1000 * (2_u64.pow(attempt)));
                        debug!(
                            attempt = attempt,
                            delay_ms = delay.as_millis(),
                            "Retrying detail collection (meta)"
                        );
                        tokio::time::sleep(delay).await;
                        continue;
                    }
                }
            }
        }
        Err(last_error.unwrap_or_else(|| anyhow::anyhow!("Unknown error")))
    }
}

/// StageActorì—ì„œ ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì‚¬ìš©ì„ ìœ„í•œ ë„ìš°ë¯¸ êµ¬ì¡°ì²´
pub struct RealCrawlingStageExecutor {
    integration_service: Arc<CrawlingIntegrationService>,
}

impl RealCrawlingStageExecutor {
    pub fn new(integration_service: Arc<CrawlingIntegrationService>) -> Self {
        Self {
            integration_service,
        }
    }

    /// StageActorì—ì„œ í˜¸ì¶œí•  ì‹¤ì œ ë‹¨ê³„ ì‹¤í–‰ ë©”ì„œë“œ
    pub async fn execute_stage(
        &self,
        stage_type: StageType,
        items: Vec<StageItem>,
        concurrency_limit: u32,
        cancellation_token: CancellationToken,
    ) -> StageResult {
        match stage_type {
            StageType::ListCollection => {
                let pages: Vec<u32> = items
                    .into_iter()
                    .filter_map(|item| match item {
                        StageItem::Page(page) => Some(page),
                        _ => None,
                    })
                    .collect();

                self.integration_service
                    .execute_list_collection_stage(pages, concurrency_limit, cancellation_token)
                    .await
            }

            StageType::DetailCollection => {
                // í˜„ì¬ëŠ” URL ì•„ì´í…œì´ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ì²˜ë¦¬
                // ì‹¤ì œë¡œëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ URLì„ ë°›ì•„ì•¼ í•¨
                let urls = Vec::new(); // TODO: ì‹¤ì œ URL ì „ë‹¬ êµ¬í˜„

                self.integration_service
                    .execute_detail_collection_stage(urls, concurrency_limit, cancellation_token)
                    .await
            }

            StageType::DataValidation => {
                // ë°ì´í„° ê²€ì¦ ë¡œì§ (í˜„ì¬ëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬)
                StageResult::Success {
                    processed_items: items.len() as u32,
                    duration_ms: 100,
                }
            }

            StageType::DatabaseSave => {
                // ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë¡œì§ (í˜„ì¬ëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬)
                StageResult::Success {
                    processed_items: items.len() as u32,
                    duration_ms: 200,
                }
            }
        }
    }
}
