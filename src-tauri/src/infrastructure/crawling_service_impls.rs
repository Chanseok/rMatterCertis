//! í¬ë¡¤ë§ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
//! 
//! domain/services/crawling_services.rsì˜ íŠ¸ë ˆì´íŠ¸ë“¤ì— ëŒ€í•œ ì‹¤ì œ êµ¬í˜„ì²´

use std::sync::Arc;
use std::time::{Duration, Instant};
use std::collections::HashMap;
use async_trait::async_trait;
use anyhow::{Result, anyhow};
use tracing::{info, warn, error, debug};
use tokio::sync::Semaphore;
use tokio::time::sleep;
use futures::future::try_join_all;
use scraper;
use regex;

use crate::domain::services::{
    StatusChecker, DatabaseAnalyzer, ProductListCollector, ProductDetailCollector,
    SiteStatus, DatabaseAnalysis, FieldAnalysis, DuplicateAnalysis, ProcessingStrategy
};
use crate::domain::services::crawling_services::{
    SiteDataChangeStatus, DataDecreaseRecommendation, RecommendedAction, SeverityLevel
};
use crate::domain::product::Product;
use crate::infrastructure::{HttpClient, MatterDataExtractor, IntegratedProductRepository};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::config::utils as config_utils;

/// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹±í•˜ê¸° ìœ„í•œ êµ¬ì¡°ì²´
#[derive(Debug, Clone)]
struct PageAnalysisCache {
    /// í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜
    product_count: u32,
    /// í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë°œê²¬ëœ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸
    max_pagination_page: u32,
    /// í˜„ì¬ í™œì„±í™”ëœ í˜ì´ì§€ ë²ˆí˜¸ (í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ í™•ì¸)
    active_page: u32,
    /// ì œí’ˆì´ ìˆëŠ”ì§€ ì—¬ë¶€
    has_products: bool,
    /// ë¶„ì„ ì™„ë£Œ ì‹œê°
    analyzed_at: std::time::Instant,
}

/// ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
/// PageDiscoveryServiceì™€ í˜‘ë ¥í•˜ì—¬ ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„
pub struct StatusCheckerImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: AppConfig,
    /// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ ìºì‹œ (í˜ì´ì§€ ë²ˆí˜¸ -> ë¶„ì„ ê²°ê³¼)
    page_cache: Arc<tokio::sync::Mutex<HashMap<u32, PageAnalysisCache>>>,
}

impl StatusCheckerImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: AppConfig,
    ) -> Self {
        // 470ì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì„¤ì •í•œ ì´ìœ  ì„¤ëª…:
        // ì´ëŠ” ê³¼ê±° CSA-IoT ì‚¬ì´íŠ¸ì—ì„œ ê´€ì°°ëœ ëŒ€ëµì ì¸ í˜ì´ì§€ ìˆ˜ì…ë‹ˆë‹¤.
        // ê·¸ëŸ¬ë‚˜ í˜„ì¬ëŠ” ë” ì‘ì€ ê°’(100)ë¶€í„° ì‹œì‘í•˜ì—¬ ë™ì ìœ¼ë¡œ íƒì§€í•©ë‹ˆë‹¤.
        // ì•±ì´ ì‚¬ìš©ë˜ë©´ì„œ ì‹¤ì œ ë§ˆì§€ë§‰ í˜ì´ì§€ë¥¼ í•™ìŠµí•˜ê³  ì €ì¥í•˜ê²Œ ë©ë‹ˆë‹¤.
        
        Self {
            http_client: Arc::new(tokio::sync::Mutex::new(http_client)),
            data_extractor: Arc::new(data_extractor),
            config,
            page_cache: Arc::new(tokio::sync::Mutex::new(HashMap::new())),
        }
    }
}

#[async_trait]
impl StatusChecker for StatusCheckerImpl {
    async fn check_site_status(&self) -> Result<SiteStatus> {
        let start_time = Instant::now();
        info!("Starting comprehensive site status check with detailed page discovery");
        
        // ìºì‹œ ì´ˆê¸°í™”
        self.clear_page_cache().await;
        
        info!("Checking site status and discovering pages...");

        // Step 1: ê¸°ë³¸ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± í™•ì¸
        let url = config_utils::matter_products_page_url_simple(1);
        
        // ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸
        let access_test = {
            let mut client = self.http_client.lock().await;
            client.fetch_html_string(&url).await
        };
        
        match access_test {
            Ok(_) => info!("Site is accessible"),
            Err(e) => {
                error!("Failed to access site: {}", e);
                return Ok(SiteStatus {
                    is_accessible: false,
                    response_time_ms: start_time.elapsed().as_millis() as u64,
                    total_pages: 0,
                    estimated_products: 0,
                    last_check_time: chrono::Utc::now(),
                    health_score: 0.0,
                    data_change_status: SiteDataChangeStatus::Inaccessible,
                    decrease_recommendation: None,
                });
            }
        }

        // Step 2: í˜ì´ì§€ ìˆ˜ íƒì§€ ë° ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜ í™•ì¸
        let (total_pages, products_on_last_page) = self.discover_total_pages().await?;

        let response_time = start_time.elapsed().as_millis() as u64;

        // Step 3: ì‚¬ì´íŠ¸ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
        let health_score = calculate_health_score(response_time, total_pages);

        info!("Site status check completed: {} pages found, {}ms total time, health score: {:.2}", 
              total_pages, response_time, health_score);

        // ì •í™•í•œ ì œí’ˆ ìˆ˜ ê³„ì‚°: (ë§ˆì§€ë§‰ í˜ì´ì§€ - 1) * í˜ì´ì§€ë‹¹ ì œí’ˆ ìˆ˜ + ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜
        use crate::infrastructure::config::defaults::DEFAULT_PRODUCTS_PER_PAGE;
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;
        
        let estimated_products = if total_pages > 1 {
            ((total_pages - 1) * products_per_page) + products_on_last_page
        } else {
            products_on_last_page
        };
        
        info!("Accurate product estimation: ({} full pages * {} products) + {} products on last page = {} total products", 
              total_pages - 1, products_per_page, products_on_last_page, estimated_products);

        // Step 4: ë°ì´í„° ë³€í™” ìƒíƒœ ë¶„ì„
        let (data_change_status, decrease_recommendation) = self.analyze_data_changes(estimated_products).await;
              
        Ok(SiteStatus {
            is_accessible: true,
            response_time_ms: response_time,
            total_pages,
            estimated_products,
            last_check_time: chrono::Utc::now(),
            health_score,
            data_change_status,
            decrease_recommendation,
        })
    }

    async fn estimate_crawling_time(&self, pages: u32) -> Duration {
        // í˜ì´ì§€ë‹¹ í‰ê·  2ì´ˆ + ì œí’ˆ ìƒì„¸í˜ì´ì§€ë‹¹ 1ì´ˆ ì¶”ì •
        let page_collection_time = pages * 2;
        let product_detail_time = pages * 20; // í˜ì´ì§€ë‹¹ 20ê°œ ì œí’ˆ * 1ì´ˆ
        let total_seconds = page_collection_time + product_detail_time;
        
        Duration::from_secs(total_seconds as u64)
    }

    async fn verify_site_accessibility(&self) -> Result<bool> {
        let status = self.check_site_status().await?;
        Ok(status.is_accessible && status.health_score > 0.5)
    }
}

impl StatusCheckerImpl {
    /// í–¥ìƒëœ í˜ì´ì§€ íƒì§€ ë¡œì§ - ì‚¬ì´íŠ¸ ì •ë³´ ë³€í™” ê°ì§€ í¬í•¨
    async fn discover_total_pages(&self) -> Result<(u32, u32)> {
        info!("ğŸ” Starting enhanced page discovery algorithm with site change detection");
        
        // 1. ì‹œì‘ í˜ì´ì§€ ê²°ì •
        let start_page = self.config.app_managed.last_known_max_page
            .unwrap_or(self.config.advanced.last_page_search_start);
        
        info!("ğŸ“ Starting from page {} (last known: {:?}, default: {})", 
              start_page, 
              self.config.app_managed.last_known_max_page,
              self.config.advanced.last_page_search_start);
        
        // 2. ì‹œì‘ í˜ì´ì§€ ë¶„ì„ (ìºì‹œ ì‚¬ìš©)
        let start_analysis = self.get_or_analyze_page(start_page).await?;
        let mut current_page = start_page;
        
        if !start_analysis.has_products {
            warn!("âš ï¸  Starting page {} has no products - checking site status", current_page);
            
            // ì²« í˜ì´ì§€ í™•ì¸ìœ¼ë¡œ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± ê²€ì¦
            let first_page_analysis = self.get_or_analyze_page(1).await?;
            if !first_page_analysis.has_products {
                error!("âŒ First page also has no products - site may be temporarily unavailable");
                return Err(anyhow::anyhow!(
                    "Site appears to be temporarily unavailable or experiencing issues. Please try again later."
                ));
            }
            
            info!("âœ… First page has products - site is accessible, cached page info may be outdated");
            warn!("ğŸ”„ Site content may have decreased - will perform full discovery");
            
            // í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
            current_page = self.find_last_valid_page_downward(current_page).await?;
            info!("âœ… Found valid starting page: {}", current_page);
        }
        
        // 3. ë°˜ë³µì  ìƒí–¥ íƒìƒ‰: í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° ê°’ì„ ì°¾ì„ ë•Œê¹Œì§€ ê³„ì†
        let mut attempts = 0;
        let max_attempts = self.config.advanced.max_search_attempts;
        
        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts ({}), stopping at page {}", max_attempts, current_page);
                break;
            }
            
            info!("ğŸ” Iteration {}/{}: Checking page {}", attempts, max_attempts, current_page);
            
            // í˜„ì¬ í˜ì´ì§€ë¥¼ ë¶„ì„ (ìºì‹œ ì‚¬ìš©)
            let analysis = match self.get_or_analyze_page(current_page).await {
                Ok(analysis) => analysis,
                Err(e) => {
                    warn!("âŒ Failed to analyze page {}: {}", current_page, e);
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ í•˜í–¥ íƒìƒ‰
                    current_page = self.find_last_valid_page_downward(current_page).await?;
                    break;
                }
            };
            
            if !analysis.has_products {
                // ì œí’ˆì´ ì—†ëŠ” ê²½ìš° ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                info!("ğŸ”» Page {} has no products, performing safe downward search", current_page);
                current_page = self.find_last_valid_page_with_safety_check(current_page).await?;
                break;
            }
            
            // í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° í˜ì´ì§€ë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
            if analysis.max_pagination_page > current_page {
                info!("ğŸ”º Found higher page {} in pagination, jumping there", analysis.max_pagination_page);
                current_page = analysis.max_pagination_page;
                // ìƒˆ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë‹¤ì‹œ íƒìƒ‰
                continue;
            } else {
                info!("ğŸ No higher pages found in pagination, {} appears to be the last page", current_page);
                break;
            }
        }
        
        // 4. ìµœì¢… ê²€ì¦: ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸ ë° ì œí’ˆ ìˆ˜ ê³„ì‚°
        let (verified_last_page, products_on_last_page) = self.verify_last_page(current_page).await?;
        
        // 5. ì„¤ì • íŒŒì¼ì— ê²°ê³¼ ì €ì¥
        if let Err(e) = self.update_last_known_page(verified_last_page).await {
            warn!("âš ï¸  Failed to update last known page in config: {}", e);
        }
        
        info!("ğŸ‰ Final verified last page: {} with {} products", verified_last_page, products_on_last_page);
        Ok((verified_last_page, products_on_last_page))
    }

    /// í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ë§ˆì§€ë§‰ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
    async fn find_last_valid_page_downward(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page.saturating_sub(1);
        let min_page = 1;

        info!("Starting downward search from page {}", current_page);

        while current_page >= min_page {
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let mut client = self.http_client.lock().await;
            match client.fetch_html_string(&test_url).await {
                Ok(html) => {
                    let doc = scraper::Html::parse_document(&html);
                    if self.has_products_on_page(&doc) {
                        info!("Found valid page with products: {}", current_page);
                        return Ok(current_page);
                    }
                },
                Err(e) => {
                    warn!("Failed to fetch page {} during downward search: {}", current_page, e);
                }
            }

            current_page = current_page.saturating_sub(1);
            
            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
        warn!("No valid pages found during downward search, returning 1");
        Ok(1)
    }

    /// ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ - ì—°ì† ë¹ˆ í˜ì´ì§€ 3ê°œ ì´ìƒ ì‹œ fatal error
    async fn find_last_valid_page_with_safety_check(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page;
        let mut consecutive_empty_pages = 0;
        const MAX_CONSECUTIVE_EMPTY: u32 = 3;
        let min_page = 1;

        info!("ğŸ” Starting safe downward search from page {} (max consecutive empty: {})", 
              current_page, MAX_CONSECUTIVE_EMPTY);

        // ë¨¼ì € ì‹œì‘ í˜ì´ì§€ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if !self.check_page_has_products(current_page).await? {
            consecutive_empty_pages += 1;
            info!("âš ï¸  Starting page {} is empty (consecutive: {})", current_page, consecutive_empty_pages);
        }

        while current_page > min_page {
            current_page = current_page.saturating_sub(1);
            
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            info!("ğŸ” Checking page {} (consecutive empty: {})", current_page, consecutive_empty_pages);
            
            let mut client = self.http_client.lock().await;
            match client.fetch_html_string(&test_url).await {
                Ok(html) => {
                    let doc = scraper::Html::parse_document(&html);
                    if self.has_products_on_page(&doc) {
                        info!("âœ… Found valid page with products: {} (after {} consecutive empty pages)", 
                              current_page, consecutive_empty_pages);
                        return Ok(current_page);
                    } else {
                        consecutive_empty_pages += 1;
                        warn!("âš ï¸  Page {} is empty (consecutive: {}/{})", 
                              current_page, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY);
                        
                        // ì—°ì†ìœ¼ë¡œ ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ì´ìƒì´ë©´ fatal error
                        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                            error!("ğŸ’¥ FATAL ERROR: Found {} consecutive empty pages starting from page {}. This indicates a serious site issue or crawling problem.", 
                                   consecutive_empty_pages, start_page);
                            
                            return Err(anyhow!(
                                "Fatal error: {} consecutive empty pages detected. Site may be down or pagination structure changed. Last checked pages: {} to {}",
                                consecutive_empty_pages, 
                                start_page,
                                current_page
                            ));
                        }
                    }
                },
                Err(e) => {
                    consecutive_empty_pages += 1;
                    warn!("âŒ Failed to fetch page {} during safe downward search: {} (consecutive: {}/{})", 
                          current_page, e, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY);
                    
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë„ ì—°ì† ì‹¤íŒ¨ë¡œ ì¹´ìš´íŠ¸
                    if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                        error!("ğŸ’¥ FATAL ERROR: {} consecutive failures (empty pages + network errors) starting from page {}.", 
                               consecutive_empty_pages, start_page);
                        
                        return Err(anyhow!(
                            "Fatal error: {} consecutive failures detected. Network issues or site problems. Last error: {}",
                            consecutive_empty_pages, 
                            e
                        ));
                    }
                }
            }
            
            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        // ìµœì†Œ í˜ì´ì§€ê¹Œì§€ ë„ë‹¬í–ˆì§€ë§Œ ì—¬ì „íˆ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ ë§ë‹¤ë©´ fatal error
        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
            error!("ğŸ’¥ FATAL ERROR: Reached minimum page but still have {} consecutive empty pages. Site appears to be completely empty or broken.", 
                   consecutive_empty_pages);
            
            return Err(anyhow!(
                "Fatal error: Site appears to be empty or broken. {} consecutive empty pages found from page {} down to page {}",
                consecutive_empty_pages, 
                start_page,
                current_page
            ));
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆì§€ë§Œ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ê²½ê³ ì™€ í•¨ê»˜ 1 ë°˜í™˜
        warn!("âš ï¸  No valid pages found during safe downward search, but only {} consecutive empty pages. Returning page 1 as fallback.", 
              consecutive_empty_pages);
        Ok(1)
    }

    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ìµœì¢… ê²€ì¦ - ë” ì² ì €í•œ ê²€ì¦ ë¡œì§
    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ê²€ì¦ ë° ì œí’ˆ ìˆ˜ í™•ì¸
    async fn verify_last_page(&self, candidate_page: u32) -> Result<(u32, u32)> {
        info!("ğŸ” Verifying candidate last page: {}", candidate_page);

        // 1. í›„ë³´ í˜ì´ì§€ ë¶„ì„ (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¶„ì„)
        let analysis = self.get_or_analyze_page(candidate_page).await?;
        let products_on_last_page = analysis.product_count;
        let has_products = analysis.has_products;
        
        info!("ğŸ“Š Last page {} has {} products", candidate_page, products_on_last_page);
        
        if !has_products {
            warn!("âš ï¸  Candidate page {} has no products, performing downward search with safety check", candidate_page);
            let actual_last_page = self.find_last_valid_page_with_safety_check(candidate_page).await?;
            // ì‹¤ì œ ë§ˆì§€ë§‰ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ ë‹¤ì‹œ í™•ì¸
            let actual_analysis = self.get_or_analyze_page(actual_last_page).await?;
            return Ok((actual_last_page, actual_analysis.product_count));
        }

        // 2. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ì—ì„œ ì´ë¯¸ ë§ˆì§€ë§‰ í˜ì´ì§€ì„ì„ í™•ì‹ í•  ìˆ˜ ìˆë‹¤ë©´ ì¶”ê°€ í™•ì¸ ìƒëµ
        // í˜„ì¬ í˜ì´ì§€ê°€ í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë°œê²¬ëœ ìµœëŒ€ í˜ì´ì§€ì™€ ê°™ë‹¤ë©´ ê²€ì¦ ì™„ë£Œ
        if analysis.max_pagination_page == candidate_page {
            info!("âœ… Page {} confirmed as last page via pagination analysis (max_pagination={})", 
                  candidate_page, analysis.max_pagination_page);
            info!("ğŸš€ Skipping additional verification - pagination analysis is reliable");
            return Ok((candidate_page, products_on_last_page));
        }
        
        // 3. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ì´ ë¶ˆí™•ì‹¤í•œ ê²½ìš°ì—ë§Œ ìµœì†Œí•œì˜ ì¶”ê°€ ê²€ì¦ ìˆ˜í–‰
        info!("ğŸ” Pagination analysis inconclusive (current={}, max_pagination={}), performing minimal verification", 
              candidate_page, analysis.max_pagination_page);
        
        // ë°”ë¡œ ë‹¤ìŒ í˜ì´ì§€ 1ê°œë§Œ í™•ì¸ (ê³¼ë„í•œ ê²€ì¦ ë°©ì§€)
        let next_page = candidate_page + 1;
        match self.check_page_has_products(next_page).await {
            Ok(true) => {
                warn!("ğŸ” Found products on page {} after candidate {}, re-discovering", 
                      next_page, candidate_page);
                // ë” ë†’ì€ í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ë°œê²¬í–ˆìœ¼ë¯€ë¡œ ê·¸ í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰
                return self.discover_from_page_with_count(next_page).await;
            },
            Ok(false) => {
                info!("âœ… Verified page {} as the last page with {} products (checked {} page ahead)", 
                      candidate_page, products_on_last_page, 1);
            },
            Err(e) => {
                debug!("âŒ Failed to check page {}: {}, assuming {} is last", next_page, e, candidate_page);
            }
        }
        
        Ok((candidate_page, products_on_last_page))
    }

    /// íŠ¹ì • í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰ ì‹œì‘ (ì œí’ˆ ìˆ˜ë„ ë°˜í™˜)
    async fn discover_from_page_with_count(&self, start_page: u32) -> Result<(u32, u32)> {
        info!("ğŸ”„ Re-discovering from page {} with product count", start_page);
        
        let mut current_page = start_page;
        let max_attempts = self.config.advanced.max_search_attempts;
        let mut attempts = 0;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts, stopping at page {}", current_page);
                break;
            }

            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let (has_products, max_page_in_pagination) = {
                let mut client = self.http_client.lock().await;
                match client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);
                        
                        info!("ğŸ“Š Page {} analysis: has_products={}, max_pagination={}", 
                              current_page, has_products, max_page);
                        
                        (has_products, max_page)
                    },
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        break;
                    }
                }
            };

            if !has_products {
                // ì œí’ˆì´ ì—†ìœ¼ë©´ ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ í›„ ì œí’ˆ ìˆ˜ í™•ì¸
                let last_page = self.find_last_valid_page_with_safety_check(current_page).await?;
                let test_url = config_utils::matter_products_page_url_simple(last_page);
                let mut client = self.http_client.lock().await;
                let html = client.fetch_html_string(&test_url).await?;
                drop(client); // ë½ í•´ì œ
                let doc = scraper::Html::parse_document(&html);
                let products_count = self.count_products(&doc);
                return Ok((last_page, products_count));
            }

            if max_page_in_pagination > current_page {
                // ë” í° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ë™
                current_page = max_page_in_pagination;
                continue;
            } else {
                // ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬, ì œí’ˆ ìˆ˜ í™•ì¸
                let test_url = config_utils::matter_products_page_url_simple(current_page);
                let mut client = self.http_client.lock().await;
                let html = client.fetch_html_string(&test_url).await?;
                drop(client); // ë½ í•´ì œ
                let doc = scraper::Html::parse_document(&html);
                let products_count = self.count_products(&doc);
                return Ok((current_page, products_count));
            }
        }

        // ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ì‹œ í˜„ì¬ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ í™•ì¸
        let test_url = config_utils::matter_products_page_url_simple(current_page);
        let mut client = self.http_client.lock().await;
        let html = client.fetch_html_string(&test_url).await?;
        drop(client); // ë½ í•´ì œ
        let doc = scraper::Html::parse_document(&html);
        let products_count = self.count_products(&doc);
        Ok((current_page, products_count))
    }

    /// íŠ¹ì • í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰ ì‹œì‘
    async fn discover_from_page(&self, start_page: u32) -> Result<u32> {
        info!("ğŸ”„ Re-discovering from page {}", start_page);
        
        let mut current_page = start_page;
        let max_attempts = self.config.advanced.max_search_attempts;
        let mut attempts = 0;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts, stopping at page {}", current_page);
                break;
            }

            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let (has_products, max_page_in_pagination) = {
                let mut client = self.http_client.lock().await;
                match client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);
                        
                        info!("ğŸ“Š Page {} analysis: has_products={}, max_pagination={}", 
                              current_page, has_products, max_page);
                        
                        (has_products, max_page)
                    },
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        break;
                    }
                }
            };

            if !has_products {
                // ì œí’ˆì´ ì—†ìœ¼ë©´ ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                return self.find_last_valid_page_with_safety_check(current_page).await;
            }

            if max_page_in_pagination > current_page {
                // ë” í° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ë™
                current_page = max_page_in_pagination;
            } else {
                // ë” í° í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ í˜„ì¬ í˜ì´ì§€ê°€ ë§ˆì§€ë§‰
                break;
            }

            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        Ok(current_page)
    }

    /// íŠ¹ì • í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸ - í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ë„ í•¨ê»˜ í™•ì¸
    async fn check_page_has_products(&self, page: u32) -> Result<bool> {
        let test_url = config_utils::matter_products_page_url_simple(page);
        
        let mut client = self.http_client.lock().await;
        match client.fetch_html_string(&test_url).await {
            Ok(html) => {
                let doc = scraper::Html::parse_document(&html);
                
                // 1. ì œí’ˆ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                let has_products = self.has_products_on_page(&doc);
                
                // 2. í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ í™•ì¸ (ë” ì¤‘ìš”í•œ ì²´í¬)
                let active_page = self.get_active_page_number(&doc);
                
                // ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ì™€ í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                let is_correct_page = active_page == page;
                
                if !is_correct_page {
                    info!("âš ï¸  Page {} was redirected to page {} (pagination mismatch)", page, active_page);
                    return Ok(false);
                }
                
                info!("âœ… Page {} verification: has_products={}, active_page={}, is_correct_page={}", 
                      page, has_products, active_page, is_correct_page);
                
                Ok(has_products && is_correct_page)
            },
            Err(_) => Ok(false),
        }
    }

    /// í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ ì¶”ì¶œ - í˜„ì¬ í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    fn get_active_page_number(&self, doc: &scraper::Html) -> u32 {
        // í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œë¥¼ ì°¾ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        // ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìš°ì„ ìˆœìœ„ ì¡°ì • (í˜ì´ì§€ë„¤ì´ì…˜ ìš°ì„  í´ë˜ìŠ¤: page-numbers.current)
        let active_selectors = [
            ".page-numbers.current", // ìš°ì„ ìˆœìœ„ ê°€ì¥ ë†’ìŒ (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •)
            "span.page-numbers.current", // ì •í™•í•œ ìš”ì†Œ ì§€ì •
            "a.page-numbers.current",
            ".current",
            ".active",
            ".pagination .current",
            ".pagination .active",
            "a.current",
            "span.current",
            "[aria-current='page']",
            ".wp-pagenavi .current",
            ".page-item.active a",
            ".page-link.active",
        ];
        
        for selector_str in &active_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                if let Some(element) = doc.select(&selector).next() {
                    // í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    let text = element.text().collect::<String>().trim().to_string();
                    if let Ok(page_num) = text.parse::<u32>() {
                        info!("ğŸ¯ Found active page number {} using selector '{}'", page_num, selector_str);
                        return page_num;
                    }
                }
            }
        }
        
        // í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° URLì—ì„œ ì¶”ì¶œ ì‹œë„
        if let Some(canonical_url) = self.get_canonical_url(doc) {
            if let Some(page_num) = self.extract_page_number(&canonical_url) {
                info!("ğŸ¯ Found page number {} from canonical URL", page_num);
                return page_num;
            }
        }
        
        // ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš° 1 ë°˜í™˜ (ì²« ë²ˆì§¸ í˜ì´ì§€ë¡œ ì¶”ì •)
        warn!("âš ï¸  Could not determine active page number, assuming page 1");
        1
    }

    /// í˜ì´ì§€ì˜ canonical URL ì¶”ì¶œ
    fn get_canonical_url(&self, doc: &scraper::Html) -> Option<String> {
        if let Ok(selector) = scraper::Selector::parse("link[rel='canonical']") {
            if let Some(element) = doc.select(&selector).next() {
                return element.value().attr("href").map(|s| s.to_string());
            }
        }
        None
    }

    /// ì„¤ì • íŒŒì¼ì— ë§ˆì§€ë§‰ í˜ì´ì§€ ë° ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    async fn update_last_known_page(&self, last_page: u32) -> Result<()> {
        use crate::infrastructure::config::ConfigManager;
        
        let config_manager = ConfigManager::new()?;
        
        // ì„¤ì • ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í´ë¡œì € ì‚¬ìš©
        config_manager.update_app_managed(|app_managed| {
            // ë§ˆì§€ë§‰ ì•Œë ¤ì§„ í˜ì´ì§€ ì—…ë°ì´íŠ¸
            app_managed.last_known_max_page = Some(last_page);
            
            // ë§ˆì§€ë§‰ ì„±ê³µí•œ í¬ë¡¤ë§ ì‹œê°„ ì—…ë°ì´íŠ¸
            app_managed.last_successful_crawl = Some(chrono::Utc::now().to_rfc3339());
            
            // ì¶”ì • ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸ (í˜ì´ì§€ë‹¹ 12ê°œ ì œí’ˆ - ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ê¸°ë°˜)
            app_managed.last_crawl_product_count = Some(last_page * 12);
            
            // í˜ì´ì§€ë‹¹ í‰ê·  ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸
            use crate::infrastructure::config::defaults::DEFAULT_PRODUCTS_PER_PAGE;
            app_managed.avg_products_per_page = Some(DEFAULT_PRODUCTS_PER_PAGE as f64);
            
            info!("ğŸ“ Updated config: last_page={}, timestamp={}", 
                  last_page, 
                  app_managed.last_successful_crawl.as_ref().unwrap_or(&"unknown".to_string()));
        }).await?;
        
        info!("âœ… Successfully updated last known page to {} in config file", last_page);
        Ok(())
    }

    /// ë°ì´í„° ë³€í™” ìƒíƒœ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
    async fn analyze_data_changes(&self, current_estimated_products: u32) -> (SiteDataChangeStatus, Option<DataDecreaseRecommendation>) {
        // ì´ì „ í¬ë¡¤ë§ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        let previous_count = self.config.app_managed.last_crawl_product_count;
        
        match previous_count {
            None => {
                info!("ğŸ†• Initial site check - no previous data available");
                (SiteDataChangeStatus::Initial { count: current_estimated_products }, None)
            },
            Some(prev_count) => {
                let change_percentage = if prev_count > 0 {
                    ((current_estimated_products as f64 - prev_count as f64) / prev_count as f64) * 100.0
                } else {
                    0.0
                };
                
                if current_estimated_products > prev_count {
                    let increase = current_estimated_products - prev_count;
                    info!("ğŸ“ˆ Site data increased: {} -> {} (+{}, +{:.1}%)", 
                          prev_count, current_estimated_products, increase, change_percentage);
                    (SiteDataChangeStatus::Increased { 
                        new_count: current_estimated_products, 
                        previous_count: prev_count 
                    }, None)
                } else if current_estimated_products == prev_count {
                    info!("ğŸ“Š Site data stable: {} products", current_estimated_products);
                    (SiteDataChangeStatus::Stable { count: current_estimated_products }, None)
                } else {
                    let decrease = prev_count - current_estimated_products;
                    let decrease_percentage = (decrease as f64 / prev_count as f64) * 100.0;
                    
                    warn!("ğŸ“‰ Site data decreased: {} -> {} (-{}, -{:.1}%)", 
                          prev_count, current_estimated_products, decrease, decrease_percentage);
                    
                    let severity = if decrease_percentage < 10.0 {
                        SeverityLevel::Low
                    } else if decrease_percentage < 30.0 {
                        SeverityLevel::Medium
                    } else if decrease_percentage < 50.0 {
                        SeverityLevel::High
                    } else {
                        SeverityLevel::Critical
                    };
                    
                    let recommendation = self.generate_decrease_recommendation(decrease_percentage, &severity);
                    
                    (SiteDataChangeStatus::Decreased { 
                        current_count: current_estimated_products,
                        previous_count: prev_count,
                        decrease_amount: decrease
                    }, Some(recommendation))
                }
            }
        }
    }
    
    /// ë°ì´í„° ê°ì†Œ ì‹œ ê¶Œì¥ì‚¬í•­ ìƒì„±
    fn generate_decrease_recommendation(&self, decrease_percentage: f64, severity: &SeverityLevel) -> DataDecreaseRecommendation {
        match severity {
            SeverityLevel::Low => DataDecreaseRecommendation {
                action_type: RecommendedAction::WaitAndRetry,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¼ì‹œì ì¸ ë³€í™”ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "ì ì‹œ í›„(5-10ë¶„) ë‹¤ì‹œ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                    "ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ìˆ˜ë™ìœ¼ë¡œ ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::Medium => DataDecreaseRecommendation {
                action_type: RecommendedAction::ManualVerification,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "CSA-IoT ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ì œí’ˆ ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                    "ì‚¬ì´íŠ¸ì—ì„œ í•„í„° ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•˜ê³  ë¶€ë¶„ ì¬í¬ë¡¤ë§ì„ ê³ ë ¤í•˜ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::High => DataDecreaseRecommendation {
                action_type: RecommendedAction::BackupAndRecrawl,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% í¬ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… í›„ ì¬í¬ë¡¤ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¦‰ì‹œ ë°±ì—…í•˜ì„¸ìš”".to_string(),
                    "CSA-IoT ì‚¬ì´íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ì‹¤ì œ ìƒí™©ì„ íŒŒì•…í•˜ì„¸ìš”".to_string(),
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¹„ìš°ê³  ì „ì²´ ì¬í¬ë¡¤ë§ì„ ìˆ˜í–‰í•˜ì„¸ìš”".to_string(),
                    "í¬ë¡¤ë§ ì™„ë£Œ í›„ ì´ì „ ë°ì´í„°ì™€ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::Critical => DataDecreaseRecommendation {
                action_type: RecommendedAction::BackupAndRecrawl,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ì‹¬ê°í•˜ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "ğŸš¨ ì¦‰ì‹œ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•˜ì„¸ìš”".to_string(),
                    "CSA-IoT ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ ì‹¤ì œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ì‚¬ì´íŠ¸ êµ¬ì¡°ë‚˜ í•„í„° ì¡°ê±´ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ë°±ì—… í™•ì¸ í›„ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì „ì²´ ì¬í¬ë¡¤ë§í•˜ì„¸ìš”".to_string(),
                    "í¬ë¡¤ë§ ì„¤ì •(selector, URL ë“±)ì„ ì¬ê²€í† í•˜ì„¸ìš”".to_string(),
                ],
            },
        }
    }

    /// í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
    fn has_products_on_page(&self, doc: &scraper::Html) -> bool {
        let product_count = self.count_products(doc);
        product_count > 0
    }

    /// í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
    fn find_max_page_in_pagination(&self, doc: &scraper::Html) -> u32 {
        let mut max_page = 1;
        
        // 1. í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ì—ì„œ ì°¾ê¸°
        let link_selectors = vec![
            "a[href*='page']",
            ".pagination a",
            ".page-numbers", // ëª¨ë“  í˜ì´ì§€ ë²ˆí˜¸ ìš”ì†Œ (aì™€ span ëª¨ë‘ í¬í•¨)
            ".page-numbers a", 
            ".pager a",
            "a[href*='paged']",
            ".page-numbers:not(.current):not(.dots)" // í˜„ì¬ í˜ì´ì§€ì™€ ì¤„ì„í‘œë¥¼ ì œì™¸í•œ í˜ì´ì§€ ë²ˆí˜¸
        ];
        
        for selector_str in &link_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                for element in doc.select(&selector) {
                    // href ì†ì„±ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    if let Some(href) = element.value().attr("href") {
                        if let Some(page_num) = self.extract_page_number(href) {
                            if page_num > max_page {
                                max_page = page_num;
                                debug!("Found higher page {} in href: {}", page_num, href);
                            }
                        }
                    }
                    
                    // í…ìŠ¤íŠ¸ì—ì„œë„ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    let text = element.text().collect::<String>().trim().to_string();
                    if let Ok(page_num) = text.parse::<u32>() {
                        if page_num > max_page && page_num < 10000 { // í•©ë¦¬ì ì¸ ìƒí•œì„ 
                            max_page = page_num;
                            debug!("Found higher page {} in text: {}", page_num, text);
                        }
                    }
                }
            }
        }
        
        debug!("Max page found in pagination: {}", max_page);
        max_page
    }

    /// URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
    fn extract_page_number(&self, url: &str) -> Option<u32> {
        // URL íŒ¨í„´: /page/123/ ë˜ëŠ” paged=123
        let patterns = [
            r"/page/(\d+)",
            r"paged=(\d+)",
            r"page=(\d+)",
            r"/(\d+)/$",  // ëì— ìˆ«ìê°€ ìˆëŠ” ê²½ìš°
        ];
        
        for pattern in &patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(caps) = re.captures(url) {
                    if let Some(num_str) = caps.get(1) {
                        if let Ok(num) = num_str.as_str().parse::<u32>() {
                            return Some(num);
                        }
                    }
                }
            }
        }
        
        None
    }

    /// í˜ì´ì§€ì—ì„œ ì œí’ˆ ê°œìˆ˜ ì¹´ìš´íŠ¸ (ëª¨ë“  ì„ íƒìë¥¼ ì‹œë„í•˜ê³  ê°€ì¥ ë§ì€ ê²°ê³¼ ë°˜í™˜)
    fn count_products(&self, doc: &scraper::Html) -> u32 {
        let mut max_count = 0;
        let mut best_selector = "none";
        
        for selector_str in &self.config.advanced.product_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                let count = doc.select(&selector).count() as u32;
                debug!("Selector '{}' found {} products", selector_str, count);
                if count > max_count {
                    max_count = count;
                    best_selector = selector_str;
                }
            } else {
                debug!("Failed to parse selector: {}", selector_str);
            }
        }
        
        // ê¸°ë³¸ ì„ íƒìë“¤ë„ ì‹œë„
        if max_count == 0 {
            if let Ok(article_selector) = scraper::Selector::parse("article") {
                let count = doc.select(&article_selector).count() as u32;
                if count > 0 {
                    max_count = count;
                    best_selector = "article (fallback)";
                    debug!("Fallback: Found {} products using generic article selector", count);
                }
            }
        }
        
        info!("Total products found on page: {} (using selector: {})", max_count, best_selector);
        max_count
    }

    /// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¶„ì„
    async fn get_or_analyze_page(&self, page_number: u32) -> Result<PageAnalysisCache> {
        // ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        {
            let cache = self.page_cache.lock().await;
            if let Some(cached) = cache.get(&page_number) {
                debug!("ğŸ“‹ Using cached analysis for page {}", page_number);
                return Ok(cached.clone());
            }
        }
        
        // ìºì‹œì— ì—†ìœ¼ë©´ ìƒˆë¡œ ë¶„ì„
        debug!("ğŸ” Analyzing page {} (not in cache)", page_number);
        let url = config_utils::matter_products_page_url_simple(page_number);
        
        let (product_count, max_pagination_page, active_page, has_products) = {
            let mut client = self.http_client.lock().await;
            let html = client.fetch_html_string(&url).await?;
            drop(client); // ë½ í•´ì œ
            
            let doc = scraper::Html::parse_document(&html);
            let product_count = self.count_products(&doc);
            let max_pagination_page = self.find_max_page_in_pagination(&doc);
            let active_page = self.get_active_page_number(&doc);
            let has_products = product_count > 0;
            
            (product_count, max_pagination_page, active_page, has_products)
        };
        
        let analysis = PageAnalysisCache {
            product_count,
            max_pagination_page,
            active_page,
            has_products,
            analyzed_at: std::time::Instant::now(),
        };
        
        // ìºì‹œì— ì €ì¥
        {
            let mut cache = self.page_cache.lock().await;
            cache.insert(page_number, analysis.clone());
        }
        
        info!("ğŸ“Š Page {} analysis: has_products={}, product_count={}, max_pagination={}", 
              page_number, has_products, product_count, max_pagination_page);
        
        Ok(analysis)
    }
    
    /// ìºì‹œë¥¼ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ìƒíƒœ ì²´í¬ ì‹œì‘ ì‹œ í˜¸ì¶œ)
    async fn clear_page_cache(&self) {
        let mut cache = self.page_cache.lock().await;
        cache.clear();
        debug!("ğŸ—‘ï¸  Page cache cleared");
    }
}

/// ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct DatabaseAnalyzerImpl {
    product_repo: Arc<IntegratedProductRepository>,
}

impl DatabaseAnalyzerImpl {
    pub fn new(product_repo: Arc<IntegratedProductRepository>) -> Self {
        Self { product_repo }
    }
}

#[async_trait]
impl DatabaseAnalyzer for DatabaseAnalyzerImpl {
    async fn analyze_current_state(&self) -> Result<DatabaseAnalysis> {
        info!("Analyzing database state...");

        let statistics = self.product_repo.get_database_statistics().await?;
        let total_products = statistics.total_products as u32;
        
        // ì¤‘ë³µ ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)
        let duplicate_count = 0; // TODO: ì‹¤ì œ ì¤‘ë³µ ê²€ì‚¬ ë¡œì§ êµ¬í˜„
        let unique_products = total_products - duplicate_count;

        // í•„ë“œ ëˆ„ë½ ë¶„ì„
        let missing_fields = FieldAnalysis {
            missing_company: 0,      // TODO: ì‹¤ì œ ëˆ„ë½ í•„ë“œ ë¶„ì„
            missing_model: 0,
            missing_matter_version: 0,
            missing_connectivity: 0,
            missing_certification_date: 0,
        };

        // ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        let data_quality_score = if total_products > 0 {
            (unique_products as f64 / total_products as f64) * 0.8 + 0.2
        } else {
            1.0
        };

        info!("Database analysis completed: {} total, {} unique products", total_products, unique_products);

        Ok(DatabaseAnalysis {
            total_products,
            unique_products,
            duplicate_count,
            last_update: None, // TODO: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ì 
            missing_fields_analysis: missing_fields,
            data_quality_score,
        })
    }

    async fn recommend_processing_strategy(&self) -> Result<ProcessingStrategy> {
        let analysis = self.analyze_current_state().await?;
        
        // ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°ì— ë”°ë¥¸ ì „ëµ ì¡°ì •
        let (batch_size, concurrency) = if analysis.total_products < 1000 {
            (20, 5)
        } else if analysis.total_products < 5000 {
            (15, 3)
        } else {
            (10, 2)
        };

        Ok(ProcessingStrategy {
            recommended_batch_size: batch_size,
            recommended_concurrency: concurrency,
            should_skip_duplicates: analysis.duplicate_count > 0,
            should_update_existing: analysis.data_quality_score < 0.8,
            priority_urls: Vec::new(), // TODO: ìš°ì„ ìˆœìœ„ URL ë¡œì§
        })
    }

    async fn analyze_duplicates(&self) -> Result<DuplicateAnalysis> {
        // TODO: ì‹¤ì œ ì¤‘ë³µ ë¶„ì„ ë¡œì§ êµ¬í˜„
        Ok(DuplicateAnalysis {
            total_duplicates: 0,
            duplicate_groups: Vec::new(),
            duplicate_percentage: 0.0,
        })
    }
}

/// ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductListCollectorImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

#[derive(Debug, Clone)]
pub struct CollectorConfig {
    pub concurrency: u32,
    pub delay_ms: u64,
    pub batch_size: u32,
    pub retry_max: u32,
}

impl ProductListCollectorImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client: Arc::new(tokio::sync::Mutex::new(http_client)),
            data_extractor: Arc::new(data_extractor),
            config,
        }
    }
}

#[async_trait]
impl ProductListCollector for ProductListCollectorImpl {
    async fn collect_all_pages(&self, total_pages: u32) -> Result<Vec<String>> {
        info!("Collecting product URLs from {} pages", total_pages);

        let _semaphore = Arc::new(Semaphore::new(self.config.concurrency as usize));
        let mut all_product_urls = Vec::new();

        // ë°°ì¹˜ë³„ë¡œ í˜ì´ì§€ ì²˜ë¦¬
        for batch_start in (1..=total_pages).step_by(self.config.batch_size as usize) {
            let batch_end = (batch_start + self.config.batch_size - 1).min(total_pages);
            let batch_pages: Vec<u32> = (batch_start..=batch_end).collect();
            
            let batch_urls = self.collect_page_batch(&batch_pages).await?;
            all_product_urls.extend(batch_urls);

            debug!("Completed batch {}-{}, total URLs: {}", batch_start, batch_end, all_product_urls.len());
        }

        info!("Product URL collection completed: {} URLs collected", all_product_urls.len());
        Ok(all_product_urls)
    }

    async fn collect_single_page(&self, page: u32) -> Result<Vec<String>> {
        let url = config_utils::matter_products_page_url_simple(page);
        debug!("Fetching page: {}", url);

        if self.config.delay_ms > 0 {
            sleep(Duration::from_millis(self.config.delay_ms)).await;
        }

        let mut client = self.http_client.lock().await;
        let html_str = client.fetch_html_string(&url).await?;
        
        let urls = self.data_extractor.extract_product_urls_from_content(&html_str)
            .map_err(|e| anyhow!("Failed to extract URLs from page {}: {}", page, e))?;

        debug!("Extracted {} URLs from page {}", urls.len(), page);
        Ok(urls)
    }

    async fn collect_page_batch(&self, pages: &[u32]) -> Result<Vec<String>> {
        let semaphore = Arc::new(Semaphore::new(self.config.concurrency as usize));
        
        let batch_tasks: Vec<_> = pages.iter().map(|&page_num| {
            let semaphore = Arc::clone(&semaphore);
            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let delay_ms = self.config.delay_ms;

            tokio::spawn(async move {
                let _permit = semaphore.acquire().await.unwrap();
                
                if delay_ms > 0 {
                    sleep(Duration::from_millis(delay_ms)).await;
                }

                let url = config_utils::matter_products_page_url_simple(page_num);
                debug!("Fetching page: {}", url);

                let mut client = http_client.lock().await;
                match client.fetch_html_string(&url).await {
                    Ok(html_str) => {
                        match data_extractor.extract_product_urls_from_content(&html_str) {
                            Ok(urls) => {
                                debug!("Extracted {} URLs from page {}", urls.len(), page_num);
                                Ok(urls)
                            },
                            Err(e) => {
                                warn!("Failed to extract URLs from page {}: {}", page_num, e);
                                Ok(Vec::new())
                            }
                        }
                    },
                    Err(e) => {
                        error!("Failed to fetch page {}: {}", page_num, e);
                        Err(e)
                    }
                }
            })
        }).collect();

        let batch_results = try_join_all(batch_tasks).await?;
        let mut all_urls = Vec::new();
        
        for result in batch_results {
            match result {
                Ok(urls) => all_urls.extend(urls),
                Err(e) => warn!("Batch task failed: {}", e),
            }
        }

        Ok(all_urls)
    }
}

/// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductDetailCollectorImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

impl ProductDetailCollectorImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client: Arc::new(tokio::sync::Mutex::new(http_client)),
            data_extractor: Arc::new(data_extractor),
            config,
        }
    }
}

#[async_trait]
impl ProductDetailCollector for ProductDetailCollectorImpl {
    async fn collect_details(&self, urls: &[String]) -> Result<Vec<Product>> {
        info!("Collecting product details from {} URLs", urls.len());

        let mut all_products = Vec::new();

        // ë°°ì¹˜ë³„ë¡œ ì œí’ˆ ì²˜ë¦¬
        for batch in urls.chunks(self.config.batch_size as usize) {
            let batch_products = self.collect_product_batch(batch).await?;
            all_products.extend(batch_products);
            
            debug!("Completed batch, total products: {}", all_products.len());
        }

        info!("Product detail collection completed: {} products collected", all_products.len());
        Ok(all_products)
    }

    async fn collect_single_product(&self, url: &str) -> Result<Product> {
        debug!("Fetching product detail: {}", url);

        if self.config.delay_ms > 0 {
            sleep(Duration::from_millis(self.config.delay_ms)).await;
        }

        let mut client = self.http_client.lock().await;
        let html_str = client.fetch_html_string(url).await?;
        
        // HTML íŒŒì‹±í•˜ì—¬ Product êµ¬ì¡°ì²´ ìƒì„±
        let html = scraper::Html::parse_document(&html_str);
        let products = self.data_extractor.extract_products_from_list(&html, 0)?;
        
        if let Some(product) = products.into_iter().next() {
            debug!("Extracted product: {:?} - {:?}", product.manufacturer, product.model);
            Ok(product)
        } else {
            Err(anyhow!("No product found at URL: {}", url))
        }
    }

    async fn collect_product_batch(&self, urls: &[String]) -> Result<Vec<Product>> {
        let semaphore = Arc::new(Semaphore::new(self.config.concurrency as usize));
        
        let batch_tasks: Vec<_> = urls.iter().map(|url| {
            let semaphore = Arc::clone(&semaphore);
            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let delay_ms = self.config.delay_ms;
            let url = url.clone();

            tokio::spawn(async move {
                let _permit = semaphore.acquire().await.unwrap();
                
                if delay_ms > 0 {
                    sleep(Duration::from_millis(delay_ms)).await;
                }

                let mut client = http_client.lock().await;
                match client.fetch_html_string(&url).await {
                    Ok(html_str) => {
                        let html = scraper::Html::parse_document(&html_str);
                        match data_extractor.extract_products_from_list(&html, 0) {
                            Ok(mut products) => {
                                if let Some(product) = products.pop() {
                                    debug!("Extracted product: {:?} - {:?}", product.manufacturer, product.model);
                                    Ok(Some(product))
                                } else {
                                    warn!("No product found at URL: {}", url);
                                    Ok(None)
                                }
                            },
                            Err(e) => {
                                warn!("Failed to extract product from {}: {}", url, e);
                                Ok(None)
                            }
                        }
                    },
                    Err(e) => {
                        error!("Failed to fetch product {}: {}", url, e);
                        Err(e)
                    }
                }
            })
        }).collect();

        let batch_results = try_join_all(batch_tasks).await?;
        let mut products = Vec::new();
        
        for result in batch_results {
            match result {
                Ok(Some(product)) => products.push(product),
                Ok(None) => {}, // ìŠ¤í‚µ
                Err(e) => warn!("Product collection task failed: {}", e),
            }
        }

        Ok(products)
    }
}

/// ì‚¬ì´íŠ¸ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
fn calculate_health_score(response_time_ms: u64, total_pages: u32) -> f64 {
    // ì‘ë‹µì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 1.0)
    let time_score = if response_time_ms < 2000 { 1.0 }
    else if response_time_ms < 5000 { 0.8 }
    else if response_time_ms < 10000 { 0.6 }
    else if response_time_ms < 20000 { 0.4 }
    else { 0.2 };
    
    // í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ì ìˆ˜ (í˜ì´ì§€ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì‚¬ì´íŠ¸ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ)
    let page_score = if total_pages >= 10 { 1.0 }
    else if total_pages >= 5 { 0.8 }
    else if total_pages >= 1 { 0.6 }
    else { 0.0 };
    
    // ê°€ì¤‘ í‰ê·  (ì‘ë‹µì‹œê°„ì´ ë” ì¤‘ìš”)
    (time_score * 0.7) + (page_score * 0.3)
}