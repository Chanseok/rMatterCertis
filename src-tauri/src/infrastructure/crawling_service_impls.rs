//! í¬ë¡¤ë§ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´ë“¤
//! 
//! domain/services/crawling_services.rsì˜ íŠ¸ë ˆì´íŠ¸ë“¤ì— ëŒ€í•œ ì‹¤ì œ êµ¬í˜„ì²´

use std::sync::Arc;
use std::time::{Duration, Instant};
use async_trait::async_trait;
use anyhow::{Result, anyhow};
use tracing::{info, warn, error, debug};
use tokio::sync::Semaphore;
use tokio::time::sleep;
use futures::future::try_join_all;

use crate::domain::services::{
    StatusChecker, DatabaseAnalyzer, ProductListCollector, ProductDetailCollector,
    SiteStatus, DatabaseAnalysis, FieldAnalysis, DuplicateAnalysis, ProcessingStrategy
};
use crate::domain::product::Product;
use crate::infrastructure::{HttpClient, MatterDataExtractor, IntegratedProductRepository};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::config::utils as config_utils;

/// ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
/// PageDiscoveryServiceì™€ í˜‘ë ¥í•˜ì—¬ ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„
pub struct StatusCheckerImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: AppConfig,
}

impl StatusCheckerImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: AppConfig,
    ) -> Self {
        // 470ì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì„¤ì •í•œ ì´ìœ  ì„¤ëª…:
        // ì´ëŠ” ê³¼ê±° CSA-IoT ì‚¬ì´íŠ¸ì—ì„œ ê´€ì°°ëœ ëŒ€ëµì ì¸ í˜ì´ì§€ ìˆ˜ì…ë‹ˆë‹¤.
        // ê·¸ëŸ¬ë‚˜ í˜„ì¬ëŠ” ë” ì‘ì€ ê°’(100)ë¶€í„° ì‹œì‘í•˜ì—¬ ë™ì ìœ¼ë¡œ íƒì§€í•©ë‹ˆë‹¤.
        // ì•±ì´ ì‚¬ìš©ë˜ë©´ì„œ ì‹¤ì œ ë§ˆì§€ë§‰ í˜ì´ì§€ë¥¼ í•™ìŠµí•˜ê³  ì €ì¥í•˜ê²Œ ë©ë‹ˆë‹¤.
        
        Self {
            http_client: Arc::new(tokio::sync::Mutex::new(http_client)),
            data_extractor: Arc::new(data_extractor),
            config,
        }
    }
}

#[async_trait]
impl StatusChecker for StatusCheckerImpl {
    async fn check_site_status(&self) -> Result<SiteStatus> {
        let start_time = Instant::now();
        info!("Checking site status and discovering pages...");

        // Step 1: ê¸°ë³¸ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± í™•ì¸
        let url = config_utils::matter_products_page_url_simple(1);
        
        // ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸
        let access_test = {
            let mut client = self.http_client.lock().await;
            client.fetch_html_string(&url).await
        };
        
        match access_test {
            Ok(_) => info!("Site is accessible"),
            Err(e) => {
                error!("Failed to access site: {}", e);
                return Ok(SiteStatus {
                    is_accessible: false,
                    response_time_ms: start_time.elapsed().as_millis() as u64,
                    total_pages: 0,
                    estimated_products: 0,
                    last_check_time: chrono::Utc::now(),
                    health_score: 0.0,
                });
            }
        }

        // Step 2: í˜ì´ì§€ ìˆ˜ íƒì§€ (PageDiscoveryServiceì˜ ë¡œì§ì„ ì•ˆì „í•˜ê²Œ í™œìš©)
        let total_pages = self.discover_total_pages().await?;

        let response_time = start_time.elapsed().as_millis() as u64;

        // Step 3: ì‚¬ì´íŠ¸ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
        let health_score = calculate_health_score(response_time, total_pages);

        info!("Site status check completed: {} pages found, {}ms total time, health score: {:.2}", 
              total_pages, response_time, health_score);

        Ok(SiteStatus {
            is_accessible: true,
            response_time_ms: response_time,
            total_pages,
            estimated_products: total_pages * 20, // í˜ì´ì§€ë‹¹ ì•½ 20ê°œ ì œí’ˆ ì¶”ì •
            last_check_time: chrono::Utc::now(),
            health_score,
        })
    }

    async fn estimate_crawling_time(&self, pages: u32) -> Duration {
        // í˜ì´ì§€ë‹¹ í‰ê·  2ì´ˆ + ì œí’ˆ ìƒì„¸í˜ì´ì§€ë‹¹ 1ì´ˆ ì¶”ì •
        let page_collection_time = pages * 2;
        let product_detail_time = pages * 20; // í˜ì´ì§€ë‹¹ 20ê°œ ì œí’ˆ * 1ì´ˆ
        let total_seconds = page_collection_time + product_detail_time;
        
        Duration::from_secs(total_seconds as u64)
    }

    async fn verify_site_accessibility(&self) -> Result<bool> {
        let status = self.check_site_status().await?;
        Ok(status.is_accessible && status.health_score > 0.5)
    }
}

impl StatusCheckerImpl {
    /// í–¥ìƒëœ í˜ì´ì§€ íƒì§€ ë¡œì§ - í˜ì´ì§€ë„¤ì´ì…˜ì„ ë°˜ë³µì ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ì •í™•í•œ ë§ˆì§€ë§‰ í˜ì´ì§€ ì°¾ê¸°
    async fn discover_total_pages(&self) -> Result<u32> {
        info!("ğŸ” Starting enhanced page discovery algorithm");
        
        // 1. ì‹œì‘ í˜ì´ì§€ ê²°ì •
        let start_page = self.config.app_managed.last_known_max_page
            .unwrap_or(self.config.advanced.last_page_search_start);
        
        info!("ğŸ“ Starting from page {} (last known: {:?}, default: {})", 
              start_page, 
              self.config.app_managed.last_known_max_page,
              self.config.advanced.last_page_search_start);
        
        // 2. ì²« ë²ˆì§¸ ë‹¨ê³„: ì‹œì‘ í˜ì´ì§€ê°€ ìœ íš¨í•œì§€ í™•ì¸
        let mut current_page = start_page;
        if !self.check_page_has_products(current_page).await? {
            info!("âš ï¸  Starting page {} has no products, searching downward", current_page);
            current_page = self.find_last_valid_page_downward(current_page).await?;
            info!("âœ… Found valid starting page: {}", current_page);
        }
        
        // 3. ë°˜ë³µì  ìƒí–¥ íƒìƒ‰: í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° ê°’ì„ ì°¾ì„ ë•Œê¹Œì§€ ê³„ì†
        let mut attempts = 0;
        let max_attempts = self.config.advanced.max_search_attempts;
        
        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts ({}), stopping at page {}", max_attempts, current_page);
                break;
            }
            
            info!("ğŸ” Iteration {}/{}: Checking page {}", attempts, max_attempts, current_page);
            
            // í˜„ì¬ í˜ì´ì§€ë¥¼ ë¡œë“œí•˜ê³  ë¶„ì„
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            debug!("ğŸ“„ Loading page: {}", test_url);
            
            let (has_products, max_page_in_pagination) = {
                let mut client = self.http_client.lock().await;
                match client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);
                        
                        info!("ğŸ“Š Page {} analysis: has_products={}, max_pagination={}", 
                              current_page, has_products, max_page);
                        
                        (has_products, max_page)
                    },
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ í•˜í–¥ íƒìƒ‰
                        current_page = self.find_last_valid_page_downward(current_page).await?;
                        break;
                    }
                }
            };
            
            if !has_products {
                // ì œí’ˆì´ ì—†ëŠ” ê²½ìš° ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                info!("ğŸ”» Page {} has no products, performing safe downward search", current_page);
                current_page = self.find_last_valid_page_with_safety_check(current_page).await?;
                break;
            }
            
            // í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° í˜ì´ì§€ë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
            if max_page_in_pagination > current_page {
                info!("ğŸ”º Found higher page {} in pagination, jumping there", max_page_in_pagination);
                current_page = max_page_in_pagination;
                // ìƒˆ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë‹¤ì‹œ íƒìƒ‰
                continue;
            } else {
                info!("ğŸ No higher pages found in pagination, {} appears to be the last page", current_page);
                break;
            }
        }
        
        // 4. ìµœì¢… ê²€ì¦: ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
        let verified_last_page = self.verify_last_page(current_page).await?;
        
        // 5. ì„¤ì • íŒŒì¼ì— ê²°ê³¼ ì €ì¥
        if let Err(e) = self.update_last_known_page(verified_last_page).await {
            warn!("âš ï¸  Failed to update last known page in config: {}", e);
        }
        
        info!("ğŸ‰ Final verified last page: {}", verified_last_page);
        Ok(verified_last_page)
    }

    /// í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ë§ˆì§€ë§‰ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
    async fn find_last_valid_page_downward(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page.saturating_sub(1);
        let min_page = 1;

        info!("Starting downward search from page {}", current_page);

        while current_page >= min_page {
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let mut client = self.http_client.lock().await;
            match client.fetch_html_string(&test_url).await {
                Ok(html) => {
                    let doc = scraper::Html::parse_document(&html);
                    if self.has_products_on_page(&doc) {
                        info!("Found valid page with products: {}", current_page);
                        return Ok(current_page);
                    }
                },
                Err(e) => {
                    warn!("Failed to fetch page {} during downward search: {}", current_page, e);
                }
            }

            current_page = current_page.saturating_sub(1);
            
            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
        warn!("No valid pages found during downward search, returning 1");
        Ok(1)
    }

    /// ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ - ì—°ì† ë¹ˆ í˜ì´ì§€ 3ê°œ ì´ìƒ ì‹œ fatal error
    async fn find_last_valid_page_with_safety_check(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page;
        let mut consecutive_empty_pages = 0;
        const MAX_CONSECUTIVE_EMPTY: u32 = 3;
        let min_page = 1;

        info!("ğŸ” Starting safe downward search from page {} (max consecutive empty: {})", 
              current_page, MAX_CONSECUTIVE_EMPTY);

        // ë¨¼ì € ì‹œì‘ í˜ì´ì§€ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if !self.check_page_has_products(current_page).await? {
            consecutive_empty_pages += 1;
            info!("âš ï¸  Starting page {} is empty (consecutive: {})", current_page, consecutive_empty_pages);
        }

        while current_page > min_page {
            current_page = current_page.saturating_sub(1);
            
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            info!("ğŸ” Checking page {} (consecutive empty: {})", current_page, consecutive_empty_pages);
            
            let mut client = self.http_client.lock().await;
            match client.fetch_html_string(&test_url).await {
                Ok(html) => {
                    let doc = scraper::Html::parse_document(&html);
                    if self.has_products_on_page(&doc) {
                        info!("âœ… Found valid page with products: {} (after {} consecutive empty pages)", 
                              current_page, consecutive_empty_pages);
                        return Ok(current_page);
                    } else {
                        consecutive_empty_pages += 1;
                        warn!("âš ï¸  Page {} is empty (consecutive: {}/{})", 
                              current_page, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY);
                        
                        // ì—°ì†ìœ¼ë¡œ ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ì´ìƒì´ë©´ fatal error
                        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                            error!("ğŸ’¥ FATAL ERROR: Found {} consecutive empty pages starting from page {}. This indicates a serious site issue or crawling problem.", 
                                   consecutive_empty_pages, start_page);
                            
                            return Err(anyhow!(
                                "Fatal error: {} consecutive empty pages detected. Site may be down or pagination structure changed. Last checked pages: {} to {}",
                                consecutive_empty_pages, 
                                start_page,
                                current_page
                            ));
                        }
                    }
                },
                Err(e) => {
                    consecutive_empty_pages += 1;
                    warn!("âŒ Failed to fetch page {} during safe downward search: {} (consecutive: {}/{})", 
                          current_page, e, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY);
                    
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë„ ì—°ì† ì‹¤íŒ¨ë¡œ ì¹´ìš´íŠ¸
                    if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                        error!("ğŸ’¥ FATAL ERROR: {} consecutive failures (empty pages + network errors) starting from page {}.", 
                               consecutive_empty_pages, start_page);
                        
                        return Err(anyhow!(
                            "Fatal error: {} consecutive failures detected. Network issues or site problems. Last error: {}",
                            consecutive_empty_pages, 
                            e
                        ));
                    }
                }
            }
            
            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        // ìµœì†Œ í˜ì´ì§€ê¹Œì§€ ë„ë‹¬í–ˆì§€ë§Œ ì—¬ì „íˆ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ ë§ë‹¤ë©´ fatal error
        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
            error!("ğŸ’¥ FATAL ERROR: Reached minimum page but still have {} consecutive empty pages. Site appears to be completely empty or broken.", 
                   consecutive_empty_pages);
            
            return Err(anyhow!(
                "Fatal error: Site appears to be empty or broken. {} consecutive empty pages found from page {} down to page {}",
                consecutive_empty_pages, 
                start_page,
                current_page
            ));
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆì§€ë§Œ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ê²½ê³ ì™€ í•¨ê»˜ 1 ë°˜í™˜
        warn!("âš ï¸  No valid pages found during safe downward search, but only {} consecutive empty pages. Returning page 1 as fallback.", 
              consecutive_empty_pages);
        Ok(1)
    }

    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ìµœì¢… ê²€ì¦ - ë” ì² ì €í•œ ê²€ì¦ ë¡œì§
    async fn verify_last_page(&self, candidate_page: u32) -> Result<u32> {
        info!("ğŸ” Verifying candidate last page: {}", candidate_page);

        // 1. í›„ë³´ í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
        let has_products = self.check_page_has_products(candidate_page).await?;
        if !has_products {
            warn!("âš ï¸  Candidate page {} has no products, performing downward search with safety check", candidate_page);
            return self.find_last_valid_page_with_safety_check(candidate_page).await;
        }

        // 2. ë‹¤ìŒ í˜ì´ì§€ë“¤ì„ í™•ì¸í•˜ì—¬ ì •ë§ ë§ˆì§€ë§‰ì¸ì§€ ê²€ì¦
        let verification_range = 5; // ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€ í™•ì¸
        
        for offset in 1..=verification_range {
            let next_page = candidate_page + offset;
            
            match self.check_page_has_products(next_page).await {
                Ok(true) => {
                    warn!("ğŸ” Found products on page {} after candidate {}, re-discovering", 
                          next_page, candidate_page);
                    
                    // ë” ë†’ì€ í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ë°œê²¬í–ˆìœ¼ë¯€ë¡œ ê·¸ í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰
                    return self.discover_from_page(next_page).await;
                },
                Ok(false) => {
                    debug!("âœ… Page {} confirmed empty", next_page);
                },
                Err(e) => {
                    debug!("âŒ Failed to check page {}: {}", next_page, e);
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                }
            }
            
            // ê° í™•ì¸ ì‚¬ì´ì— ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        info!("âœ… Verified page {} as the last page (checked {} pages ahead)", 
              candidate_page, verification_range);
        Ok(candidate_page)
    }

    /// íŠ¹ì • í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰ ì‹œì‘
    async fn discover_from_page(&self, start_page: u32) -> Result<u32> {
        info!("ğŸ”„ Re-discovering from page {}", start_page);
        
        let mut current_page = start_page;
        let max_attempts = self.config.advanced.max_search_attempts;
        let mut attempts = 0;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts, stopping at page {}", current_page);
                break;
            }

            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let (has_products, max_page_in_pagination) = {
                let mut client = self.http_client.lock().await;
                match client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);
                        
                        info!("ğŸ“Š Page {} analysis: has_products={}, max_pagination={}", 
                              current_page, has_products, max_page);
                        
                        (has_products, max_page)
                    },
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        break;
                    }
                }
            };

            if !has_products {
                // ì œí’ˆì´ ì—†ìœ¼ë©´ ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                return self.find_last_valid_page_with_safety_check(current_page).await;
            }

            if max_page_in_pagination > current_page {
                // ë” í° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ë™
                current_page = max_page_in_pagination;
            } else {
                // ë” í° í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ í˜„ì¬ í˜ì´ì§€ê°€ ë§ˆì§€ë§‰
                break;
            }

            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        Ok(current_page)
    }

    /// íŠ¹ì • í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
    async fn check_page_has_products(&self, page: u32) -> Result<bool> {
        let test_url = config_utils::matter_products_page_url_simple(page);
        
        let mut client = self.http_client.lock().await;
        match client.fetch_html_string(&test_url).await {
            Ok(html) => {
                let doc = scraper::Html::parse_document(&html);
                Ok(self.has_products_on_page(&doc))
            },
            Err(_) => Ok(false),
        }
    }

    /// ì„¤ì • íŒŒì¼ì— ë§ˆì§€ë§‰ í˜ì´ì§€ ë° ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    async fn update_last_known_page(&self, last_page: u32) -> Result<()> {
        use crate::infrastructure::config::ConfigManager;
        
        let config_manager = ConfigManager::new()?;
        
        // ì„¤ì • ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í´ë¡œì € ì‚¬ìš©
        config_manager.update_app_managed(|app_managed| {
            // ë§ˆì§€ë§‰ ì•Œë ¤ì§„ í˜ì´ì§€ ì—…ë°ì´íŠ¸
            app_managed.last_known_max_page = Some(last_page);
            
            // ë§ˆì§€ë§‰ ì„±ê³µí•œ í¬ë¡¤ë§ ì‹œê°„ ì—…ë°ì´íŠ¸
            app_managed.last_successful_crawl = Some(chrono::Utc::now().to_rfc3339());
            
            // ì¶”ì • ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸ (í˜ì´ì§€ë‹¹ 20ê°œ ì œí’ˆ ì¶”ì •)
            app_managed.last_crawl_product_count = Some(last_page * 20);
            
            // í˜ì´ì§€ë‹¹ í‰ê·  ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸
            app_managed.avg_products_per_page = Some(20.0);
            
            info!("ğŸ“ Updated config: last_page={}, timestamp={}", 
                  last_page, 
                  app_managed.last_successful_crawl.as_ref().unwrap_or(&"unknown".to_string()));
        }).await?;
        
        info!("âœ… Successfully updated last known page to {} in config file", last_page);
        Ok(())
    }

    /// í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸ (PageDiscoveryService ë¡œì§ í™œìš©)
    fn has_products_on_page(&self, doc: &scraper::Html) -> bool {
        let mut max_count = 0;
        
        for selector_str in &self.config.advanced.product_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                let count = doc.select(&selector).count() as u32;
                if count > max_count {
                    max_count = count;
                }
            }
        }
        
        // ê¸°ë³¸ ì„ íƒìë“¤ë„ ì‹œë„
        if max_count == 0 {
            if let Ok(article_selector) = scraper::Selector::parse("article") {
                max_count = doc.select(&article_selector).count() as u32;
            }
        }
        
        max_count > 0
    }

    /// í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸° (PageDiscoveryService ë¡œì§ í™œìš©)
    fn find_max_page_in_pagination(&self, doc: &scraper::Html) -> u32 {
        let link_selector = scraper::Selector::parse("a[href*='page']").unwrap();
        let mut max_page = 1;
        
        for element in doc.select(&link_selector) {
            if let Some(href) = element.value().attr("href") {
                if let Some(page_num) = self.extract_page_number(href) {
                    max_page = max_page.max(page_num);
                }
            }
        }
        
        max_page
    }

    /// URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (PageDiscoveryService ë¡œì§ í™œìš©)
    fn extract_page_number(&self, url: &str) -> Option<u32> {
        if let Some(captures) = regex::Regex::new(r"[?&]page[d]?=(\d+)")
            .ok()
            .and_then(|re| re.captures(url)) 
        {
            if let Some(num_match) = captures.get(1) {
                return num_match.as_str().parse().ok();
            }
        }
        
        if let Some(captures) = regex::Regex::new(r"/page/(\d+)")
            .ok()
            .and_then(|re| re.captures(url))
        {
            if let Some(num_match) = captures.get(1) {
                return num_match.as_str().parse().ok();
            }
        }
        
        None
    }
}

/// ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct DatabaseAnalyzerImpl {
    product_repo: Arc<IntegratedProductRepository>,
}

impl DatabaseAnalyzerImpl {
    pub fn new(product_repo: Arc<IntegratedProductRepository>) -> Self {
        Self { product_repo }
    }
}

#[async_trait]
impl DatabaseAnalyzer for DatabaseAnalyzerImpl {
    async fn analyze_current_state(&self) -> Result<DatabaseAnalysis> {
        info!("Analyzing database state...");

        let statistics = self.product_repo.get_database_statistics().await?;
        let total_products = statistics.total_products as u32;
        
        // ì¤‘ë³µ ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)
        let duplicate_count = 0; // TODO: ì‹¤ì œ ì¤‘ë³µ ê²€ì‚¬ ë¡œì§ êµ¬í˜„
        let unique_products = total_products - duplicate_count;

        // í•„ë“œ ëˆ„ë½ ë¶„ì„
        let missing_fields = FieldAnalysis {
            missing_company: 0,      // TODO: ì‹¤ì œ ëˆ„ë½ í•„ë“œ ë¶„ì„
            missing_model: 0,
            missing_matter_version: 0,
            missing_connectivity: 0,
            missing_certification_date: 0,
        };

        // ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        let data_quality_score = if total_products > 0 {
            (unique_products as f64 / total_products as f64) * 0.8 + 0.2
        } else {
            1.0
        };

        info!("Database analysis completed: {} total, {} unique products", total_products, unique_products);

        Ok(DatabaseAnalysis {
            total_products,
            unique_products,
            duplicate_count,
            last_update: None, // TODO: ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì¶”ì 
            missing_fields_analysis: missing_fields,
            data_quality_score,
        })
    }

    async fn recommend_processing_strategy(&self) -> Result<ProcessingStrategy> {
        let analysis = self.analyze_current_state().await?;
        
        // ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°ì— ë”°ë¥¸ ì „ëµ ì¡°ì •
        let (batch_size, concurrency) = if analysis.total_products < 1000 {
            (20, 5)
        } else if analysis.total_products < 5000 {
            (15, 3)
        } else {
            (10, 2)
        };

        Ok(ProcessingStrategy {
            recommended_batch_size: batch_size,
            recommended_concurrency: concurrency,
            should_skip_duplicates: analysis.duplicate_count > 0,
            should_update_existing: analysis.data_quality_score < 0.8,
            priority_urls: Vec::new(), // TODO: ìš°ì„ ìˆœìœ„ URL ë¡œì§
        })
    }

    async fn analyze_duplicates(&self) -> Result<DuplicateAnalysis> {
        // TODO: ì‹¤ì œ ì¤‘ë³µ ë¶„ì„ ë¡œì§ êµ¬í˜„
        Ok(DuplicateAnalysis {
            total_duplicates: 0,
            duplicate_groups: Vec::new(),
            duplicate_percentage: 0.0,
        })
    }
}

/// ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductListCollectorImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

#[derive(Debug, Clone)]
pub struct CollectorConfig {
    pub concurrency: u32,
    pub delay_ms: u64,
    pub batch_size: u32,
    pub retry_max: u32,
}

impl ProductListCollectorImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client: Arc::new(tokio::sync::Mutex::new(http_client)),
            data_extractor: Arc::new(data_extractor),
            config,
        }
    }
}

#[async_trait]
impl ProductListCollector for ProductListCollectorImpl {
    async fn collect_all_pages(&self, total_pages: u32) -> Result<Vec<String>> {
        info!("Collecting product URLs from {} pages", total_pages);

        let _semaphore = Arc::new(Semaphore::new(self.config.concurrency as usize));
        let mut all_product_urls = Vec::new();

        // ë°°ì¹˜ë³„ë¡œ í˜ì´ì§€ ì²˜ë¦¬
        for batch_start in (1..=total_pages).step_by(self.config.batch_size as usize) {
            let batch_end = (batch_start + self.config.batch_size - 1).min(total_pages);
            let batch_pages: Vec<u32> = (batch_start..=batch_end).collect();
            
            let batch_urls = self.collect_page_batch(&batch_pages).await?;
            all_product_urls.extend(batch_urls);

            debug!("Completed batch {}-{}, total URLs: {}", batch_start, batch_end, all_product_urls.len());
        }

        info!("Product URL collection completed: {} URLs collected", all_product_urls.len());
        Ok(all_product_urls)
    }

    async fn collect_single_page(&self, page: u32) -> Result<Vec<String>> {
        let url = config_utils::matter_products_page_url_simple(page);
        debug!("Fetching page: {}", url);

        if self.config.delay_ms > 0 {
            sleep(Duration::from_millis(self.config.delay_ms)).await;
        }

        let mut client = self.http_client.lock().await;
        let html_str = client.fetch_html_string(&url).await?;
        
        let urls = self.data_extractor.extract_product_urls_from_content(&html_str)
            .map_err(|e| anyhow!("Failed to extract URLs from page {}: {}", page, e))?;

        debug!("Extracted {} URLs from page {}", urls.len(), page);
        Ok(urls)
    }

    async fn collect_page_batch(&self, pages: &[u32]) -> Result<Vec<String>> {
        let semaphore = Arc::new(Semaphore::new(self.config.concurrency as usize));
        
        let batch_tasks: Vec<_> = pages.iter().map(|&page_num| {
            let semaphore = Arc::clone(&semaphore);
            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let delay_ms = self.config.delay_ms;

            tokio::spawn(async move {
                let _permit = semaphore.acquire().await.unwrap();
                
                if delay_ms > 0 {
                    sleep(Duration::from_millis(delay_ms)).await;
                }

                let url = config_utils::matter_products_page_url_simple(page_num);
                debug!("Fetching page: {}", url);

                let mut client = http_client.lock().await;
                match client.fetch_html_string(&url).await {
                    Ok(html_str) => {
                        match data_extractor.extract_product_urls_from_content(&html_str) {
                            Ok(urls) => {
                                debug!("Extracted {} URLs from page {}", urls.len(), page_num);
                                Ok(urls)
                            },
                            Err(e) => {
                                warn!("Failed to extract URLs from page {}: {}", page_num, e);
                                Ok(Vec::new())
                            }
                        }
                    },
                    Err(e) => {
                        error!("Failed to fetch page {}: {}", page_num, e);
                        Err(e)
                    }
                }
            })
        }).collect();

        let batch_results = try_join_all(batch_tasks).await?;
        let mut all_urls = Vec::new();
        
        for result in batch_results {
            match result {
                Ok(urls) => all_urls.extend(urls),
                Err(e) => warn!("Batch task failed: {}", e),
            }
        }

        Ok(all_urls)
    }
}

/// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductDetailCollectorImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

impl ProductDetailCollectorImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client: Arc::new(tokio::sync::Mutex::new(http_client)),
            data_extractor: Arc::new(data_extractor),
            config,
        }
    }
}

#[async_trait]
impl ProductDetailCollector for ProductDetailCollectorImpl {
    async fn collect_details(&self, urls: &[String]) -> Result<Vec<Product>> {
        info!("Collecting product details from {} URLs", urls.len());

        let mut all_products = Vec::new();

        // ë°°ì¹˜ë³„ë¡œ ì œí’ˆ ì²˜ë¦¬
        for batch in urls.chunks(self.config.batch_size as usize) {
            let batch_products = self.collect_product_batch(batch).await?;
            all_products.extend(batch_products);
            
            debug!("Completed batch, total products: {}", all_products.len());
        }

        info!("Product detail collection completed: {} products collected", all_products.len());
        Ok(all_products)
    }

    async fn collect_single_product(&self, url: &str) -> Result<Product> {
        debug!("Fetching product detail: {}", url);

        if self.config.delay_ms > 0 {
            sleep(Duration::from_millis(self.config.delay_ms)).await;
        }

        let mut client = self.http_client.lock().await;
        let html_str = client.fetch_html_string(url).await?;
        
        // HTML íŒŒì‹±í•˜ì—¬ Product êµ¬ì¡°ì²´ ìƒì„±
        let html = scraper::Html::parse_document(&html_str);
        let products = self.data_extractor.extract_products_from_list(&html, 0)?;
        
        if let Some(product) = products.into_iter().next() {
            debug!("Extracted product: {:?} - {:?}", product.manufacturer, product.model);
            Ok(product)
        } else {
            Err(anyhow!("No product found at URL: {}", url))
        }
    }

    async fn collect_product_batch(&self, urls: &[String]) -> Result<Vec<Product>> {
        let semaphore = Arc::new(Semaphore::new(self.config.concurrency as usize));
        
        let batch_tasks: Vec<_> = urls.iter().map(|url| {
            let semaphore = Arc::clone(&semaphore);
            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let delay_ms = self.config.delay_ms;
            let url = url.clone();

            tokio::spawn(async move {
                let _permit = semaphore.acquire().await.unwrap();
                
                if delay_ms > 0 {
                    sleep(Duration::from_millis(delay_ms)).await;
                }

                let mut client = http_client.lock().await;
                match client.fetch_html_string(&url).await {
                    Ok(html_str) => {
                        let html = scraper::Html::parse_document(&html_str);
                        match data_extractor.extract_products_from_list(&html, 0) {
                            Ok(mut products) => {
                                if let Some(product) = products.pop() {
                                    debug!("Extracted product: {:?} - {:?}", product.manufacturer, product.model);
                                    Ok(Some(product))
                                } else {
                                    warn!("No product found at URL: {}", url);
                                    Ok(None)
                                }
                            },
                            Err(e) => {
                                warn!("Failed to extract product from {}: {}", url, e);
                                Ok(None)
                            }
                        }
                    },
                    Err(e) => {
                        error!("Failed to fetch product {}: {}", url, e);
                        Err(e)
                    }
                }
            })
        }).collect();

        let batch_results = try_join_all(batch_tasks).await?;
        let mut products = Vec::new();
        
        for result in batch_results {
            match result {
                Ok(Some(product)) => products.push(product),
                Ok(None) => {}, // ìŠ¤í‚µ
                Err(e) => warn!("Product collection task failed: {}", e),
            }
        }

        Ok(products)
    }
}

/// ì‚¬ì´íŠ¸ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
fn calculate_health_score(response_time_ms: u64, total_pages: u32) -> f64 {
    // ì‘ë‹µì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 1.0)
    let time_score = if response_time_ms < 2000 { 1.0 }
    else if response_time_ms < 5000 { 0.8 }
    else if response_time_ms < 10000 { 0.6 }
    else if response_time_ms < 20000 { 0.4 }
    else { 0.2 };
    
    // í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ì ìˆ˜ (í˜ì´ì§€ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì‚¬ì´íŠ¸ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ)
    let page_score = if total_pages >= 10 { 1.0 }
    else if total_pages >= 5 { 0.8 }
    else if total_pages >= 1 { 0.6 }
    else { 0.0 };
    
    // ê°€ì¤‘ í‰ê·  (ì‘ë‹µì‹œê°„ì´ ë” ì¤‘ìš”)
    (time_score * 0.7) + (page_score * 0.3)
}
