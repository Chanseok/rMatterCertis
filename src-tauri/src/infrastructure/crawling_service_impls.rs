//! í¬ë¡¤ë§ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
//! 
//! domain/services/crawling_services.rsì˜ íŠ¸ë ˆì´íŠ¸ë“¤ì— ëŒ€í•œ ì‹¤ì œ êµ¬í˜„ì²´

use std::sync::Arc;
use std::time::{Duration, Instant};
use std::collections::HashMap;
use async_trait::async_trait;
use anyhow::{Result, anyhow};
use tracing::{info, warn, error, debug};
use tokio::sync::Semaphore;
use tokio::time::sleep;
use futures::future::try_join_all;
use scraper;
use regex;

use crate::domain::services::{
    StatusChecker, DatabaseAnalyzer, ProductListCollector, ProductDetailCollector,
    SiteStatus, DatabaseAnalysis, FieldAnalysis, DuplicateAnalysis, ProcessingStrategy
};
use crate::domain::services::crawling_services::{
    SiteDataChangeStatus, DataDecreaseRecommendation, RecommendedAction, SeverityLevel
};
use crate::domain::product::{Product, ProductDetail};
use crate::infrastructure::{HttpClient, MatterDataExtractor, IntegratedProductRepository};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::config::utils as config_utils;

/// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹±í•˜ê¸° ìœ„í•œ êµ¬ì¡°ì²´
#[derive(Debug, Clone)]
struct PageAnalysisCache {
    /// í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜
    product_count: u32,
    /// í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë°œê²¬ëœ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸
    max_pagination_page: u32,
    /// í˜„ì¬ í™œì„±í™”ëœ í˜ì´ì§€ ë²ˆí˜¸ (í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ í™•ì¸)
    active_page: u32,
    /// ì œí’ˆì´ ìˆëŠ”ì§€ ì—¬ë¶€
    has_products: bool,
    /// ë¶„ì„ ì™„ë£Œ ì‹œê°
    analyzed_at: std::time::Instant,
}

/// ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
/// PageDiscoveryServiceì™€ í˜‘ë ¥í•˜ì—¬ ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„
pub struct StatusCheckerImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: AppConfig,
    /// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ ìºì‹œ (í˜ì´ì§€ ë²ˆí˜¸ -> ë¶„ì„ ê²°ê³¼)
    page_cache: Arc<tokio::sync::Mutex<HashMap<u32, PageAnalysisCache>>>,
}

impl StatusCheckerImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: AppConfig,
    ) -> Self {
        // 470ì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì„¤ì •í•œ ì´ìœ  ì„¤ëª…:
        // ì´ëŠ” ê³¼ê±° CSA-IoT ì‚¬ì´íŠ¸ì—ì„œ ê´€ì°°ëœ ëŒ€ëµì ì¸ í˜ì´ì§€ ìˆ˜ì…ë‹ˆë‹¤.
        // ê·¸ëŸ¬ë‚˜ í˜„ì¬ëŠ” ë” ì‘ì€ ê°’(100)ë¶€í„° ì‹œì‘í•˜ì—¬ ë™ì ìœ¼ë¡œ íƒì§€í•©ë‹ˆë‹¤.
        // ì•±ì´ ì‚¬ìš©ë˜ë©´ì„œ ì‹¤ì œ ë§ˆì§€ë§‰ í˜ì´ì§€ë¥¼ í•™ìŠµí•˜ê³  ì €ì¥í•˜ê²Œ ë©ë‹ˆë‹¤.
        
        Self {
            http_client: Arc::new(tokio::sync::Mutex::new(http_client)),
            data_extractor: Arc::new(data_extractor),
            config,
            page_cache: Arc::new(tokio::sync::Mutex::new(HashMap::new())),
        }
    }

    /// Update the pagination context in the data extractor based on discovered page information
    pub async fn update_pagination_context(&self, total_pages: u32, items_on_last_page: u32) -> Result<()> {
        // Create pagination context
        let pagination_context = crate::infrastructure::html_parser::PaginationContext {
            total_pages,
            items_per_page: 12, // CSA-IoT site uses 12 items per page
            items_on_last_page,
            target_page_size: 12, // Our system also uses 12 items per page
        };
        
        // Update the data extractor's pagination context
        self.data_extractor.set_pagination_context(pagination_context)?;
        
        info!("ğŸ“Š Updated pagination context: total_pages={}, items_on_last_page={}", 
               total_pages, items_on_last_page);
        
        Ok(())
    }
}

#[async_trait]
impl StatusChecker for StatusCheckerImpl {
    async fn check_site_status(&self) -> Result<SiteStatus> {
        let start_time = Instant::now();
        info!("Starting comprehensive site status check with detailed page discovery");
        
        // ìºì‹œ ì´ˆê¸°í™”
        self.clear_page_cache().await;
        
        info!("Checking site status and discovering pages...");

        // Step 1: ê¸°ë³¸ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± í™•ì¸
        let url = config_utils::matter_products_page_url_simple(1);
        
        // ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸
        let access_test = {
            let mut client = self.http_client.lock().await;
            client.fetch_html_string(&url).await
        };
        
        match access_test {
            Ok(_) => info!("Site is accessible"),
            Err(e) => {
                error!("Failed to access site: {}", e);
                return Ok(SiteStatus {
                    is_accessible: false,
                    response_time_ms: start_time.elapsed().as_millis() as u64,
                    total_pages: 0,
                    estimated_products: 0,
                    last_check_time: chrono::Utc::now(),
                    health_score: 0.0,
                    data_change_status: SiteDataChangeStatus::Inaccessible,
                    decrease_recommendation: None,
                });
            }
        }

        // Step 2: í˜ì´ì§€ ìˆ˜ íƒì§€ ë° ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜ í™•ì¸
        let (total_pages, products_on_last_page) = self.discover_total_pages().await?;

        // Step 2.5: í˜ì´ì§€ë„¤ì´ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        if let Err(e) = self.update_pagination_context(total_pages, products_on_last_page).await {
            warn!("Failed to update pagination context: {}", e);
        }

        let response_time_ms = start_time.elapsed().as_millis() as u64;
        let response_time = start_time.elapsed();

        // Step 3: ì‚¬ì´íŠ¸ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
        let health_score = calculate_health_score(response_time, total_pages);

        info!("Site status check completed: {} pages found, {}ms total time, health score: {:.2}", 
              total_pages, response_time_ms, health_score);

        // ì •í™•í•œ ì œí’ˆ ìˆ˜ ê³„ì‚°: (ë§ˆì§€ë§‰ í˜ì´ì§€ - 1) * í˜ì´ì§€ë‹¹ ì œí’ˆ ìˆ˜ + ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜
        use crate::infrastructure::config::defaults::DEFAULT_PRODUCTS_PER_PAGE;
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;
        
        let estimated_products = if total_pages > 1 {
            ((total_pages - 1) * products_per_page) + products_on_last_page
        } else {
            products_on_last_page
        };
        
        info!("Accurate product estimation: ({} full pages * {} products) + {} products on last page = {} total products", 
              total_pages - 1, products_per_page, products_on_last_page, estimated_products);

        // Step 4: ë°ì´í„° ë³€í™” ìƒíƒœ ë¶„ì„
        let (data_change_status, decrease_recommendation) = self.analyze_data_changes(estimated_products).await;
              
        Ok(SiteStatus {                is_accessible: true,
                response_time_ms: response_time_ms,
                total_pages,
                estimated_products,
                last_check_time: chrono::Utc::now(),
                health_score,
            data_change_status,
            decrease_recommendation,
        })
    }

    async fn estimate_crawling_time(&self, pages: u32) -> Duration {
        // ...
        // í˜ì´ì§€ë‹¹ í‰ê·  2ì´ˆ + ì œí’ˆ ìƒì„¸í˜ì´ì§€ë‹¹ 1ì´ˆ ì¶”ì •
        let page_collection_time = pages * 2;
        let product_detail_time = pages * 20; // í˜ì´ì§€ë‹¹ 20ê°œ ì œí’ˆ * 1ì´ˆ
        let total_seconds = page_collection_time + product_detail_time;
        
        Duration::from_secs(total_seconds as u64)
    }

    async fn verify_site_accessibility(&self) -> Result<bool> {
        let status = self.check_site_status().await?;
        Ok(status.is_accessible && status.health_score > 0.5)
    }
}

impl StatusCheckerImpl {
    /// í–¥ìƒëœ í˜ì´ì§€ íƒì§€ ë¡œì§ - ì‚¬ì´íŠ¸ ì •ë³´ ë³€í™” ê°ì§€ í¬í•¨
    async fn discover_total_pages(&self) -> Result<(u32, u32)> {
        info!("ğŸ” Starting enhanced page discovery algorithm with site change detection");
        
        // 1. ì‹œì‘ í˜ì´ì§€ ê²°ì •
        let start_page = self.config.app_managed.last_known_max_page
            .unwrap_or(self.config.advanced.last_page_search_start);
        
        info!("ğŸ“ Starting from page {} (last known: {:?}, default: {})", 
              start_page, 
              self.config.app_managed.last_known_max_page,
              self.config.advanced.last_page_search_start);
        
        // 2. ì‹œì‘ í˜ì´ì§€ ë¶„ì„ (ìºì‹œ ì‚¬ìš©)
        let start_analysis = self.get_or_analyze_page(start_page).await?;
        let mut current_page = start_page;
        
        if !start_analysis.has_products {
            warn!("âš ï¸  Starting page {} has no products - checking site status", current_page);
            
            // ì²« í˜ì´ì§€ í™•ì¸ìœ¼ë¡œ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± ê²€ì¦
            let first_page_analysis = self.get_or_analyze_page(1).await?;
            if !first_page_analysis.has_products {
                error!("âŒ First page also has no products - site may be temporarily unavailable");
                return Err(anyhow::anyhow!(
                    "Site appears to be temporarily unavailable or experiencing issues. Please try again later."
                ));
            }
            
            info!("âœ… First page has products - site is accessible, cached page info may be outdated");
            warn!("ğŸ”„ Site content may have decreased - will perform full discovery");
            
            // í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
            current_page = self.find_last_valid_page_downward(current_page).await?;
            info!("âœ… Found valid starting page: {}", current_page);
        }
        
        // 3. ë°˜ë³µì  ìƒí–¥ íƒìƒ‰: í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° ê°’ì„ ì°¾ì„ ë•Œê¹Œì§€ ê³„ì†
        let mut attempts = 0;
        let max_attempts = self.config.advanced.max_search_attempts;
        
        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts ({}), stopping at page {}", max_attempts, current_page);
                break;
            }
            
            info!("ğŸ” Iteration {}/{}: Checking page {}", attempts, max_attempts, current_page);
            
            // í˜„ì¬ í˜ì´ì§€ë¥¼ ë¶„ì„ (ìºì‹œ ì‚¬ìš©)
            let analysis = match self.get_or_analyze_page(current_page).await {
                Ok(analysis) => analysis,
                Err(e) => {
                    warn!("âŒ Failed to analyze page {}: {}", current_page, e);
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ í•˜í–¥ íƒìƒ‰
                    current_page = self.find_last_valid_page_downward(current_page).await?;
                    break;
                }
            };
            
            if !analysis.has_products {
                // ì œí’ˆì´ ì—†ëŠ” ê²½ìš° ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                info!("ğŸ”» Page {} has no products, performing safe downward search", current_page);
                current_page = self.find_last_valid_page_with_safety_check(current_page).await?;
                break;
            }
            
            // í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° í˜ì´ì§€ë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
            if analysis.max_pagination_page > current_page {
                info!("ğŸ”º Found higher page {} in pagination, jumping there", analysis.max_pagination_page);
                current_page = analysis.max_pagination_page;
                // ìƒˆ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë‹¤ì‹œ íƒìƒ‰
                continue;
            } else {
                info!("ğŸ No higher pages found in pagination, {} appears to be the last page", current_page);
                break;
            }
        }
        
        // 4. ìµœì¢… ê²€ì¦: ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸ ë° ì œí’ˆ ìˆ˜ ê³„ì‚°
        let (verified_last_page, products_on_last_page) = self.verify_last_page(current_page).await?;
        
        // 5. ì„¤ì • íŒŒì¼ì— ê²°ê³¼ ì €ì¥
        if let Err(e) = self.update_last_known_page(verified_last_page).await {
            warn!("âš ï¸  Failed to update last known page in config: {}", e);
        }
        
        info!("ğŸ‰ Final verified last page: {} with {} products", verified_last_page, products_on_last_page);
        Ok((verified_last_page, products_on_last_page))
    }

    /// í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ë§ˆì§€ë§‰ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
    async fn find_last_valid_page_downward(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page.saturating_sub(1);
        let min_page = 1;

        info!("Starting downward search from page {}", current_page);

        while current_page >= min_page {
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let mut client = self.http_client.lock().await;
            match client.fetch_html_string(&test_url).await {
                Ok(html) => {
                    let doc = scraper::Html::parse_document(&html);
                    if self.has_products_on_page(&doc) {
                        info!("Found valid page with products: {}", current_page);
                        return Ok(current_page);
                    }
                },
                Err(e) => {
                    warn!("Failed to fetch page {} during downward search: {}", current_page, e);
                }
            }

            current_page = current_page.saturating_sub(1);
            
            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
        warn!("No valid pages found during downward search, returning 1");
        Ok(1)
    }

    /// ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ - ì—°ì† ë¹ˆ í˜ì´ì§€ 3ê°œ ì´ìƒ ì‹œ fatal error
    async fn find_last_valid_page_with_safety_check(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page;
        let mut consecutive_empty_pages = 0;
        const MAX_CONSECUTIVE_EMPTY: u32 = 3;
        let min_page = 1;

        info!("ğŸ” Starting safe downward search from page {} (max consecutive empty: {})", 
              current_page, MAX_CONSECUTIVE_EMPTY);

        // ë¨¼ì € ì‹œì‘ í˜ì´ì§€ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if !self.check_page_has_products(current_page).await? {
            consecutive_empty_pages += 1;
            info!("âš ï¸  Starting page {} is empty (consecutive: {})", current_page, consecutive_empty_pages);
        }

        while current_page > min_page {
            current_page = current_page.saturating_sub(1);
            
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            info!("ğŸ” Checking page {} (consecutive empty: {})", current_page, consecutive_empty_pages);
            
            let mut client = self.http_client.lock().await;
            match client.fetch_html_string(&test_url).await {
                Ok(html) => {
                    let doc = scraper::Html::parse_document(&html);
                    if self.has_products_on_page(&doc) {
                        info!("âœ… Found valid page with products: {} (after {} consecutive empty pages)", 
                              current_page, consecutive_empty_pages);
                        return Ok(current_page);
                    } else {
                        consecutive_empty_pages += 1;
                        warn!("âš ï¸  Page {} is empty (consecutive: {}/{})", 
                              current_page, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY);
                        
                        // ì—°ì†ìœ¼ë¡œ ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ì´ìƒì´ë©´ fatal error
                        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                            error!("ğŸ’¥ FATAL ERROR: Found {} consecutive empty pages starting from page {}. This indicates a serious site issue or crawling problem.", 
                                   consecutive_empty_pages, start_page);
                            
                            return Err(anyhow!(
                                "Fatal error: {} consecutive empty pages detected. Site may be down or pagination structure changed. Last checked pages: {} to {}",
                                consecutive_empty_pages, 
                                start_page,
                                current_page
                            ));
                        }
                    }
                },
                Err(e) => {
                    consecutive_empty_pages += 1;
                    warn!("âŒ Failed to fetch page {} during safe downward search: {} (consecutive: {}/{})", 
                          current_page, e, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY);
                    
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë„ ì—°ì† ì‹¤íŒ¨ë¡œ ì¹´ìš´íŠ¸
                    if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                        error!("ğŸ’¥ FATAL ERROR: {} consecutive failures (empty pages + network errors) starting from page {}.", 
                               consecutive_empty_pages, start_page);
                        
                        return Err(anyhow!(
                            "Fatal error: {} consecutive failures detected. Network issues or site problems. Last error: {}",
                            consecutive_empty_pages, 
                            e
                        ));
                    }
                }
            }
            
            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        // ìµœì†Œ í˜ì´ì§€ê¹Œì§€ ë„ë‹¬í–ˆì§€ë§Œ ì—¬ì „íˆ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ ë§ë‹¤ë©´ fatal error
        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
            error!("ğŸ’¥ FATAL ERROR: Reached minimum page but still have {} consecutive empty pages. Site appears to be completely empty or broken.", 
                   consecutive_empty_pages);
            
            return Err(anyhow!(
                "Fatal error: Site appears to be empty or broken. {} consecutive empty pages found from page {} down to page {}",
                consecutive_empty_pages, 
                start_page,
                current_page
            ));
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆì§€ë§Œ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ê²½ê³ ì™€ í•¨ê»˜ 1 ë°˜í™˜
        warn!("âš ï¸  No valid pages found during safe downward search, but only {} consecutive empty pages. Returning page 1 as fallback.", 
              consecutive_empty_pages);
        Ok(1)
    }

    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ìµœì¢… ê²€ì¦ - ë” ì² ì €í•œ ê²€ì¦ ë¡œì§
    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ê²€ì¦ ë° ì œí’ˆ ìˆ˜ í™•ì¸
    async fn verify_last_page(&self, candidate_page: u32) -> Result<(u32, u32)> {
        info!("ğŸ” Verifying candidate last page: {}", candidate_page);

        // 1. í›„ë³´ í˜ì´ì§€ ë¶„ì„ (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¶„ì„)
        let analysis = self.get_or_analyze_page(candidate_page).await?;
        let products_on_last_page = analysis.product_count;
        let has_products = analysis.has_products;
        
        info!("ğŸ“Š Last page {} has {} products", candidate_page, products_on_last_page);
        
        if !has_products {
            warn!("âš ï¸  Candidate page {} has no products, performing downward search with safety check", candidate_page);
            let actual_last_page = self.find_last_valid_page_with_safety_check(candidate_page).await?;
            // ì‹¤ì œ ë§ˆì§€ë§‰ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ ë‹¤ì‹œ í™•ì¸
            let actual_analysis = self.get_or_analyze_page(actual_last_page).await?;
            return Ok((actual_last_page, actual_analysis.product_count));
        }

        // 2. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ì—ì„œ ì´ë¯¸ ë§ˆì§€ë§‰ í˜ì´ì§€ì„ì„ í™•ì‹ í•  ìˆ˜ ìˆë‹¤ë©´ ì¶”ê°€ í™•ì¸ ìƒëµ
        // í˜„ì¬ í˜ì´ì§€ê°€ í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë°œê²¬ëœ ìµœëŒ€ í˜ì´ì§€ì™€ ê°™ë‹¤ë©´ ê²€ì¦ ì™„ë£Œ
        if analysis.max_pagination_page == candidate_page {
            info!("âœ… Page {} confirmed as last page via pagination analysis (max_pagination={})", 
                  candidate_page, analysis.max_pagination_page);
            info!("ğŸš€ Skipping additional verification - pagination analysis is reliable");
            return Ok((candidate_page, products_on_last_page));
        }
        
        // 3. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ì´ ë¶ˆí™•ì‹¤í•œ ê²½ìš°ì—ë§Œ ìµœì†Œí•œì˜ ì¶”ê°€ ê²€ì¦ ìˆ˜í–‰
        info!("ğŸ” Pagination analysis inconclusive (current={}, max_pagination={}), performing minimal verification", 
              candidate_page, analysis.max_pagination_page);
        
        // ë°”ë¡œ ë‹¤ìŒ í˜ì´ì§€ 1ê°œë§Œ í™•ì¸ (ê³¼ë„í•œ ê²€ì¦ ë°©ì§€)
        let next_page = candidate_page + 1;
        match self.check_page_has_products(next_page).await {
            Ok(true) => {
                warn!("ğŸ” Found products on page {} after candidate {}, re-discovering", 
                      next_page, candidate_page);
                // ë” ë†’ì€ í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ë°œê²¬í–ˆìœ¼ë¯€ë¡œ ê·¸ í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰
                return self.discover_from_page_with_count(next_page).await;
            },
            Ok(false) => {
                info!("âœ… Verified page {} as the last page with {} products (checked {} page ahead)", 
                      candidate_page, products_on_last_page, 1);
            },
            Err(e) => {
                debug!("âŒ Failed to check page {}: {}, assuming {} is last", next_page, e, candidate_page);
            }
        }
        
        Ok((candidate_page, products_on_last_page))
    }

    /// íŠ¹ì • í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰ ì‹œì‘ (ì œí’ˆ ìˆ˜ë„ ë°˜í™˜)
    async fn discover_from_page_with_count(&self, start_page: u32) -> Result<(u32, u32)> {
        info!("ğŸ”„ Re-discovering from page {} with product count", start_page);
        
        let mut current_page = start_page;
        let max_attempts = self.config.advanced.max_search_attempts;
        let mut attempts = 0;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts, stopping at page {}", current_page);
                break;
            }

            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let (has_products, max_page_in_pagination) = {
                let mut client = self.http_client.lock().await;
                match client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);
                        
                        info!("ğŸ“Š Page {} analysis: has_products={}, max_pagination={}", 
                              current_page, has_products, max_page);
                        
                        (has_products, max_page)
                    },
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        break;
                    }
                }
            };

            if !has_products {
                // ì œí’ˆì´ ì—†ìœ¼ë©´ ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ í›„ ì œí’ˆ ìˆ˜ í™•ì¸
                let last_page = self.find_last_valid_page_with_safety_check(current_page).await?;
                let test_url = config_utils::matter_products_page_url_simple(last_page);
                let mut client = self.http_client.lock().await;
                let html = client.fetch_html_string(&test_url).await?;
                drop(client); // ë½ í•´ì œ
                let doc = scraper::Html::parse_document(&html);
                let products_count = self.count_products(&doc);
                return Ok((last_page, products_count));
            }

            if max_page_in_pagination > current_page {
                // ë” í° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ë™
                current_page = max_page_in_pagination;
                continue;
            } else {
                // ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬, ì œí’ˆ ìˆ˜ í™•ì¸
                let test_url = config_utils::matter_products_page_url_simple(current_page);
                let mut client = self.http_client.lock().await;
                let html = client.fetch_html_string(&test_url).await?;
                drop(client); // ë½ í•´ì œ
                let doc = scraper::Html::parse_document(&html);
                let products_count = self.count_products(&doc);
                return Ok((current_page, products_count));
            }
        }

        // ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ì‹œ í˜„ì¬ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ í™•ì¸
        let test_url = config_utils::matter_products_page_url_simple(current_page);
        let mut client = self.http_client.lock().await;
        let html = client.fetch_html_string(&test_url).await?;
        drop(client); // ë½ í•´ì œ
        let doc = scraper::Html::parse_document(&html);
        let products_count = self.count_products(&doc);
        Ok((current_page, products_count))
    }

    /// íŠ¹ì • í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰ ì‹œì‘
    async fn discover_from_page(&self, start_page: u32) -> Result<u32> {
        info!("ğŸ”„ Re-discovering from page {}", start_page);
        
        let mut current_page = start_page;
        let max_attempts = self.config.advanced.max_search_attempts;
        let mut attempts = 0;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts, stopping at page {}", current_page);
                break;
            }

            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let (has_products, max_page_in_pagination) = {
                let mut client = self.http_client.lock().await;
                match client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);
                        
                        info!("ğŸ“Š Page {} analysis: has_products={}, max_pagination={}", 
                              current_page, has_products, max_page);
                        
                        (has_products, max_page)
                    },
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        break;
                    }
                }
            };

            if !has_products {
                // ì œí’ˆì´ ì—†ìœ¼ë©´ ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                return self.find_last_valid_page_with_safety_check(current_page).await;
            }

            if max_page_in_pagination > current_page {
                // ë” í° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ë™
                current_page = max_page_in_pagination;
            } else {
                // ë” í° í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ í˜„ì¬ í˜ì´ì§€ê°€ ë§ˆì§€ë§‰
                break;
            }

            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        Ok(current_page)
    }

    /// íŠ¹ì • í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸ - í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ë„ í•¨ê»˜ í™•ì¸
    async fn check_page_has_products(&self, page: u32) -> Result<bool> {
        let test_url = config_utils::matter_products_page_url_simple(page);
        
        let mut client = self.http_client.lock().await;
        match client.fetch_html_string(&test_url).await {
            Ok(html) => {
                let doc = scraper::Html::parse_document(&html);
                
                // 1. ì œí’ˆ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                let has_products = self.has_products_on_page(&doc);
                
                // 2. í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ í™•ì¸ (ë” ì¤‘ìš”í•œ ì²´í¬)
                let active_page = self.get_active_page_number(&doc);
                
                // ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ì™€ í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                let is_correct_page = active_page == page;
                
                if !is_correct_page {
                    info!("âš ï¸  Page {} was redirected to page {} (pagination mismatch)", page, active_page);
                    return Ok(false);
                }
                
                info!("âœ… Page {} verification: has_products={}, active_page={}, is_correct_page={}", 
                      page, has_products, active_page, is_correct_page);
                
                Ok(has_products && is_correct_page)
            },
            Err(_) => Ok(false),
        }
    }

    /// í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ ì¶”ì¶œ - í˜„ì¬ í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    fn get_active_page_number(&self, doc: &scraper::Html) -> u32 {
        // í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œë¥¼ ì°¾ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        // ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìš°ì„ ìˆœìœ„ ì¡°ì • (í˜ì´ì§€ë„¤ì´ì…˜ ìš°ì„  í´ë˜ìŠ¤: page-numbers.current)
        let active_selectors = [
            ".page-numbers.current", // ìš°ì„ ìˆœìœ„ ê°€ì¥ ë†’ìŒ (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •)
            "span.page-numbers.current", // ì •í™•í•œ ìš”ì†Œ ì§€ì •
            "a.page-numbers.current",
            ".current",
            ".active",
            ".pagination .current",
            ".pagination .active",
            "a.current",
            "span.current",
            "[aria-current='page']",
            ".wp-pagenavi .current",
            ".page-item.active a",
            ".page-link.active",
        ];
        
        for selector_str in &active_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                if let Some(element) = doc.select(&selector).next() {
                    // í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    let text = element.text().collect::<String>().trim().to_string();
                    if let Ok(page_num) = text.parse::<u32>() {
                        info!("ğŸ¯ Found active page number {} using selector '{}'", page_num, selector_str);
                        return page_num;
                    }
                }
            }
        }
        
        // í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° URLì—ì„œ ì¶”ì¶œ ì‹œë„
        if let Some(canonical_url) = self.get_canonical_url(doc) {
            if let Some(page_num) = self.extract_page_number(&canonical_url) {
                info!("ğŸ¯ Found page number {} from canonical URL", page_num);
                return page_num;
            }
        }
        
        // ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš° 1 ë°˜í™˜ (ì²« ë²ˆì§¸ í˜ì´ì§€ë¡œ ì¶”ì •)
        warn!("âš ï¸  Could not determine active page number, assuming page 1");
        1
    }

    /// í˜ì´ì§€ì˜ canonical URL ì¶”ì¶œ
    fn get_canonical_url(&self, doc: &scraper::Html) -> Option<String> {
        if let Ok(selector) = scraper::Selector::parse("link[rel='canonical']") {
            if let Some(element) = doc.select(&selector).next() {
                return element.value().attr("href").map(|s| s.to_string());
            }
        }
        None
    }

    /// ì„¤ì • íŒŒì¼ì— ë§ˆì§€ë§‰ í˜ì´ì§€ ë° ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    async fn update_last_known_page(&self, last_page: u32) -> Result<()> {
        use crate::infrastructure::config::ConfigManager;
        
        let config_manager = ConfigManager::new()?;
        
        // ì„¤ì • ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í´ë¡œì € ì‚¬ìš©
        config_manager.update_app_managed(|app_managed| {
            // ë§ˆì§€ë§‰ ì•Œë ¤ì§„ í˜ì´ì§€ ì—…ë°ì´íŠ¸
            app_managed.last_known_max_page = Some(last_page);
            
            // ë§ˆì§€ë§‰ ì„±ê³µí•œ í¬ë¡¤ë§ ì‹œê°„ ì—…ë°ì´íŠ¸
            app_managed.last_successful_crawl = Some(chrono::Utc::now().to_rfc3339());
            
            // ì¶”ì • ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸ (í˜ì´ì§€ë‹¹ 12ê°œ ì œí’ˆ - ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ê¸°ë°˜)
            app_managed.last_crawl_product_count = Some(last_page * 12);
            
            // í˜ì´ì§€ë‹¹ í‰ê·  ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸
            use crate::infrastructure::config::defaults::DEFAULT_PRODUCTS_PER_PAGE;
            app_managed.avg_products_per_page = Some(DEFAULT_PRODUCTS_PER_PAGE as f64);
            
            info!("ğŸ“ Updated config: last_page={}, timestamp={}", 
                  last_page, 
                  app_managed.last_successful_crawl.as_ref().unwrap_or(&"unknown".to_string()));
        }).await?;
        
        info!("âœ… Successfully updated last known page to {} in config file", last_page);
        Ok(())
    }

    /// ë°ì´í„° ë³€í™” ìƒíƒœ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
    async fn analyze_data_changes(&self, current_estimated_products: u32) -> (SiteDataChangeStatus, Option<DataDecreaseRecommendation>) {
        // ì´ì „ í¬ë¡¤ë§ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        let previous_count = self.config.app_managed.last_crawl_product_count;
        
        match previous_count {
            None => {
                info!("ğŸ†• Initial site check - no previous data available");
                (SiteDataChangeStatus::Initial { count: current_estimated_products }, None)
            },
            Some(prev_count) => {
                let change_percentage = if prev_count > 0 {
                    ((current_estimated_products as f64 - prev_count as f64) / prev_count as f64) * 100.0
                } else {
                    0.0
                };
                
                if current_estimated_products > prev_count {
                    let increase = current_estimated_products - prev_count;
                    info!("ğŸ“ˆ Site data increased: {} -> {} (+{}, +{:.1}%)", 
                          prev_count, current_estimated_products, increase, change_percentage);
                    (SiteDataChangeStatus::Increased { 
                        new_count: current_estimated_products, 
                        previous_count: prev_count 
                    }, None)
                } else if current_estimated_products == prev_count {
                    info!("ğŸ“Š Site data stable: {} products", current_estimated_products);
                    (SiteDataChangeStatus::Stable { count: current_estimated_products }, None)
                } else {
                    let decrease = prev_count - current_estimated_products;
                    let decrease_percentage = (decrease as f64 / prev_count as f64) * 100.0;
                    
                    warn!("ğŸ“‰ Site data decreased: {} -> {} (-{}, -{:.1}%)", 
                          prev_count, current_estimated_products, decrease, decrease_percentage);
                    
                    let severity = if decrease_percentage < 10.0 {
                        SeverityLevel::Low
                    } else if decrease_percentage < 30.0 {
                        SeverityLevel::Medium
                    } else if decrease_percentage < 50.0 {
                        SeverityLevel::High
                    } else {
                        SeverityLevel::Critical
                    };
                    
                    let recommendation = self.generate_decrease_recommendation(decrease_percentage, &severity);
                    
                    (SiteDataChangeStatus::Decreased { 
                        current_count: current_estimated_products,
                        previous_count: prev_count,
                        decrease_amount: decrease
                    }, Some(recommendation))
                }
            }
        }
    }
    
    /// ë°ì´í„° ê°ì†Œ ì‹œ ê¶Œì¥ì‚¬í•­ ìƒì„±
    fn generate_decrease_recommendation(&self, decrease_percentage: f64, severity: &SeverityLevel) -> DataDecreaseRecommendation {
        match severity {
            SeverityLevel::Low => DataDecreaseRecommendation {
                action_type: RecommendedAction::WaitAndRetry,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¼ì‹œì ì¸ ë³€í™”ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "ì ì‹œ í›„(5-10ë¶„) ë‹¤ì‹œ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                    "ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ìˆ˜ë™ìœ¼ë¡œ ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::Medium => DataDecreaseRecommendation {
                action_type: RecommendedAction::ManualVerification,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "CSA-IoT ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ì œí’ˆ ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                    "ì‚¬ì´íŠ¸ì—ì„œ í•„í„° ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•˜ê³  ë¶€ë¶„ ì¬í¬ë¡¤ë§ì„ ê³ ë ¤í•˜ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::High => DataDecreaseRecommendation {
                action_type: RecommendedAction::BackupAndRecrawl,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% í¬ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… í›„ ì¬í¬ë¡¤ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¦‰ì‹œ ë°±ì—…í•˜ì„¸ìš”".to_string(),
                    "CSA-IoT ì‚¬ì´íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ì‹¤ì œ ìƒí™©ì„ íŒŒì•…í•˜ì„¸ìš”".to_string(),
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¹„ìš°ê³  ì „ì²´ ì¬í¬ë¡¤ë§ì„ ìˆ˜í–‰í•˜ì„¸ìš”".to_string(),
                    "í¬ë¡¤ë§ ì™„ë£Œ í›„ ì´ì „ ë°ì´í„°ì™€ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::Critical => DataDecreaseRecommendation {
                action_type: RecommendedAction::BackupAndRecrawl,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ì‹¬ê°í•˜ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "ğŸš¨ ì¦‰ì‹œ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•˜ì„¸ìš”".to_string(),
                    "CSA-IoT ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ ì‹¤ì œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ì‚¬ì´íŠ¸ êµ¬ì¡°ë‚˜ í•„í„° ì¡°ê±´ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ë°±ì—… í™•ì¸ í›„ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì „ì²´ ì¬í¬ë¡¤ë§í•˜ì„¸ìš”".to_string(),
                    "í¬ë¡¤ë§ ì„¤ì •(selector, URL ë“±)ì„ ì¬ê²€í† í•˜ì„¸ìš”".to_string(),
                ],
            },
        }
    }

    /// í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
    fn has_products_on_page(&self, doc: &scraper::Html) -> bool {
        let product_count = self.count_products(doc);
        product_count > 0
    }

    /// í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
    fn find_max_page_in_pagination(&self, doc: &scraper::Html) -> u32 {
        let mut max_page = 1;
        
        // 1. í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ì—ì„œ ì°¾ê¸°
        let link_selectors = vec![
            "a[href*='page']",
            ".pagination a",
            ".page-numbers", // ëª¨ë“  í˜ì´ì§€ ë²ˆí˜¸ ìš”ì†Œ (aì™€ span ëª¨ë‘ í¬í•¨)
            ".page-numbers a", 
            ".pager a",
            "a[href*='paged']",
            ".page-numbers:not(.current):not(.dots)" // í˜„ì¬ í˜ì´ì§€ì™€ ì¤„ì„í‘œë¥¼ ì œì™¸í•œ í˜ì´ì§€ ë²ˆí˜¸
        ];
        
        for selector_str in &link_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                for element in doc.select(&selector) {
                    // href ì†ì„±ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    if let Some(href) = element.value().attr("href") {
                        if let Some(page_num) = self.extract_page_number(href) {
                            if page_num > max_page {
                                max_page = page_num;
                                debug!("Found higher page {} in href: {}", page_num, href);
                            }
                        }
                    }
                    
                    // í…ìŠ¤íŠ¸ì—ì„œë„ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    let text = element.text().collect::<String>().trim().to_string();
                    if let Ok(page_num) = text.parse::<u32>() {
                        if page_num > max_page && page_num < 10000 { // í•©ë¦¬ì ì¸ ìƒí•œì„ 
                            max_page = page_num;
                            debug!("Found higher page {} in text: {}", page_num, text);
                        }
                    }
                }
            }
        }
        
        debug!("Max page found in pagination: {}", max_page);
        max_page
    }

    /// URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
    fn extract_page_number(&self, url: &str) -> Option<u32> {
        // URL íŒ¨í„´: /page/123/ ë˜ëŠ” paged=123
        let patterns = [
            r"/page/(\d+)",
            r"paged=(\d+)",
            r"page=(\d+)",
            r"/(\d+)/$",  // ëì— ìˆ«ìê°€ ìˆëŠ” ê²½ìš°
        ];
        
        for pattern in &patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(caps) = re.captures(url) {
                    if let Some(num_str) = caps.get(1) {
                        if let Ok(num) = num_str.as_str().parse::<u32>() {
                            return Some(num);
                        }
                    }
                }
            }
        }
        
        None
    }

    /// í˜ì´ì§€ì—ì„œ ì œí’ˆ ê°œìˆ˜ ì¹´ìš´íŠ¸ (ëª¨ë“  ì„ íƒìë¥¼ ì‹œë„í•˜ê³  ê°€ì¥ ë§ì€ ê²°ê³¼ ë°˜í™˜)
    fn count_products(&self, doc: &scraper::Html) -> u32 {
        let mut max_count = 0;
        let mut best_selector = "none";
        
        for selector_str in &self.config.advanced.product_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                let count = doc.select(&selector).count() as u32;
                debug!("Selector '{}' found {} products", selector_str, count);
                if count > max_count {
                    max_count = count;
                    best_selector = selector_str;
                }
            } else {
                debug!("Failed to parse selector: {}", selector_str);
            }
        }
        
        info!("Total products found on page: {} (using selector: {})", max_count, best_selector);
        max_count
    }

    /// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¶„ì„
    async fn get_or_analyze_page(&self, page_number: u32) -> Result<PageAnalysisCache> {
        // ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        {
            let cache = self.page_cache.lock().await;
            if let Some(cached) = cache.get(&page_number) {
                debug!("ğŸ“‹ Using cached analysis for page {}", page_number);
                return Ok(cached.clone());
            }
        }
        
        // ìºì‹œì— ì—†ìœ¼ë©´ ìƒˆë¡œ ë¶„ì„
        debug!("ğŸ” Analyzing page {} (not in cache)", page_number);
        let url = config_utils::matter_products_page_url_simple(page_number);
        
        let (product_count, max_pagination_page, active_page, has_products) = {
            let mut client = self.http_client.lock().await;
            let html = client.fetch_html_string(&url).await?;
            drop(client); // ë½ í•´ì œ
            
            let doc = scraper::Html::parse_document(&html);
            let product_count = self.count_products(&doc);
            let max_pagination_page = self.find_max_page_in_pagination(&doc);
            let active_page = self.get_active_page_number(&doc);
            let has_products = product_count > 0;
            
            (product_count, max_pagination_page, active_page, has_products)
        };
        
        let analysis = PageAnalysisCache {
            product_count,
            max_pagination_page,
            active_page,
            has_products,
            analyzed_at: std::time::Instant::now(),
        };
        
        // ìºì‹œì— ì €ì¥
        {
            let mut cache = self.page_cache.lock().await;
            cache.insert(page_number, analysis.clone());
        }
        
        info!("ğŸ“Š Page {} analysis: has_products={}, product_count={}, max_pagination={}", 
              page_number, has_products, product_count, max_pagination_page);
        
        Ok(analysis)
    }
    
    /// ìºì‹œë¥¼ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ìƒíƒœ ì²´í¬ ì‹œì‘ ì‹œ í˜¸ì¶œ)
    async fn clear_page_cache(&self) {
        let mut cache = self.page_cache.lock().await;
        cache.clear();
        debug!("ğŸ—‘ï¸  Page cache cleared");
    }
}

/// ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct DatabaseAnalyzerImpl {
    product_repo: Arc<IntegratedProductRepository>,
}

impl DatabaseAnalyzerImpl {
    pub fn new(product_repo: Arc<IntegratedProductRepository>) -> Self {
        Self { product_repo }
    }

    /// ì‹¤ì œ ì¤‘ë³µ ì œí’ˆ ìˆ˜ ê³„ì‚°
    async fn count_duplicate_products(&self) -> Result<u32> {
        // certificate_idë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì°¾ê¸°
        let products = self.product_repo.get_all_products().await?;
        let mut cert_id_count = std::collections::HashMap::new();
        
        for product in products {
            if let Some(cert_id) = &product.certificate_id {
                *cert_id_count.entry(cert_id.clone()).or_insert(0) += 1;
            }
        }
        
        // ì¤‘ë³µëœ ì œí’ˆ ìˆ˜ ê³„ì‚° (ê·¸ë£¹ í¬ê¸° - 1)
        let duplicate_count: u32 = cert_id_count.values()
            .filter(|&&count| count > 1)
            .map(|&count| count - 1)
            .sum();
            
        debug!("Found {} duplicate products based on certificate_id", duplicate_count);
        Ok(duplicate_count)
    }

    /// ì‹¤ì œ í•„ë“œ ëˆ„ë½ ë¶„ì„
    async fn analyze_missing_fields(&self) -> Result<FieldAnalysis> {
        let products = self.product_repo.get_all_products().await?;
        let total = products.len() as u32;
        
        let mut missing_company = 0u32;
        let mut missing_model = 0u32;
        let mut missing_matter_version = 0u32;
        let mut missing_connectivity = 0u32;
        let mut missing_certification_date = 0u32;
        
        for product in products {
            if product.manufacturer.is_none() || product.manufacturer.as_ref().map_or(true, |s| s.is_empty()) {
                missing_company += 1;
            }
            if product.model.is_none() || product.model.as_ref().map_or(true, |s| s.is_empty()) {
                missing_model += 1;
            }
            // Note: Product êµ¬ì¡°ì²´ì— matter_version í•„ë“œê°€ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í‚µ
            missing_matter_version = 0;
            // Note: Product êµ¬ì¡°ì²´ì— connectivity í•„ë“œê°€ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í‚µ
            missing_connectivity = 0;
            // Note: certification_dateëŠ” ProductDetailì—ë§Œ ìˆìœ¼ë¯€ë¡œ ìŠ¤í‚µ
            missing_certification_date = 0;
        }
        
        info!("ğŸ“Š Field analysis: {}/{} missing company, {}/{} missing model, {}/{} missing matter_version",
              missing_company, total, missing_model, total, missing_matter_version, total);
        
        Ok(FieldAnalysis {
            missing_company,
            missing_model,
            missing_matter_version,
            missing_connectivity,
            missing_certification_date,
        })
    }

    /// ì‹¤ì œ ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    fn calculate_data_quality_score(&self, total: u32, unique: u32, missing_fields: &FieldAnalysis) -> f64 {
        if total == 0 {
            return 1.0;
        }
        
        // ì¤‘ë³µë¥  ì ìˆ˜ (70% ê°€ì¤‘ì¹˜)
        let uniqueness_score = (unique as f64 / total as f64) * 0.7;
        
        // í•„ë“œ ì™„ì„±ë„ ì ìˆ˜ (30% ê°€ì¤‘ì¹˜)
        let total_fields = total * 5; // 5ê°œ ì£¼ìš” í•„ë“œ
        let missing_total = missing_fields.missing_company + 
                           missing_fields.missing_model + 
                           missing_fields.missing_matter_version + 
                           missing_fields.missing_connectivity + 
                           missing_fields.missing_certification_date;
        
        let completeness_score = if total_fields > 0 {
            ((total_fields - missing_total) as f64 / total_fields as f64) * 0.3
        } else {
            0.3
        };
        
        let final_score = uniqueness_score + completeness_score;
        debug!("Quality score: uniqueness={:.3} + completeness={:.3} = {:.3}", 
               uniqueness_score, completeness_score, final_score);
        
        final_score.min(1.0).max(0.0)
    }

    /// ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
    async fn get_last_update_time(&self) -> Result<Option<chrono::DateTime<chrono::Utc>>> {
        // ê°€ì¥ ìµœê·¼ì— ì—…ë°ì´íŠ¸ëœ ì œí’ˆì˜ ì‹œê°„ì„ ê°€ì ¸ì˜¤ê¸°
        match self.product_repo.get_latest_updated_product().await {
            Ok(Some(product)) => Ok(Some(product.updated_at)),
            Ok(None) => Ok(None),
            Err(_) => Ok(None), // ì—ëŸ¬ ì‹œ None ë°˜í™˜
        }
    }

    /// ëˆ„ë½ëœ í•„ë“œê°€ ë§ì€ ì œí’ˆë“¤ì˜ ìš°ì„ ìˆœìœ„ URL ìƒì„±
    async fn get_priority_urls_for_missing_fields(&self) -> Result<Vec<String>> {
        let products = self.product_repo.get_all_products().await?;
        let mut priority_products = Vec::new();
        
        for product in products {
            let missing_count = [
                product.manufacturer.is_none() || product.manufacturer.as_ref().map_or(true, |s| s.is_empty()),
                product.model.is_none() || product.model.as_ref().map_or(true, |s| s.is_empty()),
                // matter_versionê³¼ connectivity í•„ë“œê°€ Productì— ì—†ìœ¼ë¯€ë¡œ falseë¡œ ì„¤ì •
                false, // matter_version
                false, // connectivity
                // certification_dateëŠ” ProductDetailì—ë§Œ ìˆìœ¼ë¯€ë¡œ falseë¡œ ì„¤ì •
                false, // certification_date
            ].iter().filter(|&&missing| missing).count();
            
            // 3ê°œ ì´ìƒ í•„ë“œê°€ ëˆ„ë½ëœ ì œí’ˆë“¤ì„ ìš°ì„ ìˆœìœ„ë¡œ ì„¤ì •
            if missing_count >= 3 {
                priority_products.push(product.url.clone());
            }
        }
        
        // ìµœëŒ€ 50ê°œê¹Œì§€ë§Œ ìš°ì„ ìˆœìœ„ë¡œ ì„¤ì •
        priority_products.truncate(50);
        
        info!("ğŸ“‹ Generated {} priority URLs for products with missing fields", priority_products.len());
        Ok(priority_products)
    }

    /// ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸°
    async fn find_duplicate_groups(&self) -> Result<Vec<Vec<String>>> {
        let products = self.product_repo.get_all_products().await?;
        let mut cert_id_groups: std::collections::HashMap<String, Vec<String>> = std::collections::HashMap::new();
        
        for product in products {
            if let Some(cert_id) = &product.certificate_id {
                cert_id_groups.entry(cert_id.clone())
                    .or_insert_with(Vec::new)
                    .push(product.url.clone());
            }
        }
        
        // 2ê°œ ì´ìƒì˜ ì œí’ˆì´ ìˆëŠ” ê·¸ë£¹ë§Œ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
        let duplicate_groups: Vec<Vec<String>> = cert_id_groups.into_values()
            .filter(|group| group.len() > 1)
            .collect();
        
        debug!("Found {} duplicate groups", duplicate_groups.len());
        Ok(duplicate_groups)
    }
}

#[async_trait]
impl DatabaseAnalyzer for DatabaseAnalyzerImpl {
    async fn analyze_current_state(&self) -> Result<DatabaseAnalysis> {
        let products = self.product_repo.get_all_products().await?;
        let total = products.len() as u32;
        
        // ì¤‘ë³µ ì œí’ˆ ê³„ì‚°
        let duplicate_count = self.count_duplicate_products().await?;
        let unique = total.saturating_sub(duplicate_count);
        
        // í•„ë“œ ëˆ„ë½ ë¶„ì„
        let missing_fields_analysis = self.analyze_missing_fields().await?;
        
        // ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        let data_quality_score = self.calculate_data_quality_score(total, unique, &missing_fields_analysis);
        
        // ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„
        let last_update = self.get_last_update_time().await?;
        
        info!("ğŸ“Š Database Analysis: total={}, unique={}, duplicates={}, quality={:.3}", 
              total, unique, duplicate_count, data_quality_score);
        
        Ok(DatabaseAnalysis {
            total_products: total,
            unique_products: unique,
            duplicate_count,
            last_update,
            missing_fields_analysis,
            data_quality_score,
        })
    }
    
    async fn recommend_processing_strategy(&self) -> Result<ProcessingStrategy> {
        let analysis = self.analyze_current_state().await?;
        
        // ë°°ì¹˜ í¬ê¸° ì¶”ì²œ (ë°ì´í„° í’ˆì§ˆì— ë”°ë¼ ì¡°ì •)
        let recommended_batch_size = if analysis.data_quality_score > 0.8 {
            20 // ë†’ì€ í’ˆì§ˆì¼ ë•ŒëŠ” í° ë°°ì¹˜
        } else if analysis.data_quality_score > 0.5 {
            10 // ì¤‘ê°„ í’ˆì§ˆ
        } else {
            5  // ë‚®ì€ í’ˆì§ˆì¼ ë•ŒëŠ” ì‘ì€ ë°°ì¹˜
        };
        
        // ë™ì‹œì„± ì¶”ì²œ
        let recommended_concurrency = if analysis.total_products > 1000 {
            3
        } else {
            2
        };
        
        // ì¤‘ë³µ ê±´ë„ˆë›°ê¸° ì—¬ë¶€
        let should_skip_duplicates = analysis.duplicate_count > analysis.total_products / 10;
        
        // ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸ ì—¬ë¶€
        let should_update_existing = analysis.data_quality_score < 0.7;
        
        // ìš°ì„ ìˆœìœ„ URL ìƒì„±
        let priority_urls = self.get_priority_urls_for_missing_fields().await?;
        
        info!("ğŸ¯ Strategy: batch_size={}, concurrency={}, skip_duplicates={}, update_existing={}", 
              recommended_batch_size, recommended_concurrency, should_skip_duplicates, should_update_existing);
        
        Ok(ProcessingStrategy {
            recommended_batch_size,
            recommended_concurrency,
            should_skip_duplicates,
            should_update_existing,
            priority_urls,
        })
    }
    
    async fn analyze_duplicates(&self) -> Result<DuplicateAnalysis> {
        let total_products = self.product_repo.get_all_products().await?.len() as u32;
        let total_duplicates = self.count_duplicate_products().await?;
        let duplicate_percentage = if total_products > 0 {
            (total_duplicates as f64 / total_products as f64) * 100.0
        } else {
            0.0
        };
        
        // ì¤‘ë³µ ê·¸ë£¹ ì°¾ê¸° (ê°„ì†Œí™”ëœ ë²„ì „)
        let duplicate_groups = vec![]; // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” find_duplicate_groups() ì‚¬ìš©
        
        info!("ğŸ”„ Duplicate Analysis: {}/{} duplicates ({:.1}%)", 
              total_duplicates, total_products, duplicate_percentage);
        
        Ok(DuplicateAnalysis {
            total_duplicates,
            duplicate_groups,
            duplicate_percentage,
        })
    }
}

/// ì»¬ë ‰í„° ì„¤ì •
#[derive(Debug, Clone)]
pub struct CollectorConfig {
    pub batch_size: u32,
    pub max_concurrent: u32,
    pub concurrency: u32, // alias for max_concurrent  
    pub delay_between_requests: Duration,
    pub delay_ms: u64, // alias for delay_between_requests in milliseconds
    pub retry_attempts: u32,
    pub retry_max: u32, // alias for retry_attempts
}

impl Default for CollectorConfig {
    fn default() -> Self {
        Self {
            batch_size: 10,
            max_concurrent: 3,
            concurrency: 3,
            delay_between_requests: Duration::from_millis(500),
            delay_ms: 500,
            retry_attempts: 3,
            retry_max: 3,
        }
    }
}

/// í—¬ìŠ¤ ìŠ¤ì½”ì–´ ê³„ì‚° í•¨ìˆ˜
fn calculate_health_score(response_time: Duration, total_pages: u32) -> f64 {
    // ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 0.7)
    let response_score = if response_time.as_millis() <= 500 {
        0.7
    } else if response_time.as_millis() <= 1000 {
        0.5
    } else if response_time.as_millis() <= 2000 {
        0.3
    } else {
        0.1
    };
    
    // í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 0.3)
    let page_score = if total_pages > 0 {
        0.3
    } else {
        0.0
    };
    
    response_score + page_score
}

/// ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductListCollectorImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

impl ProductListCollectorImpl {
    pub fn new(
        http_client: Arc<tokio::sync::Mutex<HttpClient>>,
        data_extractor: Arc<MatterDataExtractor>,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client,
            data_extractor,
            config,
        }
    }
}

#[async_trait]
impl ProductListCollector for ProductListCollectorImpl {
    async fn collect_all_pages(&self, total_pages: u32) -> Result<Vec<String>> {
        let mut all_urls = Vec::new();
        
        for page in 1..=total_pages {
            match self.collect_single_page(page).await {
                Ok(urls) => {
                    all_urls.extend(urls);
                    debug!("ğŸ“„ Collected {} URLs from page {}", all_urls.len(), page);
                }
                Err(e) => {
                    warn!("âš ï¸  Failed to collect URLs from page {}: {}", page, e);
                }
            }
            
            tokio::time::sleep(self.config.delay_between_requests).await;
        }
        
        info!("ğŸ“‹ Total URLs collected: {}", all_urls.len());
        Ok(all_urls)
    }
    
    async fn collect_single_page(&self, page: u32) -> Result<Vec<String>> {
        let url = config_utils::matter_products_page_url_simple(page);
        let mut client = self.http_client.lock().await;
        let html = client.fetch_html_string(&url).await?;
        drop(client);
        
        let doc = scraper::Html::parse_document(&html);
        let urls = self.data_extractor.extract_product_urls(&doc, "https://csa-iot.org")?;
        
        debug!("ğŸ”— Extracted {} URLs from page {}", urls.len(), page);
        Ok(urls)
    }
    
    async fn collect_page_batch(&self, pages: &[u32]) -> Result<Vec<String>> {
        let mut all_urls = Vec::new();
        
        for page in pages {
            match self.collect_single_page(*page).await {
                Ok(urls) => all_urls.extend(urls),
                Err(e) => warn!("Failed to collect from page {}: {}", page, e),
            }
            
            tokio::time::sleep(self.config.delay_between_requests).await;
        }
        
        Ok(all_urls)
    }
}

/// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductDetailCollectorImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

impl ProductDetailCollectorImpl {
    pub fn new(
        http_client: Arc<tokio::sync::Mutex<HttpClient>>,
        data_extractor: Arc<MatterDataExtractor>,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client,
            data_extractor,
            config,
        }
    }
}

#[async_trait]
impl ProductDetailCollector for ProductDetailCollectorImpl {
    async fn collect_details(&self, urls: &[String]) -> Result<Vec<ProductDetail>> {
        let mut products = Vec::new();
        
        for url in urls {
            match self.collect_single_product(url).await {
                Ok(product) => products.push(product),
                Err(e) => warn!("Failed to collect product from {}: {}", url, e),
            }
            
            tokio::time::sleep(self.config.delay_between_requests).await;
        }
        
        Ok(products)
    }
    
    async fn collect_single_product(&self, url: &str) -> Result<ProductDetail> {
        let mut client = self.http_client.lock().await;
        let html = client.fetch_html_string(url).await?;
        drop(client);
        
        let doc = scraper::Html::parse_document(&html);
        let product_detail = self.data_extractor.extract_product_detail(&doc, url.to_string())?;
        
        debug!("ğŸ“¦ Extracted product: {}", product_detail.certification_id.as_deref().unwrap_or("Unknown"));
        Ok(product_detail)
    }
    
    async fn collect_product_batch(&self, urls: &[String]) -> Result<Vec<ProductDetail>> {
        let mut products = Vec::new();
        
        for url in urls {
            match self.collect_single_product(url).await {
                Ok(product) => products.push(product),
                Err(e) => warn!("Failed to collect product from {}: {}", url, e),
            }
            
            tokio::time::sleep(self.config.delay_between_requests).await;
        }
        
        Ok(products)
    }
}

/// ProductDetailì„ Productë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
pub fn product_detail_to_product(detail: crate::domain::product::ProductDetail) -> Product {
    Product {
        url: detail.url,
        manufacturer: detail.manufacturer,
        model: detail.model,
        certificate_id: detail.certification_id,
        page_id: detail.page_id,
        index_in_page: detail.index_in_page,
        created_at: detail.created_at,
        updated_at: detail.updated_at,
    }
}

