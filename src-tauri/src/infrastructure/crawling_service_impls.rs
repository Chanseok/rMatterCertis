//! í¬ë¡¤ë§ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
//!
//! domain/services/crawling_services.rsì˜ íŠ¸ë ˆì´íŠ¸ë“¤ì— ëŒ€í•œ ì‹¤ì œ êµ¬í˜„ì²´

use anyhow::{Result, anyhow};
use async_trait::async_trait;
use chrono;
use regex;
use scraper;
use std::any::Any;
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::Semaphore;
use tokio_util::sync::CancellationToken;
use tracing::{debug, error, info, warn};

use crate::crawl_engine::services::crawling_planner::CrawlingPlanner;
use crate::domain::product::{Product, ProductDetail};
use crate::domain::product_url::ProductUrl;
use crate::domain::services::crawling_services::{
    CrawlingRangeRecommendation, DataDecreaseRecommendation, DatabaseAnalysis, RecommendedAction,
    SeverityLevel, SiteDataChangeStatus,
};
use crate::domain::services::{
    DatabaseAnalyzer, DuplicateAnalysis, FieldAnalysis, ProcessingStrategy, ProductDetailCollector,
    ProductListCollector, SiteStatus, StatusChecker,
};
use crate::infrastructure::config::utils as config_utils;
use crate::infrastructure::config::{AppConfig, CrawlingConfig};
use crate::infrastructure::{HttpClient, IntegratedProductRepository, MatterDataExtractor};
// Canonical pagination calculator (legacy utils::PageIdCalculator via domain alias)
use crate::domain::pagination::CanonicalPageIdCalculator;

// ìƒìˆ˜ ì •ì˜
const DEFAULT_PRODUCTS_PER_PAGE: u32 = 12;

// Reintroduced struct definitions (accidentally disrupted during method removal phase)
pub struct StatusCheckerImpl {
    pub http_client: Arc<HttpClient>,
    pub data_extractor: Arc<MatterDataExtractor>,
    pub config: AppConfig,
    page_cache: Arc<tokio::sync::Mutex<HashMap<u32, PageAnalysisCache>>>,
    pub product_repo: Option<Arc<IntegratedProductRepository>>,
}

#[derive(Clone, Debug)]
struct PageAnalysisCache {
    max_page_in_pagination: u32,
    product_count: u32,
    has_products: bool,
}

impl StatusCheckerImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: AppConfig,
    ) -> Self {
        Self {
            http_client: Arc::new(http_client),
            data_extractor: Arc::new(data_extractor),
            config,
            page_cache: Arc::new(tokio::sync::Mutex::new(HashMap::new())),
            product_repo: None,
        }
    }

    pub fn create_configured_http_client(&self) -> Result<HttpClient> {
        HttpClient::create_from_global_config()
            .map_err(|e| anyhow!("HttpClient create failed: {}", e))
    }

    pub async fn clear_page_cache(&self) {
        let mut guard = self.page_cache.lock().await;
        guard.clear();
    }
}

impl StatusCheckerImpl {
    /// Associate a product repository after initial creation (legacy helper)
    pub fn with_product_repo(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: AppConfig,
        product_repo: Arc<IntegratedProductRepository>,
    ) -> Self {
        let mut instance = StatusCheckerImpl::new(http_client, data_extractor, config);
        instance.product_repo = Some(product_repo);
        instance
    }

    /// Update the pagination context in the data extractor based on discovered page information
    pub async fn update_pagination_context(
        &self,
        total_pages: u32,
        items_on_last_page: u32,
    ) -> Result<()> {
        let validated_config =
            crate::application::validated_crawling_config::ValidatedCrawlingConfig::from_app_config(
                &self.config,
            );
        let products_per_page = validated_config.products_per_page;
        let pagination_context = crate::infrastructure::html_parser::PaginationContext {
            total_pages,
            items_per_page: products_per_page,
            items_on_last_page,
            target_page_size: products_per_page,
        };
        self.data_extractor
            .set_pagination_context(pagination_context)?;
        info!(
            "ğŸ“Š Updated pagination context: total_pages={}, items_on_last_page={}, products_per_page={}",
            total_pages, items_on_last_page, products_per_page
        );
        Ok(())
    }
}

#[async_trait]
impl StatusChecker for StatusCheckerImpl {
    async fn check_site_status(&self) -> Result<SiteStatus> {
        let start_time = Instant::now();
        info!("Starting comprehensive site status check with detailed page discovery");

        // ìºì‹œ ì´ˆê¸°í™”
        self.clear_page_cache_internal().await;

        info!("Checking site status and discovering pages...");

        // Step 1: ê¸°ë³¸ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± í™•ì¸
        let url = config_utils::matter_products_page_url_simple(1);

        // ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸
        let access_test = {
            // Use configured HttpClient instead of hardcoded default
            let _client = self.create_configured_http_client()?;
            let result = self.http_client.fetch_response(&url).await?.text().await;
            result
        };

        match access_test {
            Ok(_) => info!("Site is accessible"),
            Err(e) => {
                error!("Failed to access site: {}", e);
                return Ok(SiteStatus {
                    is_accessible: false,
                    response_time_ms: start_time.elapsed().as_millis() as u64,
                    total_pages: 0,
                    estimated_products: 0,
                    products_on_last_page: 0,
                    last_check_time: chrono::Utc::now(),
                    health_score: 0.0,
                    data_change_status: SiteDataChangeStatus::Inaccessible,
                    decrease_recommendation: None,
                    crawling_range_recommendation: CrawlingRangeRecommendation::None,
                });
            }
        }

        // Step 2: í˜ì´ì§€ ìˆ˜ íƒì§€ ë° ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜ í™•ì¸
        let (total_pages, products_on_last_page) = self.discover_total_pages().await?;

        // Step 2.5: í˜ì´ì§€ë„¤ì´ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        if let Err(e) = self
            .update_pagination_context(total_pages, products_on_last_page)
            .await
        {
            warn!("Failed to update pagination context: {}", e);
        }

        let response_time_ms = start_time.elapsed().as_millis() as u64;
        let response_time = start_time.elapsed();

        // Step 3: ì‚¬ì´íŠ¸ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
        let health_score = calculate_health_score(response_time, total_pages);

        info!(
            "Site status check completed: {} pages found, {}ms total time, health score: {:.2}",
            total_pages, response_time_ms, health_score
        );

        // ì •í™•í•œ ì œí’ˆ ìˆ˜ ê³„ì‚°: (ë§ˆì§€ë§‰ í˜ì´ì§€ - 1) * í˜ì´ì§€ë‹¹ ì œí’ˆ ìˆ˜ + ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;

        let estimated_products = if total_pages > 1 {
            ((total_pages - 1) * products_per_page) + products_on_last_page
        } else {
            products_on_last_page
        };

        info!(
            "Accurate product estimation: ({} full pages * {} products) + {} products on last page = {} total products",
            total_pages - 1,
            products_per_page,
            products_on_last_page,
            estimated_products
        );

        // Step 4: ë°ì´í„° ë³€í™” ìƒíƒœ ë¶„ì„
        let (data_change_status, decrease_recommendation) =
            self.analyze_data_changes(estimated_products).await;

        // Step 5: í¬ë¡¤ë§ ë²”ìœ„ ê¶Œì¥ì‚¬í•­ ê³„ì‚° - ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ ì‚¬ìš©
        info!("ğŸ” Calculating crawling range recommendation from site status and DB analysis...");
        info!(
            "ğŸ—ï¸ [NEW ARCHITECTURE] Using SystemConfig-based intelligent strategy instead of hardcoded values"
        );

        let system_config = Arc::new(crate::crawl_engine::context::SystemConfig::default());
        info!(
            "âœ… [NEW ARCHITECTURE] SystemConfig initialized: batch_sizes.small_db_multiplier={}",
            system_config.performance.batch_sizes.small_db_multiplier
        );
        info!(
            "âœ… [NEW ARCHITECTURE] SystemConfig initialized: concurrency.high_load_multiplier={}",
            system_config.performance.concurrency.high_load_multiplier
        );

        // CrawlingPlanner ì´ˆê¸°í™” ë° í…ŒìŠ¤íŠ¸ (ìºì‹œëœ ì‚¬ì´íŠ¸ ìƒíƒœ ì‚¬ìš©)
        let status_checker_arc = Arc::new(StatusCheckerImpl {
            http_client: self.http_client.clone(),
            data_extractor: self.data_extractor.clone(),
            config: self.config.clone(),
            page_cache: Arc::new(tokio::sync::Mutex::new(HashMap::new())),
            product_repo: self.product_repo.clone(),
        });
        // ğŸ”§ ì˜¬ë°”ë¥¸ DatabaseAnalyzer ì‚¬ìš©: StatusCheckerImpl ëŒ€ì‹  DatabaseAnalyzerImpl ì‚¬ìš©
        let db_analyzer_arc: Arc<dyn DatabaseAnalyzer> =
            if let Some(ref product_repo) = self.product_repo {
                Arc::new(DatabaseAnalyzerImpl::new(product_repo.clone()))
            } else {
                status_checker_arc.clone() // fallback
            };
        let status_checker_for_planner: Arc<dyn StatusChecker> = status_checker_arc.clone();

        let crawling_planner =
            CrawlingPlanner::new(status_checker_for_planner, db_analyzer_arc, system_config);

        // ìºì‹œëœ ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ CrawlingPlannerì— ì „ë‹¬ (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
        let cached_site_status = SiteStatus {
            is_accessible: true,
            response_time_ms,
            total_pages,
            estimated_products,
            products_on_last_page,
            last_check_time: chrono::Utc::now(),
            health_score,
            data_change_status: data_change_status.clone(),
            decrease_recommendation: decrease_recommendation.clone(),
            crawling_range_recommendation: CrawlingRangeRecommendation::Full, // ì„ì‹œê°’
        };

        // ì‹¤ì œ CrawlingPlannerë¥¼ ì‚¬ìš©í•´ì„œ ë¶„ì„ ì‹œë„ (ìºì‹œëœ ë°ì´í„° ì‚¬ìš©)
        match crawling_planner
            .analyze_system_state_with_cache(Some(cached_site_status))
            .await
        {
            Ok((site_status_new, db_analysis_new)) => {
                info!(
                    "ğŸ‰ [NEW ARCHITECTURE] CrawlingPlanner analysis successful! Site pages: {}, DB products: {}",
                    site_status_new.total_pages, db_analysis_new.total_products
                );
            }
            Err(e) => {
                info!(
                    "âš ï¸ [NEW ARCHITECTURE] CrawlingPlanner analysis failed, using fallback: {}",
                    e
                );
            }
        }

        let crawling_range_recommendation = self
            .calculate_crawling_range_recommendation_internal(
                total_pages,
                products_on_last_page,
                estimated_products,
            )
            .await?;

        Ok(SiteStatus {
            is_accessible: true,
            response_time_ms,
            total_pages,
            estimated_products,
            products_on_last_page,
            last_check_time: chrono::Utc::now(),
            health_score,
            data_change_status,
            decrease_recommendation,
            crawling_range_recommendation,
        })
    }

    async fn calculate_crawling_range_recommendation(
        &self,
        site_status: &SiteStatus,
        db_analysis: &DatabaseAnalysis,
    ) -> Result<CrawlingRangeRecommendation> {
        info!("ğŸ” Calculating crawling range recommendation from site status and DB analysis...");
        info!(
            "ğŸ“Š DB Analysis shows: total_products={}, unique_products={}",
            db_analysis.total_products, db_analysis.unique_products
        );

        // Cross-check with local status to ensure consistency
        let local_status = self.get_local_db_status().await?;

        // Verify consistency between different DB access methods
        let db_total = db_analysis.total_products;
        if db_total != local_status.total_saved_products {
            warn!(
                "âš ï¸  DB inconsistency detected: analysis={}, local_status={}",
                db_analysis.total_products, local_status.total_saved_products
            );
            // Use the higher value for safer operation
            let effective_total = db_total.max(local_status.total_saved_products);
            info!("ğŸ”§ Using effective total: {}", effective_total);
        }

        // If database is empty, recommend full crawl
        if db_analysis.total_products == 0 && local_status.is_empty {
            info!("ğŸ“Š Local DB is confirmed empty - recommending full crawl");
            return Ok(CrawlingRangeRecommendation::Full);
        }

        // If there's inconsistency but some data exists, use partial crawl
        if db_analysis.total_products == 0 && !local_status.is_empty {
            warn!("âš ï¸  Inconsistent DB state: analysis says empty but local status says not empty");
            warn!("âš ï¸  This suggests a DB access issue - using local status for safety");
            // Continue with partial crawl logic using local_status data
        }

        // Calculate how many new products might have been added
        let effective_total = db_analysis
            .total_products
            .max(local_status.total_saved_products);
        let estimated_new_products = if site_status.estimated_products > effective_total {
            site_status.estimated_products - effective_total
        } else {
            0
        };

        if estimated_new_products == 0 {
            info!("ğŸ“Š No new products detected - recommending minimal verification crawl");
            return Ok(CrawlingRangeRecommendation::Partial(5)); // 5 pages for verification
        }

        // Calculate pages needed for new products
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;
        let pages_needed = (estimated_new_products as f64 / products_per_page as f64).ceil() as u32;
        let limited_pages = pages_needed.min(self.config.user.crawling.page_range_limit);

        info!(
            "ğŸ“Š Estimated {} new products, recommending {} pages crawl",
            estimated_new_products, limited_pages
        );
        Ok(CrawlingRangeRecommendation::Partial(limited_pages))
    }

    async fn estimate_crawling_time(&self, pages: u32) -> Duration {
        // ...
        // í˜ì´ì§€ë‹¹ í‰ê·  2ì´ˆ + ì œí’ˆ ìƒì„¸í˜ì´ì§€ë‹¹ 1ì´ˆ ì¶”ì •
        let page_collection_time = pages * 2;
        let product_detail_time = pages * 20; // í˜ì´ì§€ë‹¹ 20ê°œ ì œí’ˆ * 1ì´ˆ
        let total_seconds = page_collection_time + product_detail_time;

        Duration::from_secs(total_seconds as u64)
    }

    async fn verify_site_accessibility(&self) -> Result<bool> {
        let status = self.check_site_status().await?;
        // health_scoreëŠ” ì„±ëŠ¥ ì •ë³´ì¼ ë¿, í¬ë¡¤ë§ ê°€ëŠ¥ ì—¬ë¶€ì™€ëŠ” ë¬´ê´€
        // ì‚¬ì´íŠ¸ ì ‘ê·¼ ê°€ëŠ¥ì„±ê³¼ ê¸°ë³¸ì ì¸ í˜ì´ì§€ êµ¬ì¡°ë§Œ í™•ì¸
        Ok(status.is_accessible && status.total_pages > 0)
    }
}

impl StatusCheckerImpl {
    /// í–¥ìƒëœ í˜ì´ì§€ íƒì§€ ë¡œì§ - ì‚¬ì´íŠ¸ ì •ë³´ ë³€í™” ê°ì§€ í¬í•¨
    async fn discover_total_pages(&self) -> Result<(u32, u32)> {
        info!("ğŸ” Starting enhanced page discovery algorithm with site change detection");

        // 1. ì‹œì‘ í˜ì´ì§€ ê²°ì •
        let start_page = self
            .config
            .app_managed
            .last_known_max_page
            .unwrap_or(self.config.advanced.last_page_search_start);

        info!(
            "ğŸ“ Starting from page {} (last known: {:?}, default: {})",
            start_page,
            self.config.app_managed.last_known_max_page,
            self.config.advanced.last_page_search_start
        );

        // 2. ì‹œì‘ í˜ì´ì§€ ë¶„ì„ (ìºì‹œ ì‚¬ìš©)
        let start_analysis = self.get_or_analyze_page(start_page).await?;
        let mut current_page = start_page;

        if !start_analysis.has_products {
            warn!(
                "âš ï¸  Starting page {} has no products - checking site status",
                current_page
            );
            // ì²« í˜ì´ì§€ í™•ì¸ìœ¼ë¡œ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± ê²€ì¦
            let first_page_analysis = self.get_or_analyze_page(1).await?;
            if !first_page_analysis.has_products {
                error!("âŒ First page also has no products - site may be temporarily unavailable");
                return Err(anyhow::anyhow!(
                    "Site appears to be temporarily unavailable or experiencing issues. Please try again later."
                ));
            }

            info!(
                "âœ… First page has products - site is accessible, cached page info may be outdated"
            );
            warn!("ğŸ”„ Site content may have decreased - will perform full discovery");

            // í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
            current_page = self.find_last_valid_page_downward(current_page).await?;
            info!("âœ… Found valid starting page: {}", current_page);
        }

        // 3. ë°˜ë³µì  ìƒí–¥ íƒìƒ‰: í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° ê°’ì„ ì°¾ì„ ë•Œê¹Œì§€ ê³„ì†
        let mut attempts = 0;
        let max_attempts = self.config.advanced.max_search_attempts;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!(
                    "ğŸ”„ Reached maximum attempts ({}), stopping at page {}",
                    max_attempts, current_page
                );
                break;
            }

            info!(
                "ğŸ” Iteration {}/{}: Checking page {}",
                attempts, max_attempts, current_page
            );

            // í˜„ì¬ í˜ì´ì§€ë¥¼ ë¶„ì„ (ìºì‹œ ì‚¬ìš©)
            let analysis = match self.get_or_analyze_page(current_page).await {
                Ok(analysis) => analysis,
                Err(e) => {
                    warn!("âŒ Failed to analyze page {}: {}", current_page, e);
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ í•˜í–¥ íƒìƒ‰
                    current_page = self.find_last_valid_page_downward(current_page).await?;
                    break;
                }
            };

            if !analysis.has_products {
                // ì œí’ˆì´ ì—†ëŠ” ê²½ìš° ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                info!(
                    "ğŸ”» Page {} has no products, performing safe downward search",
                    current_page
                );
                current_page = self
                    .find_last_valid_page_with_safety_check(current_page)
                    .await?;
                break;
            }

            // í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° í˜ì´ì§€ë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
            if analysis.max_page_in_pagination > current_page {
                info!(
                    "ğŸ”º Found higher page {} in pagination, jumping there",
                    analysis.max_page_in_pagination
                );
                current_page = analysis.max_page_in_pagination;
                // ìƒˆ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë‹¤ì‹œ íƒìƒ‰
                continue;
            }
            info!(
                "ğŸ No higher pages found in pagination, {} appears to be the last page",
                current_page
            );
            break;
        }

        // 4. ìµœì¢… ê²€ì¦: ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸ ë° ì œí’ˆ ìˆ˜ ê³„ì‚°
        let (verified_last_page, products_on_last_page) =
            self.verify_last_page(current_page).await?;

        // 5. ì„¤ì • íŒŒì¼ì— ê²°ê³¼ ì €ì¥
        if let Err(e) = self
            .update_last_known_page(verified_last_page, Some(products_on_last_page))
            .await
        {
            warn!("âš ï¸  Failed to update last known page in config: {}", e);
        }

        info!(
            "ğŸ‰ Final verified last page: {} with {} products",
            verified_last_page, products_on_last_page
        );
        Ok((verified_last_page, products_on_last_page))
    }

    /// í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ë§ˆì§€ë§‰ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
    async fn find_last_valid_page_downward(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page.saturating_sub(1);
        let min_page = 1;

        info!("Starting downward search from page {}", current_page);

        while current_page >= min_page {
            let test_url = config_utils::matter_products_page_url_simple(current_page);

            // Use configured HttpClient
            let _client = self.create_configured_http_client()?;
            match self.http_client.fetch_response(&test_url).await {
                Ok(response) => match response.text().await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        if self.has_products_on_page(&doc) {
                            info!("Found valid page with products: {}", current_page);
                            return Ok(current_page);
                        }
                    }
                    Err(e) => {
                        error!("Failed to get HTML for page {}: {}", current_page, e);
                    }
                },
                Err(e) => {
                    warn!(
                        "Failed to fetch page {} during downward search: {}",
                        current_page, e
                    );
                }
            }

            current_page = current_page.saturating_sub(1);

            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(
                self.config.user.request_delay_ms,
            ))
            .await;
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
        warn!("No valid pages found during downward search, returning 1");
        Ok(1)
    }

    /// ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ - ì—°ì† ë¹ˆ í˜ì´ì§€ 3ê°œ ì´ìƒ ì‹œ fatal error
    async fn find_last_valid_page_with_safety_check(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page;
        let mut consecutive_empty_pages = 0;
        const MAX_CONSECUTIVE_EMPTY: u32 = 3;
        let min_page = 1;

        info!(
            "ğŸ” Starting safe downward search from page {} (max consecutive empty: {})",
            current_page, MAX_CONSECUTIVE_EMPTY
        );

        // ë¨¼ì € ì‹œì‘ í˜ì´ì§€ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if !self.check_page_has_products(current_page).await? {
            consecutive_empty_pages += 1;
            info!(
                "âš ï¸  Starting page {} is empty (consecutive: {})",
                current_page, consecutive_empty_pages
            );
        }

        while current_page > min_page {
            current_page = current_page.saturating_sub(1);

            let test_url = config_utils::matter_products_page_url_simple(current_page);
            info!(
                "ğŸ” Checking page {} (consecutive empty: {})",
                current_page, consecutive_empty_pages
            );

            // Use configured HttpClient
            let _client = self.create_configured_http_client()?; // unused (legacy path)
            match self.http_client.fetch_response(&test_url).await {
                Ok(response) => match response.text().await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        if self.has_products_on_page(&doc) {
                            info!(
                                "âœ… Found valid page with products: {} (after {} consecutive empty pages)",
                                current_page, consecutive_empty_pages
                            );
                            return Ok(current_page);
                        }
                        consecutive_empty_pages += 1;
                        warn!(
                            "âš ï¸  Page {} is empty (consecutive: {}/{})",
                            current_page, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY
                        );

                        // ì—°ì†ìœ¼ë¡œ ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ì´ìƒì´ë©´ fatal error
                        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                            error!(
                                "ğŸ’¥ FATAL ERROR: Found {} consecutive empty pages starting from page {}. This indicates a serious site issue or crawling problem.",
                                consecutive_empty_pages, start_page
                            );

                            return Err(anyhow!(
                                "Fatal error: {} consecutive empty pages detected. Site may be down or pagination structure changed. Last checked pages: {} to {}",
                                consecutive_empty_pages,
                                start_page,
                                current_page
                            ));
                        }
                    }
                    Err(e) => {
                        consecutive_empty_pages += 1;
                        warn!(
                            "âŒ Failed to get HTML for page {} during safe downward search: {} (consecutive: {}/{})",
                            current_page, e, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY
                        );

                        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                            error!(
                                "ğŸ’¥ FATAL ERROR: {} consecutive failures starting from page {}.",
                                consecutive_empty_pages, start_page
                            );

                            return Err(anyhow!(
                                "Fatal error: {} consecutive failures detected. HTML parsing issues or site problems. Last error: {}",
                                consecutive_empty_pages,
                                e
                            ));
                        }
                    }
                },
                Err(e) => {
                    consecutive_empty_pages += 1;
                    warn!(
                        "âŒ Failed to fetch page {} during safe downward search: {} (consecutive: {}/{})",
                        current_page, e, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY
                    );

                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë„ ì—°ì† ì‹¤íŒ¨ë¡œ ì¹´ìš´íŠ¸
                    if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                        error!(
                            "ğŸ’¥ FATAL ERROR: {} consecutive failures (empty pages + network errors) starting from page {}.",
                            consecutive_empty_pages, start_page
                        );

                        return Err(anyhow!(
                            "Fatal error: {} consecutive failures detected. Network issues or site problems. Last error: {}",
                            consecutive_empty_pages,
                            e
                        ));
                    }
                }
            }

            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(
                self.config.user.request_delay_ms,
            ))
            .await;
        }

        // ìµœì†Œ í˜ì´ì§€ê¹Œì§€ ë„ë‹¬í–ˆì§€ë§Œ ì—¬ì „íˆ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ ë§ë‹¤ë©´ fatal error
        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
            error!(
                "ğŸ’¥ FATAL ERROR: Reached minimum page but still have {} consecutive empty pages. Site appears to be completely empty or broken.",
                consecutive_empty_pages
            );

            return Err(anyhow!(
                "Fatal error: Site appears to be empty or broken. {} consecutive empty pages found from page {} down to page {}",
                consecutive_empty_pages,
                start_page,
                current_page
            ));
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆì§€ë§Œ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ê²½ê³ ì™€ í•¨ê»˜ 1 ë°˜í™˜
        warn!(
            "âš ï¸  No valid pages found during safe downward search, but only {} consecutive empty pages. Returning page 1 as fallback.",
            consecutive_empty_pages
        );
        Ok(1)
    }

    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ìµœì¢… ê²€ì¦ - ë” ì² ì €í•œ ê²€ì¦ ë¡œì§
    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ê²€ì¦ ë° ì œí’ˆ ìˆ˜ í™•ì¸
    async fn verify_last_page(&self, candidate_page: u32) -> Result<(u32, u32)> {
        info!("ğŸ” Verifying candidate last page: {}", candidate_page);

        // 1. í›„ë³´ í˜ì´ì§€ ë¶„ì„ (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¶„ì„)
        let analysis = self.get_or_analyze_page(candidate_page).await?;
        let products_on_last_page = analysis.product_count;
        let has_products = analysis.has_products;

        info!(
            "ğŸ“Š Last page {} has {} products",
            candidate_page, products_on_last_page
        );

        if !has_products {
            warn!(
                "âš ï¸  Candidate page {} has no products, performing downward search with safety check",
                candidate_page
            );
            let actual_last_page = self
                .find_last_valid_page_with_safety_check(candidate_page)
                .await?;
            // ì‹¤ì œ ë§ˆì§€ë§‰ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ ë‹¤ì‹œ í™•ì¸
            let actual_analysis = self.get_or_analyze_page(actual_last_page).await?;
            return Ok((actual_last_page, actual_analysis.product_count));
        }

        // 2. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ì—ì„œ ì´ë¯¸ ë§ˆì§€ë§‰ í˜ì´ì§€ì„ì„ í™•ì‹ í•  ìˆ˜ ìˆë‹¤ë©´ ì¶”ê°€ í™•ì¸ ìƒëµ
        // í˜„ì¬ í˜ì´ì§€ê°€ í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë°œê²¬ëœ ìµœëŒ€ í˜ì´ì§€ì™€ ê°™ë‹¤ë©´ ê²€ì¦ ì™„ë£Œ
        if analysis.max_page_in_pagination == candidate_page {
            info!(
                "âœ… Page {} confirmed as last page via pagination analysis (max_pagination={})",
                candidate_page, analysis.max_page_in_pagination
            );
            info!("ğŸš€ Skipping additional verification - pagination analysis is reliable");
            return Ok((candidate_page, products_on_last_page));
        }

        // 3. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ì´ ë¶ˆí™•ì‹¤í•œ ê²½ìš°ì—ë§Œ ìµœì†Œí•œì˜ ì¶”ê°€ ê²€ì¦ ìˆ˜í–‰
        info!(
            "ğŸ” Pagination analysis inconclusive (current={}, max_pagination={}), performing minimal verification",
            candidate_page, analysis.max_page_in_pagination
        );

        // ë°”ë¡œ ë‹¤ìŒ í˜ì´ì§€ 1ê°œë§Œ í™•ì¸ (ê³¼ë„í•œ ê²€ì¦ ë°©ì§€)
        let next_page = candidate_page + 1;
        match self.check_page_has_products(next_page).await {
            Ok(true) => {
                warn!(
                    "ğŸ” Found products on page {} after candidate {}, re-discovering",
                    next_page, candidate_page
                );
                // ë” ë†’ì€ í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ë°œê²¬í–ˆìœ¼ë¯€ë¡œ ê·¸ í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰
                return self.discover_from_page_with_count(next_page).await;
            }
            Ok(false) => {
                info!(
                    "âœ… Verified page {} as the last page with {} products (checked {} page ahead)",
                    candidate_page, products_on_last_page, 1
                );
            }
            Err(e) => {
                debug!(
                    "âŒ Failed to check page {}: {}, assuming {} is last",
                    next_page, e, candidate_page
                );
            }
        }

        Ok((candidate_page, products_on_last_page))
    }

    /// íŠ¹ì • í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰ ì‹œì‘ (ì œí’ˆ ìˆ˜ë„ ë°˜í™˜)
    async fn discover_from_page_with_count(&self, start_page: u32) -> Result<(u32, u32)> {
        info!(
            "ğŸ”„ Re-discovering from page {} with product count",
            start_page
        );

        let mut current_page = start_page;
        let max_attempts = self.config.advanced.max_search_attempts;
        let mut attempts = 0;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!(
                    "ğŸ”„ Reached maximum attempts, stopping at page {}",
                    current_page
                );
                break;
            }

            let test_url = config_utils::matter_products_page_url_simple(current_page);

            let (has_products, max_page_in_pagination) = {
                match self.http_client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);

                        info!(
                            "ğŸ“Š Page {} analysis: has_products={}, max_pagination={}",
                            current_page, has_products, max_page
                        );

                        (has_products, max_page)
                    }
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        break;
                    }
                }
            };

            if !has_products {
                // ì œí’ˆì´ ì—†ìœ¼ë©´ ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ í›„ ì œí’ˆ ìˆ˜ í™•ì¸
                let last_page = self
                    .find_last_valid_page_with_safety_check(current_page)
                    .await?;
                let test_url = config_utils::matter_products_page_url_simple(last_page);

                let html = self.http_client.fetch_html_string(&test_url).await?;

                let doc = scraper::Html::parse_document(&html);
                let products_count = self.count_products(&doc);
                return Ok((last_page, products_count));
            }

            if max_page_in_pagination > current_page {
                // ë” í° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ë™
                current_page = max_page_in_pagination;
                continue;
            }
            // ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬, ì œí’ˆ ìˆ˜ í™•ì¸
            let test_url = config_utils::matter_products_page_url_simple(current_page);

            let html = self.http_client.fetch_html_string(&test_url).await?;

            let doc = scraper::Html::parse_document(&html);
            let products_count = self.count_products(&doc);
            return Ok((current_page, products_count));
        }

        // ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ì‹œ í˜„ì¬ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ í™•ì¸
        let test_url = config_utils::matter_products_page_url_simple(current_page);

        let html = self.http_client.fetch_html_string(&test_url).await?;

        let doc = scraper::Html::parse_document(&html);
        let products_count = self.count_products(&doc);
        Ok((current_page, products_count))
    }

    /// íŠ¹ì • í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸ - í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ë„ í•¨ê»˜ í™•ì¸
    async fn check_page_has_products(&self, page: u32) -> Result<bool> {
        let test_url = config_utils::matter_products_page_url_simple(page);

        // Use configured HttpClient
        let _client = self.create_configured_http_client()?;
        match self.http_client.fetch_response(&test_url).await {
            Ok(response) => {
                match response.text().await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);

                        // 1. ì œí’ˆ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                        let has_products = self.has_products_on_page(&doc);

                        // 2. í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ í™•ì¸ (ë” ì¤‘ìš”í•œ ì²´í¬)
                        let active_page = self.get_active_page_number(&doc);

                        // ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ì™€ í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                        let is_correct_page = active_page == page;

                        if !is_correct_page {
                            info!(
                                "âš ï¸  Page {} was redirected to page {} (pagination mismatch)",
                                page, active_page
                            );
                            return Ok(false);
                        }

                        info!(
                            "âœ… Page {} verification: has_products={}, active_page={}, is_correct_page={}",
                            page, has_products, active_page, is_correct_page
                        );

                        Ok(has_products && is_correct_page)
                    }
                    Err(_) => Ok(false),
                }
            }
            Err(_) => Ok(false),
        }
    }

    /// í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ ì¶”ì¶œ - í˜„ì¬ í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    fn get_active_page_number(&self, doc: &scraper::Html) -> u32 {
        // í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œë¥¼ ì°¾ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        // ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìš°ì„ ìˆœìœ„ ì¡°ì • (í˜ì´ì§€ë„¤ì´ì…˜ ìš°ì„  í´ë˜ìŠ¤: page-numbers.current)
        let active_selectors = [
            ".page-numbers.current",     // ìš°ì„ ìˆœìœ„ ê°€ì¥ ë†’ìŒ (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •)
            "span.page-numbers.current", // ì •í™•í•œ ìš”ì†Œ ì§€ì •
            "a.page-numbers.current",
            ".current",
            ".active",
            ".pagination .current",
            ".pagination .active",
            "a.current",
            "span.current",
            "[aria-current='page']",
            ".wp-pagenavi .current",
            ".page-item.active a",
            ".page-link.active",
        ];

        for selector_str in &active_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                if let Some(element) = doc.select(&selector).next() {
                    // í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    let text = element.text().collect::<String>().trim().to_string();
                    if let Ok(page_num) = text.parse::<u32>() {
                        info!(
                            "ğŸ¯ Found active page number {} using selector '{}'",
                            page_num, selector_str
                        );
                        return page_num;
                    }
                }
            }
        }

        // í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° URLì—ì„œ ì¶”ì¶œ ì‹œë„
        if let Some(canonical_url) = self.get_canonical_url(doc) {
            if let Some(page_num) = self.extract_page_number(&canonical_url) {
                info!("ğŸ¯ Found page number {} from canonical URL", page_num);
                return page_num;
            }
        }

        // ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš° 1 ë°˜í™˜ (ì²« ë²ˆì§¸ í˜ì´ì§€ë¡œ ì¶”ì •)
        warn!("âš ï¸  Could not determine active page number, assuming page 1");
        1
    }

    /// í˜ì´ì§€ì˜ canonical URL ì¶”ì¶œ
    fn get_canonical_url(&self, doc: &scraper::Html) -> Option<String> {
        if let Ok(selector) = scraper::Selector::parse("link[rel='canonical']") {
            if let Some(element) = doc.select(&selector).next() {
                return element.value().attr("href").map(|s| s.to_string());
            }
        }
        None
    }

    /// ì„¤ì • íŒŒì¼ì— ë§ˆì§€ë§‰ í˜ì´ì§€ ë° ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    async fn update_last_known_page(
        &self,
        last_page: u32,
        items_on_last_page: Option<u32>,
    ) -> Result<()> {
        use crate::infrastructure::config::ConfigManager;

        let config_manager = ConfigManager::new()?;

        // ì„¤ì • ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í´ë¡œì € ì‚¬ìš©
        config_manager.update_app_managed(|app_managed| {
            // ë§ˆì§€ë§‰ ì•Œë ¤ì§„ í˜ì´ì§€ ì—…ë°ì´íŠ¸
            app_managed.last_known_max_page = Some(last_page);

            // ë§ˆì§€ë§‰ ì„±ê³µí•œ í¬ë¡¤ë§ ì‹œê°„ ì—…ë°ì´íŠ¸
            app_managed.last_successful_crawl = Some(chrono::Utc::now().to_rfc3339());

            // ì´ ì œí’ˆ ìˆ˜ ì •í™• ê³„ì‚°
            let items_per_page: u32 = DEFAULT_PRODUCTS_PER_PAGE;
        let last_partial = items_on_last_page.unwrap_or(items_per_page);
        let accurate_total = if last_page == 0 { 0 } else { (last_page - 1) * items_per_page + last_partial };
            app_managed.last_crawl_product_count = Some(accurate_total);

            // í˜ì´ì§€ë‹¹ í‰ê·  ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸
            app_managed.avg_products_per_page = Some(DEFAULT_PRODUCTS_PER_PAGE as f64);

        info!("ğŸ“ Updated config: last_page={}, items_on_last_page={}, accurate_total_products={}, timestamp={}", 
            last_page,
            last_partial,
            accurate_total,
            app_managed.last_successful_crawl.as_ref().unwrap_or(&"unknown".to_string()));
        }).await?;

        info!(
            "âœ… Successfully updated last known page to {} in config file",
            last_page
        );
        Ok(())
    }

    /// ë°ì´í„° ë³€í™” ìƒíƒœ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
    async fn analyze_data_changes(
        &self,
        current_estimated_products: u32,
    ) -> (SiteDataChangeStatus, Option<DataDecreaseRecommendation>) {
        // ì´ì „ í¬ë¡¤ë§ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        let previous_count = self.config.app_managed.last_crawl_product_count;

        match previous_count {
            None => {
                info!("ğŸ†• Initial site check - no previous data available");
                (
                    SiteDataChangeStatus::Initial {
                        count: current_estimated_products,
                    },
                    None,
                )
            }
            Some(prev_count) => {
                let change_percentage = if prev_count > 0 {
                    ((current_estimated_products as f64 - prev_count as f64) / prev_count as f64)
                        * 100.0
                } else {
                    0.0
                };
                // í—ˆìš© ì˜¤ì°¨ (ë§ˆì§€ë§‰ í˜ì´ì§€ partial ì°¨ì´ ë“±) - 0.5% ë¯¸ë§Œ ë³€í™”ëŠ” Stable ì²˜ë¦¬
                let decrease_tolerance_pct = 0.5_f64;

                if current_estimated_products > prev_count {
                    let increase = current_estimated_products - prev_count;
                    info!(
                        "ğŸ“ˆ Site data increased: {} -> {} (+{}, +{:.1}%)",
                        prev_count, current_estimated_products, increase, change_percentage
                    );
                    (
                        SiteDataChangeStatus::Increased {
                            new_count: current_estimated_products,
                            previous_count: prev_count,
                        },
                        None,
                    )
                } else if current_estimated_products == prev_count
                    || change_percentage.abs() < decrease_tolerance_pct
                {
                    if change_percentage.abs() < decrease_tolerance_pct
                        && current_estimated_products != prev_count
                    {
                        info!(
                            "ğŸ“Š Site data change {:.2}% within tolerance (<{:.2}%), treating as stable ({} -> {})",
                            change_percentage,
                            decrease_tolerance_pct,
                            prev_count,
                            current_estimated_products
                        );
                    } else {
                        info!(
                            "ğŸ“Š Site data stable: {} products",
                            current_estimated_products
                        );
                    }
                    (
                        SiteDataChangeStatus::Stable {
                            count: current_estimated_products,
                        },
                        None,
                    )
                } else {
                    let decrease = prev_count - current_estimated_products;
                    let decrease_percentage = (decrease as f64 / prev_count as f64) * 100.0;

                    warn!(
                        "ğŸ“‰ Site data decreased: {} -> {} (-{}, -{:.1}%)",
                        prev_count, current_estimated_products, decrease, decrease_percentage
                    );

                    let severity = if decrease_percentage < 10.0 {
                        SeverityLevel::Low
                    } else if decrease_percentage < 30.0 {
                        SeverityLevel::Medium
                    } else if decrease_percentage < 50.0 {
                        SeverityLevel::High
                    } else {
                        SeverityLevel::Critical
                    };

                    let recommendation =
                        self.generate_decrease_recommendation(decrease_percentage, &severity);

                    (
                        SiteDataChangeStatus::Decreased {
                            current_count: current_estimated_products,
                            previous_count: prev_count,
                            decrease_amount: decrease,
                        },
                        Some(recommendation),
                    )
                }
            }
        }
    }

    /// ë°ì´í„° ê°ì†Œ ì‹œ ê¶Œì¥ì‚¬í•­ ìƒì„±
    fn generate_decrease_recommendation(
        &self,
        decrease_percentage: f64,
        severity: &SeverityLevel,
    ) -> DataDecreaseRecommendation {
        match severity {
            SeverityLevel::Low => DataDecreaseRecommendation {
                action_type: RecommendedAction::WaitAndRetry,
                description: format!(
                    "ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¼ì‹œì ì¸ ë³€í™”ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    decrease_percentage
                ),
                severity: severity.clone(),
                action_steps: vec![
                    "ì ì‹œ í›„(5-10ë¶„) ë‹¤ì‹œ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                    "ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ìˆ˜ë™ìœ¼ë¡œ ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::Medium => DataDecreaseRecommendation {
                action_type: RecommendedAction::ManualVerification,
                description: format!(
                    "ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                    decrease_percentage
                ),
                severity: severity.clone(),
                action_steps: vec![
                    "CSA-IoT ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ì œí’ˆ ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                    "ì‚¬ì´íŠ¸ì—ì„œ í•„í„° ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•˜ê³  ë¶€ë¶„ ì¬í¬ë¡¤ë§ì„ ê³ ë ¤í•˜ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::High => DataDecreaseRecommendation {
                action_type: RecommendedAction::BackupAndRecrawl,
                description: format!(
                    "ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% í¬ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… í›„ ì¬í¬ë¡¤ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.",
                    decrease_percentage
                ),
                severity: severity.clone(),
                action_steps: vec![
                    "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¦‰ì‹œ ë°±ì—…í•˜ì„¸ìš”".to_string(),
                    "CSA-IoT ì‚¬ì´íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ì‹¤ì œ ìƒí™©ì„ íŒŒì•…í•˜ì„¸ìš”".to_string(),
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¹„ìš°ê³  ì „ì²´ ì¬í¬ë¡¤ë§ì„ ìˆ˜í–‰í•˜ì„¸ìš”".to_string(),
                    "í¬ë¡¤ë§ ì™„ë£Œ í›„ ì´ì „ ë°ì´í„°ì™€ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::Critical => DataDecreaseRecommendation {
                action_type: RecommendedAction::BackupAndRecrawl,
                description: format!(
                    "ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ì‹¬ê°í•˜ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    decrease_percentage
                ),
                severity: severity.clone(),
                action_steps: vec![
                    "ğŸš¨ ì¦‰ì‹œ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•˜ì„¸ìš”".to_string(),
                    "CSA-IoT ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ ì‹¤ì œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ì‚¬ì´íŠ¸ êµ¬ì¡°ë‚˜ í•„í„° ì¡°ê±´ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ë°±ì—… í™•ì¸ í›„ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì „ì²´ ì¬í¬ë¡¤ë§í•˜ì„¸ìš”".to_string(),
                    "í¬ë¡¤ë§ ì„¤ì •(selector, URL ë“±)ì„ ì¬ê²€í† í•˜ì„¸ìš”".to_string(),
                ],
            },
        }
    }

    /// í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
    fn has_products_on_page(&self, doc: &scraper::Html) -> bool {
        let product_count = self.count_products(doc);
        product_count > 0
    }

    /// í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
    fn find_max_page_in_pagination(&self, doc: &scraper::Html) -> u32 {
        let mut max_page = 1;

        // 1. í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ì—ì„œ ì°¾ê¸°
        let link_selectors = vec![
            "a[href*='page']",
            ".pagination a",
            ".page-numbers", // ëª¨ë“  í˜ì´ì§€ ë²ˆí˜¸ ìš”ì†Œ (aì™€ span ëª¨ë‘ í¬í•¨)
            ".page-numbers a",
            ".pager a",
            "a[href*='paged']",
            ".page-numbers:not(.current):not(.dots)", // í˜„ì¬ í˜ì´ì§€ì™€ ì¤„ì„í‘œë¥¼ ì œì™¸í•œ í˜ì´ì§€ ë²ˆí˜¸
        ];

        for selector_str in &link_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                for element in doc.select(&selector) {
                    // href ì†ì„±ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    if let Some(href) = element.value().attr("href") {
                        if let Some(page_num) = self.extract_page_number(href) {
                            if page_num > max_page {
                                max_page = page_num;
                                debug!("Found higher page {} in href: {}", page_num, href);
                            }
                        }
                    }

                    // í…ìŠ¤íŠ¸ì—ì„œë„ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    let text = element.text().collect::<String>().trim().to_string();
                    if let Ok(page_num) = text.parse::<u32>() {
                        if page_num > max_page && page_num < 10000 {
                            // í•©ë¦¬ì ì¸ ìƒí•œì„ 
                            max_page = page_num;
                            debug!("Found higher page {} in text: {}", page_num, text);
                        }
                    }
                }
            }
        }

        debug!("Max page found in pagination: {}", max_page);
        max_page
    }

    /// URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
    fn extract_page_number(&self, url: &str) -> Option<u32> {
        // URL íŒ¨í„´: /page/123/ ë˜ëŠ” paged=123
        let patterns = [
            r"/page/(\d+)/?", // CSA-IoT ì‚¬ì´íŠ¸ì˜ /page/123/ íŒ¨í„´
            r"paged=(\d+)",
            r"page=(\d+)",
            r"/(\d+)/$",      // ëì— ìˆ«ìê°€ ìˆëŠ” ê²½ìš°
            r"page/(\d+)/\?", // page/123/?... íŒ¨í„´
        ];

        for pattern in &patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(caps) = re.captures(url) {
                    if let Some(num_str) = caps.get(1) {
                        if let Ok(num) = num_str.as_str().parse::<u32>() {
                            return Some(num);
                        }
                    }
                }
            }
        }

        None
    }

    /// í˜ì´ì§€ì—ì„œ ì œí’ˆ ê°œìˆ˜ ì¹´ìš´íŠ¸ (ëª¨ë“  ì„ íƒìë¥¼ ì‹œë„í•˜ê³  ê°€ì¥ ë§ì€ ê²°ê³¼ ë°˜í™˜)
    fn count_products(&self, doc: &scraper::Html) -> u32 {
        let mut max_count = 0;
        let mut best_selector = "none";

        for selector_str in &self.config.advanced.product_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                let count = doc.select(&selector).count() as u32;
                debug!("Selector '{}' found {} products", selector_str, count);
                if count > max_count {
                    max_count = count;
                    best_selector = selector_str;
                }
            } else {
                debug!("Failed to parse selector: {}", selector_str);
            }
        }

        info!(
            "Total products found on page: {} (using selector: {})",
            max_count, best_selector
        );
        max_count
    }

    /// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¶„ì„
    async fn get_or_analyze_page(&self, page_number: u32) -> Result<PageAnalysisCache> {
        // ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        {
            let cache = self.page_cache.lock().await;
            if let Some(cached) = cache.get(&page_number) {
                debug!("ğŸ“‹ Using cached analysis for page {}", page_number);
                return Ok(cached.clone());
            }
        }

        // ìºì‹œì— ì—†ìœ¼ë©´ ìƒˆë¡œ ë¶„ì„
        debug!("ğŸ” Analyzing page {} (not in cache)", page_number);
        let url = config_utils::matter_products_page_url_simple(page_number);

        let (product_count, max_pagination_page, _active_page, has_products) = {
            // Use consistent HttpClient
            let _client = self.create_configured_http_client()?;
            let response = self.http_client.fetch_response(&url).await?;
            let html_string: String = response.text().await?;

            let doc = scraper::Html::parse_document(&html_string);
            let product_count = self.count_products(&doc);
            let max_pagination_page = self.find_max_page_in_pagination(&doc);
            let active_page = self.get_active_page_number(&doc);
            let has_products = product_count > 0;

            (
                product_count,
                max_pagination_page,
                active_page,
                has_products,
            )
        };

        let analysis = PageAnalysisCache {
            max_page_in_pagination: max_pagination_page,
            product_count,
            has_products,
        };

        // ìºì‹œì— ì €ì¥
        {
            let mut cache = self.page_cache.lock().await;
            cache.insert(page_number, analysis.clone());
        }

        info!(
            "ğŸ“Š Page {} analysis: has_products={}, product_count={}, max_pagination={}",
            page_number, has_products, product_count, max_pagination_page
        );

        Ok(analysis)
    }

    /// ìºì‹œë¥¼ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ìƒíƒœ ì²´í¬ ì‹œì‘ ì‹œ í˜¸ì¶œ)
    async fn clear_page_cache_internal(&self) {
        let mut cache = self.page_cache.lock().await;
        cache.clear();
        debug!("ğŸ—‘ï¸  Page cache cleared");
    }

    /// í¬ë¡¤ë§ ë²”ìœ„ ê¶Œì¥ì‚¬í•­ ê³„ì‚°
    /// ë¡œì»¬ DB ìƒíƒœì™€ ì‚¬ì´íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í¬ë¡¤ë§ ëŒ€ìƒ í˜ì´ì§€ ë²”ìœ„ë¥¼ ê³„ì‚° (ë‚´ë¶€ìš©)
    async fn calculate_crawling_range_recommendation_internal(
        &self,
        total_pages_on_site: u32,
        products_on_last_page: u32,
        estimated_products: u32,
    ) -> Result<CrawlingRangeRecommendation> {
        info!("ğŸ” Calculating crawling range recommendation...");

        // í˜„ì¬ ë¡œì»¬ DB ìƒíƒœ í™•ì¸
        let local_db_status = self.get_local_db_status().await?;

        // DBê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì „ì²´ í¬ë¡¤ë§ ê¶Œì¥
        if local_db_status.is_empty {
            info!("ğŸ“Š Local DB is empty - recommending full crawl");
            return Ok(CrawlingRangeRecommendation::Full);
        }

        // ì‚¬ì´íŠ¸ ë°ì´í„° ë³€í™” ë¶„ì„
        let data_change_analysis = self.analyze_site_data_changes(estimated_products).await;

        // í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚°
        let crawling_range = self
            .calculate_next_crawling_pages(
                &local_db_status,
                total_pages_on_site,
                products_on_last_page,
                estimated_products,
                &data_change_analysis,
            )
            .await?;

        info!("ğŸ“Š Crawling range recommendation: {:?}", crawling_range);
        Ok(crawling_range)
    }

    /// ë¡œì»¬ DB ìƒíƒœ ì¡°íšŒ
    async fn get_local_db_status(&self) -> Result<LocalDbStatus> {
        match &self.product_repo {
            Some(repo) => {
                let products = repo.get_all_products().await?;

                if products.is_empty() {
                    return Ok(LocalDbStatus {
                        is_empty: true,
                        max_page_id: 0,
                        max_index_in_page: 0,
                        total_saved_products: 0,
                    });
                }

                // ê°€ì¥ ë†’ì€ pageIdì™€ í•´ë‹¹ í˜ì´ì§€ì—ì„œì˜ ìµœëŒ€ indexInPage ì°¾ê¸°
                let mut max_page_id = 0i32;
                let mut max_index_in_page = 0i32;

                for product in &products {
                    if let (Some(page_id), Some(index_in_page)) =
                        (product.page_id, product.index_in_page)
                    {
                        if page_id > max_page_id {
                            max_page_id = page_id;
                            max_index_in_page = index_in_page;
                        } else if page_id == max_page_id && index_in_page > max_index_in_page {
                            max_index_in_page = index_in_page;
                        }
                    }
                }

                info!(
                    "ğŸ“Š Local DB status: max_page_id={}, max_index_in_page={}, total_products={}",
                    max_page_id,
                    max_index_in_page,
                    products.len()
                );

                Ok(LocalDbStatus {
                    is_empty: false,
                    max_page_id: max_page_id.max(0) as u32,
                    max_index_in_page: max_index_in_page.max(0) as u32,
                    total_saved_products: products.len() as u32,
                })
            }
            None => {
                warn!("âš ï¸  Product repository not available - assuming empty DB");

                // DB ë¶„ì„ê³¼ ë¡œì»¬ ìƒíƒœê°€ ë¶ˆì¼ì¹˜í•  ìˆ˜ ìˆìŒì„ ê²½ê³ 
                warn!(
                    "âš ï¸  DB inconsistency possible: repository unavailable but analysis may show different results"
                );

                Ok(LocalDbStatus {
                    is_empty: true,
                    max_page_id: 0,
                    max_index_in_page: 0,
                    total_saved_products: 0,
                })
            }
        }
    }

    /// ì‚¬ì´íŠ¸ ë°ì´í„° ë³€í™” ë¶„ì„
    async fn analyze_site_data_changes(
        &self,
        current_estimated_products: u32,
    ) -> DataChangeAnalysis {
        let previous_count = self.config.app_managed.last_crawl_product_count;

        match previous_count {
            None => DataChangeAnalysis::Initial,
            Some(prev_count) => {
                let _change_percentage = if prev_count > 0 {
                    ((current_estimated_products as f64 - prev_count as f64) / prev_count as f64)
                        * 100.0
                } else {
                    0.0
                };

                if current_estimated_products > prev_count {
                    DataChangeAnalysis::Increased {
                        new_products: current_estimated_products - prev_count,
                    }
                } else if current_estimated_products == prev_count {
                    DataChangeAnalysis::Stable
                } else {
                    DataChangeAnalysis::Decreased {
                        lost_products: prev_count - current_estimated_products,
                    }
                }
            }
        }
    }

    /// ë‹¤ìŒ í¬ë¡¤ë§ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚°
    async fn calculate_next_crawling_pages(
        &self,
        local_db_status: &LocalDbStatus,
        total_pages_on_site: u32,
        products_on_last_page: u32,
        _estimated_products: u32,
        data_change_analysis: &DataChangeAnalysis,
    ) -> Result<CrawlingRangeRecommendation> {
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;

        // ë°ì´í„° ë³€í™”ì— ë”°ë¥¸ í¬ë¡¤ë§ ì „ëµ ê²°ì •
        match data_change_analysis {
            DataChangeAnalysis::Initial => {
                info!("ğŸ“Š Initial crawling - recommending full crawl");
                return Ok(CrawlingRangeRecommendation::Full);
            }
            DataChangeAnalysis::Decreased { lost_products, .. } => {
                warn!(
                    "ğŸ“‰ Site data decreased by {} products - recommending full recrawl",
                    lost_products
                );
                return Ok(CrawlingRangeRecommendation::Full);
            }
            DataChangeAnalysis::Increased { new_products, .. } => {
                // ìƒˆë¡œìš´ ì œí’ˆì´ ë§ì´ ì¶”ê°€ëœ ê²½ìš° ë¶€ë¶„ í¬ë¡¤ë§
                let recommended_pages =
                    (*new_products as f64 / products_per_page as f64).ceil() as u32;
                let limited_pages =
                    recommended_pages.min(self.config.user.crawling.page_range_limit);

                info!(
                    "ğŸ“ˆ Site data increased by {} products - recommending partial crawl of {} pages",
                    new_products, limited_pages
                );
                return Ok(CrawlingRangeRecommendation::Partial(limited_pages));
            }
            DataChangeAnalysis::Stable => {
                // ì•ˆì •ì ì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì ìš©
            }
        }

        // ê¸°ì¡´ ë¡œì§: ë¡œì»¬ DB ìƒíƒœ ê¸°ë°˜ ê³„ì‚°
        if local_db_status.is_empty {
            return Ok(CrawlingRangeRecommendation::Full);
        }

        // 1ë‹¨ê³„: ë¡œì»¬ DBì— ë§ˆì§€ë§‰ìœ¼ë¡œ ì €ì¥ëœ ì œí’ˆì˜ 'ì—­ìˆœ ì ˆëŒ€ ì¸ë±ìŠ¤' ê³„ì‚°
        let last_saved_index =
            (local_db_status.max_page_id * products_per_page) + local_db_status.max_index_in_page;
        info!("ğŸ“Š Last saved product index: {}", last_saved_index);

        // 2ë‹¨ê³„: ë‹¤ìŒì— í¬ë¡¤ë§í•´ì•¼ í•  ì œí’ˆì˜ 'ì—­ìˆœ ì ˆëŒ€ ì¸ë±ìŠ¤' ê²°ì •
        let next_product_index = last_saved_index + 1;
        info!("ğŸ“Š Next product index to crawl: {}", last_saved_index);

        // 3ë‹¨ê³„: 'ì—­ìˆœ ì ˆëŒ€ ì¸ë±ìŠ¤'ë¥¼ ì›¹ì‚¬ì´íŠ¸ í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë³€í™˜
        let total_products =
            ((total_pages_on_site - 1) * products_per_page) + products_on_last_page;

        // ë‹¤ìŒ ì œí’ˆì´ ì „ì²´ ì œí’ˆ ìˆ˜ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° (ëª¨ë“  ì œí’ˆ í¬ë¡¤ë§ ì™„ë£Œ)
        if next_product_index >= total_products {
            info!("ğŸ“Š All products have been crawled - no crawling needed");
            return Ok(CrawlingRangeRecommendation::None);
        }

        // 'ìˆœì°¨ ì¸ë±ìŠ¤'ë¡œ ë³€í™˜ (ìµœì‹  ì œí’ˆì´ 0)
        let forward_index = (total_products - 1) - next_product_index;

        // ì›¹ì‚¬ì´íŠ¸ í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚°
        let target_page_number = (forward_index / products_per_page) + 1;

        info!("ğŸ“Š Target page to start crawling: {}", target_page_number);

        // 4ë‹¨ê³„: í¬ë¡¤ë§ í˜ì´ì§€ ë²”ìœ„ ê²°ì •
        let max_crawl_pages = self.config.user.crawling.page_range_limit;
        let start_page = target_page_number;
        let end_page = if start_page >= max_crawl_pages {
            start_page - max_crawl_pages + 1
        } else {
            1
        };

        let actual_pages_to_crawl = if start_page >= end_page {
            start_page - end_page + 1
        } else {
            start_page
        };

        info!(
            "ğŸ“Š Crawling range: pages {} to {} (total: {} pages)",
            start_page, end_page, actual_pages_to_crawl
        );

        Ok(CrawlingRangeRecommendation::Partial(actual_pages_to_crawl))
    }
}

/// ë¡œì»¬ DB ìƒíƒœ ì •ë³´
#[derive(Debug, Clone)]
struct LocalDbStatus {
    is_empty: bool,
    max_page_id: u32,
    max_index_in_page: u32,
    total_saved_products: u32,
}

/// ë°ì´í„° ë³€í™” ë¶„ì„ ê²°ê³¼
#[derive(Debug, Clone)]
enum DataChangeAnalysis {
    Initial,
    Increased { new_products: u32 },
    Decreased { lost_products: u32 },
    Stable,
}

/// ì»¬ë ‰í„° ì„¤ì • (Modern Rust 2024 ì¤€ìˆ˜)
///
/// ValidatedCrawlingConfigì—ì„œ ê²€ì¦ëœ ê°’ì„ ì‚¬ìš©í•˜ì—¬ í•˜ë“œì½”ë”©ì„ ë°©ì§€í•©ë‹ˆë‹¤.
#[derive(Debug, Clone)]
pub struct CollectorConfig {
    pub batch_size: u32,
    pub max_concurrent: u32,
    pub concurrency: u32, // alias for max_concurrent
    pub delay_between_requests: Duration,
    pub delay_ms: u64, // alias for delay_between_requests in milliseconds
    pub retry_attempts: u32,
    pub retry_max: u32, // alias for retry_attempts
}

impl CollectorConfig {
    /// ValidatedCrawlingConfigì—ì„œ CollectorConfig ìƒì„±
    ///
    /// # Arguments
    /// * `validated_config` - ê²€ì¦ëœ í¬ë¡¤ë§ ì„¤ì •
    ///
    /// # Returns
    /// ì„¤ì •ê°’ì´ ì ìš©ëœ CollectorConfig
    #[must_use]
    pub fn from_validated(
        validated_config: &crate::application::validated_crawling_config::ValidatedCrawlingConfig,
    ) -> Self {
        let delay_ms = validated_config.request_delay().as_millis() as u64;

        Self {
            batch_size: validated_config.batch_size(),
            max_concurrent: validated_config.max_concurrent(),
            concurrency: validated_config.max_concurrent(), // alias
            delay_between_requests: validated_config.request_delay(),
            delay_ms,
            retry_attempts: validated_config.max_retries(),
            retry_max: validated_config.max_retries(), // alias
        }
    }
}

impl Default for CollectorConfig {
    /// ê¸°ë³¸ê°’ì€ ValidatedCrawlingConfig::default()ì—ì„œ ê°€ì ¸ì˜´
    /// í•˜ë“œì½”ë”©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ValidatedCrawlingConfigë¥¼ ì‚¬ìš©
    fn default() -> Self {
        let validated_config =
            crate::application::validated_crawling_config::ValidatedCrawlingConfig::default();
        Self::from_validated(&validated_config)
    }
}

/// í—¬ìŠ¤ ìŠ¤ì½”ì–´ ê³„ì‚° í•¨ìˆ˜
fn calculate_health_score(response_time: Duration, total_pages: u32) -> f64 {
    // ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 0.7) - ë” ê´€ëŒ€í•œ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •
    let response_score = if response_time.as_millis() <= 2000 {
        0.7 // 2ì´ˆ ì´í•˜ëŠ” ì–‘í˜¸
    } else if response_time.as_millis() <= 5000 {
        0.5 // 5ì´ˆ ì´í•˜ëŠ” ë³´í†µ
    } else if response_time.as_millis() <= 10000 {
        0.3 // 10ì´ˆ ì´í•˜ëŠ” ëŠë¦¼
    } else {
        0.1 // 10ì´ˆ ì´ˆê³¼ëŠ” ë¬¸ì œ
    };

    // í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 0.3) - í˜ì´ì§€ ë°œê²¬ ì—¬ë¶€ê°€ ë” ì¤‘ìš”
    let page_score = if total_pages > 0 { 0.3 } else { 0.0 };

    response_score + page_score
}

/// ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductListCollectorImpl {
    http_client: Arc<HttpClient>, // ğŸ”¥ Mutex ì œê±° - í˜ì´ì§€ ìˆ˜ì§‘ë„ ì§„ì •í•œ ë™ì‹œì„± êµ¬í˜„
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
    status_checker: Arc<StatusCheckerImpl>,
}

impl ProductListCollectorImpl {
    pub fn new(
        http_client: Arc<HttpClient>, // ğŸ”¥ Mutex ì œê±°
        data_extractor: Arc<MatterDataExtractor>,
        config: CollectorConfig,
        status_checker: Arc<StatusCheckerImpl>,
    ) -> Self {
        Self {
            http_client,
            data_extractor,
            config,
            status_checker,
        }
    }

    /// ğŸ”¥ ë™ì‹œì„±ì„ ë³´ì¥í•˜ëŠ” ì´ë²¤íŠ¸ ê¸°ë°˜ í˜ì´ì§€ ìˆ˜ì§‘ ë©”ì„œë“œ (ë¹„ë™ê¸° ì´ë²¤íŠ¸ í ì‚¬ìš©)
    pub async fn collect_page_range_with_async_events(
        &self,
        start_page: u32,
        end_page: u32,
        cancellation_token: Option<CancellationToken>,
        session_id: String,
        batch_id: String,
    ) -> Result<Vec<ProductUrl>> {
        use tokio::sync::mpsc;

        // ğŸ”¥ ë¹„ë™ê¸° ì´ë²¤íŠ¸ í ìƒì„± (ë…¼ë¸”ë¡œí‚¹)
        let (event_tx, mut event_rx) = mpsc::unbounded_channel::<PageEvent>();

        // ğŸ”¥ ì´ë²¤íŠ¸ ì²˜ë¦¬ê¸°ë¥¼ ë³„ë„ íƒœìŠ¤í¬ë¡œ ë¶„ë¦¬ (ë©”ì¸ ì‘ì—…ê³¼ ë…ë¦½ì )
        let session_id_clone = session_id.clone();
        let batch_id_clone = batch_id.clone();
        tokio::spawn(async move {
            while let Some(page_event) = event_rx.recv().await {
                // ì´ë²¤íŠ¸ ì²˜ë¦¬ëŠ” ë©”ì¸ ì‘ì—… íë¦„ê³¼ ì™„ì „íˆ ë…ë¦½ì 
                // ì‹¤íŒ¨í•´ë„ ë©”ì¸ ì‘ì—…ì— ì˜í–¥ ì—†ìŒ
                if let Err(e) =
                    Self::handle_page_event(page_event, &session_id_clone, &batch_id_clone).await
                {
                    debug!("Event handling failed (non-critical): {}", e);
                }
            }
        });

        // Handle descending range (older to newer) - typical for our use case
        let pages: Vec<u32> = if start_page > end_page {
            (end_page..=start_page).rev().collect()
        } else {
            (start_page..=end_page).collect()
        };

        info!(
            "ğŸ” Collecting from {} pages in range {} to {} with true concurrent execution + async events",
            pages.len(),
            start_page,
            end_page
        );

        // ì‚¬ì´íŠ¸ ë¶„ì„ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ì •í™•í•œ ì´ í˜ì´ì§€ ìˆ˜ì™€ ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜ í™•ì¸
        let (total_pages, products_on_last_page) =
            self.status_checker.discover_total_pages().await?;
        let last_page_number = total_pages;
        let products_in_last_page = products_on_last_page;

        // CanonicalPageIdCalculator ì´ˆê¸°í™” (legacy êµ¬í˜„ alias)
        let page_calculator =
            CanonicalPageIdCalculator::new(last_page_number, products_in_last_page as usize);
        let max_concurrent = self.config.max_concurrent as usize;

        // ì§„ì •í•œ ë™ì‹œì„± ì‹¤í–‰ì„ ìœ„í•œ ì„¸ë§ˆí¬ì–´ ê¸°ë°˜ ì²˜ë¦¬
        let semaphore = Arc::new(tokio::sync::Semaphore::new(max_concurrent));
        let mut tasks = Vec::new();

        info!(
            "ğŸš€ Creating {} concurrent tasks with semaphore control (max: {})",
            pages.len(),
            max_concurrent
        );

        for page in pages {
            // ì·¨ì†Œ í† í° í™•ì¸
            if let Some(ref token) = cancellation_token {
                if token.is_cancelled() {
                    warn!("ğŸ›‘ Task creation cancelled for page {}", page);
                    break;
                }
            }

            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let semaphore_clone = Arc::clone(&semaphore);
            let calculator = page_calculator.clone();
            let event_tx_clone = event_tx.clone();
            let cancellation_token_clone = cancellation_token.clone();

            // ê° íƒœìŠ¤í¬ëŠ” ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰
            let task = tokio::spawn(async move {
                // ğŸ”¥ ë…¼ë¸”ë¡œí‚¹ ì´ë²¤íŠ¸ ë°œì†¡ (ì‹¤íŒ¨í•´ë„ ë©”ì¸ ì‘ì—… ê³„ì†)
                let _ = event_tx_clone.send(PageEvent::Started { page_number: page });

                // ì‹¤í–‰ í—ˆê°€ë¥¼ ë°›ì„ ë•Œê¹Œì§€ ëŒ€ê¸° (ì§„ì •í•œ ë™ì‹œì„± ì œì–´)
                let _permit = match semaphore_clone.acquire().await {
                    Ok(permit) => {
                        debug!("ğŸ”“ Acquired permit for page {}", page);
                        permit
                    }
                    Err(_) => {
                        let _ = event_tx_clone.send(PageEvent::Failed {
                            page_number: page,
                            error: "Semaphore acquisition failed".to_string(),
                        });
                        return Err(anyhow!("Semaphore acquisition failed"));
                    }
                };

                // ì·¨ì†Œ í™•ì¸
                if let Some(ref token) = cancellation_token_clone {
                    if token.is_cancelled() {
                        let _ = event_tx_clone.send(PageEvent::Cancelled { page_number: page });
                        return Err(anyhow!("Task cancelled"));
                    }
                }

                // ì‹¤ì œ í˜ì´ì§€ ìˆ˜ì§‘ ì‘ì—… (ì™„ì „íˆ ë…ë¦½ì )
                let start_time = std::time::Instant::now();
                let result = Self::collect_single_page_independently(
                    http_client,
                    data_extractor,
                    calculator,
                    page,
                )
                .await;

                let duration_ms = start_time.elapsed().as_millis() as u64;

                // ğŸ”¥ ê²°ê³¼ì— ë”°ë¥¸ ë…¼ë¸”ë¡œí‚¹ ì´ë²¤íŠ¸ ë°œì†¡
                match &result {
                    Ok(products) => {
                        let _ = event_tx_clone.send(PageEvent::Completed {
                            page_number: page,
                            products_found: products.len() as u32,
                            duration_ms,
                        });
                    }
                    Err(e) => {
                        let _ = event_tx_clone.send(PageEvent::Failed {
                            page_number: page,
                            error: e.to_string(),
                        });
                    }
                }

                debug!("ğŸ”— Page {} processing completed (permit released)", page);
                result.map(|products| (page, products))
            });

            tasks.push(task);
        }

        info!(
            "âœ… Created {} tasks, waiting for all to complete with concurrent execution",
            tasks.len()
        );

        // ëª¨ë“  íƒœìŠ¤í¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ (ì§„ì •í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
        let results = futures::future::join_all(tasks).await;

        // ê²°ê³¼ ìˆ˜ì§‘
        let mut all_urls = Vec::new();
        let mut successful_pages = 0;
        let mut failed_pages = 0;

        for result in results {
            match result {
                Ok(Ok((page, mut urls))) => {
                    debug!("âœ… Page {} completed with {} URLs", page, urls.len());
                    all_urls.append(&mut urls);
                    successful_pages += 1;
                }
                Ok(Err(e)) => {
                    warn!("âŒ Page processing failed: {}", e);
                    failed_pages += 1;
                }
                Err(e) => {
                    warn!("âŒ Task join failed: {}", e);
                    failed_pages += 1;
                }
            }
        }

        info!(
            "ğŸ¯ Phase 5 concurrent collection completed: {} pages successful, {} failed, {} total URLs",
            successful_pages,
            failed_pages,
            all_urls.len()
        );

        Ok(all_urls)
    }

    /// ğŸ”¥ ì™„ì „íˆ ë…ë¦½ì ì¸ ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘ (ì˜ì¡´ì„± ìµœì†Œí™”)
    async fn collect_single_page_independently(
        http_client: Arc<HttpClient>, // ğŸ”¥ Mutex ì œê±° - í˜ì´ì§€ ìˆ˜ì§‘ë„ ì§„ì •í•œ ë™ì‹œì„±
        data_extractor: Arc<MatterDataExtractor>,
        calculator: CanonicalPageIdCalculator,
        page: u32,
    ) -> Result<Vec<ProductUrl>> {
        let url = format!(
            "https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver",
            page
        );

        // ğŸ”¥ Mutex ì œê±° - HTTP í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì‚¬ìš©ìœ¼ë¡œ ì§„ì •í•œ ë™ì‹œì„±
        let response = http_client.fetch_response(&url).await?;
        let html_string: String = response.text().await?;

        let doc = scraper::Html::parse_document(&html_string);
        let url_strings = data_extractor.extract_product_urls(&doc, "https://csa-iot.org")?;

        // Convert URLs to ProductUrl with proper pageId and indexInPage calculation
        let product_urls: Vec<ProductUrl> = url_strings
            .into_iter()
            .enumerate()
            .map(|(index, url)| {
                let calculation = calculator.calculate(page, index);
                ProductUrl {
                    url,
                    page_id: calculation.page_id,
                    index_in_page: calculation.index_in_page,
                }
            })
            .collect();

        // ğŸ” Debug summary for verification of page_id/index_in_page mapping
        if !product_urls.is_empty() {
            let min_page_id = product_urls.iter().map(|p| p.page_id).min().unwrap_or(0);
            let max_page_id = product_urls.iter().map(|p| p.page_id).max().unwrap_or(0);
            let min_index = product_urls
                .iter()
                .map(|p| p.index_in_page)
                .min()
                .unwrap_or(0);
            let max_index = product_urls
                .iter()
                .map(|p| p.index_in_page)
                .max()
                .unwrap_or(0);
            let sample: Vec<String> = product_urls
                .iter()
                .take(6)
                .enumerate()
                .map(|(i, p)| format!("i{}=>p{}_i{}", i, p.page_id, p.index_in_page))
                .collect();
            debug!(
                "ğŸ“ Page {} mapping summary: count={}, page_id=[{}..{}], index_in_page=[{}..{}], sample={:?}",
                page,
                product_urls.len(),
                min_page_id,
                max_page_id,
                min_index,
                max_index,
                sample
            );
        } else {
            debug!(
                "ğŸ“ Page {} produced no product URLs for mapping summary",
                page
            );
        }

        Ok(product_urls)
    }

    /// ğŸ”¥ ì´ë²¤íŠ¸ ì²˜ë¦¬ê¸° (ë¹„ë™ê¸°, ë…¼ë¸”ë¡œí‚¹)
    async fn handle_page_event(event: PageEvent, _session_id: &str, _batch_id: &str) -> Result<()> {
        // ì‹¤ì œ ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŒ… ë¡œì§
        // ì´ í•¨ìˆ˜ëŠ” ë©”ì¸ ì‘ì—…ê³¼ ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ë¨
        match event {
            PageEvent::Started { page_number } => {
                debug!("ğŸ“„ Page {} started", page_number);
                // SystemStateBroadcaster::emit_product_list_page_event() í˜¸ì¶œ
            }
            PageEvent::Completed {
                page_number,
                products_found,
                duration_ms,
            } => {
                debug!(
                    "âœ… Page {} completed: {} products in {}ms",
                    page_number, products_found, duration_ms
                );
                // ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
            }
            PageEvent::Failed { page_number, error } => {
                debug!("âŒ Page {} failed: {}", page_number, error);
                // ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡
            }
            PageEvent::Cancelled { page_number } => {
                debug!("ğŸ›‘ Page {} cancelled", page_number);
                // ì·¨ì†Œ ì´ë²¤íŠ¸ ë°œì†¡
            }
        }
        Ok(())
    }
}

/// ğŸ”¥ ProductDetail íƒœìŠ¤í¬ ì´ë²¤íŠ¸ íƒ€ì…
#[derive(Debug, Clone)]
enum ProductDetailEvent {
    TaskStarted {
        product_url: String,
        product_name: Option<String>,
        task_id: String,
    },
    HttpRequestStarted {
        product_url: String,
        task_id: String,
    },
    ParsingStarted {
        product_url: String,
        task_id: String,
        html_size: usize,
    },
    TaskCompleted {
        product_url: String,
        product_name: Option<String>,
        task_id: String,
        processing_time: std::time::Duration,
        extracted_fields: u32,
    },
    TaskFailed {
        product_url: String,
        task_id: String,
        error: String,
        processing_time: std::time::Duration,
    },
}

#[async_trait]
impl ProductListCollector for ProductListCollectorImpl {
    async fn collect_page_batch(
        &self,
        pages: &[u32],
        total_pages: u32,
        products_on_last_page: u32,
    ) -> Result<Vec<ProductUrl>> {
        info!(
            "ğŸ” Collecting batch of {} pages with stateless design",
            pages.len()
        );

        let mut all_urls = Vec::new();
        for &page in pages {
            match self
                .collect_single_page(page, total_pages, products_on_last_page)
                .await
            {
                Ok(mut urls) => {
                    all_urls.append(&mut urls);
                    debug!("âœ… Page {} completed with {} URLs", page, urls.len());
                }
                Err(e) => {
                    error!("âŒ Failed to collect page {}: {}", page, e);
                    continue;
                }
            }
        }

        info!(
            "ğŸ“Š Batch collection completed: {} total URLs from {} pages",
            all_urls.len(),
            pages.len()
        );
        Ok(all_urls)
    }

    fn as_any(&self) -> &dyn Any {
        self
    }
    async fn collect_all_pages(
        &self,
        total_pages: u32,
        products_on_last_page: u32,
    ) -> Result<Vec<ProductUrl>> {
        info!(
            "ğŸ” Collecting from {} pages with stateless parallel processing",
            total_pages
        );

        // Use the existing parallel implementation from collect_page_range
        self.collect_page_range(1, total_pages, total_pages, products_on_last_page)
            .await
    }

    async fn collect_page_range(
        &self,
        start_page: u32,
        end_page: u32,
        total_pages: u32,
        products_on_last_page: u32,
    ) -> Result<Vec<ProductUrl>> {
        // Handle descending range (older to newer) - typical for our use case
        let pages: Vec<u32> = if start_page > end_page {
            // Descending range: start from oldest (highest page number) to newest (lower page number)
            (end_page..=start_page).rev().collect()
        } else {
            // Ascending range: start from lowest to highest page number
            (start_page..=end_page).collect()
        };

        info!(
            "ğŸ” Collecting from {} pages in range {} to {} with stateless execution",
            pages.len(),
            start_page,
            end_page
        );

        // âœ… Clean Code: ëª…ì‹œì  íŒŒë¼ë¯¸í„° ì‚¬ìš© (ìƒíƒœ ì˜ì¡´ì„± ì œê±°)
        info!(
            "ğŸ“Š Using explicit parameters: total_pages={}, products_on_last_page={}",
            total_pages, products_on_last_page
        );

        // CanonicalPageIdCalculator ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ìƒì„±)
        let page_calculator =
            CanonicalPageIdCalculator::new(total_pages, products_on_last_page as usize);

        let max_concurrent = self.config.max_concurrent as usize;

        // Phase 5 Implementation: ì§„ì •í•œ ë™ì‹œì„± ì‹¤í–‰ì„ ìœ„í•œ ì„¸ë§ˆí¬ì–´ ê¸°ë°˜ ì²˜ë¦¬
        // 1. ì„¸ë§ˆí¬ì–´ ìƒì„±: max_concurrent ê°œì˜ permitë§Œ í—ˆìš©
        let semaphore = Arc::new(Semaphore::new(max_concurrent));

        // 2. ëª¨ë“  í˜ì´ì§€ì— ëŒ€í•´ ì¦‰ì‹œ íƒœìŠ¤í¬ ìƒì„± (í•˜ì§€ë§Œ ì„¸ë§ˆí¬ì–´ë¡œ ì œì–´)
        let mut tasks = Vec::new();

        info!(
            "ğŸš€ Creating {} concurrent tasks with semaphore control (max: {})",
            pages.len(),
            max_concurrent
        );

        for page in pages {
            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let status_checker = Arc::clone(&self.status_checker);
            let semaphore_clone = Arc::clone(&semaphore);
            let calculator = page_calculator.clone(); // CanonicalPageIdCalculator í´ë¡ 

            // 3. ê° íƒœìŠ¤í¬ëŠ” ì„¸ë§ˆí¬ì–´ permitì„ íšë“í•œ í›„ ì‹¤í–‰
            let task = tokio::spawn(async move {
                // ì‹¤í–‰ í—ˆê°€ë¥¼ ë°›ì„ ë•Œê¹Œì§€ ëŒ€ê¸° (ì§„ì •í•œ ë™ì‹œì„± ì œì–´)
                let _permit = match semaphore_clone.acquire().await {
                    Ok(permit) => {
                        debug!("ğŸ”“ Acquired permit for page {}", page);
                        permit
                    }
                    Err(_) => {
                        error!("Failed to acquire semaphore permit for page {}", page);
                        return Err(anyhow!("Semaphore acquisition failed"));
                    }
                };

                // âœ… PageIdCalculatorë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ ë° URL ìƒì„±
                let url = format!(
                    "https://csa-iot.org/csa-iot_products/page/{}/?p_keywords&p_type%5B0%5D=14&p_program_type%5B0%5D=1049&p_certificate&p_family&p_firmware_ver",
                    page
                );
                // Use consistent HttpClient for true concurrency
                let response = http_client.fetch_response(&url).await?;
                let html_string: String = response.text().await?;

                let doc = scraper::Html::parse_document(&html_string);
                let url_strings =
                    data_extractor.extract_product_urls(&doc, "https://csa-iot.org")?;

                // í™œì„± í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ (ë¦¬ë‹¤ì´ë ‰íŠ¸/í˜ì´ì§€ë„¤ì´ì…˜ ë¶ˆì¼ì¹˜ ë°©ì§€)
                let active_page = status_checker.get_active_page_number(&doc);
                if active_page != page {
                    tracing::warn!(
                        "âš ï¸ Requested page {} but active pagination indicates {}. Using {} for page_id calculation.",
                        page,
                        active_page,
                        active_page
                    );
                }
                let effective_page = active_page.max(1);

                // âœ… PageIdCalculatorë¥¼ ì‚¬ìš©í•œ ProductUrl ìƒì„±
                let product_urls: Vec<ProductUrl> = url_strings
                    .into_iter()
                    .enumerate()
                    .map(|(index, url)| {
                        let calculation = calculator.calculate(effective_page, index);
                        ProductUrl {
                            url,
                            page_id: calculation.page_id,
                            index_in_page: calculation.index_in_page,
                        }
                    })
                    .collect();

                debug!(
                    "ğŸ”— Extracted {} URLs from page {} (permit released)",
                    product_urls.len(),
                    page
                );
                Ok::<(u32, Vec<ProductUrl>), anyhow::Error>((page, product_urls))
                // _permitì´ ì—¬ê¸°ì„œ ìë™ìœ¼ë¡œ dropë˜ì–´ ë‹¤ìŒ íƒœìŠ¤í¬ê°€ ì‹¤í–‰ë  ìˆ˜ ìˆìŒ
            });

            tasks.push(task);
        }

        info!(
            "âœ… Created {} tasks, waiting for all to complete with concurrent execution",
            tasks.len()
        );

        // 4. ëª¨ë“  íƒœìŠ¤í¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ (ì§„ì •í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
        let results = futures::future::join_all(tasks).await;

        // ê²°ê³¼ ìˆ˜ì§‘
        let mut all_urls = Vec::new();
        let mut successful_pages = 0;
        let mut failed_pages = 0;

        for result in results {
            match result {
                Ok(Ok((page, urls))) => {
                    all_urls.extend(urls);
                    successful_pages += 1;
                    debug!("âœ… Page {} completed successfully", page);
                }
                Ok(Err(e)) => {
                    error!("âŒ Page collection failed: {}", e);
                    failed_pages += 1;
                }
                Err(e) => {
                    error!("âŒ Task join failed: {}", e);
                    failed_pages += 1;
                }
            }
        }

        info!(
            "ğŸ“Š Concurrent collection completed: {} successful, {} failed, {} total URLs",
            successful_pages,
            failed_pages,
            all_urls.len()
        );

        Ok(all_urls)
    }

    async fn collect_single_page(
        &self,
        page: u32,
        total_pages: u32,
        products_on_last_page: u32,
    ) -> Result<Vec<ProductUrl>> {
        // âœ… Clean Code: ëª…ì‹œì  íŒŒë¼ë¯¸í„° ì‚¬ìš© (ìƒíƒœ ì˜ì¡´ì„± ì œê±°)

        info!(
            "ğŸ“Š Using cached site analysis for single page {}: total_pages={}, products_on_last_page={}",
            page, total_pages, products_on_last_page
        );

        let page_calculator =
            CanonicalPageIdCalculator::new(total_pages, products_on_last_page as usize);

        let url = crate::infrastructure::config::utils::matter_products_page_url_simple(page);
        // Use policy-based HttpClient to respect status-based retry and Retry-After
        let response = self.http_client.fetch_response_with_policy(&url).await?;
        let html_string: String = response.text().await?;

        let doc = scraper::Html::parse_document(&html_string);
        let url_strings = self
            .data_extractor
            .extract_product_urls(&doc, "https://csa-iot.org")?;

        // í™œì„± í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸ í›„ ë³´ì •
        let active_page = self.status_checker.get_active_page_number(&doc);
        if active_page != page {
            tracing::warn!(
                "âš ï¸ Requested page {} but active pagination indicates {}. Using {} for page_id calculation.",
                page,
                active_page,
                active_page
            );
        }
        let effective_page = active_page.max(1);

        // âœ… PageIdCalculatorë¥¼ ì‚¬ìš©í•œ ProductUrl ìƒì„±
        let product_urls: Vec<ProductUrl> = url_strings
            .into_iter()
            .enumerate()
            .map(|(index, url)| {
                let calculation = page_calculator.calculate(effective_page, index);
                ProductUrl {
                    url,
                    page_id: calculation.page_id,
                    index_in_page: calculation.index_in_page,
                }
            })
            .collect();

        debug!(
            "ğŸ”— Extracted {} URLs from page {}",
            product_urls.len(),
            page
        );
        Ok(product_urls)
    }

    async fn collect_page_range_with_cancellation(
        &self,
        start_page: u32,
        end_page: u32,
        total_pages: u32,
        products_on_last_page: u32,
        cancellation_token: CancellationToken,
    ) -> Result<Vec<ProductUrl>> {
        // Handle descending range (older to newer) - typical for our use case
        let pages: Vec<u32> = if start_page > end_page {
            // Descending range: start from oldest (highest page number) to newest (lower page number)
            (end_page..=start_page).rev().collect()
        } else {
            // Ascending range: start from lowest to highest page number
            (start_page..=end_page).collect()
        };

        info!(
            "ğŸ” Collecting from {} pages in range {} to {} with cancellation support and stateless execution",
            pages.len(),
            start_page,
            end_page
        );

        // âœ… Clean Code: ëª…ì‹œì  íŒŒë¼ë¯¸í„° ì‚¬ìš© (ìƒíƒœ ì˜ì¡´ì„± ì œê±°)

        info!(
            "ğŸ“Š Using explicit parameters: total_pages={}, products_on_last_page={}",
            total_pages, products_on_last_page
        );

        // CanonicalPageIdCalculator ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ìƒì„±)
        let page_calculator =
            CanonicalPageIdCalculator::new(total_pages, products_on_last_page as usize);

        let max_concurrent = self.config.max_concurrent as usize;

        // Phase 5 Implementation: ì§„ì •í•œ ë™ì‹œì„± ì‹¤í–‰ì„ ìœ„í•œ ì„¸ë§ˆí¬ì–´ ê¸°ë°˜ ì²˜ë¦¬
        // 1. ì„¸ë§ˆí¬ì–´ ìƒì„±: max_concurrent ê°œì˜ permitë§Œ í—ˆìš©
        let semaphore = Arc::new(Semaphore::new(max_concurrent));

        // 2. ëª¨ë“  í˜ì´ì§€ì— ëŒ€í•´ ì¦‰ì‹œ íƒœìŠ¤í¬ ìƒì„± (í•˜ì§€ë§Œ ì„¸ë§ˆí¬ì–´ë¡œ ì œì–´)
        let mut tasks = Vec::new();

        info!(
            "ğŸš€ Creating {} concurrent tasks with semaphore control (max: {})",
            pages.len(),
            max_concurrent
        );

        for page in pages {
            // ì·¨ì†Œ í† í° í™•ì¸
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Task creation cancelled for page {}", page);
                break;
            }

            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let status_checker = Arc::clone(&self.status_checker);
            let token_clone = cancellation_token.clone();
            let semaphore_clone = Arc::clone(&semaphore);
            let calculator = page_calculator.clone(); // CanonicalPageIdCalculator í´ë¡ 

            // 3. ê° íƒœìŠ¤í¬ëŠ” ì„¸ë§ˆí¬ì–´ permitì„ íšë“í•œ í›„ ì‹¤í–‰
            let task = tokio::spawn(async move {
                // ì‹¤í–‰ í—ˆê°€ë¥¼ ë°›ì„ ë•Œê¹Œì§€ ëŒ€ê¸°
                let _permit = match semaphore_clone.acquire().await {
                    Ok(permit) => {
                        debug!("ğŸ”“ Acquired permit for page {}", page);
                        permit
                    }
                    Err(_) => {
                        error!("Failed to acquire semaphore permit for page {}", page);
                        return Err(anyhow!("Semaphore acquisition failed"));
                    }
                };

                // ì‘ì—… ì‹œì‘ ì „ ì·¨ì†Œ í™•ì¸
                if token_clone.is_cancelled() {
                    warn!("ğŸ›‘ Task cancelled before execution for page {}", page);
                    return Err(anyhow!("Task cancelled"));
                }

                // ì‹¤ì œ í˜ì´ì§€ ìˆ˜ì§‘ ì‘ì—…
                let url =
                    crate::infrastructure::config::utils::matter_products_page_url_simple(page);
                // Use consistent HttpClient for true concurrency
                let response = http_client.fetch_response(&url).await?;
                let html_string: String = response.text().await?;

                // ì¤‘ê°„ì— ì·¨ì†Œ í™•ì¸
                if token_clone.is_cancelled() {
                    warn!("ğŸ›‘ Task cancelled during processing for page {}", page);
                    return Err(anyhow!("Task cancelled during processing"));
                }

                let doc = scraper::Html::parse_document(&html_string);
                let url_strings =
                    data_extractor.extract_product_urls(&doc, "https://csa-iot.org")?;

                // í™œì„± í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ë° ë³´ì •
                let active_page = status_checker.get_active_page_number(&doc);
                if active_page != page {
                    tracing::warn!(
                        "âš ï¸ Requested page {} but active pagination indicates {}. Using {} for page_id calculation.",
                        page,
                        active_page,
                        active_page
                    );
                }
                let effective_page = active_page.max(1);

                // âœ… PageIdCalculatorë¥¼ ì‚¬ìš©í•œ ProductUrl ìƒì„±
                let product_urls: Vec<ProductUrl> = url_strings
                    .into_iter()
                    .enumerate()
                    .map(|(index, url)| {
                        let calculation = calculator.calculate(effective_page, index);
                        ProductUrl {
                            url,
                            page_id: calculation.page_id,
                            index_in_page: calculation.index_in_page,
                        }
                    })
                    .collect();

                debug!(
                    "ğŸ”— Extracted {} URLs from page {} (permit released)",
                    product_urls.len(),
                    page
                );
                Ok::<(u32, Vec<ProductUrl>), anyhow::Error>((page, product_urls))
                // _permitì´ ì—¬ê¸°ì„œ ìë™ìœ¼ë¡œ dropë˜ì–´ ë‹¤ìŒ íƒœìŠ¤í¬ê°€ ì‹¤í–‰ë  ìˆ˜ ìˆìŒ
            });

            tasks.push(task);
        }

        info!(
            "âœ… Created {} tasks, waiting for all to complete with concurrent execution",
            tasks.len()
        );

        // 4. ëª¨ë“  íƒœìŠ¤í¬ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ (ì§„ì •í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
        let results = futures::future::join_all(tasks).await;

        // ê²°ê³¼ ìˆ˜ì§‘
        let mut all_urls = Vec::new();
        let mut successful_pages = 0;
        let mut failed_pages = 0;

        for result in results {
            match result {
                Ok(Ok((page, urls))) => {
                    all_urls.extend(urls);
                    successful_pages += 1;
                    debug!("âœ… Page {} completed successfully", page);
                }
                Ok(Err(e)) => {
                    error!("âŒ Page collection failed: {}", e);
                    failed_pages += 1;
                }
                Err(e) => {
                    error!("âŒ Task join failed: {}", e);
                    failed_pages += 1;
                }
            }
        }

        info!(
            "ğŸ“Š Concurrent collection completed: {} successful, {} failed, {} total URLs",
            successful_pages,
            failed_pages,
            all_urls.len()
        );

        Ok(all_urls)
    }
}

/// ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct DatabaseAnalyzerImpl {
    product_repo: Arc<IntegratedProductRepository>,
}

impl DatabaseAnalyzerImpl {
    pub fn new(product_repo: Arc<IntegratedProductRepository>) -> Self {
        Self { product_repo }
    }
}

#[async_trait]
impl DatabaseAnalyzer for DatabaseAnalyzerImpl {
    async fn analyze_current_state(&self) -> Result<DatabaseAnalysis> {
        // ğŸ’¾ ì‹¤ì œ DBì—ì„œ ì œí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
        info!("ğŸ” [DatabaseAnalyzer] Starting database analysis...");

        // ğŸ”§ Debug: IntegratedProductRepositoryê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        info!("ğŸ” [DatabaseAnalyzer] Using database pool from IntegratedProductRepository");

        // ğŸ” Test database connection first
        info!("ğŸ” [DatabaseAnalyzer] Testing database connection...");

        // ğŸš€ Performance optimization: Use count query instead of loading all products
        let total_products = match self.product_repo.get_product_count().await {
            Ok(count) => {
                info!(
                    "âœ… [DatabaseAnalyzer] Successfully retrieved total count from database: {}",
                    count
                );
                count as usize
            }
            Err(e) => {
                error!(
                    "âŒ [DatabaseAnalyzer] Failed to get product count from database: {:?}",
                    e
                );
                error!("âŒ [DatabaseAnalyzer] Error details: {}", e);
                error!("âŒ [DatabaseAnalyzer] Error source: {:?}", e.source());
                error!(
                    "âŒ [DatabaseAnalyzer] This is the exact error location that generates 'Product repository not available'"
                );

                // ğŸ”§ Additional debugging: Try to check if the database exists
                info!("ğŸ” [DatabaseAnalyzer] Attempting additional diagnostics...");

                warn!("âš ï¸  Product repository not available - assuming empty DB");
                warn!(
                    "âš ï¸  DB inconsistency possible: repository unavailable but analysis may show different results"
                );
                return Ok(DatabaseAnalysis {
                    total_products: 0,
                    unique_products: 0,
                    missing_products_count: 0,
                    duplicate_count: 0,
                    last_update: Some(chrono::Utc::now()),
                    missing_fields_analysis: FieldAnalysis {
                        missing_company: 0,
                        missing_model: 0,
                        missing_matter_version: 0,
                        missing_connectivity: 0,
                        missing_certification_date: 0,
                    },
                    data_quality_score: 0.0,
                });
            }
        };

        info!(
            "ğŸ“Š [DatabaseAnalyzer] Database analysis completed: {} total products",
            total_products
        );

        // ê¸°ë³¸ ë¶„ì„ ë°˜í™˜ - í•„ë“œ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ìˆ˜ì •
        Ok(DatabaseAnalysis {
            total_products: total_products as u32,
            unique_products: total_products as u32,
            missing_products_count: 0, // duplicate_countë¥¼ missing_products_countë¡œ ë³€ê²½
            duplicate_count: 0,
            last_update: Some(chrono::Utc::now()),
            missing_fields_analysis: FieldAnalysis {
                missing_company: 0,
                missing_model: 0,
                missing_matter_version: 0,
                missing_connectivity: 0,
                missing_certification_date: 0,
            },
            data_quality_score: if total_products > 0 { 0.85 } else { 0.0 },
        })
    }

    async fn recommend_processing_strategy(&self) -> Result<ProcessingStrategy> {
        Ok(ProcessingStrategy {
            recommended_batch_size: 100,
            recommended_concurrency: 10,
            should_skip_duplicates: true,
            should_update_existing: false,
            priority_urls: Vec::new(),
        })
    }

    async fn analyze_duplicates(&self) -> Result<DuplicateAnalysis> {
        Ok(DuplicateAnalysis {
            total_duplicates: 0,
            duplicate_groups: Vec::new(),
            duplicate_percentage: 0.0,
        })
    }
}

/// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductDetailCollectorImpl {
    http_client: Arc<HttpClient>, // ğŸ”¥ Mutex ì œê±° - GlobalRateLimiterê°€ ë™ì‹œì„± ê´€ë¦¬
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

impl ProductDetailCollectorImpl {
    pub fn new(
        http_client: Arc<HttpClient>, // ğŸ”¥ Mutex ì œê±°
        data_extractor: Arc<MatterDataExtractor>,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client,
            data_extractor,
            config,
        }
    }

    /// ğŸ”¥ ProductDetail ì´ë²¤íŠ¸ ì²˜ë¦¬ê¸° (ë¹„ë™ê¸°, ë…¼ë¸”ë¡œí‚¹)
    async fn handle_product_detail_event(
        event: ProductDetailEvent,
        session_id: &str,
        batch_id: &str,
    ) -> Result<()> {
        // ğŸ”¥ ì´ë²¤íŠ¸ ì²˜ë¦¬ë¥¼ ë¡œê·¸ë¡œë§Œ ë‚¨ê¸°ê³  ì‹¤ì œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ëŠ” ServiceBasedCrawlingEngineì—ì„œ ì²˜ë¦¬
        // SystemStateBroadcasterëŠ” AppHandleì´ í•„ìš”í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

        match event {
            ProductDetailEvent::TaskStarted {
                product_url,
                product_name,
                task_id,
            } => {
                info!("ğŸš€ Product task started: {} ({})", product_url, task_id);
                debug!(
                    "ğŸ“ Product: {} | Task: {} | Session: {} | Batch: {}",
                    product_name.unwrap_or_else(|| "Unknown".to_string()),
                    task_id,
                    session_id,
                    batch_id
                );
            }
            ProductDetailEvent::HttpRequestStarted {
                product_url,
                task_id,
            } => {
                debug!(
                    "ğŸŒ HTTP request started for product: {} (task: {})",
                    product_url, task_id
                );
            }
            ProductDetailEvent::ParsingStarted {
                product_url,
                task_id,
                html_size,
            } => {
                debug!(
                    "ğŸ” Parsing started for product: {} (task: {}, HTML size: {})",
                    product_url, task_id, html_size
                );
            }
            ProductDetailEvent::TaskCompleted {
                product_url,
                product_name,
                task_id,
                processing_time,
                extracted_fields,
            } => {
                info!(
                    "âœ… Product task completed: {} ({}) - {} fields extracted in {:?}",
                    product_url, task_id, extracted_fields, processing_time
                );
                debug!(
                    "ğŸ“Š Product: {} | Fields: {} | Time: {:?} | Session: {} | Batch: {}",
                    product_name.unwrap_or_else(|| "Unknown".to_string()),
                    extracted_fields,
                    processing_time,
                    session_id,
                    batch_id
                );
            }
            ProductDetailEvent::TaskFailed {
                product_url,
                task_id,
                error,
                processing_time,
            } => {
                warn!(
                    "âŒ Product task failed: {} ({}) - {} (took {:?})",
                    product_url, task_id, error, processing_time
                );
                debug!(
                    "ğŸ’¥ Error: {} | Time: {:?} | Session: {} | Batch: {}",
                    error, processing_time, session_id, batch_id
                );
            }
        }

        Ok(())
    }
}

#[async_trait]
impl ProductDetailCollector for ProductDetailCollectorImpl {
    async fn collect_details(&self, product_urls: &[ProductUrl]) -> Result<Vec<ProductDetail>> {
        debug!("Collecting details for {} products", product_urls.len());

        let semaphore = Arc::new(Semaphore::new(self.config.max_concurrent as usize));
        let http_client = Arc::clone(&self.http_client);
        let data_extractor = Arc::clone(&self.data_extractor);
        let mut tasks = Vec::new();

        for product_url in product_urls {
            let http_client_clone = Arc::clone(&http_client);
            let data_extractor_clone = Arc::clone(&data_extractor);
            let url = product_url.url.clone();
            let page_id = product_url.page_id; // Capture page_id
            let index_in_page = product_url.index_in_page; // Capture index_in_page
            let permit = Arc::clone(&semaphore);
            let max_retries = self.config.retry_attempts.max(1);

            let task = tokio::spawn(async move {
                let _permit = permit.acquire().await.unwrap();

                // Retry-aware fetch + minimal parse retry
                let mut attempts: u32 = 0;
                let html_string: String;
                loop {
                    attempts += 1;
                    match http_client_clone.fetch_response_with_policy(&url).await {
                        Ok(response) => match response.text().await {
                            Ok(s) => {
                                html_string = s;
                                break;
                            }
                            Err(e) => {
                                if attempts < max_retries {
                                    tokio::time::sleep(Duration::from_millis(
                                        500 * attempts as u64,
                                    ))
                                    .await;
                                    continue;
                                } else {
                                    return Err(anyhow::anyhow!(
                                        "Failed to read response text: {}",
                                        e
                                    ));
                                }
                            }
                        },
                        Err(e) => {
                            if attempts < max_retries {
                                tokio::time::sleep(Duration::from_millis(500 * attempts as u64))
                                    .await;
                                continue;
                            } else {
                                return Err(e);
                            }
                        }
                    }
                }

                let doc = scraper::Html::parse_document(&html_string);
                let mut detail = data_extractor_clone.extract_product_detail(&doc, url.clone())?;

                // ğŸ”¥ Set page_id and index_in_page from ProductUrl
                detail.page_id = Some(page_id);
                detail.index_in_page = Some(index_in_page);

                // ğŸ”¥ Generate and set id field
                detail.id = Some(format!("p{:04}i{:02}", page_id, index_in_page));

                Ok::<ProductDetail, anyhow::Error>(detail)
            });

            tasks.push(task);
        }

        let results = futures::future::join_all(tasks).await;
        let mut details = Vec::new();

        for result in results {
            match result {
                Ok(Ok(detail)) => details.push(detail),
                Ok(Err(e)) => warn!("Failed to collect product detail: {}", e),
                Err(e) => warn!("Task failed: {}", e),
            }
        }

        debug!("Successfully collected {} product details", details.len());
        Ok(details)
    }

    async fn collect_details_with_cancellation(
        &self,
        product_urls: &[ProductUrl],
        cancellation_token: CancellationToken,
    ) -> Result<Vec<ProductDetail>> {
        info!(
            "Collecting details for {} products with cancellation support",
            product_urls.len()
        );

        let semaphore = Arc::new(Semaphore::new(self.config.max_concurrent as usize));
        let http_client = Arc::clone(&self.http_client);
        let data_extractor = Arc::clone(&self.data_extractor);
        let mut tasks = Vec::new();

        for product_url in product_urls {
            let http_client_clone = Arc::clone(&http_client);
            let data_extractor_clone = Arc::clone(&data_extractor);
            let url = product_url.url.clone();
            let page_id = product_url.page_id; // Capture page_id
            let index_in_page = product_url.index_in_page; // Capture index_in_page
            let permit = Arc::clone(&semaphore);
            let _delay = self.config.delay_ms;
            let token = cancellation_token.clone();

            let task = tokio::spawn(async move {
                if token.is_cancelled() {
                    return Err(anyhow!("Task cancelled"));
                }

                let _permit = permit.acquire().await.unwrap();

                // Remove individual delay for true concurrency
                // tokio::select! {
                //     _ = tokio::time::sleep(Duration::from_millis(delay)) => {},
                //     _ = token.cancelled() => return Err(anyhow!("Task cancelled during delay")),
                // }

                if token.is_cancelled() {
                    return Err(anyhow!("Task cancelled"));
                }

                // ğŸ”¥ Use retry-aware HTTP method
                let response = http_client_clone.fetch_response_with_policy(&url).await?;
                let html_string: String = response.text().await?;

                if token.is_cancelled() {
                    return Err(anyhow!("Task cancelled"));
                }

                let doc = scraper::Html::parse_document(&html_string);
                let mut detail = data_extractor_clone.extract_product_detail(&doc, url.clone())?;

                // ğŸ”¥ Set page_id and index_in_page from ProductUrl
                detail.page_id = Some(page_id);
                detail.index_in_page = Some(index_in_page);

                // ğŸ”¥ Generate and set id field
                detail.id = Some(format!("p{:04}i{:02}", page_id, index_in_page));

                Ok::<ProductDetail, anyhow::Error>(detail)
            });

            tasks.push(task);
        }

        let results = futures::future::join_all(tasks).await;
        let mut details = Vec::new();

        for result in results {
            match result {
                Ok(Ok(detail)) => details.push(detail),
                Ok(Err(e)) => {
                    if !cancellation_token.is_cancelled() {
                        warn!("Failed to collect product detail: {}", e);
                    }
                }
                Err(e) => {
                    if !cancellation_token.is_cancelled() {
                        warn!("Task failed: {}", e);
                    }
                }
            }
        }

        info!("Successfully collected {} product details", details.len());
        Ok(details)
    }

    async fn collect_single_product(&self, product_url: &ProductUrl) -> Result<ProductDetail> {
        self.collect_details(&[product_url.clone()])
            .await?
            .into_iter()
            .next()
            .ok_or_else(|| anyhow::anyhow!("Failed to collect product detail"))
    }

    async fn collect_product_batch(
        &self,
        product_urls: &[ProductUrl],
    ) -> Result<Vec<ProductDetail>> {
        self.collect_details(product_urls).await
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }
}

impl ProductDetailCollectorImpl {
    /// ğŸ”¥ ë™ì‹œì„±ì„ ë³´ì¥í•˜ëŠ” ì´ë²¤íŠ¸ ê¸°ë°˜ ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ë©”ì„œë“œ (ë¹„ë™ê¸° ì´ë²¤íŠ¸ í ì‚¬ìš©)
    ///
    /// Errors
    /// Returns an error if HTTP fetching or parsing fails for all tasks in a way that prevents
    /// producing any results. Partial failures are logged and skipped; successful details are
    /// still returned.
    ///
    /// Panics
    /// This function does not intentionally panic. Internal task joins are awaited safely.
    #[allow(clippy::too_many_lines)]
    #[allow(clippy::cognitive_complexity)]
    #[allow(clippy::missing_errors_doc)]
    #[allow(clippy::missing_panics_doc)]
    pub async fn collect_details_with_async_events(
        &self,
        product_urls: &[ProductUrl],
        cancellation_token: Option<CancellationToken>,
        session_id: String,
        batch_id: String,
    ) -> Result<Vec<ProductDetail>> {
        info!(
            "ğŸš€ Collecting details for {} products with async events",
            product_urls.len()
        );

        // ğŸ”¥ ë¹„ë™ê¸° ì´ë²¤íŠ¸ í ìƒì„± (ProductDetail íƒœìŠ¤í¬ìš©)
        let (event_tx, mut event_rx) = tokio::sync::mpsc::unbounded_channel::<ProductDetailEvent>();

        // ğŸ”¥ ì´ë²¤íŠ¸ ì²˜ë¦¬ê¸° ìƒì„± (ì™„ì „íˆ ë…ë¦½ì )
        let session_id_clone = session_id.clone();
        let batch_id_clone = batch_id.clone();
        let event_handler = tokio::spawn(async move {
            while let Some(event) = event_rx.recv().await {
                if let Err(e) =
                    Self::handle_product_detail_event(event, &session_id_clone, &batch_id_clone)
                        .await
                {
                    warn!("ğŸ”¥ Event handler error: {}", e);
                }
            }
        });

        let semaphore = Arc::new(Semaphore::new(self.config.max_concurrent as usize));
        let http_client = Arc::clone(&self.http_client);
        let data_extractor = Arc::clone(&self.data_extractor);
        let mut tasks = Vec::new();

        for product_url in product_urls {
            let http_client_clone = Arc::clone(&http_client);
            let data_extractor_clone = Arc::clone(&data_extractor);
            let url = product_url.url.clone();
            let page_id = product_url.page_id;
            let index_in_page = product_url.index_in_page;
            let permit = Arc::clone(&semaphore);
            let token = cancellation_token.clone();
            let event_tx_clone = event_tx.clone();

            let task = tokio::spawn(async move {
                if let Some(ref token) = token {
                    if token.is_cancelled() {
                        return Err(anyhow!("Task cancelled"));
                    }
                }

                let start_time = std::time::Instant::now();
                let task_id = format!("product-{}", url);

                // ğŸ”¥ íƒœìŠ¤í¬ ì‹œì‘ ì´ë²¤íŠ¸ (ë…¼ë¸”ë¡œí‚¹)
                let _ = event_tx_clone.send(ProductDetailEvent::TaskStarted {
                    product_url: url.clone(),
                    product_name: None, // Will be filled after parsing
                    task_id: task_id.clone(),
                });

                let _permit = permit.acquire().await.unwrap();

                if let Some(ref token) = token {
                    if token.is_cancelled() {
                        return Err(anyhow!("Task cancelled"));
                    }
                }

                // HTTP ìš”ì²­ ì‹œì‘ ì´ë²¤íŠ¸
                let _ = event_tx_clone.send(ProductDetailEvent::HttpRequestStarted {
                    product_url: url.clone(),
                    task_id: task_id.clone(),
                });

                // ğŸ”¥ Use retry-aware HTTP method
                let response = match http_client_clone.fetch_response_with_policy(&url).await {
                    Ok(response) => response,
                    Err(e) => {
                        let _ = event_tx_clone.send(ProductDetailEvent::TaskFailed {
                            product_url: url.clone(),
                            task_id: task_id.clone(),
                            error: format!("HTTP request failed: {}", e),
                            processing_time: start_time.elapsed(),
                        });
                        return Err(e);
                    }
                };

                let html = match response.text().await {
                    Ok(html) => html,
                    Err(e) => {
                        let _ = event_tx_clone.send(ProductDetailEvent::TaskFailed {
                            product_url: url.clone(),
                            task_id: task_id.clone(),
                            error: format!("Failed to read response: {}", e),
                            processing_time: start_time.elapsed(),
                        });
                        return Err(anyhow::anyhow!("Failed to read response: {}", e));
                    }
                };

                if let Some(ref token) = token {
                    if token.is_cancelled() {
                        return Err(anyhow!("Task cancelled"));
                    }
                }

                // íŒŒì‹± ì‹œì‘ ì´ë²¤íŠ¸
                let _ = event_tx_clone.send(ProductDetailEvent::ParsingStarted {
                    product_url: url.clone(),
                    task_id: task_id.clone(),
                    html_size: html.len(),
                });

                let doc = scraper::Html::parse_document(&html);
                let mut detail =
                    match data_extractor_clone.extract_product_detail(&doc, url.clone()) {
                        Ok(detail) => detail,
                        Err(e) => {
                            let _ = event_tx_clone.send(ProductDetailEvent::TaskFailed {
                                product_url: url.clone(),
                                task_id: task_id.clone(),
                                error: format!("Parsing failed: {}", e),
                                processing_time: start_time.elapsed(),
                            });
                            return Err(e);
                        }
                    };

                // ğŸ”¥ Set page_id and index_in_page from ProductUrl
                detail.page_id = Some(page_id);
                detail.index_in_page = Some(index_in_page);
                detail.id = Some(format!("p{:04}i{:02}", page_id, index_in_page));

                // ğŸ”¥ íƒœìŠ¤í¬ ì™„ë£Œ ì´ë²¤íŠ¸ (ë…¼ë¸”ë¡œí‚¹)
                let _ = event_tx_clone.send(ProductDetailEvent::TaskCompleted {
                    product_url: url.clone(),
                    product_name: detail.manufacturer.clone().or_else(|| detail.model.clone()),
                    task_id: task_id.clone(),
                    processing_time: start_time.elapsed(),
                    extracted_fields: calculate_extracted_fields(&detail),
                });

                Ok::<ProductDetail, anyhow::Error>(detail)
            });

            tasks.push(task);
        }

        let results = futures::future::join_all(tasks).await;
        let mut details = Vec::new();

        for result in results {
            match result {
                Ok(Ok(detail)) => details.push(detail),
                Ok(Err(e)) => {
                    if cancellation_token
                        .as_ref()
                        .map_or(true, |t| !t.is_cancelled())
                    {
                        warn!("Failed to collect product detail: {}", e);
                    }
                }
                Err(e) => {
                    if cancellation_token
                        .as_ref()
                        .map_or(true, |t| !t.is_cancelled())
                    {
                        warn!("Task failed: {}", e);
                    }
                }
            }
        }

        // ğŸ”¥ ì´ë²¤íŠ¸ í ì •ë¦¬
        drop(event_tx);
        let _ = event_handler.await;

        info!(
            "âœ… Successfully collected {} product details with async events",
            details.len()
        );
        Ok(details)
    }
}

/// ì§€ëŠ¥í˜• í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚°ê¸°
pub struct CrawlingRangeCalculator {
    product_repo: Arc<IntegratedProductRepository>,
    config: AppConfig,
}

impl CrawlingRangeCalculator {
    pub fn new(product_repo: Arc<IntegratedProductRepository>, config: AppConfig) -> Self {
        Self {
            product_repo,
            config,
        }
    }

    /// ë‹¤ìŒ í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚°
    pub async fn calculate_next_crawling_range(
        &self,
        total_pages: u32,
        products_on_last_page: u32,
    ) -> Result<Option<(u32, u32)>> {
        info!(
            "ğŸ¯ ì…ë ¥ íŒŒë¼ë¯¸í„°: total_pages={}, products_on_last_page={}",
            total_pages, products_on_last_page
        );

        // ì‚¬ìš©ì ì„¤ì •ì—ì„œ í˜ì´ì§€ ë²”ìœ„ ì œí•œ ê°€ì ¸ì˜¤ê¸°
        let user_page_limit = self.config.user.crawling.page_range_limit;
        let intelligent_mode = &self.config.user.crawling.intelligent_mode;

        info!(
            "âš™ï¸ User settings: page_range_limit={}, intelligent_mode.enabled={}, max_range_limit={}",
            user_page_limit, intelligent_mode.enabled, intelligent_mode.max_range_limit
        );

        // ì‹¤ì œ ì‚¬ìš©í•  í˜ì´ì§€ ì œí•œ ê³„ì‚°
        let effective_page_limit =
            if intelligent_mode.enabled && intelligent_mode.override_config_limit {
                // ì§€ëŠ¥í˜• ëª¨ë“œì—ì„œ overrideê°€ í—ˆìš©ëœ ê²½ìš°, max_range_limitê³¼ user_page_limit ì¤‘ ì‘ì€ ê°’ ì‚¬ìš©
                user_page_limit.min(intelligent_mode.max_range_limit)
            } else {
                // ì¼ë°˜ ëª¨ë“œì´ê±°ë‚˜ overrideê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°, ì‚¬ìš©ì ì„¤ì •ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                user_page_limit
            };

        info!(
            "ğŸ“Š Effective page limit for this crawling: {}",
            effective_page_limit
        );

        // ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í˜„ì¬ ì œí’ˆ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        let all_products = match self.product_repo.get_all_products().await {
            Ok(products) => {
                info!(
                    "âœ… Successfully retrieved {} products from database",
                    products.len()
                );
                products
            }
            Err(e) => {
                error!("âŒ Failed to get products from database: {}", e);
                Vec::new()
            }
        };

        if all_products.is_empty() {
            info!(
                "ğŸ“‹ Database is empty - starting from the last page (page {})",
                total_pages
            );
            let end_page = (total_pages.saturating_sub(effective_page_limit - 1)).max(1);
            return Ok(Some((total_pages, end_page)));
        }

        // ê°€ì¥ ë†’ì€ page_id ì°¾ê¸° (ì—­ìˆœì´ë¯€ë¡œ ê°€ì¥ ì‘ì€ ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸)
        let max_page_id = all_products
            .iter()
            .filter_map(|p| p.page_id)
            .max()
            .unwrap_or(0);

        info!("ğŸ” Current max page_id in database: {}", max_page_id);

        // page_idì—ì„œ ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë³€í™˜
        // page_id 0 = 485í˜ì´ì§€, page_id 1 = 484í˜ì´ì§€, ..., page_id 5 = 480í˜ì´ì§€
        // Overflow ë°©ì§€: max_page_idê°€ total_pagesë³´ë‹¤ í´ ìˆ˜ ìˆìŒ (ì‚¬ì´íŠ¸ ë³€ê²½ ë“±)
        let last_crawled_page = if max_page_id as u32 >= total_pages {
            warn!(
                "âš ï¸  Database max_page_id ({}) >= total_pages ({}), assuming no valid crawled pages",
                max_page_id, total_pages
            );
            0 // ìœ íš¨í•œ í¬ë¡¤ë§ëœ í˜ì´ì§€ê°€ ì—†ë‹¤ê³  ê°„ì£¼
        } else {
            total_pages - max_page_id as u32
        };
        info!(
            "ğŸ“ Last crawled page: {} (page_id: {})",
            last_crawled_page, max_page_id
        );

        // ë‹¤ìŒ í¬ë¡¤ë§í•  ë²”ìœ„ ê³„ì‚°
        // í˜„ì¬ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ì§‘ ìƒíƒœ í™•ì¸
        let current_page_products = all_products
            .iter()
            .filter(|p| p.page_id == Some(max_page_id))
            .count();

        let expected_products_on_current_page = if last_crawled_page == total_pages {
            // ë§ˆì§€ë§‰ í˜ì´ì§€ (485í˜ì´ì§€)ë¼ë©´ products_on_last_pageë§Œí¼ ìˆì–´ì•¼ í•¨
            products_on_last_page as usize
        } else {
            // ë‹¤ë¥¸ í˜ì´ì§€ë¼ë©´ 12ê°œê°€ ìˆì–´ì•¼ í•¨
            DEFAULT_PRODUCTS_PER_PAGE as usize
        };

        info!(
            "ğŸ” Current page {} has {}/{} products",
            last_crawled_page, current_page_products, expected_products_on_current_page
        );

        // ë‹¤ìŒ í¬ë¡¤ë§ ì‹œì‘ í˜ì´ì§€ ê²°ì •
        let start_page = if current_page_products < expected_products_on_current_page {
            // í˜„ì¬ í˜ì´ì§€ê°€ ì™„ì „íˆ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ë‹¤ë©´ í˜„ì¬ í˜ì´ì§€ë¶€í„° ì‹œì‘
            last_crawled_page.max(1)
        } else {
            // í˜„ì¬ í˜ì´ì§€ê°€ ì™„ë£Œë˜ì—ˆë‹¤ë©´ ë‹¤ìŒ í˜ì´ì§€ë¶€í„° ì‹œì‘
            if last_crawled_page > 1 {
                last_crawled_page - 1
            } else {
                info!("ğŸ All pages have been crawled");
                return Ok(None);
            }
        };

        // í¬ë¡¤ë§ ë²”ìœ„ ì œí•œ ì ìš© (ì‚¬ìš©ì ì„¤ì • ì¡´ì¤‘)
        let end_page = (start_page.saturating_sub(effective_page_limit - 1)).max(1);

        info!(
            "âœ… Next crawling range: {}í˜ì´ì§€ â†’ {}í˜ì´ì§€ (ì—­ìˆœ, ìµœëŒ€ {}í˜ì´ì§€)",
            start_page, end_page, effective_page_limit
        );
        Ok(Some((start_page, end_page)))
    }
}

/// ProductDetailì„ Productë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
pub fn product_detail_to_product(detail: ProductDetail) -> Product {
    let mut product = Product {
        id: detail.id.clone(), // Use detail's id if available
        url: detail.url,
        manufacturer: detail.manufacturer,
        model: detail.model,
        certificate_id: detail.certificate_id,
        page_id: detail.page_id,
        index_in_page: detail.index_in_page,
        created_at: detail.created_at,
        updated_at: detail.updated_at,
    };

    // Generate ID if not already set
    if product.id.is_none() {
        product.generate_id();
    }

    product
}

/// ğŸ”¥ ProductDetailì—ì„œ ì¶”ì¶œëœ í•„ë“œ ê°œìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
fn calculate_extracted_fields(detail: &crate::domain::product::ProductDetail) -> u32 {
    let mut count = 0u32;

    if detail.manufacturer.is_some() {
        count += 1;
    }
    if detail.model.is_some() {
        count += 1;
    }
    if detail.device_type.is_some() {
        count += 1;
    }
    if detail.certificate_id.is_some() {
        count += 1;
    }
    if detail.certification_date.is_some() {
        count += 1;
    }
    if detail.software_version.is_some() {
        count += 1;
    }
    if detail.hardware_version.is_some() {
        count += 1;
    }
    if detail.vid.is_some() {
        count += 1;
    }
    if detail.pid.is_some() {
        count += 1;
    }
    if detail.family_sku.is_some() {
        count += 1;
    }
    if detail.family_variant_sku.is_some() {
        count += 1;
    }
    if detail.firmware_version.is_some() {
        count += 1;
    }
    if detail.family_id.is_some() {
        count += 1;
    }
    if detail.tis_trp_tested.is_some() {
        count += 1;
    }
    if detail.specification_version.is_some() {
        count += 1;
    }
    if detail.transport_interface.is_some() {
        count += 1;
    }
    if detail.primary_device_type_id.is_some() {
        count += 1;
    }
    if detail.application_categories.is_some() {
        count += 1;
    }
    if detail.description.is_some() {
        count += 1;
    }
    if detail.compliance_document_url.is_some() {
        count += 1;
    }
    if detail.program_type.is_some() {
        count += 1;
    }

    count
}

// Additional trait implementations for service-based architecture

#[async_trait]
impl DatabaseAnalyzer for StatusCheckerImpl {
    async fn analyze_current_state(&self) -> Result<DatabaseAnalysis> {
        // Placeholder implementation for service-based architecture
        Ok(DatabaseAnalysis {
            total_products: 0,
            unique_products: 0,
            missing_products_count: 0, // duplicate_countë¥¼ missing_products_countë¡œ ë³€ê²½
            duplicate_count: 0,
            missing_fields_analysis: FieldAnalysis {
                missing_company: 0,
                missing_model: 0,
                missing_matter_version: 0,
                missing_connectivity: 0,
                missing_certification_date: 0,
            },
            data_quality_score: 0.0,
            last_update: Some(chrono::Utc::now()),
        })
    }

    async fn recommend_processing_strategy(&self) -> Result<ProcessingStrategy> {
        Ok(ProcessingStrategy {
            recommended_batch_size: 100,
            recommended_concurrency: 10,
            should_skip_duplicates: true,
            should_update_existing: false,
            priority_urls: Vec::new(),
        })
    }

    async fn analyze_duplicates(&self) -> Result<DuplicateAnalysis> {
        Ok(DuplicateAnalysis {
            total_duplicates: 0,
            duplicate_groups: Vec::new(),
            duplicate_percentage: 0.0,
        })
    }
}

#[async_trait]
impl ProductDetailCollector for ProductListCollectorImpl {
    async fn collect_details(&self, _product_urls: &[ProductUrl]) -> Result<Vec<ProductDetail>> {
        // Placeholder implementation for service-based architecture
        Ok(Vec::new())
    }

    async fn collect_details_with_cancellation(
        &self,
        _product_urls: &[ProductUrl],
        _cancellation_token: CancellationToken,
    ) -> Result<Vec<ProductDetail>> {
        // Placeholder implementation for service-based architecture
        Ok(Vec::new())
    }

    async fn collect_single_product(&self, _product_url: &ProductUrl) -> Result<ProductDetail> {
        // Placeholder implementation for service-based architecture
        Err(anyhow!("Not implemented"))
    }

    async fn collect_product_batch(
        &self,
        _product_urls: &[ProductUrl],
    ) -> Result<Vec<ProductDetail>> {
        // Placeholder implementation for service-based architecture
        Ok(Vec::new())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }
}

impl CrawlingRangeCalculator {
    /// ê°„ë‹¨í•œ ì§„í–‰ ìƒí™© ë¶„ì„ (smart_crawling ëª…ë ¹ì–´ìš©)
    pub async fn analyze_simple_progress(
        &self,
        total_pages_on_site: u32,
        products_on_last_page: u32,
    ) -> Result<crate::domain::events::CrawlingProgress> {
        // ë¡œì»¬ DB ìƒíƒœ í™•ì¸
        let all_products = match self.product_repo.get_all_products().await {
            Ok(products) => {
                info!(
                    "ğŸ“Š Successfully retrieved {} products from database",
                    products.len()
                );
                products
            }
            Err(e) => {
                error!("âŒ Failed to get products from database: {}", e);
                Vec::new()
            }
        };
        let saved_products = all_products.len() as u32;

        // ì´ ì œí’ˆ ìˆ˜ ì¶”ì •
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;
        // ì•ˆì „í•œ ì´ ì œí’ˆ ìˆ˜ ì¶”ì • (ì–¸ë”í”Œë¡œìš°/ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
        let pages_except_last = total_pages_on_site.saturating_sub(1);
        let total_estimated_products = pages_except_last
            .saturating_mul(products_per_page)
            .saturating_add(products_on_last_page.min(products_per_page));

        // ì§„í–‰ë¥  ê³„ì‚°
        let percentage = if total_estimated_products > 0 {
            (saved_products as f64 / total_estimated_products as f64) * 100.0
        } else {
            0.0
        };

        // ê°€ì¥ ë†’ì€ pageIdì™€ indexInPage ì°¾ê¸°
        let mut max_page_id = 0i32;
        let mut max_index_in_page = 0i32;

        for product in &all_products {
            let page_id = product.page_id.unwrap_or(0);
            let index_in_page = product.index_in_page.unwrap_or(0);

            if page_id > max_page_id {
                max_page_id = page_id;
                max_index_in_page = index_in_page;
            } else if page_id == max_page_id && index_in_page > max_index_in_page {
                max_index_in_page = index_in_page;
            }
        }

        // ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë³€í™˜ (page_id 0 = ë§ˆì§€ë§‰ í˜ì´ì§€) â€” ì–¸ë”í”Œë¡œìš° ë°©ì§€
        let actual_last_crawled_page = if max_page_id >= 0 {
            let mp = max_page_id as u32;
            if mp > total_pages_on_site {
                warn!(
                    "âš ï¸ Detected inconsistency: max_page_id ({}) > total_pages_on_site ({}). Using 0 for actual_last_crawled_page.",
                    mp, total_pages_on_site
                );
            }
            total_pages_on_site.saturating_sub(mp)
        } else {
            0
        };

        info!(
            "ğŸ“Š Progress: {}/{} products ({:.1}%), last crawled page: {} (page_id: {})",
            saved_products,
            total_estimated_products,
            percentage,
            actual_last_crawled_page,
            max_page_id
        );

        Ok(crate::domain::events::CrawlingProgress {
            current: saved_products,
            total: total_estimated_products,
            percentage,
            current_stage: if percentage >= 100.0 {
                crate::domain::events::CrawlingStage::DatabaseSave
            } else {
                crate::domain::events::CrawlingStage::Idle
            },
            current_step: format!(
                "Saved {} of {} products",
                saved_products, total_estimated_products
            ),
            status: if percentage >= 100.0 {
                crate::domain::events::CrawlingStatus::Completed
            } else {
                crate::domain::events::CrawlingStatus::Idle
            },
            message: format!("Progress: {:.1}%", percentage),
            remaining_time: None,
            elapsed_time: 0,
            new_items: 0,
            updated_items: 0,
            current_batch: Some(max_page_id as u32),
            total_batches: Some(total_pages_on_site),
            errors: 0,
            timestamp: chrono::Utc::now(),
        })
    }

    pub async fn analyze_crawling_progress(
        &self,
        _url: &str,
        _config: &CrawlingConfig,
        _database_analysis: &DatabaseAnalysis,
    ) -> Result<crate::domain::events::CrawlingProgress> {
        // Placeholder implementation
        Ok(crate::domain::events::CrawlingProgress {
            current: 0,
            total: 1,
            percentage: 0.0,
            current_stage: crate::domain::events::CrawlingStage::Idle,
            current_step: "Waiting".to_string(),
            status: crate::domain::events::CrawlingStatus::Idle,
            message: "Ready".to_string(),
            remaining_time: None,
            elapsed_time: 0,
            new_items: 0,
            updated_items: 0,
            current_batch: Some(0),
            total_batches: Some(1),
            errors: 0,
            timestamp: chrono::Utc::now(),
        })
    }
}

/// ğŸ”¥ í˜ì´ì§€ ì²˜ë¦¬ ì´ë²¤íŠ¸ (ë…¼ë¸”ë¡œí‚¹ íìš©)
#[derive(Debug, Clone)]
enum PageEvent {
    Started {
        page_number: u32,
    },
    Completed {
        page_number: u32,
        products_found: u32,
        duration_ms: u64,
    },
    Failed {
        page_number: u32,
        error: String,
    },
    Cancelled {
        page_number: u32,
    },
}
