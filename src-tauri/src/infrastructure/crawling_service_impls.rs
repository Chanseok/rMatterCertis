//! í¬ë¡¤ë§ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
//! 
//! domain/services/crawling_services.rsì˜ íŠ¸ë ˆì´íŠ¸ë“¤ì— ëŒ€í•œ ì‹¤ì œ êµ¬í˜„ì²´

use std::sync::Arc;
use std::time::{Duration, Instant};
use std::collections::HashMap;
use async_trait::async_trait;
use anyhow::{Result, anyhow};
use tracing::{info, warn, error, debug};
use tokio::sync::Semaphore;
use tokio::time::sleep;
use tokio_util::sync::CancellationToken;
use futures::future::try_join_all;
use scraper;
use regex;

use crate::domain::services::{
    StatusChecker, DatabaseAnalyzer, ProductListCollector, ProductDetailCollector,
    SiteStatus, DatabaseAnalysis, FieldAnalysis, DuplicateAnalysis, ProcessingStrategy
};
use crate::domain::services::crawling_services::{
    SiteDataChangeStatus, DataDecreaseRecommendation, RecommendedAction, SeverityLevel, CrawlingRangeRecommendation
};
use crate::domain::product::{Product, ProductDetail};
use crate::infrastructure::{HttpClient, MatterDataExtractor, IntegratedProductRepository};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::config::utils as config_utils;
use crate::infrastructure::config::defaults;

/// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹±í•˜ê¸° ìœ„í•œ êµ¬ì¡°ì²´
#[derive(Debug, Clone)]
struct PageAnalysisCache {
    /// í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜
    product_count: u32,
    /// í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë°œê²¬ëœ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸
    max_pagination_page: u32,
    /// í˜„ì¬ í™œì„±í™”ëœ í˜ì´ì§€ ë²ˆí˜¸ (í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ í™•ì¸)
    active_page: u32,
    /// ì œí’ˆì´ ìˆëŠ”ì§€ ì—¬ë¶€
    has_products: bool,
    /// ë¶„ì„ ì™„ë£Œ ì‹œê°
    analyzed_at: std::time::Instant,
}

/// ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
/// PageDiscoveryServiceì™€ í˜‘ë ¥í•˜ì—¬ ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„
pub struct StatusCheckerImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: AppConfig,
    /// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ ìºì‹œ (í˜ì´ì§€ ë²ˆí˜¸ -> ë¶„ì„ ê²°ê³¼)
    page_cache: Arc<tokio::sync::Mutex<HashMap<u32, PageAnalysisCache>>>,
    /// ì œí’ˆ ë ˆí¬ì§€í† ë¦¬ (ë¡œì»¬ DB ìƒíƒœ ì¡°íšŒìš©)
    product_repo: Option<Arc<IntegratedProductRepository>>,
}

impl StatusCheckerImpl {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: AppConfig,
    ) -> Self {
        // 470ì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì„¤ì •í•œ ì´ìœ  ì„¤ëª…:
        // ì´ëŠ” ê³¼ê±° CSA-IoT ì‚¬ì´íŠ¸ì—ì„œ ê´€ì°°ëœ ëŒ€ëµì ì¸ í˜ì´ì§€ ìˆ˜ì…ë‹ˆë‹¤.
        // ê·¸ëŸ¬ë‚˜ í˜„ì¬ëŠ” ë” ì‘ì€ ê°’(100)ë¶€í„° ì‹œì‘í•˜ì—¬ ë™ì ìœ¼ë¡œ íƒì§€í•©ë‹ˆë‹¤.
        // ì•±ì´ ì‚¬ìš©ë˜ë©´ì„œ ì‹¤ì œ ë§ˆì§€ë§‰ í˜ì´ì§€ë¥¼ í•™ìŠµí•˜ê³  ì €ì¥í•˜ê²Œ ë©ë‹ˆë‹¤.
        
        Self {
            http_client: Arc::new(tokio::sync::Mutex::new(http_client)),
            data_extractor: Arc::new(data_extractor),
            config,
            page_cache: Arc::new(tokio::sync::Mutex::new(HashMap::new())),
            product_repo: None,
        }
    }

    pub fn with_product_repo(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        config: AppConfig,
        product_repo: Arc<IntegratedProductRepository>,
    ) -> Self {
        let mut instance = Self::new(http_client, data_extractor, config);
        instance.product_repo = Some(product_repo);
        instance
    }

    /// Update the pagination context in the data extractor based on discovered page information
    pub async fn update_pagination_context(&self, total_pages: u32, items_on_last_page: u32) -> Result<()> {
        // Create pagination context
        let pagination_context = crate::infrastructure::html_parser::PaginationContext {
            total_pages,
            items_per_page: 12, // CSA-IoT site uses 12 items per page
            items_on_last_page,
            target_page_size: 12, // Our system also uses 12 items per page
        };
        
        // Update the data extractor's pagination context
        self.data_extractor.set_pagination_context(pagination_context)?;
        
        info!("ğŸ“Š Updated pagination context: total_pages={}, items_on_last_page={}", 
               total_pages, items_on_last_page);
        
        Ok(())
    }
}

#[async_trait]
impl StatusChecker for StatusCheckerImpl {
    async fn check_site_status(&self) -> Result<SiteStatus> {
        let start_time = Instant::now();
        info!("Starting comprehensive site status check with detailed page discovery");
        
        // ìºì‹œ ì´ˆê¸°í™”
        self.clear_page_cache().await;
        
        info!("Checking site status and discovering pages...");

        // Step 1: ê¸°ë³¸ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± í™•ì¸
        let url = config_utils::matter_products_page_url_simple(1);
        
        // ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸
        let access_test = {
            let mut client = self.http_client.lock().await;
            client.fetch_html_string(&url).await
        };
        
        match access_test {
            Ok(_) => info!("Site is accessible"),
            Err(e) => {
                error!("Failed to access site: {}", e);
                return Ok(SiteStatus {
                    is_accessible: false,
                    response_time_ms: start_time.elapsed().as_millis() as u64,
                    total_pages: 0,
                    estimated_products: 0,
                    products_on_last_page: 0,
                    last_check_time: chrono::Utc::now(),
                    health_score: 0.0,
                    data_change_status: SiteDataChangeStatus::Inaccessible,
                    decrease_recommendation: None,
                    crawling_range_recommendation: CrawlingRangeRecommendation::None,
                });
            }
        }

        // Step 2: í˜ì´ì§€ ìˆ˜ íƒì§€ ë° ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜ í™•ì¸
        let (total_pages, products_on_last_page) = self.discover_total_pages().await?;

        // Step 2.5: í˜ì´ì§€ë„¤ì´ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        if let Err(e) = self.update_pagination_context(total_pages, products_on_last_page).await {
            warn!("Failed to update pagination context: {}", e);
        }

        let response_time_ms = start_time.elapsed().as_millis() as u64;
        let response_time = start_time.elapsed();

        // Step 3: ì‚¬ì´íŠ¸ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
        let health_score = calculate_health_score(response_time, total_pages);

        info!("Site status check completed: {} pages found, {}ms total time, health score: {:.2}", 
              total_pages, response_time_ms, health_score);

        // ì •í™•í•œ ì œí’ˆ ìˆ˜ ê³„ì‚°: (ë§ˆì§€ë§‰ í˜ì´ì§€ - 1) * í˜ì´ì§€ë‹¹ ì œí’ˆ ìˆ˜ + ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜
        use crate::infrastructure::config::defaults::DEFAULT_PRODUCTS_PER_PAGE;
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;
        
        let estimated_products = if total_pages > 1 {
            ((total_pages - 1) * products_per_page) + products_on_last_page
        } else {
            products_on_last_page
        };
        
        info!("Accurate product estimation: ({} full pages * {} products) + {} products on last page = {} total products", 
              total_pages - 1, products_per_page, products_on_last_page, estimated_products);

        // Step 4: ë°ì´í„° ë³€í™” ìƒíƒœ ë¶„ì„
        let (data_change_status, decrease_recommendation) = self.analyze_data_changes(estimated_products).await;
        
        // Step 5: í¬ë¡¤ë§ ë²”ìœ„ ê¶Œì¥ì‚¬í•­ ê³„ì‚°
        let crawling_range_recommendation = self.calculate_crawling_range_recommendation_internal(
            total_pages, 
            products_on_last_page, 
            estimated_products
        ).await?;
              
        Ok(SiteStatus {
            is_accessible: true,
            response_time_ms: response_time_ms,
            total_pages,
            estimated_products,
            products_on_last_page,
            last_check_time: chrono::Utc::now(),
            health_score,
            data_change_status,
            decrease_recommendation,
            crawling_range_recommendation,
        })
    }

    async fn calculate_crawling_range_recommendation(
        &self, 
        site_status: &SiteStatus, 
        db_analysis: &DatabaseAnalysis
    ) -> Result<CrawlingRangeRecommendation> {
        info!("ğŸ” Calculating crawling range recommendation from site status and DB analysis...");
        
        // If database is empty, recommend full crawl
        if db_analysis.total_products == 0 {
            info!("ğŸ“Š Local DB is empty - recommending full crawl");
            return Ok(CrawlingRangeRecommendation::Full);
        }
        
        // Calculate how many new products might have been added
        let estimated_new_products = if site_status.estimated_products > db_analysis.total_products as u32 {
            site_status.estimated_products - db_analysis.total_products as u32
        } else {
            0
        };
        
        if estimated_new_products == 0 {
            info!("ğŸ“Š No new products detected - recommending minimal verification crawl");
            return Ok(CrawlingRangeRecommendation::Partial(5)); // 5 pages for verification
        }
        
        // Calculate pages needed for new products
        use crate::infrastructure::config::defaults::DEFAULT_PRODUCTS_PER_PAGE;
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;
        let pages_needed = (estimated_new_products as f64 / products_per_page as f64).ceil() as u32;
        let limited_pages = pages_needed.min(self.config.user.max_pages);
        
        info!("ğŸ“Š Estimated {} new products, recommending {} pages crawl", estimated_new_products, limited_pages);
        Ok(CrawlingRangeRecommendation::Partial(limited_pages))
    }

    async fn estimate_crawling_time(&self, pages: u32) -> Duration {
        // ...
        // í˜ì´ì§€ë‹¹ í‰ê·  2ì´ˆ + ì œí’ˆ ìƒì„¸í˜ì´ì§€ë‹¹ 1ì´ˆ ì¶”ì •
        let page_collection_time = pages * 2;
        let product_detail_time = pages * 20; // í˜ì´ì§€ë‹¹ 20ê°œ ì œí’ˆ * 1ì´ˆ
        let total_seconds = page_collection_time + product_detail_time;
        
        Duration::from_secs(total_seconds as u64)
    }

    async fn verify_site_accessibility(&self) -> Result<bool> {
        let status = self.check_site_status().await?;
        // health_scoreëŠ” ì„±ëŠ¥ ì •ë³´ì¼ ë¿, í¬ë¡¤ë§ ê°€ëŠ¥ ì—¬ë¶€ì™€ëŠ” ë¬´ê´€
        // ì‚¬ì´íŠ¸ ì ‘ê·¼ ê°€ëŠ¥ì„±ê³¼ ê¸°ë³¸ì ì¸ í˜ì´ì§€ êµ¬ì¡°ë§Œ í™•ì¸
        Ok(status.is_accessible && status.total_pages > 0)
    }
}

impl StatusCheckerImpl {
    /// í–¥ìƒëœ í˜ì´ì§€ íƒì§€ ë¡œì§ - ì‚¬ì´íŠ¸ ì •ë³´ ë³€í™” ê°ì§€ í¬í•¨
    async fn discover_total_pages(&self) -> Result<(u32, u32)> {
        info!("ğŸ” Starting enhanced page discovery algorithm with site change detection");
        
        // 1. ì‹œì‘ í˜ì´ì§€ ê²°ì •
        let start_page = self.config.app_managed.last_known_max_page
            .unwrap_or(self.config.advanced.last_page_search_start);
        
        info!("ğŸ“ Starting from page {} (last known: {:?}, default: {})", 
              start_page, 
              self.config.app_managed.last_known_max_page,
              self.config.advanced.last_page_search_start);
        
        // 2. ì‹œì‘ í˜ì´ì§€ ë¶„ì„ (ìºì‹œ ì‚¬ìš©)
        let start_analysis = self.get_or_analyze_page(start_page).await?;
        let mut current_page = start_page;
        
        if !start_analysis.has_products {
            warn!("âš ï¸  Starting page {} has no products - checking site status", current_page);
            
            // ì²« í˜ì´ì§€ í™•ì¸ìœ¼ë¡œ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± ê²€ì¦
            let first_page_analysis = self.get_or_analyze_page(1).await?;
            if !first_page_analysis.has_products {
                error!("âŒ First page also has no products - site may be temporarily unavailable");
                return Err(anyhow::anyhow!(
                    "Site appears to be temporarily unavailable or experiencing issues. Please try again later."
                ));
            }
            
            info!("âœ… First page has products - site is accessible, cached page info may be outdated");
            warn!("ğŸ”„ Site content may have decreased - will perform full discovery");
            
            // í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
            current_page = self.find_last_valid_page_downward(current_page).await?;
            info!("âœ… Found valid starting page: {}", current_page);
        }
        
        // 3. ë°˜ë³µì  ìƒí–¥ íƒìƒ‰: í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° ê°’ì„ ì°¾ì„ ë•Œê¹Œì§€ ê³„ì†
        let mut attempts = 0;
        let max_attempts = self.config.advanced.max_search_attempts;
        
        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts ({}), stopping at page {}", max_attempts, current_page);
                break;
            }
            
            info!("ğŸ” Iteration {}/{}: Checking page {}", attempts, max_attempts, current_page);
            
            // í˜„ì¬ í˜ì´ì§€ë¥¼ ë¶„ì„ (ìºì‹œ ì‚¬ìš©)
            let analysis = match self.get_or_analyze_page(current_page).await {
                Ok(analysis) => analysis,
                Err(e) => {
                    warn!("âŒ Failed to analyze page {}: {}", current_page, e);
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ í•˜í–¥ íƒìƒ‰
                    current_page = self.find_last_valid_page_downward(current_page).await?;
                    break;
                }
            };
            
            if !analysis.has_products {
                // ì œí’ˆì´ ì—†ëŠ” ê²½ìš° ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                info!("ğŸ”» Page {} has no products, performing safe downward search", current_page);
                current_page = self.find_last_valid_page_with_safety_check(current_page).await?;
                break;
            }
            
            // í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë” í° í˜ì´ì§€ë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
            if analysis.max_pagination_page > current_page {
                info!("ğŸ”º Found higher page {} in pagination, jumping there", analysis.max_pagination_page);
                current_page = analysis.max_pagination_page;
                // ìƒˆ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë‹¤ì‹œ íƒìƒ‰
                continue;
            } else {
                info!("ğŸ No higher pages found in pagination, {} appears to be the last page", current_page);
                break;
            }
        }
        
        // 4. ìµœì¢… ê²€ì¦: ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸ ë° ì œí’ˆ ìˆ˜ ê³„ì‚°
        let (verified_last_page, products_on_last_page) = self.verify_last_page(current_page).await?;
        
        // 5. ì„¤ì • íŒŒì¼ì— ê²°ê³¼ ì €ì¥
        if let Err(e) = self.update_last_known_page(verified_last_page).await {
            warn!("âš ï¸  Failed to update last known page in config: {}", e);
        }
        
        info!("ğŸ‰ Final verified last page: {} with {} products", verified_last_page, products_on_last_page);
        Ok((verified_last_page, products_on_last_page))
    }

    /// í•˜í–¥ íƒìƒ‰ìœ¼ë¡œ ë§ˆì§€ë§‰ ìœ íš¨í•œ í˜ì´ì§€ ì°¾ê¸°
    async fn find_last_valid_page_downward(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page.saturating_sub(1);
        let min_page = 1;

        info!("Starting downward search from page {}", current_page);

        while current_page >= min_page {
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let mut client = self.http_client.lock().await;
            match client.fetch_html_string(&test_url).await {
                Ok(html) => {
                    let doc = scraper::Html::parse_document(&html);
                    if self.has_products_on_page(&doc) {
                        info!("Found valid page with products: {}", current_page);
                        return Ok(current_page);
                    }
                },
                Err(e) => {
                    warn!("Failed to fetch page {} during downward search: {}", current_page, e);
                }
            }

            current_page = current_page.saturating_sub(1);
            
            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
        warn!("No valid pages found during downward search, returning 1");
        Ok(1)
    }

    /// ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ - ì—°ì† ë¹ˆ í˜ì´ì§€ 3ê°œ ì´ìƒ ì‹œ fatal error
    async fn find_last_valid_page_with_safety_check(&self, start_page: u32) -> Result<u32> {
        let mut current_page = start_page;
        let mut consecutive_empty_pages = 0;
        const MAX_CONSECUTIVE_EMPTY: u32 = 3;
        let min_page = 1;

        info!("ğŸ” Starting safe downward search from page {} (max consecutive empty: {})", 
              current_page, MAX_CONSECUTIVE_EMPTY);

        // ë¨¼ì € ì‹œì‘ í˜ì´ì§€ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if !self.check_page_has_products(current_page).await? {
            consecutive_empty_pages += 1;
            info!("âš ï¸  Starting page {} is empty (consecutive: {})", current_page, consecutive_empty_pages);
        }

        while current_page > min_page {
            current_page = current_page.saturating_sub(1);
            
            let test_url = config_utils::matter_products_page_url_simple(current_page);
            info!("ğŸ” Checking page {} (consecutive empty: {})", current_page, consecutive_empty_pages);
            
            let mut client = self.http_client.lock().await;
            match client.fetch_html_string(&test_url).await {
                Ok(html) => {
                    let doc = scraper::Html::parse_document(&html);
                    if self.has_products_on_page(&doc) {
                        info!("âœ… Found valid page with products: {} (after {} consecutive empty pages)", 
                              current_page, consecutive_empty_pages);
                        return Ok(current_page);
                    } else {
                        consecutive_empty_pages += 1;
                        warn!("âš ï¸  Page {} is empty (consecutive: {}/{})", 
                              current_page, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY);
                        
                        // ì—°ì†ìœ¼ë¡œ ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ì´ìƒì´ë©´ fatal error
                        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                            error!("ğŸ’¥ FATAL ERROR: Found {} consecutive empty pages starting from page {}. This indicates a serious site issue or crawling problem.", 
                                   consecutive_empty_pages, start_page);
                            
                            return Err(anyhow!(
                                "Fatal error: {} consecutive empty pages detected. Site may be down or pagination structure changed. Last checked pages: {} to {}",
                                consecutive_empty_pages, 
                                start_page,
                                current_page
                            ));
                        }
                    }
                },
                Err(e) => {
                    consecutive_empty_pages += 1;
                    warn!("âŒ Failed to fetch page {} during safe downward search: {} (consecutive: {}/{})", 
                          current_page, e, consecutive_empty_pages, MAX_CONSECUTIVE_EMPTY);
                    
                    // ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë„ ì—°ì† ì‹¤íŒ¨ë¡œ ì¹´ìš´íŠ¸
                    if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
                        error!("ğŸ’¥ FATAL ERROR: {} consecutive failures (empty pages + network errors) starting from page {}.", 
                               consecutive_empty_pages, start_page);
                        
                        return Err(anyhow!(
                            "Fatal error: {} consecutive failures detected. Network issues or site problems. Last error: {}",
                            consecutive_empty_pages, 
                            e
                        ));
                    }
                }
            }
            
            // ìš”ì²­ ê°„ ì§€ì—°
            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        // ìµœì†Œ í˜ì´ì§€ê¹Œì§€ ë„ë‹¬í–ˆì§€ë§Œ ì—¬ì „íˆ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ ë§ë‹¤ë©´ fatal error
        if consecutive_empty_pages >= MAX_CONSECUTIVE_EMPTY {
            error!("ğŸ’¥ FATAL ERROR: Reached minimum page but still have {} consecutive empty pages. Site appears to be completely empty or broken.", 
                   consecutive_empty_pages);
            
            return Err(anyhow!(
                "Fatal error: Site appears to be empty or broken. {} consecutive empty pages found from page {} down to page {}",
                consecutive_empty_pages, 
                start_page,
                current_page
            ));
        }

        // ëª¨ë“  í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆì§€ë§Œ ì—°ì† ë¹ˆ í˜ì´ì§€ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ê²½ê³ ì™€ í•¨ê»˜ 1 ë°˜í™˜
        warn!("âš ï¸  No valid pages found during safe downward search, but only {} consecutive empty pages. Returning page 1 as fallback.", 
              consecutive_empty_pages);
        Ok(1)
    }

    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ìµœì¢… ê²€ì¦ - ë” ì² ì €í•œ ê²€ì¦ ë¡œì§
    /// ë§ˆì§€ë§‰ í˜ì´ì§€ ê²€ì¦ ë° ì œí’ˆ ìˆ˜ í™•ì¸
    async fn verify_last_page(&self, candidate_page: u32) -> Result<(u32, u32)> {
        info!("ğŸ” Verifying candidate last page: {}", candidate_page);

        // 1. í›„ë³´ í˜ì´ì§€ ë¶„ì„ (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¶„ì„)
        let analysis = self.get_or_analyze_page(candidate_page).await?;
        let products_on_last_page = analysis.product_count;
        let has_products = analysis.has_products;
        
        info!("ğŸ“Š Last page {} has {} products", candidate_page, products_on_last_page);
        
        if !has_products {
            warn!("âš ï¸  Candidate page {} has no products, performing downward search with safety check", candidate_page);
            let actual_last_page = self.find_last_valid_page_with_safety_check(candidate_page).await?;
            // ì‹¤ì œ ë§ˆì§€ë§‰ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ ë‹¤ì‹œ í™•ì¸
            let actual_analysis = self.get_or_analyze_page(actual_last_page).await?;
            return Ok((actual_last_page, actual_analysis.product_count));
        }

        // 2. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ì—ì„œ ì´ë¯¸ ë§ˆì§€ë§‰ í˜ì´ì§€ì„ì„ í™•ì‹ í•  ìˆ˜ ìˆë‹¤ë©´ ì¶”ê°€ í™•ì¸ ìƒëµ
        // í˜„ì¬ í˜ì´ì§€ê°€ í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ë°œê²¬ëœ ìµœëŒ€ í˜ì´ì§€ì™€ ê°™ë‹¤ë©´ ê²€ì¦ ì™„ë£Œ
        if analysis.max_pagination_page == candidate_page {
            info!("âœ… Page {} confirmed as last page via pagination analysis (max_pagination={})", 
                  candidate_page, analysis.max_pagination_page);
            info!("ğŸš€ Skipping additional verification - pagination analysis is reliable");
            return Ok((candidate_page, products_on_last_page));
        }
        
        // 3. í˜ì´ì§€ë„¤ì´ì…˜ ë¶„ì„ì´ ë¶ˆí™•ì‹¤í•œ ê²½ìš°ì—ë§Œ ìµœì†Œí•œì˜ ì¶”ê°€ ê²€ì¦ ìˆ˜í–‰
        info!("ğŸ” Pagination analysis inconclusive (current={}, max_pagination={}), performing minimal verification", 
              candidate_page, analysis.max_pagination_page);
        
        // ë°”ë¡œ ë‹¤ìŒ í˜ì´ì§€ 1ê°œë§Œ í™•ì¸ (ê³¼ë„í•œ ê²€ì¦ ë°©ì§€)
        let next_page = candidate_page + 1;
        match self.check_page_has_products(next_page).await {
            Ok(true) => {
                warn!("ğŸ” Found products on page {} after candidate {}, re-discovering", 
                      next_page, candidate_page);
                // ë” ë†’ì€ í˜ì´ì§€ì—ì„œ ì œí’ˆì„ ë°œê²¬í–ˆìœ¼ë¯€ë¡œ ê·¸ í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰
                return self.discover_from_page_with_count(next_page).await;
            },
            Ok(false) => {
                info!("âœ… Verified page {} as the last page with {} products (checked {} page ahead)", 
                      candidate_page, products_on_last_page, 1);
            },
            Err(e) => {
                debug!("âŒ Failed to check page {}: {}, assuming {} is last", next_page, e, candidate_page);
            }
        }
        
        Ok((candidate_page, products_on_last_page))
    }

    /// íŠ¹ì • í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰ ì‹œì‘ (ì œí’ˆ ìˆ˜ë„ ë°˜í™˜)
    async fn discover_from_page_with_count(&self, start_page: u32) -> Result<(u32, u32)> {
        info!("ğŸ”„ Re-discovering from page {} with product count", start_page);
        
        let mut current_page = start_page;
        let max_attempts = self.config.advanced.max_search_attempts;
        let mut attempts = 0;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts, stopping at page {}", current_page);
                break;
            }

            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let (has_products, max_page_in_pagination) = {
                let mut client = self.http_client.lock().await;
                match client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);
                        
                        info!("ğŸ“Š Page {} analysis: has_products={}, max_pagination={}", 
                              current_page, has_products, max_page);
                        
                        (has_products, max_page)
                    },
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        break;
                    }
                }
            };

            if !has_products {
                // ì œí’ˆì´ ì—†ìœ¼ë©´ ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰ í›„ ì œí’ˆ ìˆ˜ í™•ì¸
                let last_page = self.find_last_valid_page_with_safety_check(current_page).await?;
                let test_url = config_utils::matter_products_page_url_simple(last_page);
                let mut client = self.http_client.lock().await;
                let html = client.fetch_html_string(&test_url).await?;
                drop(client); // ë½ í•´ì œ
                let doc = scraper::Html::parse_document(&html);
                let products_count = self.count_products(&doc);
                return Ok((last_page, products_count));
            }

            if max_page_in_pagination > current_page {
                // ë” í° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ë™
                current_page = max_page_in_pagination;
                continue;
            } else {
                // ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬, ì œí’ˆ ìˆ˜ í™•ì¸
                let test_url = config_utils::matter_products_page_url_simple(current_page);
                let mut client = self.http_client.lock().await;
                let html = client.fetch_html_string(&test_url).await?;
                drop(client); // ë½ í•´ì œ
                let doc = scraper::Html::parse_document(&html);
                let products_count = self.count_products(&doc);
                return Ok((current_page, products_count));
            }
        }

        // ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ë„ë‹¬ ì‹œ í˜„ì¬ í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ í™•ì¸
        let test_url = config_utils::matter_products_page_url_simple(current_page);
        let mut client = self.http_client.lock().await;
        let html = client.fetch_html_string(&test_url).await?;
        drop(client); // ë½ í•´ì œ
        let doc = scraper::Html::parse_document(&html);
        let products_count = self.count_products(&doc);
        Ok((current_page, products_count))
    }

    /// íŠ¹ì • í˜ì´ì§€ë¶€í„° ë‹¤ì‹œ íƒìƒ‰ ì‹œì‘
    async fn discover_from_page(&self, start_page: u32) -> Result<u32> {
        info!("ğŸ”„ Re-discovering from page {}", start_page);
        
        let mut current_page = start_page;
        let max_attempts = self.config.advanced.max_search_attempts;
        let mut attempts = 0;

        loop {
            attempts += 1;
            if attempts > max_attempts {
                warn!("ğŸ”„ Reached maximum attempts, stopping at page {}", current_page);
                break;
            }

            let test_url = config_utils::matter_products_page_url_simple(current_page);
            
            let (has_products, max_page_in_pagination) = {
                let mut client = self.http_client.lock().await;
                match client.fetch_html_string(&test_url).await {
                    Ok(html) => {
                        let doc = scraper::Html::parse_document(&html);
                        let has_products = self.has_products_on_page(&doc);
                        let max_page = self.find_max_page_in_pagination(&doc);
                        
                        info!("ğŸ“Š Page {} analysis: has_products={}, max_pagination={}", 
                              current_page, has_products, max_page);
                        
                        (has_products, max_page)
                    },
                    Err(e) => {
                        warn!("âŒ Failed to fetch page {}: {}", current_page, e);
                        break;
                    }
                }
            };

            if !has_products {
                // ì œí’ˆì´ ì—†ìœ¼ë©´ ì•ˆì „ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ í•˜í–¥ íƒìƒ‰
                return self.find_last_valid_page_with_safety_check(current_page).await;
            }

            if max_page_in_pagination > current_page {
                // ë” í° í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì´ë™
                current_page = max_page_in_pagination;
            } else {
                // ë” í° í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ í˜„ì¬ í˜ì´ì§€ê°€ ë§ˆì§€ë§‰
                break;
            }

            tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
        }

        Ok(current_page)
    }

    /// íŠ¹ì • í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸ - í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ë„ í•¨ê»˜ í™•ì¸
    async fn check_page_has_products(&self, page: u32) -> Result<bool> {
        let test_url = config_utils::matter_products_page_url_simple(page);
        
        let mut client = self.http_client.lock().await;
        match client.fetch_html_string(&test_url).await {
            Ok(html) => {
                let doc = scraper::Html::parse_document(&html);
                
                // 1. ì œí’ˆ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                let has_products = self.has_products_on_page(&doc);
                
                // 2. í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ í™•ì¸ (ë” ì¤‘ìš”í•œ ì²´í¬)
                let active_page = self.get_active_page_number(&doc);
                
                // ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ì™€ í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                let is_correct_page = active_page == page;
                
                if !is_correct_page {
                    info!("âš ï¸  Page {} was redirected to page {} (pagination mismatch)", page, active_page);
                    return Ok(false);
                }
                
                info!("âœ… Page {} verification: has_products={}, active_page={}, is_correct_page={}", 
                      page, has_products, active_page, is_correct_page);
                
                Ok(has_products && is_correct_page)
            },
            Err(_) => Ok(false),
        }
    }

    /// í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ê°’ ì¶”ì¶œ - í˜„ì¬ í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    fn get_active_page_number(&self, doc: &scraper::Html) -> u32 {
        // í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì†Œë¥¼ ì°¾ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        // ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìš°ì„ ìˆœìœ„ ì¡°ì • (í˜ì´ì§€ë„¤ì´ì…˜ ìš°ì„  í´ë˜ìŠ¤: page-numbers.current)
        let active_selectors = [
            ".page-numbers.current", // ìš°ì„ ìˆœìœ„ ê°€ì¥ ë†’ìŒ (ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì •)
            "span.page-numbers.current", // ì •í™•í•œ ìš”ì†Œ ì§€ì •
            "a.page-numbers.current",
            ".current",
            ".active",
            ".pagination .current",
            ".pagination .active",
            "a.current",
            "span.current",
            "[aria-current='page']",
            ".wp-pagenavi .current",
            ".page-item.active a",
            ".page-link.active",
        ];
        
        for selector_str in &active_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                if let Some(element) = doc.select(&selector).next() {
                    // í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    let text = element.text().collect::<String>().trim().to_string();
                    if let Ok(page_num) = text.parse::<u32>() {
                        info!("ğŸ¯ Found active page number {} using selector '{}'", page_num, selector_str);
                        return page_num;
                    }
                }
            }
        }
        
        // í™œì„± í˜ì´ì§€ë„¤ì´ì…˜ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° URLì—ì„œ ì¶”ì¶œ ì‹œë„
        if let Some(canonical_url) = self.get_canonical_url(doc) {
            if let Some(page_num) = self.extract_page_number(&canonical_url) {
                info!("ğŸ¯ Found page number {} from canonical URL", page_num);
                return page_num;
            }
        }
        
        // ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš° 1 ë°˜í™˜ (ì²« ë²ˆì§¸ í˜ì´ì§€ë¡œ ì¶”ì •)
        warn!("âš ï¸  Could not determine active page number, assuming page 1");
        1
    }

    /// í˜ì´ì§€ì˜ canonical URL ì¶”ì¶œ
    fn get_canonical_url(&self, doc: &scraper::Html) -> Option<String> {
        if let Ok(selector) = scraper::Selector::parse("link[rel='canonical']") {
            if let Some(element) = doc.select(&selector).next() {
                return element.value().attr("href").map(|s| s.to_string());
            }
        }
        None
    }

    /// ì„¤ì • íŒŒì¼ì— ë§ˆì§€ë§‰ í˜ì´ì§€ ë° ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
    async fn update_last_known_page(&self, last_page: u32) -> Result<()> {
        use crate::infrastructure::config::ConfigManager;
        
        let config_manager = ConfigManager::new()?;
        
        // ì„¤ì • ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í´ë¡œì € ì‚¬ìš©
        config_manager.update_app_managed(|app_managed| {
            // ë§ˆì§€ë§‰ ì•Œë ¤ì§„ í˜ì´ì§€ ì—…ë°ì´íŠ¸
            app_managed.last_known_max_page = Some(last_page);
            
            // ë§ˆì§€ë§‰ ì„±ê³µí•œ í¬ë¡¤ë§ ì‹œê°„ ì—…ë°ì´íŠ¸
            app_managed.last_successful_crawl = Some(chrono::Utc::now().to_rfc3339());
            
            // ì¶”ì • ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸ (í˜ì´ì§€ë‹¹ 12ê°œ ì œí’ˆ - ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ê¸°ë°˜)
            app_managed.last_crawl_product_count = Some(last_page * 12);
            
            // í˜ì´ì§€ë‹¹ í‰ê·  ì œí’ˆ ìˆ˜ ì—…ë°ì´íŠ¸
            use crate::infrastructure::config::defaults::DEFAULT_PRODUCTS_PER_PAGE;
            app_managed.avg_products_per_page = Some(DEFAULT_PRODUCTS_PER_PAGE as f64);
            
            info!("ğŸ“ Updated config: last_page={}, timestamp={}", 
                  last_page, 
                  app_managed.last_successful_crawl.as_ref().unwrap_or(&"unknown".to_string()));
        }).await?;
        
        info!("âœ… Successfully updated last known page to {} in config file", last_page);
        Ok(())
    }

    /// ë°ì´í„° ë³€í™” ìƒíƒœ ë¶„ì„ ë° ê¶Œì¥ì‚¬í•­ ìƒì„±
    async fn analyze_data_changes(&self, current_estimated_products: u32) -> (SiteDataChangeStatus, Option<DataDecreaseRecommendation>) {
        // ì´ì „ í¬ë¡¤ë§ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        let previous_count = self.config.app_managed.last_crawl_product_count;
        
        match previous_count {
            None => {
                info!("ğŸ†• Initial site check - no previous data available");
                (SiteDataChangeStatus::Initial { count: current_estimated_products }, None)
            },
            Some(prev_count) => {
                let change_percentage = if prev_count > 0 {
                    ((current_estimated_products as f64 - prev_count as f64) / prev_count as f64) * 100.0
                } else {
                    0.0
                };
                
                if current_estimated_products > prev_count {
                    let increase = current_estimated_products - prev_count;
                    info!("ğŸ“ˆ Site data increased: {} -> {} (+{}, +{:.1}%)", 
                          prev_count, current_estimated_products, increase, change_percentage);
                    (SiteDataChangeStatus::Increased { 
                        new_count: current_estimated_products, 
                        previous_count: prev_count 
                    }, None)
                } else if current_estimated_products == prev_count {
                    info!("ğŸ“Š Site data stable: {} products", current_estimated_products);
                    (SiteDataChangeStatus::Stable { count: current_estimated_products }, None)
                } else {
                    let decrease = prev_count - current_estimated_products;
                    let decrease_percentage = (decrease as f64 / prev_count as f64) * 100.0;
                    
                    warn!("ğŸ“‰ Site data decreased: {} -> {} (-{}, -{:.1}%)", 
                          prev_count, current_estimated_products, decrease, decrease_percentage);
                    
                    let severity = if decrease_percentage < 10.0 {
                        SeverityLevel::Low
                    } else if decrease_percentage < 30.0 {
                        SeverityLevel::Medium
                    } else if decrease_percentage < 50.0 {
                        SeverityLevel::High
                    } else {
                        SeverityLevel::Critical
                    };
                    
                    let recommendation = self.generate_decrease_recommendation(decrease_percentage, &severity);
                    
                    (SiteDataChangeStatus::Decreased { 
                        current_count: current_estimated_products,
                        previous_count: prev_count,
                        decrease_amount: decrease
                    }, Some(recommendation))
                }
            }
        }
    }
    
    /// ë°ì´í„° ê°ì†Œ ì‹œ ê¶Œì¥ì‚¬í•­ ìƒì„±
    fn generate_decrease_recommendation(&self, decrease_percentage: f64, severity: &SeverityLevel) -> DataDecreaseRecommendation {
        match severity {
            SeverityLevel::Low => DataDecreaseRecommendation {
                action_type: RecommendedAction::WaitAndRetry,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¼ì‹œì ì¸ ë³€í™”ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "ì ì‹œ í›„(5-10ë¶„) ë‹¤ì‹œ ìƒíƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                    "ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ìˆ˜ë™ìœ¼ë¡œ ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::Medium => DataDecreaseRecommendation {
                action_type: RecommendedAction::ManualVerification,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "CSA-IoT ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ì œí’ˆ ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”".to_string(),
                    "ì‚¬ì´íŠ¸ì—ì„œ í•„í„° ì„¤ì •ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•˜ê³  ë¶€ë¶„ ì¬í¬ë¡¤ë§ì„ ê³ ë ¤í•˜ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::High => DataDecreaseRecommendation {
                action_type: RecommendedAction::BackupAndRecrawl,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% í¬ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… í›„ ì¬í¬ë¡¤ë§ì„ ê¶Œì¥í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¦‰ì‹œ ë°±ì—…í•˜ì„¸ìš”".to_string(),
                    "CSA-IoT ì‚¬ì´íŠ¸ë¥¼ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ì‹¤ì œ ìƒí™©ì„ íŒŒì•…í•˜ì„¸ìš”".to_string(),
                    "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¹„ìš°ê³  ì „ì²´ ì¬í¬ë¡¤ë§ì„ ìˆ˜í–‰í•˜ì„¸ìš”".to_string(),
                    "í¬ë¡¤ë§ ì™„ë£Œ í›„ ì´ì „ ë°ì´í„°ì™€ ë¹„êµ ë¶„ì„í•˜ì„¸ìš”".to_string(),
                ],
            },
            SeverityLevel::Critical => DataDecreaseRecommendation {
                action_type: RecommendedAction::BackupAndRecrawl,
                description: format!("ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ {:.1}% ì‹¬ê°í•˜ê²Œ ê°ì†Œí–ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.", decrease_percentage),
                severity: severity.clone(),
                action_steps: vec![
                    "ğŸš¨ ì¦‰ì‹œ í˜„ì¬ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë°±ì—…í•˜ì„¸ìš”".to_string(),
                    "CSA-IoT ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ ì‹¤ì œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ì‚¬ì´íŠ¸ êµ¬ì¡°ë‚˜ í•„í„° ì¡°ê±´ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”".to_string(),
                    "ë°±ì—… í™•ì¸ í›„ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ê³  ì „ì²´ ì¬í¬ë¡¤ë§í•˜ì„¸ìš”".to_string(),
                    "í¬ë¡¤ë§ ì„¤ì •(selector, URL ë“±)ì„ ì¬ê²€í† í•˜ì„¸ìš”".to_string(),
                ],
            },
        }
    }

    /// í˜ì´ì§€ì— ì œí’ˆì´ ìˆëŠ”ì§€ í™•ì¸
    fn has_products_on_page(&self, doc: &scraper::Html) -> bool {
        let product_count = self.count_products(doc);
        product_count > 0
    }

    /// í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
    fn find_max_page_in_pagination(&self, doc: &scraper::Html) -> u32 {
        let mut max_page = 1;
        
        // 1. í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ì—ì„œ ì°¾ê¸°
        let link_selectors = vec![
            "a[href*='page']",
            ".pagination a",
            ".page-numbers", // ëª¨ë“  í˜ì´ì§€ ë²ˆí˜¸ ìš”ì†Œ (aì™€ span ëª¨ë‘ í¬í•¨)
            ".page-numbers a", 
            ".pager a",
            "a[href*='paged']",
            ".page-numbers:not(.current):not(.dots)" // í˜„ì¬ í˜ì´ì§€ì™€ ì¤„ì„í‘œë¥¼ ì œì™¸í•œ í˜ì´ì§€ ë²ˆí˜¸
        ];
        
        for selector_str in &link_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                for element in doc.select(&selector) {
                    // href ì†ì„±ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    if let Some(href) = element.value().attr("href") {
                        if let Some(page_num) = self.extract_page_number(href) {
                            if page_num > max_page {
                                max_page = page_num;
                                debug!("Found higher page {} in href: {}", page_num, href);
                            }
                        }
                    }
                    
                    // í…ìŠ¤íŠ¸ì—ì„œë„ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                    let text = element.text().collect::<String>().trim().to_string();
                    if let Ok(page_num) = text.parse::<u32>() {
                        if page_num > max_page && page_num < 10000 { // í•©ë¦¬ì ì¸ ìƒí•œì„ 
                            max_page = page_num;
                            debug!("Found higher page {} in text: {}", page_num, text);
                        }
                    }
                }
            }
        }
        
        debug!("Max page found in pagination: {}", max_page);
        max_page
    }

    /// URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
    fn extract_page_number(&self, url: &str) -> Option<u32> {
        // URL íŒ¨í„´: /page/123/ ë˜ëŠ” paged=123
        let patterns = [
            r"/page/(\d+)",
            r"paged=(\d+)",
            r"page=(\d+)",
            r"/(\d+)/$",  // ëì— ìˆ«ìê°€ ìˆëŠ” ê²½ìš°
        ];
        
        for pattern in &patterns {
            if let Ok(re) = regex::Regex::new(pattern) {
                if let Some(caps) = re.captures(url) {
                    if let Some(num_str) = caps.get(1) {
                        if let Ok(num) = num_str.as_str().parse::<u32>() {
                            return Some(num);
                        }
                    }
                }
            }
        }
        
        None
    }

    /// í˜ì´ì§€ì—ì„œ ì œí’ˆ ê°œìˆ˜ ì¹´ìš´íŠ¸ (ëª¨ë“  ì„ íƒìë¥¼ ì‹œë„í•˜ê³  ê°€ì¥ ë§ì€ ê²°ê³¼ ë°˜í™˜)
    fn count_products(&self, doc: &scraper::Html) -> u32 {
        let mut max_count = 0;
        let mut best_selector = "none";
        
        for selector_str in &self.config.advanced.product_selectors {
            if let Ok(selector) = scraper::Selector::parse(selector_str) {
                let count = doc.select(&selector).count() as u32;
                debug!("Selector '{}' found {} products", selector_str, count);
                if count > max_count {
                    max_count = count;
                    best_selector = selector_str;
                }
            } else {
                debug!("Failed to parse selector: {}", selector_str);
            }
        }
        
        info!("Total products found on page: {} (using selector: {})", max_count, best_selector);
        max_count
    }

    /// í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¶„ì„
    async fn get_or_analyze_page(&self, page_number: u32) -> Result<PageAnalysisCache> {
        // ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        {
            let cache = self.page_cache.lock().await;
            if let Some(cached) = cache.get(&page_number) {
                debug!("ğŸ“‹ Using cached analysis for page {}", page_number);
                return Ok(cached.clone());
            }
        }
        
        // ìºì‹œì— ì—†ìœ¼ë©´ ìƒˆë¡œ ë¶„ì„
        debug!("ğŸ” Analyzing page {} (not in cache)", page_number);
        let url = config_utils::matter_products_page_url_simple(page_number);
        
        let (product_count, max_pagination_page, active_page, has_products) = {
            let mut client = self.http_client.lock().await;
            let html = client.fetch_html_string(&url).await?;
            drop(client); // ë½ í•´ì œ
            
            let doc = scraper::Html::parse_document(&html);
            let product_count = self.count_products(&doc);
            let max_pagination_page = self.find_max_page_in_pagination(&doc);
            let active_page = self.get_active_page_number(&doc);
            let has_products = product_count > 0;
            
            (product_count, max_pagination_page, active_page, has_products)
        };
        
        let analysis = PageAnalysisCache {
            product_count,
            max_pagination_page,
            active_page,
            has_products,
            analyzed_at: std::time::Instant::now(),
        };
        
        // ìºì‹œì— ì €ì¥
        {
            let mut cache = self.page_cache.lock().await;
            cache.insert(page_number, analysis.clone());
        }
        
        info!("ğŸ“Š Page {} analysis: has_products={}, product_count={}, max_pagination={}", 
              page_number, has_products, product_count, max_pagination_page);
        
        Ok(analysis)
    }
    
    /// ìºì‹œë¥¼ ì´ˆê¸°í™” (ìƒˆë¡œìš´ ìƒíƒœ ì²´í¬ ì‹œì‘ ì‹œ í˜¸ì¶œ)
    async fn clear_page_cache(&self) {
        let mut cache = self.page_cache.lock().await;
        cache.clear();
        debug!("ğŸ—‘ï¸  Page cache cleared");
    }
    
    /// í¬ë¡¤ë§ ë²”ìœ„ ê¶Œì¥ì‚¬í•­ ê³„ì‚°
    /// ë¡œì»¬ DB ìƒíƒœì™€ ì‚¬ì´íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í¬ë¡¤ë§ ëŒ€ìƒ í˜ì´ì§€ ë²”ìœ„ë¥¼ ê³„ì‚° (ë‚´ë¶€ìš©)
    async fn calculate_crawling_range_recommendation_internal(
        &self,
        total_pages_on_site: u32,
        products_on_last_page: u32,
        estimated_products: u32,
    ) -> Result<CrawlingRangeRecommendation> {
        info!("ğŸ” Calculating crawling range recommendation...");
        
        // í˜„ì¬ ë¡œì»¬ DB ìƒíƒœ í™•ì¸
        let local_db_status = self.get_local_db_status().await?;
        
        // DBê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì „ì²´ í¬ë¡¤ë§ ê¶Œì¥
        if local_db_status.is_empty {
            info!("ğŸ“Š Local DB is empty - recommending full crawl");
            return Ok(CrawlingRangeRecommendation::Full);
        }
        
        // ì‚¬ì´íŠ¸ ë°ì´í„° ë³€í™” ë¶„ì„
        let data_change_analysis = self.analyze_site_data_changes(estimated_products).await;
        
        // í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚°
        let crawling_range = self.calculate_next_crawling_pages(
            &local_db_status,
            total_pages_on_site,
            products_on_last_page,
            estimated_products,
            &data_change_analysis,
        ).await?;
        
        info!("ğŸ“Š Crawling range recommendation: {:?}", crawling_range);
        Ok(crawling_range)
    }
    
    /// ë¡œì»¬ DB ìƒíƒœ ì¡°íšŒ
    async fn get_local_db_status(&self) -> Result<LocalDbStatus> {
        match &self.product_repo {
            Some(repo) => {
                let products = repo.get_all_products().await?;
                
                if products.is_empty() {
                    return Ok(LocalDbStatus {
                        is_empty: true,
                        max_page_id: 0,
                        max_index_in_page: 0,
                        total_saved_products: 0,
                    });
                }
                
                // ê°€ì¥ ë†’ì€ pageIdì™€ í•´ë‹¹ í˜ì´ì§€ì—ì„œì˜ ìµœëŒ€ indexInPage ì°¾ê¸°
                let mut max_page_id = 0i32;
                let mut max_index_in_page = 0i32;
                
                for product in &products {
                    if let (Some(page_id), Some(index_in_page)) = (product.page_id, product.index_in_page) {
                        if page_id > max_page_id {
                            max_page_id = page_id;
                            max_index_in_page = index_in_page;
                        } else if page_id == max_page_id && index_in_page > max_index_in_page {
                            max_index_in_page = index_in_page;
                        }
                    }
                }
                
                info!("ğŸ“Š Local DB status: max_page_id={}, max_index_in_page={}, total_products={}", 
                      max_page_id, max_index_in_page, products.len());
                
                Ok(LocalDbStatus {
                    is_empty: false,
                    max_page_id: max_page_id.max(0) as u32,
                    max_index_in_page: max_index_in_page.max(0) as u32,
                    total_saved_products: products.len() as u32,
                })
            },
            None => {
                warn!("âš ï¸  Product repository not available - assuming empty DB");
                Ok(LocalDbStatus {
                    is_empty: true,
                    max_page_id: 0,
                    max_index_in_page: 0,
                    total_saved_products: 0,
                })
            }
        }
    }
    
    /// ì‚¬ì´íŠ¸ ë°ì´í„° ë³€í™” ë¶„ì„
    async fn analyze_site_data_changes(&self, current_estimated_products: u32) -> DataChangeAnalysis {
        let previous_count = self.config.app_managed.last_crawl_product_count;
        
        match previous_count {
            None => DataChangeAnalysis::Initial,
            Some(prev_count) => {
                let change_percentage = if prev_count > 0 {
                    ((current_estimated_products as f64 - prev_count as f64) / prev_count as f64) * 100.0
                } else {
                    0.0
                };
                
                if current_estimated_products > prev_count {
                    DataChangeAnalysis::Increased { 
                        new_products: current_estimated_products - prev_count,
                        change_percentage,
                    }
                } else if current_estimated_products == prev_count {
                    DataChangeAnalysis::Stable
                } else {
                    DataChangeAnalysis::Decreased {
                        lost_products: prev_count - current_estimated_products,
                        change_percentage: -change_percentage,
                    }
                }
            }
        }
    }
    
    /// ë‹¤ìŒ í¬ë¡¤ë§ í˜ì´ì§€ ë²”ìœ„ ê³„ì‚°
    async fn calculate_next_crawling_pages(
        &self,
        local_db_status: &LocalDbStatus,
        total_pages_on_site: u32,
        products_on_last_page: u32,
        estimated_products: u32,
        data_change_analysis: &DataChangeAnalysis,
    ) -> Result<CrawlingRangeRecommendation> {
        use crate::infrastructure::config::defaults::DEFAULT_PRODUCTS_PER_PAGE;
        let products_per_page = DEFAULT_PRODUCTS_PER_PAGE;
        
        // ë°ì´í„° ë³€í™”ì— ë”°ë¥¸ í¬ë¡¤ë§ ì „ëµ ê²°ì •
        match data_change_analysis {
            DataChangeAnalysis::Initial => {
                info!("ğŸ“Š Initial crawling - recommending full crawl");
                return Ok(CrawlingRangeRecommendation::Full);
            },
            DataChangeAnalysis::Decreased { lost_products, .. } => {
                warn!("ğŸ“‰ Site data decreased by {} products - recommending full recrawl", lost_products);
                return Ok(CrawlingRangeRecommendation::Full);
            },
            DataChangeAnalysis::Increased { new_products, .. } => {
                // ìƒˆë¡œìš´ ì œí’ˆì´ ë§ì´ ì¶”ê°€ëœ ê²½ìš° ë¶€ë¶„ í¬ë¡¤ë§
                let recommended_pages = (*new_products as f64 / products_per_page as f64).ceil() as u32;
                let limited_pages = recommended_pages.min(self.config.user.crawling.page_range_limit);
                
                info!("ğŸ“ˆ Site data increased by {} products - recommending partial crawl of {} pages", 
                      new_products, limited_pages);
                return Ok(CrawlingRangeRecommendation::Partial(limited_pages));
            },
            DataChangeAnalysis::Stable => {
                // ì•ˆì •ì ì¸ ê²½ìš° ê¸°ì¡´ ë¡œì§ ì ìš©
            }
        }
        
        // ê¸°ì¡´ ë¡œì§: ë¡œì»¬ DB ìƒíƒœ ê¸°ë°˜ ê³„ì‚°
        if local_db_status.is_empty {
            return Ok(CrawlingRangeRecommendation::Full);
        }
        
        // 1ë‹¨ê³„: ë¡œì»¬ DBì— ë§ˆì§€ë§‰ìœ¼ë¡œ ì €ì¥ëœ ì œí’ˆì˜ 'ì—­ìˆœ ì ˆëŒ€ ì¸ë±ìŠ¤' ê³„ì‚°
        let last_saved_index = (local_db_status.max_page_id * products_per_page) + local_db_status.max_index_in_page;
        info!("ğŸ“Š Last saved product index: {}", last_saved_index);
        
        // 2ë‹¨ê³„: ë‹¤ìŒì— í¬ë¡¤ë§í•´ì•¼ í•  ì œí’ˆì˜ 'ì—­ìˆœ ì ˆëŒ€ ì¸ë±ìŠ¤' ê²°ì •
        let next_product_index = last_saved_index + 1;
        info!("ğŸ“Š Next product index to crawl: {}", last_saved_index);
        
        // 3ë‹¨ê³„: 'ì—­ìˆœ ì ˆëŒ€ ì¸ë±ìŠ¤'ë¥¼ ì›¹ì‚¬ì´íŠ¸ í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë³€í™˜
        let total_products = ((total_pages_on_site - 1) * products_per_page) + products_on_last_page;
        
        // ë‹¤ìŒ ì œí’ˆì´ ì „ì²´ ì œí’ˆ ìˆ˜ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° (ëª¨ë“  ì œí’ˆ í¬ë¡¤ë§ ì™„ë£Œ)
        if next_product_index >= total_products {
            info!("ğŸ“Š All products have been crawled - no crawling needed");
            return Ok(CrawlingRangeRecommendation::None);
        }
        
        // 'ìˆœì°¨ ì¸ë±ìŠ¤'ë¡œ ë³€í™˜ (ìµœì‹  ì œí’ˆì´ 0)
        let forward_index = (total_products - 1) - next_product_index;
        
        // ì›¹ì‚¬ì´íŠ¸ í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚°
        let target_page_number = (forward_index / products_per_page) + 1;
        
        info!("ğŸ“Š Target page to start crawling: {}", target_page_number);
        
        // 4ë‹¨ê³„: í¬ë¡¤ë§ í˜ì´ì§€ ë²”ìœ„ ê²°ì •
        let max_crawl_pages = self.config.user.crawling.page_range_limit;
        let start_page = target_page_number;
        let end_page = if start_page >= max_crawl_pages {
            start_page - max_crawl_pages + 1
        } else {
            1
        };
        
        let actual_pages_to_crawl = if start_page >= end_page {
            start_page - end_page + 1
        } else {
            start_page
        };
        
        info!("ğŸ“Š Crawling range: pages {} to {} (total: {} pages)", 
              start_page, end_page, actual_pages_to_crawl);
        
        Ok(CrawlingRangeRecommendation::Partial(actual_pages_to_crawl))
    }
}

/// ë¡œì»¬ DB ìƒíƒœ ì •ë³´
#[derive(Debug, Clone)]
struct LocalDbStatus {
    is_empty: bool,
    max_page_id: u32,
    max_index_in_page: u32,
    total_saved_products: u32,
}

/// ë°ì´í„° ë³€í™” ë¶„ì„ ê²°ê³¼
#[derive(Debug, Clone)]
enum DataChangeAnalysis {
    Initial,
    Increased { new_products: u32, change_percentage: f64 },
    Decreased { lost_products: u32, change_percentage: f64 },
    Stable,
}

/// ì»¬ë ‰í„° ì„¤ì •
#[derive(Debug, Clone)]
pub struct CollectorConfig {
    pub batch_size: u32,
    pub max_concurrent: u32,
    pub concurrency: u32, // alias for max_concurrent  
    pub delay_between_requests: Duration,
    pub delay_ms: u64, // alias for delay_between_requests in milliseconds
    pub retry_attempts: u32,
    pub retry_max: u32, // alias for retry_attempts
}

impl Default for CollectorConfig {
    fn default() -> Self {
        Self {
            batch_size: 10,
            max_concurrent: 3,
            concurrency: 3,
            delay_between_requests: Duration::from_millis(500),
            delay_ms: 500,
            retry_attempts: 3,
            retry_max: 3,
        }
    }
}

/// í—¬ìŠ¤ ìŠ¤ì½”ì–´ ê³„ì‚° í•¨ìˆ˜
fn calculate_health_score(response_time: Duration, total_pages: u32) -> f64 {
    // ì‘ë‹µ ì‹œê°„ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 0.7) - ë” ê´€ëŒ€í•œ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •
    let response_score = if response_time.as_millis() <= 2000 {
        0.7  // 2ì´ˆ ì´í•˜ëŠ” ì–‘í˜¸
    } else if response_time.as_millis() <= 5000 {
        0.5  // 5ì´ˆ ì´í•˜ëŠ” ë³´í†µ
    } else if response_time.as_millis() <= 10000 {
        0.3  // 10ì´ˆ ì´í•˜ëŠ” ëŠë¦¼
    } else {
        0.1  // 10ì´ˆ ì´ˆê³¼ëŠ” ë¬¸ì œ
    };
    
    // í˜ì´ì§€ ìˆ˜ ê¸°ë°˜ ì ìˆ˜ (0.0 ~ 0.3) - í˜ì´ì§€ ë°œê²¬ ì—¬ë¶€ê°€ ë” ì¤‘ìš”
    let page_score = if total_pages > 0 {
        0.3
    } else {
        0.0
    };
    
    response_score + page_score
}

/// ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductListCollectorImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

impl ProductListCollectorImpl {
    pub fn new(
        http_client: Arc<tokio::sync::Mutex<HttpClient>>,
        data_extractor: Arc<MatterDataExtractor>,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client,
            data_extractor,
            config,
        }
    }
}

#[async_trait]
impl ProductListCollector for ProductListCollectorImpl {
    async fn collect_all_pages(&self, total_pages: u32) -> Result<Vec<String>> {
        info!("ğŸ” Collecting from {} pages with parallel processing", total_pages);
        
        // Use the existing parallel implementation from collect_page_range
        self.collect_page_range(1, total_pages).await
    }
    
    async fn collect_page_range(&self, start_page: u32, end_page: u32) -> Result<Vec<String>> {
        let mut all_urls = Vec::new();
        
        // Handle descending range (older to newer) - typical for our use case
        let pages: Vec<u32> = if start_page > end_page {
            // Descending range: start from oldest (highest page number) to newest (lower page number)
            (end_page..=start_page).rev().collect()
        } else {
            // Ascending range: start from lowest to highest page number
            (start_page..=end_page).collect()
        };
        
        info!("ğŸ” Collecting from {} pages in range {} to {} (order: {})", 
              pages.len(), 
              start_page, 
              end_page,
              if start_page > end_page { "oldest first" } else { "newest first" });
        
        let max_concurrent = self.config.max_concurrent as usize;
        
        // Process pages in true parallel batches with proper concurrency control
        for chunk in pages.chunks(max_concurrent) {
            let mut tasks = Vec::new();
            
            info!("ğŸš€ Starting parallel batch of {} pages", chunk.len());
            
            for &page in chunk {
                let http_client = Arc::clone(&self.http_client);
                let data_extractor = Arc::clone(&self.data_extractor);
                
                let task = tokio::spawn(async move {
                    let url = config_utils::matter_products_page_url_simple(page);
                    let mut client = http_client.lock().await;
                    let html = client.fetch_html_string(&url).await?;
                    drop(client);
                    
                    let doc = scraper::Html::parse_document(&html);
                    let urls = data_extractor.extract_product_urls(&doc, "https://csa-iot.org")?;
                    
                    debug!("ğŸ”— Extracted {} URLs from page {}", urls.len(), page);
                    Ok::<(u32, Vec<String>), anyhow::Error>((page, urls))
                });
                
                tasks.push(task);
            }
            
            // Wait for all tasks in this batch to complete concurrently
            let results = futures::future::join_all(tasks).await;
            
            for result in results {
                match result {
                    Ok(Ok((page, urls))) => {
                        let urls_len = urls.len();
                        all_urls.extend(urls);
                        debug!("ğŸ“„ Collected {} URLs from page {} (total so far: {})", urls_len, page, all_urls.len());
                    }
                    Ok(Err(e)) => warn!("âš ï¸  Failed to collect URLs: {}", e),
                    Err(e) => warn!("Task failed: {}", e),
                }
            }
            
            info!("ğŸ“Š Completed parallel batch: {}/{} pages processed, {} URLs collected so far", 
                  chunk.len(), pages.len(), all_urls.len());
            
            // Apply rate limiting between batches, not between individual requests
            if chunk.len() == max_concurrent && !pages.chunks(max_concurrent).last().unwrap().contains(&chunk[0]) {
                tokio::time::sleep(self.config.delay_between_requests).await;
                info!("â±ï¸  Rate limiting delay applied between batches");
            }
        }
        
        info!("ğŸ“‹ Total URLs collected from page range {}-{}: {}", start_page, end_page, all_urls.len());
        Ok(all_urls)
    }
    
    async fn collect_single_page(&self, page: u32) -> Result<Vec<String>> {
        let url = config_utils::matter_products_page_url_simple(page);
        let mut client = self.http_client.lock().await;
        let html = client.fetch_html_string(&url).await?;
        drop(client);
        
        let doc = scraper::Html::parse_document(&html);
        let urls = self.data_extractor.extract_product_urls(&doc, "https://csa-iot.org")?;
        
        debug!("ğŸ”— Extracted {} URLs from page {}", urls.len(), page);
        Ok(urls)
    }
    
    async fn collect_page_range_with_cancellation(&self, start_page: u32, end_page: u32, cancellation_token: CancellationToken) -> Result<Vec<String>> {
        let mut all_urls = Vec::new();
        
        // Handle descending range (older to newer) - typical for our use case
        let pages: Vec<u32> = if start_page > end_page {
            // Descending range: start from oldest (highest page number) to newest (lower page number)
            (end_page..=start_page).rev().collect()
        } else {
            // Ascending range: start from lowest to highest page number
            (start_page..=end_page).collect()
        };
        
        info!("ğŸ” Collecting from {} pages in range {} to {} with cancellation support", 
              pages.len(), start_page, end_page);
        
        let max_concurrent = self.config.max_concurrent as usize;
        
        // Process pages in true parallel batches with proper concurrency control
        for chunk in pages.chunks(max_concurrent) {
            // ë°°ì¹˜ ì‹œì‘ ì „ ì·¨ì†Œ í™•ì¸
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Page range collection cancelled at batch");
                return Err(anyhow::anyhow!("Page range collection cancelled"));
            }
            
            let mut tasks = Vec::new();
            
            info!("ğŸš€ Starting cancellable parallel batch of {} pages", chunk.len());
            
            for &page in chunk {
                let http_client = Arc::clone(&self.http_client);
                let data_extractor = Arc::clone(&self.data_extractor);
                let token_clone = cancellation_token.clone();
                
                let task = tokio::spawn(async move {
                    // ì‘ì—… ì‹œì‘ ì „ ì·¨ì†Œ í™•ì¸
                    if token_clone.is_cancelled() {
                        debug!("ğŸ›‘ Page {} collection cancelled before start", page);
                        return Err(anyhow::anyhow!("Page collection cancelled"));
                    }
                    
                    let url = config_utils::matter_products_page_url_simple(page);
                    let mut client = http_client.lock().await;
                    let html = client.fetch_html_string(&url).await?;
                    drop(client);
                    
                    // HTTP ìš”ì²­ ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
                    if token_clone.is_cancelled() {
                        debug!("ğŸ›‘ Page {} collection cancelled after HTTP request", page);
                        return Err(anyhow::anyhow!("Page collection cancelled after HTTP request"));
                    }
                    
                    let doc = scraper::Html::parse_document(&html);
                    let urls = data_extractor.extract_product_urls(&doc, "https://csa-iot.org")?;
                    
                    debug!("ğŸ”— Extracted {} URLs from page {}", urls.len(), page);
                    Ok::<(u32, Vec<String>), anyhow::Error>((page, urls))
                });
                
                tasks.push(task);
            }
            
            // ë°°ì¹˜ ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (ì·¨ì†Œ í† í°ê³¼ í•¨ê»˜)
            let results = tokio::select! {
                results = futures::future::join_all(tasks) => results,
                _ = cancellation_token.cancelled() => {
                    warn!("ğŸ›‘ Page range collection cancelled during batch execution");
                    return Err(anyhow::anyhow!("Page range collection cancelled during batch execution"));
                }
            };
            
            for result in results {
                match result {
                    Ok(Ok((page, urls))) => {
                        let urls_len = urls.len();
                        all_urls.extend(urls);
                        debug!("ğŸ“„ Collected {} URLs from page {} (total so far: {})", urls_len, page, all_urls.len());
                    }
                    Ok(Err(e)) => {
                        if e.to_string().contains("cancelled") {
                            warn!("ğŸ›‘ Page collection was cancelled");
                            return Err(e);
                        } else {
                            warn!("âš ï¸  Failed to collect URLs: {}", e);
                        }
                    }
                    Err(e) => warn!("Task failed: {}", e),
                }
            }
            
            // ë°°ì¹˜ ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Page range collection cancelled after batch completion");
                return Err(anyhow::anyhow!("Page range collection cancelled"));
            }
            
            info!("ğŸ“Š Completed cancellable parallel batch: {}/{} pages processed, {} URLs collected so far", 
                  chunk.len(), pages.len(), all_urls.len());
            
            // Apply rate limiting between batches with cancellation support
            if chunk.len() == max_concurrent && !pages.chunks(max_concurrent).last().unwrap().contains(&chunk[0]) {
                tokio::select! {
                    _ = tokio::time::sleep(self.config.delay_between_requests) => {
                        info!("â±ï¸  Rate limiting delay applied between batches");
                    },
                    _ = cancellation_token.cancelled() => {
                        warn!("ğŸ›‘ Page range collection cancelled during rate limiting delay");
                        return Err(anyhow::anyhow!("Page range collection cancelled during delay"));
                    }
                }
            }
        }
        
        info!("ğŸ“‹ Total URLs collected from page range {}-{} with cancellation: {}", start_page, end_page, all_urls.len());
        Ok(all_urls)
    }
    
    async fn collect_page_batch(&self, pages: &[u32]) -> Result<Vec<String>> {
        let mut all_urls = Vec::new();
        
        for page in pages {
            match self.collect_single_page(*page).await {
                Ok(urls) => all_urls.extend(urls),
                Err(e) => warn!("Failed to collect from page {}: {}", page, e),
            }
            
            tokio::time::sleep(self.config.delay_between_requests).await;
        }
        
        Ok(all_urls)
    }
}

/// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct ProductDetailCollectorImpl {
    http_client: Arc<tokio::sync::Mutex<HttpClient>>,
    data_extractor: Arc<MatterDataExtractor>,
    config: CollectorConfig,
}

impl ProductDetailCollectorImpl {
    pub fn new(
        http_client: Arc<tokio::sync::Mutex<HttpClient>>,
        data_extractor: Arc<MatterDataExtractor>,
        config: CollectorConfig,
    ) -> Self {
        Self {
            http_client,
            data_extractor,
            config,
        }
    }
}

#[async_trait]
impl ProductDetailCollector for ProductDetailCollectorImpl {
    async fn collect_details(&self, urls: &[String]) -> Result<Vec<ProductDetail>> {
        // ë°±ì—… ì•ˆì „ì¥ì¹˜: ì·¨ì†Œ í† í°ì´ ì—†ì–´ë„ ìµœì†Œí•œì˜ ì²´í¬ë¥¼ ìœ„í•´ ê¸°ë³¸ í† í° ìƒì„±
        let default_token = CancellationToken::new();
        warn!("âš ï¸  collect_details called without cancellation token - using default token as fallback");
        
        // ì·¨ì†Œ ê°€ëŠ¥í•œ ë©”ì„œë“œë¡œ ìœ„ì„
        self.collect_details_with_cancellation(urls, default_token).await
    }
    
    async fn collect_details_with_cancellation(&self, urls: &[String], cancellation_token: CancellationToken) -> Result<Vec<ProductDetail>> {
        let mut products = Vec::new();
        let max_concurrent = self.config.max_concurrent as usize;
        
        info!("ğŸš€ Starting REAL concurrent product detail collection with cancellation: {} URLs with {} concurrent workers", 
              urls.len(), max_concurrent);
        
        // Use a semaphore to limit actual concurrent HTTP requests
        let semaphore = Arc::new(Semaphore::new(max_concurrent));
        
        // Create ALL tasks immediately for true parallel execution
        let mut all_tasks = Vec::new();
        
        for (index, url) in urls.iter().enumerate() {
            // Early cancellation check
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Cancellation detected before starting task {}", index);
                break;
            }
            
            let url = url.clone();
            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let cancellation_token = cancellation_token.clone();
            let semaphore = Arc::clone(&semaphore);
            
            let task = tokio::spawn(async move {
                // Acquire semaphore permit (limits concurrent HTTP requests)
                let permit = match semaphore.try_acquire() {
                    Ok(permit) => permit,
                    Err(_) => {
                        // If can't acquire immediately, wait with cancellation
                        let acquire_future = semaphore.acquire();
                        tokio::select! {
                            permit = acquire_future => {
                                match permit {
                                    Ok(permit) => permit,
                                    Err(_) => {
                                        debug!("ğŸ›‘ Task {} cancelled while waiting for semaphore", index);
                                        return Err(anyhow::anyhow!("Cancelled while waiting for semaphore"));
                                    }
                                }
                            }
                            _ = cancellation_token.cancelled() => {
                                debug!("ğŸ›‘ Task {} cancelled while waiting for semaphore", index);
                                return Err(anyhow::anyhow!("Cancelled while waiting for semaphore"));
                            }
                        }
                    }
                };
                
                // Immediate cancellation check after acquiring permit
                if cancellation_token.is_cancelled() {
                    debug!("ğŸ›‘ Task {} cancelled before HTTP request", index);
                    return Err(anyhow::anyhow!("Operation cancelled before request"));
                }
                
                info!("ğŸŒ Starting HTTP request for URL {}: {}", index, url);
                
                // HTTP request with timeout and cancellation
                let html = {
                    let client_future = http_client.lock();
                    let mut client = tokio::select! {
                        client = client_future => client,
                        _ = cancellation_token.cancelled() => {
                            warn!("ğŸ›‘ HTTP client acquisition CANCELLED for URL {}", index);
                            return Err(anyhow::anyhow!("HTTP client acquisition cancelled"));
                        }
                    };
                    
                    let fetch_future = client.fetch_html_string(&url);
                    let timeout_future = tokio::time::sleep(tokio::time::Duration::from_secs(30));
                    
                    // Race between HTTP request, timeout, and cancellation
                    let result = tokio::select! {
                        result = fetch_future => {
                            match result {
                                Ok(html) => {
                                    info!("âœ… HTTP request completed for URL {}", index);
                                    Ok(html)
                                }
                                Err(e) => {
                                    warn!("âŒ HTTP request failed for URL {}: {}", index, e);
                                    Err(e)
                                }
                            }
                        }
                        _ = timeout_future => {
                            warn!("â° HTTP request TIMEOUT for URL {}", index);
                            Err(anyhow::anyhow!("HTTP request timeout"))
                        }
                        _ = cancellation_token.cancelled() => {
                            warn!("ğŸ›‘ HTTP request CANCELLED for URL {}: {}", index, url);
                            Err(anyhow::anyhow!("HTTP request cancelled by user"))
                        }
                    };
                    
                    drop(client); // Release HTTP client lock immediately
                    drop(permit); // Release semaphore permit immediately
                    result?
                };
                
                // Final cancellation check before processing
                if cancellation_token.is_cancelled() {
                    warn!("ğŸ›‘ Processing cancelled for URL {}", index);
                    return Err(anyhow::anyhow!("Processing cancelled"));
                }
                
                // Parse and extract product details
                let doc = scraper::Html::parse_document(&html);
                let product_detail = data_extractor.extract_product_detail(&doc, url.clone())?;
                
                info!("ğŸ“¦ Product extracted successfully for URL {}: {}", index, 
                      product_detail.certification_id.as_deref().unwrap_or("Unknown"));
                Ok::<ProductDetail, anyhow::Error>(product_detail)
            });
            
            all_tasks.push(task);
        }
        
        info!("ğŸš€ Launched {} concurrent tasks with semaphore limit of {}, waiting for completion...", 
              all_tasks.len(), max_concurrent);
        
        // Use tokio::select! to race between task completion and cancellation
        let results = tokio::select! {
            results = futures::future::join_all(all_tasks) => results,
            _ = cancellation_token.cancelled() => {
                warn!("ğŸ›‘ Task collection CANCELLED by user - tasks may still be running");
                return Err(anyhow::anyhow!("Task collection cancelled by user"));
            }
        };
        
        // Process results
        let mut successful_count = 0;
        let mut cancelled_count = 0;
        let mut failed_count = 0;
        
        for (index, result) in results.into_iter().enumerate() {
            match result {
                Ok(Ok(product)) => {
                    products.push(product);
                    successful_count += 1;
                    debug!("âœ… Task {} completed successfully", index);
                }
                Ok(Err(e)) => {
                    if e.to_string().contains("cancelled") {
                        cancelled_count += 1;
                        debug!("ğŸ›‘ Task {} was cancelled: {}", index, e);
                    } else {
                        failed_count += 1;
                        warn!("âŒ Task {} failed: {}", index, e);
                    }
                }
                Err(e) => {
                    failed_count += 1;
                    warn!("ğŸ’¥ Task {} panicked: {}", index, e);
                }
            }
        }
        
        // Final status report
        if cancellation_token.is_cancelled() {
            warn!("ğŸ›‘ Collection CANCELLED: {} successful, {} cancelled, {} failed out of {} total", 
                  successful_count, cancelled_count, failed_count, urls.len());
        } else {
            info!("âœ… Collection COMPLETED: {} successful, {} failed out of {} total", 
                  successful_count, failed_count, urls.len());
        }
        
        Ok(products)
    }

    async fn collect_single_product(&self, url: &str) -> Result<ProductDetail> {
        let mut client = self.http_client.lock().await;
        let html = client.fetch_html_string(url).await?;
        drop(client);
        
        let doc = scraper::Html::parse_document(&html);
        let product_detail = self.data_extractor.extract_product_detail(&doc, url.to_string())?;
        
        debug!("ğŸ“¦ Extracted product: {}", product_detail.certification_id.as_deref().unwrap_or("Unknown"));
        Ok(product_detail)
    }
    
    async fn collect_product_batch(&self, urls: &[String]) -> Result<Vec<ProductDetail>> {
        let mut products = Vec::new();
        
        for url in urls {
            match self.collect_single_product(url).await {
                Ok(product) => products.push(product),
                Err(e) => warn!("Failed to collect product from {}: {}", url, e),
            }
            
            tokio::time::sleep(self.config.delay_between_requests).await;
        }
        
        Ok(products)
    }
}

/// ProductDetailì„ Productë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
pub fn product_detail_to_product(detail: crate::domain::product::ProductDetail) -> Product {
    Product {
        url: detail.url,
        manufacturer: detail.manufacturer,
        model: detail.model,
        certificate_id: detail.certification_id,
        page_id: detail.page_id,
        index_in_page: detail.index_in_page,
        created_at: detail.created_at,
        updated_at: detail.updated_at,
    }
}

/// í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚° ë° ê´€ë¦¬ ì„œë¹„ìŠ¤
pub struct CrawlingRangeCalculator {
    product_repo: Arc<IntegratedProductRepository>,
    config: AppConfig,
}

impl CrawlingRangeCalculator {
    pub fn new(product_repo: Arc<IntegratedProductRepository>, config: AppConfig) -> Self {
        Self {
            product_repo,
            config,
        }
    }

    /// ë‹¤ìŒ í¬ë¡¤ë§ ëŒ€ìƒ í˜ì´ì§€ ë²”ìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    /// 
    /// ì´ ë©”ì„œë“œëŠ” prompts6ì—ì„œ ì„¤ëª…í•œ ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
    /// 1. ë¡œì»¬ DBì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ì €ì¥ëœ ì œí’ˆì˜ pageIdì™€ indexInPageë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    /// 2. ì‚¬ì´íŠ¸ ì •ë³´ (ì´ í˜ì´ì§€ ìˆ˜, ë§ˆì§€ë§‰ í˜ì´ì§€ ì œí’ˆ ìˆ˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í¬ë¡¤ë§ ë²”ìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    /// 3. í¬ë¡¤ë§ í˜ì´ì§€ ì œí•œì„ ì ìš©í•©ë‹ˆë‹¤.
    /// 
    /// Returns: Some((start_page, end_page)) ë˜ëŠ” None (ëª¨ë“  ì œí’ˆì´ í¬ë¡¤ë§ë¨)
    pub async fn calculate_next_crawling_range(
        &self,
        total_pages_on_site: u32,
        products_on_last_page: u32,
    ) -> Result<Option<(u32, u32)>> {
        let crawl_page_limit = self.config.user.crawling.page_range_limit;
        let products_per_page = defaults::DEFAULT_PRODUCTS_PER_PAGE;

        let range = self.product_repo.calculate_next_crawling_range(
            total_pages_on_site,
            products_on_last_page,
            crawl_page_limit,
            products_per_page,
        ).await?;

        if let Some((start_page, end_page)) = range {
            info!("ğŸ¯ Next crawling range: pages {} to {} (limit: {})", 
                  start_page, end_page, crawl_page_limit);
            
            // ì¶”ê°€ ê²€ì¦: í•´ë‹¹ ë²”ìœ„ê°€ ì´ë¯¸ í¬ë¡¤ë§ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if self.product_repo.is_page_range_crawled(start_page, end_page, products_per_page).await? {
                warn!("âš ï¸  Calculated range {} to {} appears to be already crawled, skipping", 
                      start_page, end_page);
                return Ok(None);
            }
        } else {
            info!("ğŸ All products have been crawled - no more work to do");
        }

        Ok(range)
    }

    /// íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ê°€ í¬ë¡¤ë§ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    pub async fn is_range_crawled(&self, start_page: u32, end_page: u32) -> Result<bool> {
        let products_per_page = defaults::DEFAULT_PRODUCTS_PER_PAGE;
        self.product_repo.is_page_range_crawled(start_page, end_page, products_per_page).await
    }

    /// í¬ë¡¤ë§ ì§„í–‰ ìƒí™©ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    pub async fn analyze_crawling_progress(
        &self,
        total_pages_on_site: u32,
        products_on_last_page: u32,
    ) -> Result<CrawlingProgress> {
        let products_per_page = defaults::DEFAULT_PRODUCTS_PER_PAGE;
        
        // ì „ì²´ ì œí’ˆ ìˆ˜ ê³„ì‚°
        let total_products = ((total_pages_on_site - 1) * products_per_page) + products_on_last_page;
        
        // í˜„ì¬ DBì— ì €ì¥ëœ ì œí’ˆ ìˆ˜
        let saved_products = self.product_repo.get_product_count().await? as u32;
        
        // ì§„í–‰ë¥  ê³„ì‚°
        let progress_percentage = if total_products > 0 {
            (saved_products as f64 / total_products as f64 * 100.0).min(100.0)
        } else {
            0.0
        };
        
        // ë§ˆì§€ë§‰ ì €ì¥ëœ ì œí’ˆ ì •ë³´
        let (max_page_id, max_index_in_page) = self.product_repo.get_max_page_id_and_index().await?;
        
        // ë‹¤ìŒ í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚°
        let next_range = self.calculate_next_crawling_range(total_pages_on_site, products_on_last_page).await?;
        
        info!("ğŸ“Š Crawling Progress Analysis:");
        info!("  Total products on site: {}", total_products);
        info!("  Products saved in DB: {}", saved_products);
        info!("  Progress: {:.2}%", progress_percentage);
        info!("  Last saved: pageId={:?}, indexInPage={:?}", max_page_id, max_index_in_page);
        info!("  Next range: {:?}", next_range);
        
        Ok(CrawlingProgress {
            total_products,
            saved_products,
            progress_percentage,
            max_page_id,
            max_index_in_page,
            next_range,
            is_completed: next_range.is_none(),
        })
    }
}

/// í¬ë¡¤ë§ ì§„í–‰ ìƒí™© ì •ë³´
#[derive(Debug, Clone)]
pub struct CrawlingProgress {
    pub total_products: u32,
    pub saved_products: u32,
    pub progress_percentage: f64,
    pub max_page_id: Option<i32>,
    pub max_index_in_page: Option<i32>,
    pub next_range: Option<(u32, u32)>,
    pub is_completed: bool,
}

/// í†µí•©ëœ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ - ë²”ìœ„ ê³„ì‚° ë¡œì§ì„ í¬í•¨
pub struct SmartCrawlingService {
    range_calculator: Arc<CrawlingRangeCalculator>,
    status_checker: Arc<StatusCheckerImpl>,
    list_collector: Arc<ProductListCollectorImpl>,
    detail_collector: Arc<ProductDetailCollectorImpl>,
    product_repo: Arc<IntegratedProductRepository>,
    config: AppConfig,
}

impl SmartCrawlingService {
    pub fn new(
        range_calculator: Arc<CrawlingRangeCalculator>,
        status_checker: Arc<StatusCheckerImpl>,
        list_collector: Arc<ProductListCollectorImpl>,
        detail_collector: Arc<ProductDetailCollectorImpl>,
        product_repo: Arc<IntegratedProductRepository>,
        config: AppConfig,
    ) -> Self {
        Self {
            range_calculator,
            status_checker,
            list_collector,
            detail_collector,
            product_repo,
            config,
        }
    }

    /// ìŠ¤ë§ˆíŠ¸ í¬ë¡¤ë§ ì‹¤í–‰ - ìë™ìœ¼ë¡œ ë‹¤ìŒ í¬ë¡¤ë§ ë²”ìœ„ë¥¼ ê³„ì‚°í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    pub async fn run_smart_crawling(&self) -> Result<CrawlingProgress> {
        info!("ğŸš€ Starting smart crawling with automatic range calculation");
        
        // 1. ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬
        let site_status = self.status_checker.check_site_status().await?;
        info!("ğŸ“Š Site status: {} pages discovered", site_status.total_pages);
        
        // 2. ë‹¤ìŒ í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚°
        let next_range = self.range_calculator.calculate_next_crawling_range(
            site_status.total_pages,
            site_status.products_on_last_page,
        ).await?;
        
        match next_range {
            Some((start_page, end_page)) => {
                info!("ğŸ¯ Crawling pages {} to {}", start_page, end_page);
                
                // 3. í˜ì´ì§€ ë²”ìœ„ í¬ë¡¤ë§
                let pages_to_crawl: Vec<u32> = if start_page >= end_page {
                    // ì •ìƒì ì¸ ì—­ìˆœ í¬ë¡¤ë§ (ë†’ì€ ë²ˆí˜¸ì—ì„œ ë‚®ì€ ë²ˆí˜¸ë¡œ)
                    (end_page..=start_page).rev().collect()
                } else {
                    // ìˆœì°¨ í¬ë¡¤ë§ (ë‚®ì€ ë²ˆí˜¸ì—ì„œ ë†’ì€ ë²ˆí˜¸ë¡œ)
                    (start_page..=end_page).collect()
                };
                
                // 4. ì œí’ˆ URL ìˆ˜ì§‘
                let mut all_urls = Vec::new();
                for page in pages_to_crawl {
                    match self.list_collector.collect_single_page(page).await {
                        Ok(urls) => {
                            all_urls.extend(urls);
                            info!("ğŸ“„ Collected {} URLs from page {}", all_urls.len(), page);
                        }
                        Err(e) => {
                            warn!("âš ï¸  Failed to collect URLs from page {}: {}", page, e);
                        }
                    }
                    
                    // í˜ì´ì§€ ê°„ ì§€ì—°
                    tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
                }
                
                info!("ğŸ”— Total URLs collected: {}", all_urls.len());
                
                // 5. ì œí’ˆ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                let mut processed_count = 0;
                let total_urls = all_urls.len();
                
                for url in all_urls {
                    match self.detail_collector.collect_single_product(&url).await {
                        Ok(product_detail) => {
                            // ì œí’ˆì„ DBì— ì €ì¥
                            let product = product_detail_to_product(product_detail.clone());
                            
                            if let Err(e) = self.product_repo.create_or_update_product(&product).await {
                                warn!("âš ï¸  Failed to save product {}: {}", url, e);
                            } else {
                                processed_count += 1;
                                if processed_count % 10 == 0 {
                                    info!("ğŸ“¦ Processed {}/{} products", processed_count, total_urls);
                                }
                            }
                        }
                        Err(e) => {
                            warn!("âš ï¸  Failed to collect product details from {}: {}", url, e);
                        }
                    }
                    
                    // ìš”ì²­ ê°„ ì§€ì—°
                    tokio::time::sleep(tokio::time::Duration::from_millis(self.config.user.request_delay_ms)).await;
                }
                
                info!("âœ… Smart crawling completed: processed {}/{} products", processed_count, total_urls);
            }
            None => {
                info!("ğŸ All products have been crawled - no more work to do");
            }
        }
        
        // 6. ìµœì¢… ì§„í–‰ ìƒí™© ë¶„ì„
        let progress = self.range_calculator.analyze_crawling_progress(
            site_status.total_pages,
            site_status.products_on_last_page,
        ).await?;
        
        Ok(progress)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::infrastructure::integrated_product_repository::IntegratedProductRepository;
    use crate::infrastructure::crawling_service_impls::CrawlingRangeCalculator;
    use crate::infrastructure::config::AppConfig;
    use sqlx::SqlitePool;
    use std::sync::Arc;

    #[tokio::test]
    async fn test_step_by_step_calculation() {
        // Test the step-by-step calculation as described in prompts6
        
        // Given data from prompts6
        let max_page_id = 10i32;
        let max_index_in_page = 6i32;
        let total_pages_on_site = 481u32;
        let products_on_last_page = 10u32;
        let crawl_page_limit = 10u32;
        let products_per_page = 12u32;
        
        println!("ğŸ“Š Step-by-step calculation test:");
        println!("Input data:");
        println!("  max_page_id: {}", max_page_id);
        println!("  max_index_in_page: {}", max_index_in_page);
        println!("  total_pages_on_site: {}", total_pages_on_site);
        println!("  products_on_last_page: {}", products_on_last_page);
        println!("  crawl_page_limit: {}", crawl_page_limit);
        println!("  products_per_page: {}", products_per_page);
        
        // Step 1: Calculate last saved index
        let last_saved_index = (max_page_id as u32 * products_per_page) + max_index_in_page as u32;
        println!("\nStep 1: lastSavedIndex = ({} * {}) + {} = {}", 
                 max_page_id, products_per_page, max_index_in_page, last_saved_index);
        assert_eq!(last_saved_index, 126, "Last saved index should be 126");
        
        // Step 2: Calculate next product index
        let next_product_index = last_saved_index + 1;
        println!("Step 2: nextProductIndex = {} + 1 = {}", last_saved_index, next_product_index);
        assert_eq!(next_product_index, 127, "Next product index should be 127");
        
        // Step 3: Calculate total products
        let total_products = ((total_pages_on_site - 1) * products_per_page) + products_on_last_page;
        println!("Step 3: totalProducts = (({} - 1) * {}) + {} = {}", 
                 total_pages_on_site, products_per_page, products_on_last_page, total_products);
        assert_eq!(total_products, 5770, "Total products should be 5770");
        
        // Step 4: Convert to forward index
        let forward_index = (total_products - 1) - next_product_index;
        println!("Step 4: forwardIndex = ({} - 1) - {} = {}", 
                 total_products, next_product_index, forward_index);
        assert_eq!(forward_index, 5642, "Forward index should be 5642");
        
        // Step 5: Calculate target page number
        let target_page_number = (forward_index / products_per_page) + 1;
        println!("Step 5: targetPageNumber = ({} / {}) + 1 = {}", 
                 forward_index, products_per_page, target_page_number);
        assert_eq!(target_page_number, 471, "Target page number should be 471");
        
        // Step 6: Apply crawl page limit
        let start_page = target_page_number;
        let end_page = if start_page >= crawl_page_limit {
            start_page - crawl_page_limit + 1
        } else {
            1
        };
        println!("Step 6: startPage = {}, endPage = {} - {} + 1 = {}", 
                 start_page, start_page, crawl_page_limit, end_page);
        
        assert_eq!(start_page, 471, "Start page should be 471");
        assert_eq!(end_page, 462, "End page should be 462");
        
        println!("\nâœ… All calculation steps match prompts6 specification!");
        println!("ğŸ¯ Final result: crawl pages {} to {}", start_page, end_page);
    }
}

/// ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì„œë¹„ìŠ¤ êµ¬í˜„ì²´
pub struct DatabaseAnalyzerImpl {
    product_repo: Arc<IntegratedProductRepository>,
}

impl DatabaseAnalyzerImpl {
    pub fn new(product_repo: Arc<IntegratedProductRepository>) -> Self {
        Self { product_repo }
    }

    /// ì‹¤ì œ ì¤‘ë³µ ì œí’ˆ ìˆ˜ ê³„ì‚°
    async fn count_duplicate_products(&self) -> Result<u32> {
        // certificate_idë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì°¾ê¸°
        let products = self.product_repo.get_all_products().await?;
        let mut cert_id_count = std::collections::HashMap::new();
        
        for product in products {
            if let Some(cert_id) = &product.certificate_id {
                *cert_id_count.entry(cert_id.clone()).or_insert(0) += 1;
            }
        }
        
        // ì¤‘ë³µëœ ì œí’ˆ ìˆ˜ ê³„ì‚° (ê·¸ë£¹ í¬ê¸° - 1)
        let duplicate_count: u32 = cert_id_count.values()
            .filter(|&&count| count > 1)
            .map(|&count| count - 1)
            .sum();
            
        debug!("Found {} duplicate products based on certificate_id", duplicate_count);
        Ok(duplicate_count)
    }

    /// ì‹¤ì œ í•„ë“œ ëˆ„ë½ ë¶„ì„
    async fn analyze_missing_fields(&self) -> Result<FieldAnalysis> {
        let products = self.product_repo.get_all_products().await?;
        let total = products.len() as u32;
        
        let mut missing_company = 0u32;
        let mut missing_model = 0u32;
        let missing_matter_version = 0u32;
        let missing_connectivity = 0u32;
        let missing_certification_date = 0u32;
        
        for product in products {
            if product.manufacturer.is_none() || product.manufacturer.as_ref().map_or(true, |s| s.is_empty()) {
                missing_company += 1;
            }
            if product.model.is_none() || product.model.as_ref().map_or(true, |s| s.is_empty()) {
                missing_model += 1;
            }
        }
        
        info!("ğŸ“Š Field analysis: {}/{} missing company, {}/{} missing model",
              missing_company, total, missing_model, total);
        
        Ok(FieldAnalysis {
            missing_company,
            missing_model,
            missing_matter_version,
            missing_connectivity,
            missing_certification_date,
        })
    }
}

#[async_trait]
impl DatabaseAnalyzer for DatabaseAnalyzerImpl {
    async fn analyze_current_state(&self) -> Result<DatabaseAnalysis> {
        let products = self.product_repo.get_all_products().await?;
        let total_products = products.len() as u32;
        
        let duplicate_count = self.count_duplicate_products().await?;
        let unique_products = total_products.saturating_sub(duplicate_count);
        
        let missing_fields_analysis = self.analyze_missing_fields().await?;
        
        // ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
        let quality_score = if total_products > 0 {
            let completeness_score = 1.0 - (missing_fields_analysis.missing_company as f64 + missing_fields_analysis.missing_model as f64) / (total_products as f64 * 2.0);
            let uniqueness_score = unique_products as f64 / total_products as f64;
            (completeness_score + uniqueness_score) / 2.0
        } else {
            0.0
        };
        
        // ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ (ê°€ì¥ ìµœê·¼ ì œí’ˆì˜ created_at ì‚¬ìš©)
        let last_update = products.iter()
            .map(|p| p.created_at)
            .max();
        
        info!("ğŸ“Š Database analysis: total={}, unique={}, duplicates={}, quality={:.2}", 
              total_products, unique_products, duplicate_count, quality_score);
        
        Ok(DatabaseAnalysis {
            total_products,
            unique_products,
            duplicate_count,
            last_update,
            missing_fields_analysis,
            data_quality_score: quality_score,
        })
    }
    
    async fn recommend_processing_strategy(&self) -> Result<ProcessingStrategy> {
        let analysis = self.analyze_current_state().await?;
        
        // ë°ì´í„° ìƒíƒœì— ë”°ë¥¸ ì²˜ë¦¬ ì „ëµ ê²°ì •
        let (batch_size, concurrency) = if analysis.total_products < 1000 {
            (50, 3)  // ì†Œê·œëª¨: ì‘ì€ ë°°ì¹˜, ë‚®ì€ ë™ì‹œì„±
        } else if analysis.total_products < 5000 {
            (100, 5) // ì¤‘ê·œëª¨: ì¤‘ê°„ ë°°ì¹˜, ì¤‘ê°„ ë™ì‹œì„±
        } else {
            (200, 8) // ëŒ€ê·œëª¨: í° ë°°ì¹˜, ë†’ì€ ë™ì‹œì„±
        };
        
        let should_skip_duplicates = analysis.duplicate_count > analysis.total_products / 10; // 10% ì´ìƒ ì¤‘ë³µ
        let should_update_existing = analysis.data_quality_score < 0.8; // í’ˆì§ˆì´ 80% ë¯¸ë§Œ
        
        Ok(ProcessingStrategy {
            recommended_batch_size: batch_size,
            recommended_concurrency: concurrency,
            should_skip_duplicates,
            should_update_existing,
            priority_urls: Vec::new(), // ìš°ì„ ìˆœìœ„ URLì€ ë¹„ì›Œë‘ 
        })
    }
    
    async fn analyze_duplicates(&self) -> Result<DuplicateAnalysis> {
        let products = self.product_repo.get_all_products().await?;
        let total_products = products.len() as u32;
        
        if total_products == 0 {
            return Ok(DuplicateAnalysis {
                total_duplicates: 0,
                duplicate_groups: Vec::new(),
                duplicate_percentage: 0.0,
            });
        }
        
        let duplicate_count = self.count_duplicate_products().await?;
        let duplicate_percentage = (duplicate_count as f64 / total_products as f64) * 100.0;
        
        info!("ğŸ“Š Duplicate analysis: {}/{} duplicates ({:.1}%)", 
              duplicate_count, total_products, duplicate_percentage);
        
        Ok(DuplicateAnalysis {
            total_duplicates: duplicate_count,
            duplicate_groups: Vec::new(), // ê°„ë‹¨í•œ êµ¬í˜„ì—ì„œëŠ” ê·¸ë£¹ ì •ë³´ ìƒëµ
            duplicate_percentage,
        })
    }
}

