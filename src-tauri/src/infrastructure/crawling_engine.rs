//! ë°°ì¹˜ í¬ë¡¤ë§ ì—”ì§„ - 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
//! 
//! ì´ ëª¨ë“ˆì€ guide/crawling ë¬¸ì„œë“¤ì˜ ë…¸í•˜ìš°ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬í˜„ëœ 
//! ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë°°ì¹˜ í¬ë¡¤ë§ ì—”ì§„ì…ë‹ˆë‹¤.

use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::Semaphore;
use tokio::time::sleep;
use tracing::{info, warn, error, debug};
use anyhow::{Result, anyhow};
use futures::future::try_join_all;

use crate::domain::events::{CrawlingProgress, CrawlingStage, CrawlingStatus};
use crate::application::EventEmitter;
use crate::infrastructure::{HttpClient, MatterDataExtractor, IntegratedProductRepository, csa_iot};

/// ë°°ì¹˜ í¬ë¡¤ë§ ì„¤ì •
#[derive(Debug, Clone)]
pub struct BatchCrawlingConfig {
    pub start_page: u32,
    pub end_page: u32,
    pub concurrency: u32,
    pub delay_ms: u64,
    pub batch_size: u32,
    pub retry_max: u32,
    pub timeout_ms: u64,
}

impl Default for BatchCrawlingConfig {
    fn default() -> Self {
        Self {
            start_page: 1,
            end_page: 1, // Changed from 100 - will be overridden by intelligent range calculation
            concurrency: 3,
            delay_ms: 1000,
            batch_size: 10,
            retry_max: 3,
            timeout_ms: 30000,
        }
    }
}

/// 4ë‹¨ê³„ ë°°ì¹˜ í¬ë¡¤ë§ ì—”ì§„
pub struct BatchCrawlingEngine {
    http_client: Arc<HttpClient>,  // ğŸ”¥ Mutex ì œê±° - GlobalRateLimiterê°€ ë™ì‹œì„± ê´€ë¦¬
    data_extractor: Arc<MatterDataExtractor>,
    product_repo: Arc<IntegratedProductRepository>,
    event_emitter: Arc<Option<EventEmitter>>,
    config: BatchCrawlingConfig,
    session_id: String,
}

impl BatchCrawlingEngine {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        product_repo: Arc<IntegratedProductRepository>,
        event_emitter: Arc<Option<EventEmitter>>,
        config: BatchCrawlingConfig,
        session_id: String,
    ) -> Self {
        Self {
            http_client: Arc::new(http_client),  // ğŸ”¥ Mutex ì œê±°
            data_extractor: Arc::new(data_extractor),
            product_repo,
            event_emitter,
            config,
            session_id,
        }
    }

    /// 4ë‹¨ê³„ ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
    pub async fn execute(&self) -> Result<()> {
        let start_time = Instant::now();
        info!("Starting 4-stage batch crawling for session: {}", self.session_id);

        // Stage 1: ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸
        let total_pages = self.stage1_discover_total_pages().await?;
        
        // Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)
        let product_urls = self.stage2_collect_product_list(total_pages).await?;
        
        // Stage 3: ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
        let products = self.stage3_collect_product_details(&product_urls).await?;
        
        // Stage 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        let (processed_count, new_items, updated_items, errors) = self.stage4_save_to_database(products).await?;

        let duration = start_time.elapsed();
        info!("Batch crawling completed in {:?}", duration);
        
        // ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
        self.emit_completion_event(duration, processed_count, new_items, updated_items, errors).await?;
        
        Ok(())
    }

    /// Stage 1: ì´ í˜ì´ì§€ ìˆ˜ ë°œê²¬
    async fn stage1_discover_total_pages(&self) -> Result<u32> {
        info!("Stage 1: Discovering total pages");
        
        self.emit_progress(CrawlingStage::TotalPages, 0, 1, 0.0, 
            "ì´ í˜ì´ì§€ ìˆ˜ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘...").await?;

        let url = format!("{}?page=1", csa_iot::PRODUCTS_PAGE_MATTER_ONLY);
        let html = self.fetch_page(&url).await?;
        
        // ì´ í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ (MatterDataExtractor í™œìš©)
        let total_pages = match self.data_extractor.extract_total_pages(&html) {
            Ok(pages) => pages.min(self.config.end_page),
            Err(_) => {
                warn!("Could not extract total pages, using configured end_page");
                self.config.end_page
            }
        };

        self.emit_progress(CrawlingStage::TotalPages, 1, 1, 100.0, 
            &format!("ì´ {}í˜ì´ì§€ ë°œê²¬", total_pages)).await?;

        info!("Stage 1 completed: {} total pages", total_pages);
        Ok(total_pages)
    }

    /// Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)
    async fn stage2_collect_product_list(&self, total_pages: u32) -> Result<Vec<String>> {
        info!("Stage 2: Collecting product list from {} pages", total_pages);
        
        let effective_start = self.config.start_page;
        let effective_end = total_pages.min(self.config.end_page);
        let total_pages_to_process = effective_end - effective_start + 1;

        self.emit_progress(CrawlingStage::ProductList, 0, total_pages_to_process, 0.0,
            "ì œí’ˆ ëª©ë¡ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘...").await?;

        let semaphore = Arc::new(Semaphore::new(self.config.concurrency as usize));
        let mut all_product_urls = Vec::new();
        let mut completed_pages = 0u32;

        // ë°°ì¹˜ë³„ë¡œ í˜ì´ì§€ ì²˜ë¦¬
        for batch_start in (effective_start..=effective_end).step_by(self.config.batch_size as usize) {
            let batch_end = (batch_start + self.config.batch_size - 1).min(effective_end);
            let batch_pages: Vec<u32> = (batch_start..=batch_end).collect();
            
            debug!("Processing batch: pages {} to {}", batch_start, batch_end);

            let batch_tasks: Vec<_> = batch_pages.into_iter().map(|page_num| {
                let semaphore = Arc::clone(&semaphore);
                let http_client = Arc::clone(&self.http_client);
                let data_extractor = Arc::clone(&self.data_extractor);
                let delay_ms = self.config.delay_ms;

                tokio::spawn(async move {
                    let _permit = semaphore.acquire().await.unwrap();
                    
                    if delay_ms > 0 {
                        sleep(Duration::from_millis(delay_ms)).await;
                    }

                    let url = format!("{}?page={}", csa_iot::PRODUCTS_PAGE_MATTER_ONLY, page_num);
                    debug!("Fetching page: {}", url);

                    // ğŸ”¥ Mutex ì œê±° - ì§ì ‘ HttpClient ì‚¬ìš©ìœ¼ë¡œ ì§„ì •í•œ ë™ì‹œì„±
                    match http_client.fetch_html_string(&url).await {
                        Ok(html_str) => {
                            match data_extractor.extract_product_urls_from_content(&html_str) {
                                Ok(urls) => {
                                    debug!("Extracted {} URLs from page {}", urls.len(), page_num);
                                    Ok(urls)
                                },
                                Err(e) => {
                                    warn!("Failed to extract URLs from page {}: {}", page_num, e);
                                    Ok(Vec::new())
                                }
                            }
                        },
                        Err(e) => {
                            error!("Failed to fetch page {}: {}", page_num, e);
                            Err(e)
                        }
                    }
                })
            }).collect();

            // ë°°ì¹˜ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
            let batch_results = try_join_all(batch_tasks).await?;
            for result in batch_results {
                match result {
                    Ok(urls) => all_product_urls.extend(urls),
                    Err(e) => warn!("Batch task failed: {}", e),
                }
            }

            completed_pages += batch_end - batch_start + 1;
            let progress = (completed_pages as f64 / total_pages_to_process as f64) * 100.0;
            
            self.emit_progress(CrawlingStage::ProductList, completed_pages, total_pages_to_process, 
                progress, &format!("{}/{} í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ", completed_pages, total_pages_to_process)).await?;
        }

        info!("Stage 2 completed: {} product URLs collected", all_product_urls.len());
        Ok(all_product_urls)
    }

    /// Stage 3: ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ (ê°œì„ ëœ ë³‘ë ¬ ì²˜ë¦¬ - Spawn All, Control with Semaphore)
    async fn stage3_collect_product_details(&self, product_urls: &[String]) -> Result<Vec<serde_json::Value>> {
        info!("Stage 3: Collecting product details from {} URLs (improved concurrency)", product_urls.len());
        
        let total_products = product_urls.len() as u32;
        self.emit_progress(CrawlingStage::ProductDetails, 0, total_products, 0.0,
            "ì œí’ˆ ìƒì„¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘...").await?;

        let semaphore = Arc::new(Semaphore::new(self.config.concurrency as usize));

        // Spawn ALL tasks at once - proposal6.md recommendation
        let tasks: Vec<_> = product_urls.iter().enumerate().map(|(idx, url)| {
            let semaphore = Arc::clone(&semaphore);
            let http_client = Arc::clone(&self.http_client);
            let data_extractor = Arc::clone(&self.data_extractor);
            let url = url.clone();
            let delay_ms = self.config.delay_ms;

            tokio::spawn(async move {
                let _permit = semaphore.acquire().await.unwrap();
                
                if delay_ms > 0 && idx > 0 {
                    sleep(Duration::from_millis(delay_ms)).await;
                }

                debug!("Fetching product details: {}", url);
                
                // ğŸ”¥ Mutex ì œê±° - ì§ì ‘ HttpClient ì‚¬ìš©ìœ¼ë¡œ ì§„ì •í•œ ë™ì‹œì„±
                match http_client.fetch_html_string(&url).await {
                    Ok(html_str) => {
                        match data_extractor.extract_product_data(&html_str) {
                            Ok(product) => {
                                debug!("Successfully extracted product data from: {}", url);
                                Ok(Some(product))
                            },
                            Err(e) => {
                                warn!("Failed to extract product data from {}: {}", url, e);
                                Ok(None)
                            }
                        }
                    },
                    Err(e) => {
                        error!("Failed to fetch product page {}: {}", url, e);
                        Err(e)
                    }
                }
            })
        }).collect();

        // Wait for ALL tasks to complete - no chunking
        let all_results = try_join_all(tasks).await?;
        
        // Collect results
        let mut all_products = Vec::new();
        let mut successful_count = 0u32;
        let mut failed_count = 0u32;

        for result in all_results {
            match result {
                Ok(Some(product)) => {
                    all_products.push(product);
                    successful_count += 1;
                },
                Ok(None) => {
                    failed_count += 1;
                },
                Err(e) => {
                    warn!("Product detail task failed: {}", e);
                    failed_count += 1;
                },
            }
        }

        // Final progress update
        self.emit_progress(CrawlingStage::ProductDetails, total_products, total_products, 
            100.0, &format!("ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: ì„±ê³µ {}, ì‹¤íŒ¨ {}", successful_count, failed_count)).await?;

        info!("Stage 3 completed: {} products detailed collected (no chunking)", all_products.len());
        Ok(all_products)
    }

    /// Stage 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    async fn stage4_save_to_database(&self, products: Vec<serde_json::Value>) -> Result<(u32, u32, u32, u32)> {
        info!("Stage 4: Saving {} products to database", products.len());
        
        let total_products = products.len() as u32;
        self.emit_progress(CrawlingStage::Database, 0, total_products, 0.0,
            "ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì¤‘...").await?;

        let mut saved_count = 0u32;
        let mut new_items = 0u32;
        let mut updated_items = 0u32;
        let mut error_count = 0u32;

        // ë°°ì¹˜ë³„ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        for (batch_idx, batch) in products.chunks(self.config.batch_size as usize).enumerate() {
            debug!("Saving batch {}: {} products", batch_idx + 1, batch.len());
            
            for (idx, product) in batch.iter().enumerate() {
                match self.product_repo.upsert_product(product.clone()).await {
                    Ok(is_new) => {
                        if is_new {
                            new_items += 1;
                        } else {
                            updated_items += 1;
                        }
                        saved_count += 1;
                    },
                    Err(e) => {
                        error!("Failed to save product {}: {}", idx + 1, e);
                        error_count += 1;
                        
                        // ì˜¤ë¥˜ ì´ë²¤íŠ¸ ë°œì†¡
                        let error_id = format!("db_save_error_{}", uuid::Uuid::new_v4());
                        let error_msg = format!("ì œí’ˆ #{} ì €ì¥ ì‹¤íŒ¨: {}", idx + 1, e);
                        let _ = self.emit_error(&error_id, &error_msg, CrawlingStage::Database, true).await;
                    }
                }

                if idx % 10 == 0 {
                    let progress = (saved_count as f64 / total_products as f64) * 100.0;
                    self.emit_progress(CrawlingStage::Database, saved_count, total_products, 
                        progress, &format!("{}/{} ì œí’ˆ ì €ì¥ ì™„ë£Œ (ì‹ ê·œ: {}, ì—…ë°ì´íŠ¸: {}, ì˜¤ë¥˜: {})", 
                        saved_count, total_products, new_items, updated_items, error_count)).await?;
                }
            }
        }

        info!("Stage 4 completed: {} products saved (new: {}, updated: {}, errors: {})", 
              saved_count, new_items, updated_items, error_count);
        
        self.emit_progress(CrawlingStage::Database, total_products, total_products, 100.0,
            &format!("ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: ì´ {} ì œí’ˆ (ì‹ ê·œ: {}, ì—…ë°ì´íŠ¸: {}, ì˜¤ë¥˜: {})", 
            saved_count, new_items, updated_items, error_count)).await?;

        // ì²˜ë¦¬ëœ í•­ëª© ìˆ˜, ì‹ ê·œ í•­ëª© ìˆ˜, ì—…ë°ì´íŠ¸ëœ í•­ëª© ìˆ˜, ì˜¤ë¥˜ ìˆ˜ ë°˜í™˜
        Ok((saved_count, new_items, updated_items, error_count))
    }

    /// ì§„í–‰ìƒí™© ì´ë²¤íŠ¸ ë°œì†¡ (ê³„ì‚°ëœ í•„ë“œ í¬í•¨)
    async fn emit_progress(
        &self, 
        stage: CrawlingStage, 
        current: u32, 
        total: u32, 
        _percentage: f64, // ê³„ì‚°ëœ í•„ë“œì´ë¯€ë¡œ ë¬´ì‹œ
        message: &str
    ) -> Result<()> {
        // Start timeì„ í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ê°€ì • (ì‹¤ì œë¡œëŠ” BatchCrawlingEngineì—ì„œ ê´€ë¦¬í•´ì•¼ í•¨)
        let start_time = chrono::Utc::now() - chrono::Duration::seconds(60); // ì„ì‹œê°’
        
        let progress = CrawlingProgress::new_with_calculation(
            current,
            total,
            stage,
            message.to_string(),
            CrawlingStatus::Running,
            message.to_string(),
            start_time,
            0, // new_items - TODO: ì‹¤ì œ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            0, // updated_items - TODO: ì‹¤ì œ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            0, // errors - TODO: ì‹¤ì œ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        );

        if let Some(ref emitter) = *self.event_emitter {
            if let Err(e) = emitter.emit_progress(progress).await {
                warn!("ì´ë²¤íŠ¸ ë°œì†¡ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {}", e);
                // ì´ë²¤íŠ¸ ë°œì†¡ ì‹¤íŒ¨ëŠ” í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•ŠìŒ
            }
        }

        Ok(())
    }

    /// ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
    async fn emit_completion_event(&self, duration: Duration, processed_count: u32, new_items: u32, updated_items: u32, errors: u32) -> Result<()> {
        if let Some(ref emitter) = *self.event_emitter {
            let result = crate::domain::events::CrawlingResult {
                total_processed: processed_count,
                new_items,
                updated_items,
                errors,
                duration_ms: duration.as_millis() as u64,
                stages_completed: vec![
                    CrawlingStage::TotalPages,
                    CrawlingStage::ProductList,
                    CrawlingStage::ProductDetails,
                    CrawlingStage::Database,
                ],
                start_time: chrono::Utc::now() - chrono::Duration::milliseconds(duration.as_millis() as i64),
                end_time: chrono::Utc::now(),
                performance_metrics: crate::domain::events::PerformanceMetrics {
                    avg_processing_time_ms: if processed_count > 0 {
                        duration.as_millis() as f64 / processed_count as f64
                    } else {
                        0.0
                    },
                    items_per_second: if duration.as_secs() > 0 {
                        processed_count as f64 / duration.as_secs() as f64
                    } else {
                        0.0
                    },
                    memory_usage_mb: 0.0, // TODO: ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
                    network_requests: 0,  // TODO: ì‹¤ì œ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ìˆ˜ ì¶”ì 
                    cache_hit_rate: 0.0,  // TODO: ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°
                },
            };

            if let Err(e) = emitter.emit_completed(result).await {
                warn!("ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡ ì‹¤íŒ¨: {}", e);
                // ì´ë²¤íŠ¸ ë°œì†¡ ì‹¤íŒ¨ëŠ” í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•ŠìŒ
            }
        }

        Ok(())
    }

    /// ìŠ¤í…Œì´ì§€ ë³€ê²½ ì´ë²¤íŠ¸ ë°œì†¡
    #[allow(dead_code)]
    async fn emit_stage_change(&self, from: CrawlingStage, to: CrawlingStage, message: &str) -> Result<()> {
        if let Some(ref emitter) = *self.event_emitter {
            if let Err(e) = emitter.emit_stage_change(from, to, message.to_string()).await {
                warn!("ìŠ¤í…Œì´ì§€ ë³€ê²½ ì´ë²¤íŠ¸ ë°œì†¡ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {}", e);
            }
        }
        Ok(())
    }

    /// ì˜¤ë¥˜ ì´ë²¤íŠ¸ ë°œì†¡
    async fn emit_error(&self, error_id: &str, message: &str, stage: CrawlingStage, recoverable: bool) -> Result<()> {
        if let Some(ref emitter) = *self.event_emitter {
            if let Err(e) = emitter.emit_error(
                error_id.to_string(), 
                message.to_string(), 
                stage, 
                recoverable
            ).await {
                warn!("ì˜¤ë¥˜ ì´ë²¤íŠ¸ ë°œì†¡ ì‹¤íŒ¨ (ë¬´ì‹œë¨): {}", e);
            }
        }
        Ok(())
    }

    /// HTTP í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ì¬ì‹œë„ í¬í•¨)
    async fn fetch_page(&self, url: &str) -> Result<String> {
        let mut retries = 0;
        let max_retries = self.config.retry_max;

        loop {
            // ğŸ”¥ Mutex ì œê±° - ì§ì ‘ HttpClient ì‚¬ìš©ìœ¼ë¡œ ì§„ì •í•œ ë™ì‹œì„±
            match self.http_client.fetch_html_string(url).await {
                Ok(html) => return Ok(html),
                Err(e) => {
                    retries += 1;
                    if retries > max_retries {
                        return Err(anyhow!("Failed to fetch {} after {} retries: {}", url, max_retries, e));
                    }
                    
                    let delay = Duration::from_millis(1000 * (1 << retries.min(5))); // ì§€ìˆ˜ ë°±ì˜¤í”„
                    warn!("Retrying {} ({}/{}) after {:?}", url, retries, max_retries, delay);
                    sleep(delay).await;
                }
            }
        }
    }
}
