//! ì¬ì‹œë„ ê´€ë¦¬ì - INTEGRATED_PHASE2_PLAN Week 1 Day 3-4 êµ¬í˜„
//! 
//! ì´ ëª¨ë“ˆì€ í¬ë¡¤ë§ ì‘ì—…ì˜ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì„ ì œê³µí•˜ë©°,
//! ë‹¤ì–‘í•œ ì—ëŸ¬ íƒ€ì…ì— ë”°ë¥¸ ì ì‘ì  ì¬ì‹œë„ ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤.

use std::time::Duration;
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use tokio::sync::{Mutex, RwLock};
use anyhow::Result;
use tracing::{info, warn, debug};
use chrono::{DateTime, Utc};

use crate::domain::events::CrawlingStage;

/// ì¬ì‹œë„ ê´€ë¦¬ì
#[derive(Clone)]
pub struct RetryManager {
    max_retries: u32,
    retry_queue: Arc<Mutex<VecDeque<RetryItem>>>,
    failure_classifier: Arc<dyn FailureClassifier>,
    retry_history: Arc<RwLock<HashMap<String, Vec<RetryAttempt>>>>,
}

/// ì¬ì‹œë„ ì•„ì´í…œ
#[derive(Debug, Clone)]
pub struct RetryItem {
    pub item_id: String,
    pub stage: CrawlingStage,
    pub attempt_count: u32,
    pub last_error: String,
    pub next_retry_time: DateTime<Utc>,
    pub exponential_backoff: Duration,
    pub original_url: String,
    pub metadata: HashMap<String, String>,
}

/// ì¬ì‹œë„ ì‹œë„ ê¸°ë¡
#[derive(Debug, Clone)]
pub struct RetryAttempt {
    pub attempt_number: u32,
    pub attempted_at: DateTime<Utc>,
    pub error_type: ErrorClassification,
    pub backoff_duration: Duration,
    pub success: bool,
}

/// ì‹¤íŒ¨ ë¶„ë¥˜ê¸° íŠ¸ë ˆì´íŠ¸
#[async_trait::async_trait]
pub trait FailureClassifier: Send + Sync {
    async fn classify_error(&self, error: &str, stage: CrawlingStage) -> ErrorClassification;
    async fn calculate_backoff(&self, attempt_count: u32) -> Duration;
    async fn should_retry(&self, classification: &ErrorClassification, attempt_count: u32) -> bool;
}

/// ì—ëŸ¬ ë¶„ë¥˜
#[derive(Debug, Clone)]
pub enum ErrorClassification {
    Recoverable { 
        retry_after: Duration,
        category: RecoverableErrorCategory,
    },
    NonRecoverable { 
        reason: String,
        category: NonRecoverableErrorCategory,
    },
    RateLimited { 
        retry_after: Duration,
        severity: RateLimitSeverity,
    },
    NetworkError { 
        retry_after: Duration,
        error_type: NetworkErrorType,
    },
}

#[derive(Debug, Clone)]
pub enum RecoverableErrorCategory {
    TemporaryServerError,    // 5xx ì‘ë‹µ
    ParsingError,            // HTML êµ¬ì¡° ë³€ê²½ ë“±
    DatabaseConnectionLost,  // DB ì—°ê²° ëŠê¹€
    ResourceBusy,           // ë¦¬ì†ŒìŠ¤ ì¼ì‹œì  ì‚¬ìš© ë¶ˆê°€
}

#[derive(Debug, Clone)]
pub enum NonRecoverableErrorCategory {
    AuthenticationError,     // 401, 403
    NotFound,               // 404
    InvalidConfiguration,    // ì„¤ì • ì˜¤ë¥˜
    CriticalSystemError,    // ì‹œìŠ¤í…œ ë ˆë²¨ ì—ëŸ¬
}

#[derive(Debug, Clone)]
pub enum RateLimitSeverity {
    Light,      // ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
    Moderate,   // ì¤‘ê°„ ì •ë„ ëŒ€ê¸°
    Severe,     // ê¸´ ëŒ€ê¸° í•„ìš”
}

#[derive(Debug, Clone)]
pub enum NetworkErrorType {
    Timeout,
    ConnectionRefused,
    DnsResolution,
    SslError,
}

impl RetryManager {
    /// ìƒˆ ì¬ì‹œë„ ê´€ë¦¬ì ìƒì„±
    pub fn new(max_retries: u32) -> Self {
        let failure_classifier = Arc::new(StandardFailureClassifier::new());
        
        Self {
            max_retries,
            retry_queue: Arc::new(Mutex::new(VecDeque::new())),
            failure_classifier,
            retry_history: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// ì‹¤íŒ¨í•œ ì‘ì—…ì„ ì¬ì‹œë„ íì— ì¶”ê°€
    pub async fn add_failed_item(&self, 
        item_id: String,
        stage: CrawlingStage,
        error: String,
        url: String,
        metadata: HashMap<String, String>
    ) -> Result<bool> {
        info!("ğŸ”„ Adding failed item to retry queue: {} (stage: {:?})", item_id, stage);
        
        // ì—ëŸ¬ ë¶„ë¥˜
        let classification = self.failure_classifier.classify_error(&error, stage.clone()).await;
        
        // í˜„ì¬ ì‹œë„ íšŸìˆ˜ í™•ì¸
        let current_attempts = self.get_retry_count(&item_id).await;
        
        // ì¬ì‹œë„ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if !self.failure_classifier.should_retry(&classification, current_attempts).await {
            warn!("âŒ Item {} cannot be retried: {:?}", item_id, classification);
            return Ok(false);
        }
        
        if current_attempts >= self.max_retries {
            warn!("âŒ Item {} exceeded max retries ({})", item_id, self.max_retries);
            return Ok(false);
        }
        
        // ë°±ì˜¤í”„ ì‹œê°„ ê³„ì‚°
        let backoff = self.failure_classifier.calculate_backoff(current_attempts + 1).await;
        let next_retry_time = chrono::Utc::now() + chrono::Duration::from_std(backoff)?;
        
        // ì¬ì‹œë„ ì•„ì´í…œ ìƒì„±
        let retry_item = RetryItem {
            item_id: item_id.clone(),
            stage,
            attempt_count: current_attempts + 1,
            last_error: error.clone(),
            next_retry_time,
            exponential_backoff: backoff,
            original_url: url,
            metadata,
        };
        
        // íì— ì¶”ê°€
        {
            let mut queue = self.retry_queue.lock().await;
            queue.push_back(retry_item);
        }
        
        // ê¸°ë¡ ì—…ë°ì´íŠ¸
        self.record_retry_attempt(&item_id, classification.clone(), backoff, false).await;
        
        info!("âœ… Item {} scheduled for retry in {:?}", item_id, backoff);
        Ok(true)
    }
    
    /// ì¬ì‹œë„ ê°€ëŠ¥í•œ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
    pub async fn get_ready_items(&self) -> Result<Vec<RetryItem>> {
        let mut queue = self.retry_queue.lock().await;
        let now = chrono::Utc::now();
        let mut ready_items = Vec::new();
        let mut remaining_items = VecDeque::new();
        
        while let Some(item) = queue.pop_front() {
            if item.next_retry_time <= now {
                debug!("ğŸŸ¢ Item {} is ready for retry", item.item_id);
                ready_items.push(item);
            } else {
                remaining_items.push_back(item);
            }
        }
        
        *queue = remaining_items;
        
        if !ready_items.is_empty() {
            info!("ğŸ“¤ Retrieved {} items ready for retry", ready_items.len());
        }
        
        Ok(ready_items)
    }
    
    /// ì¬ì‹œë„ ì„±ê³µ ê¸°ë¡
    pub async fn mark_retry_success(&self, item_id: &str) -> Result<()> {
        info!("âœ… Retry succeeded for item: {}", item_id);
        self.record_retry_attempt(item_id, 
            ErrorClassification::Recoverable { 
                retry_after: Duration::from_secs(0), 
                category: RecoverableErrorCategory::TemporaryServerError 
            }, 
            Duration::from_secs(0), 
            true
        ).await;
        Ok(())
    }
    
    /// ì¬ì‹œë„ ê¸°ë¡ ì €ì¥
    async fn record_retry_attempt(&self, item_id: &str, classification: ErrorClassification, backoff: Duration, success: bool) {
        let mut history = self.retry_history.write().await;
        let attempts = history.entry(item_id.to_string()).or_insert_with(Vec::new);
        
        attempts.push(RetryAttempt {
            attempt_number: attempts.len() as u32 + 1,
            attempted_at: chrono::Utc::now(),
            error_type: classification,
            backoff_duration: backoff,
            success,
        });
    }
    
    /// ì•„ì´í…œì˜ ì¬ì‹œë„ íšŸìˆ˜ ì¡°íšŒ
    async fn get_retry_count(&self, item_id: &str) -> u32 {
        let history = self.retry_history.read().await;
        history.get(item_id).map(|attempts| attempts.len() as u32).unwrap_or(0)
    }
    
    /// ì¬ì‹œë„ í†µê³„ ì¡°íšŒ
    pub async fn get_retry_stats(&self) -> RetryStats {
        let history = self.retry_history.read().await;
        let queue = self.retry_queue.lock().await;
        
        let total_items = history.len();
        let pending_retries = queue.len();
        let successful_retries = history.values()
            .flatten()
            .filter(|attempt| attempt.success)
            .count();
        let failed_retries = history.values()
            .flatten()
            .filter(|attempt| !attempt.success)
            .count();
            
        RetryStats {
            total_items,
            pending_retries,
            successful_retries,
            failed_retries,
            max_retries: self.max_retries,
        }
    }
}

/// ì¬ì‹œë„ í†µê³„
#[derive(Debug, Clone, serde::Serialize)]
pub struct RetryStats {
    pub total_items: usize,
    pub pending_retries: usize,
    pub successful_retries: usize,
    pub failed_retries: usize,
    pub max_retries: u32,
}

/// í‘œì¤€ ì‹¤íŒ¨ ë¶„ë¥˜ê¸° êµ¬í˜„
pub struct StandardFailureClassifier {
    base_backoff: Duration,
    max_backoff: Duration,
}

impl StandardFailureClassifier {
    pub fn new() -> Self {
        Self {
            base_backoff: Duration::from_secs(1),
            max_backoff: Duration::from_secs(300), // 5ë¶„ ìµœëŒ€
        }
    }
}

#[async_trait::async_trait]
impl FailureClassifier for StandardFailureClassifier {
    async fn classify_error(&self, error: &str, _stage: CrawlingStage) -> ErrorClassification {
        let error_lower = error.to_lowercase();
        
        // HTTP ìƒíƒœ ì½”ë“œ ê¸°ë°˜ ë¶„ë¥˜
        if error_lower.contains("429") || error_lower.contains("rate limit") {
            return ErrorClassification::RateLimited {
                retry_after: Duration::from_secs(60),
                severity: RateLimitSeverity::Moderate,
            };
        }
        
        if error_lower.contains("401") || error_lower.contains("403") {
            return ErrorClassification::NonRecoverable {
                reason: "Authentication or authorization error".to_string(),
                category: NonRecoverableErrorCategory::AuthenticationError,
            };
        }
        
        if error_lower.contains("404") {
            return ErrorClassification::NonRecoverable {
                reason: "Resource not found".to_string(),
                category: NonRecoverableErrorCategory::NotFound,
            };
        }
        
        if error_lower.contains("5") && (error_lower.contains("00") || error_lower.contains("02") || error_lower.contains("03")) {
            return ErrorClassification::Recoverable {
                retry_after: Duration::from_secs(5),
                category: RecoverableErrorCategory::TemporaryServerError,
            };
        }
        
        // ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë¶„ë¥˜
        if error_lower.contains("timeout") {
            return ErrorClassification::NetworkError {
                retry_after: Duration::from_secs(10),
                error_type: NetworkErrorType::Timeout,
            };
        }
        
        if error_lower.contains("connection refused") {
            return ErrorClassification::NetworkError {
                retry_after: Duration::from_secs(30),
                error_type: NetworkErrorType::ConnectionRefused,
            };
        }
        
        // íŒŒì‹± ì—ëŸ¬
        if error_lower.contains("parse") || error_lower.contains("html") {
            return ErrorClassification::Recoverable {
                retry_after: Duration::from_secs(2),
                category: RecoverableErrorCategory::ParsingError,
            };
        }
        
        // ê¸°ë³¸ê°’: ì¼ë°˜ì ì¸ ë³µêµ¬ ê°€ëŠ¥ ì—ëŸ¬
        ErrorClassification::Recoverable {
            retry_after: Duration::from_secs(5),
            category: RecoverableErrorCategory::TemporaryServerError,
        }
    }
    
    async fn calculate_backoff(&self, attempt_count: u32) -> Duration {
        // ì§€ìˆ˜ ë°±ì˜¤í”„: base_backoff * 2^(attempt_count - 1) + jitter
        let exponential = self.base_backoff.as_secs() * 2_u64.pow(attempt_count.saturating_sub(1));
        let jitter = (attempt_count as u64) % 3; // ê°„ë‹¨í•œ ì§€í„° (rand ëŒ€ì‹ )
        let total_secs = (exponential + jitter).min(self.max_backoff.as_secs());
        
        Duration::from_secs(total_secs)
    }
    
    async fn should_retry(&self, classification: &ErrorClassification, attempt_count: u32) -> bool {
        match classification {
            ErrorClassification::NonRecoverable { .. } => false,
            ErrorClassification::RateLimited { severity, .. } => {
                match severity {
                    RateLimitSeverity::Severe => attempt_count <= 1, // ì‹¬ê°í•œ rate limitì€ 1íšŒë§Œ
                    _ => attempt_count <= 3,
                }
            },
            ErrorClassification::Recoverable { .. } => attempt_count <= 5,
            ErrorClassification::NetworkError { .. } => attempt_count <= 3,
        }
    }
}
