//! ê°œì„ ëœ ë°°ì¹˜ í¬ë¡¤ë§ ì—”ì§„ - ì„œë¹„ìŠ¤ ë ˆì´ì–´ ë¶„ë¦¬ ë²„ì „
//! 
//! ì´ ëª¨ë“ˆì€ guide/crawling ë¬¸ì„œì˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ê° ë‹¨ê³„ë¥¼ 
//! ë…ë¦½ì ì¸ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬í˜„í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í¬ë¡¤ë§ ì—”ì§„ì…ë‹ˆë‹¤.

use std::sync::Arc;
use std::time::{Duration, Instant};
use std::collections::HashMap;
use anyhow::{Result, anyhow};
use tracing::{info, warn, debug, error};
use tokio_util::sync::CancellationToken;
use chrono::Utc;

// ğŸ”¥ ì´ë²¤íŠ¸ ì½œë°± íƒ€ì… ì •ì˜ ì¶”ê°€
pub type PageEventCallback = Arc<dyn Fn(u32, String, u32, bool) -> Result<()> + Send + Sync>;
pub type RetryEventCallback = Arc<dyn Fn(String, String, String, u32, u32, String) -> Result<()> + Send + Sync>;

use crate::domain::services::crawling_services::{
    StatusChecker, DatabaseAnalyzer, ProductListCollector, ProductDetailCollector,
    SiteStatus, DatabaseAnalysis
};
use crate::domain::events::{CrawlingProgress, CrawlingStage, CrawlingStatus};
use crate::domain::product::{Product, ProductDetail};
use crate::domain::product_url::ProductUrl;
use crate::application::EventEmitter;
use crate::infrastructure::{HttpClient, MatterDataExtractor, IntegratedProductRepository, RetryManager};
use crate::infrastructure::crawling_service_impls::{
    StatusCheckerImpl, ProductListCollectorImpl, ProductDetailCollectorImpl,
    CrawlingRangeCalculator, CollectorConfig, product_detail_to_product
};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::system_broadcaster::SystemStateBroadcaster;
use crate::events::{AtomicTaskEvent, TaskStatus};

// ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ import
use crate::new_architecture::events::task_lifecycle::{
    TaskLifecycleEvent, TaskExecutionContext,
    ResourceAllocation, ResourceUsage, ErrorCategory, RetryStrategy,
    ConcurrencyEvent
};

/// ë°°ì¹˜ í¬ë¡¤ë§ ì„¤ì •
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct BatchCrawlingConfig {
    pub start_page: u32,
    pub end_page: u32,
    pub concurrency: u32,
    pub list_page_concurrency: u32,
    pub product_detail_concurrency: u32,
    pub delay_ms: u64,
    pub batch_size: u32,
    pub retry_max: u32,
    pub timeout_ms: u64,
    pub disable_intelligent_range: bool, // ğŸ­ Actor ì‹œìŠ¤í…œìš©: ì§€ëŠ¥í˜• ë²”ìœ„ ì¬ê³„ì‚° ë¹„í™œì„±í™”
    #[serde(skip)]
    pub cancellation_token: Option<CancellationToken>,
}

impl BatchCrawlingConfig {
    /// Create BatchCrawlingConfig from ValidatedCrawlingConfig for Modern Rust 2024 compliance
    #[must_use]
    pub fn from_validated(validated_config: &crate::application::validated_crawling_config::ValidatedCrawlingConfig) -> Self {
        Self {
            start_page: 1,
            end_page: 1, // Will be set by range calculator
            concurrency: validated_config.max_concurrent(),
            list_page_concurrency: validated_config.list_page_max_concurrent,
            product_detail_concurrency: validated_config.product_detail_max_concurrent,
            delay_ms: validated_config.request_delay_ms,
            batch_size: validated_config.batch_size(),
            retry_max: validated_config.max_retries(),
            timeout_ms: validated_config.request_timeout_ms,
            disable_intelligent_range: false, // ê¸°ë³¸ê°’ì€ ì§€ëŠ¥í˜• ë²”ìœ„ ì‚¬ìš©
            cancellation_token: None,
        }
    }
}

impl Default for BatchCrawlingConfig {
    fn default() -> Self {
        // Use ValidatedCrawlingConfig for all defaults instead of hardcoded values
        let validated_config = crate::application::validated_crawling_config::ValidatedCrawlingConfig::default();
        
        Self {
            start_page: 1,
            end_page: 1, // âœ… ê¸°ë³¸ê°’ì„ 1ë¡œ ì„¤ì • (ì‹¤ì œ ê³„ì‚°ëœ ë²”ìœ„ ì‚¬ìš©)
            concurrency: validated_config.max_concurrent(),
            list_page_concurrency: validated_config.list_page_max_concurrent,
            product_detail_concurrency: validated_config.product_detail_max_concurrent,
            delay_ms: validated_config.request_delay_ms,
            batch_size: validated_config.batch_size(),
            retry_max: validated_config.max_retries(),
            timeout_ms: validated_config.request_timeout_ms,
            disable_intelligent_range: false, // ê¸°ë³¸ê°’ì€ ì§€ëŠ¥í˜• ë²”ìœ„ ì‚¬ìš©
            cancellation_token: None,
        }
    }
}

impl BatchCrawlingConfig {
    /// Phase 4: ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚° ê²°ê³¼ë¥¼ configì— ì ìš©
    pub fn update_range_from_calculation(&mut self, optimal_range: Option<(u32, u32)>) {
        if let Some((start_page, end_page)) = optimal_range {
            info!("ğŸ”„ Updating crawling range from {}..{} to {}..{}", 
                  self.start_page, self.end_page, start_page, end_page);
            self.start_page = start_page;
            self.end_page = end_page;
        } else {
            info!("ğŸ”„ No optimal range available, keeping current range {}..{}", 
                  self.start_page, self.end_page);
        }
    }
    
    /// í˜„ì¬ ì„¤ì •ëœ ë²”ìœ„ ì •ë³´ ë°˜í™˜
    pub fn get_page_range(&self) -> (u32, u32) {
        (self.start_page, self.end_page)
    }
}

/// ì„¸ë¶„í™”ëœ í¬ë¡¤ë§ ì´ë²¤íŠ¸ íƒ€ì…
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub enum DetailedCrawlingEvent {
    SessionStarted { 
        session_id: String, 
        config: BatchCrawlingConfig 
    },
    StageStarted { 
        stage: String, 
        message: String 
    },
    StageCompleted { 
        stage: String, 
        items_processed: usize 
    },
    PageCompleted { 
        page: u32, 
        products_found: u32 
    },
    ProductProcessed { 
        url: String, 
        success: bool 
    },
    BatchCompleted { 
        batch: u32, 
        total: u32 
    },
    ErrorOccurred { 
        stage: String, 
        error: String, 
        recoverable: bool 
    },
    SessionCompleted {
        session_id: String,
        duration: Duration,
        total_products: u32,
        success_rate: f64,
    },
    
    // ğŸ”¥ ìƒˆë¡œìš´ ì„¸ë¶„í™”ëœ ë°°ì¹˜ ì´ë²¤íŠ¸
    BatchCreated {
        batch_id: u32,
        total_batches: u32,
        start_page: u32,
        end_page: u32,
        description: String,
    },
    BatchStarted {
        batch_id: u32,
        total_batches: u32,
        pages_in_batch: u32,
    },
    
    // ğŸ”¥ ìƒˆë¡œìš´ í˜ì´ì§€ ì¬ì‹œë„ ì´ë²¤íŠ¸
    PageStarted {
        page: u32,
        batch_id: u32,
        url: String,
    },
    PageRetryAttempt {
        page: u32,
        batch_id: u32,
        url: String,
        attempt: u32,
        max_attempts: u32,
        reason: String,
    },
    PageRetrySuccess {
        page: u32,
        batch_id: u32,
        url: String,
        final_attempt: u32,
        products_found: u32,
    },
    PageRetryFailed {
        page: u32,
        batch_id: u32,
        url: String,
        total_attempts: u32,
        final_error: String,
    },
    
    // ğŸ”¥ ìƒˆë¡œìš´ ì œí’ˆ ì¬ì‹œë„ ì´ë²¤íŠ¸
    ProductStarted {
        url: String,
        batch_id: u32,
        product_index: u32,
        total_products: u32,
    },
    ProductRetryAttempt {
        url: String,
        batch_id: u32,
        attempt: u32,
        max_attempts: u32,
        reason: String,
    },
    ProductRetrySuccess {
        url: String,
        batch_id: u32,
        final_attempt: u32,
    },
    ProductRetryFailed {
        url: String,
        batch_id: u32,
        total_attempts: u32,
        final_error: String,
    },
    
    // ğŸš€ ìƒˆë¡œìš´ ì„¸ë¶„í™”ëœ í˜ì´ì§€ë³„ ì´ë²¤íŠ¸
    PageCollectionStarted {
        page: u32,
        batch_id: u32,
        url: String,
        estimated_products: Option<u32>,
    },
    PageCollectionCompleted {
        page: u32,
        batch_id: u32,
        url: String,
        products_found: u32,
        duration_ms: u64,
    },
    
    // ğŸš€ ìƒˆë¡œìš´ ì„¸ë¶„í™”ëœ ì œí’ˆë³„ ìƒì„¸ ìˆ˜ì§‘ ì´ë²¤íŠ¸
    ProductDetailCollectionStarted {
        url: String,
        product_index: u32,
        total_products: u32,
        batch_id: u32,
    },
    ProductDetailProcessingStarted {
        url: String,
        product_index: u32,
        parsing_stage: String,
    },
    ProductDetailCollectionCompleted {
        url: String,
        product_index: u32,
        success: bool,
        duration_ms: u64,
        data_extracted: bool,
    },
    
    // ğŸš€ ìƒˆë¡œìš´ ë°°ì¹˜ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì´ë²¤íŠ¸
    DatabaseBatchSaveStarted {
        batch_id: u32,
        products_count: u32,
        batch_size: u32,
    },
    DatabaseBatchSaveCompleted {
        batch_id: u32,
        products_saved: u32,
        new_items: u32,
        updated_items: u32,
        errors: u32,
        duration_ms: u64,
    },
}

/// DetailedCrawlingEventë¥¼ TaskLifecycleEventë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
impl DetailedCrawlingEvent {
    /// DetailedCrawlingEventë¥¼ TaskLifecycleEventì™€ TaskExecutionContextë¡œ ë³€í™˜
    pub fn to_task_lifecycle_event(&self, session_id: &str) -> Option<(TaskExecutionContext, TaskLifecycleEvent)> {
        let now = Utc::now();
        
        match self {
            DetailedCrawlingEvent::PageStarted { page, batch_id, url } => {
                let context = TaskExecutionContext {
                    session_id: session_id.to_string(),
                    batch_id: format!("batch_{}", batch_id),
                    stage_name: "page_crawling".to_string(),
                    task_id: format!("page_{}_{}", batch_id, page),
                    task_url: url.clone(),
                    start_time: now,
                    worker_id: Some(format!("page_worker_{}", page % 4)), // ê°„ë‹¨í•œ ì›Œì»¤ í• ë‹¹
                };
                
                let event = TaskLifecycleEvent::Started {
                    worker_id: context.worker_id.clone().unwrap_or_default(),
                    retry_attempt: 0,
                    allocated_resources: ResourceAllocation {
                        memory_bytes: 50 * 1024 * 1024, // 50MB
                        cpu_percent: 25.0,
                        network_bandwidth_kbps: Some(1000),
                    },
                };
                
                Some((context, event))
            },
            
            DetailedCrawlingEvent::PageCompleted { page, products_found } => {
                let context = TaskExecutionContext {
                    session_id: session_id.to_string(),
                    batch_id: "unknown_batch".to_string(),
                    stage_name: "page_crawling".to_string(),
                    task_id: format!("page_{}", page),
                    task_url: format!("https://matter.co.kr/page/{}", page),
                    start_time: now,
                    worker_id: Some(format!("page_worker_{}", page % 4)),
                };
                
                let event = TaskLifecycleEvent::Succeeded {
                    duration_ms: 2000, // ì˜ˆìƒ ì†Œìš” ì‹œê°„
                    result_summary: format!("{}ê°œ ì œí’ˆ ë°œê²¬", products_found),
                    items_processed: *products_found,
                    final_throughput: *products_found as f64 / 2.0, // ì´ˆë‹¹ ì²˜ë¦¬ìœ¨
                    resource_usage: ResourceUsage {
                        peak_memory_bytes: 45 * 1024 * 1024,
                        avg_cpu_percent: 20.0,
                        total_network_bytes: 512 * 1024,
                        disk_io_operations: 50,
                    },
                };
                
                Some((context, event))
            },
            
            DetailedCrawlingEvent::ProductStarted { url, batch_id, product_index, total_products: _ } => {
                let context = TaskExecutionContext {
                    session_id: session_id.to_string(),
                    batch_id: format!("batch_{}", batch_id),
                    stage_name: "product_detail_crawling".to_string(),
                    task_id: format!("product_{}_{}", batch_id, product_index),
                    task_url: url.clone(),
                    start_time: now,
                    worker_id: Some(format!("product_worker_{}", product_index % 8)),
                };
                
                let event = TaskLifecycleEvent::Started {
                    worker_id: context.worker_id.clone().unwrap_or_default(),
                    retry_attempt: 0,
                    allocated_resources: ResourceAllocation {
                        memory_bytes: 20 * 1024 * 1024, // 20MB
                        cpu_percent: 12.5,
                        network_bandwidth_kbps: Some(500),
                    },
                };
                
                Some((context, event))
            },
            
            DetailedCrawlingEvent::ProductProcessed { url, success } => {
                let context = TaskExecutionContext {
                    session_id: session_id.to_string(),
                    batch_id: "unknown_batch".to_string(),
                    stage_name: "product_detail_crawling".to_string(),
                    task_id: format!("product_{}", url.chars().rev().take(8).collect::<String>()),
                    task_url: url.clone(),
                    start_time: now,
                    worker_id: Some("product_worker".to_string()),
                };
                
                let event = if *success {
                    TaskLifecycleEvent::Succeeded {
                        duration_ms: 1500,
                        result_summary: "ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ".to_string(),
                        items_processed: 1,
                        final_throughput: 0.67, // ì•½ 1.5ì´ˆë‹¹ 1ê°œ
                        resource_usage: ResourceUsage {
                            peak_memory_bytes: 15 * 1024 * 1024,
                            avg_cpu_percent: 10.0,
                            total_network_bytes: 256 * 1024,
                            disk_io_operations: 20,
                        },
                    }
                } else {
                    TaskLifecycleEvent::Failed {
                        error_message: "ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨".to_string(),
                        error_code: "PRODUCT_FETCH_ERROR".to_string(),
                        error_category: ErrorCategory::Network,
                        is_recoverable: true,
                        stack_trace: None,
                        resource_usage: ResourceUsage {
                            peak_memory_bytes: 10 * 1024 * 1024,
                            avg_cpu_percent: 5.0,
                            total_network_bytes: 64 * 1024,
                            disk_io_operations: 5,
                        },
                    }
                };
                
                Some((context, event))
            },
            
            DetailedCrawlingEvent::PageRetryAttempt { page, batch_id, url, attempt, max_attempts, reason } => {
                let context = TaskExecutionContext {
                    session_id: session_id.to_string(),
                    batch_id: format!("batch_{}", batch_id),
                    stage_name: "page_crawling".to_string(),
                    task_id: format!("page_{}_{}", batch_id, page),
                    task_url: url.clone(),
                    start_time: now,
                    worker_id: Some(format!("retry_worker_{}", attempt)),
                };
                
                let event = TaskLifecycleEvent::Retrying {
                    attempt: *attempt,
                    max_attempts: *max_attempts,
                    delay_ms: 1000 * (2_u64.pow(*attempt - 1)), // ì§€ìˆ˜ ë°±ì˜¤í”„
                    reason: reason.clone(),
                    retry_strategy: RetryStrategy::ExponentialBackoff {
                        base_ms: 1000,
                        multiplier: 2.0,
                    },
                };
                
                Some((context, event))
            },
            
            DetailedCrawlingEvent::ProductRetryAttempt { url, batch_id, attempt, max_attempts, reason } => {
                let context = TaskExecutionContext {
                    session_id: session_id.to_string(),
                    batch_id: format!("batch_{}", batch_id),
                    stage_name: "product_detail_crawling".to_string(),
                    task_id: format!("product_retry_{}", url.chars().rev().take(8).collect::<String>()),
                    task_url: url.clone(),
                    start_time: now,
                    worker_id: Some(format!("retry_worker_{}", attempt)),
                };
                
                let event = TaskLifecycleEvent::Retrying {
                    attempt: *attempt,
                    max_attempts: *max_attempts,
                    delay_ms: 500 * (*attempt as u64), // ì„ í˜• ë°±ì˜¤í”„
                    reason: reason.clone(),
                    retry_strategy: RetryStrategy::LinearBackoff {
                        initial_ms: 500,
                        increment_ms: 500,
                    },
                };
                
                Some((context, event))
            },
            
            DetailedCrawlingEvent::ErrorOccurred { stage, error, recoverable } => {
                let context = TaskExecutionContext {
                    session_id: session_id.to_string(),
                    batch_id: "error_context".to_string(),
                    stage_name: stage.clone(),
                    task_id: format!("error_{}", now.timestamp()),
                    task_url: "unknown".to_string(),
                    start_time: now,
                    worker_id: None,
                };
                
                let event = TaskLifecycleEvent::Failed {
                    error_message: error.clone(),
                    error_code: "STAGE_ERROR".to_string(),
                    error_category: ErrorCategory::Business,
                    is_recoverable: *recoverable,
                    stack_trace: None,
                    resource_usage: ResourceUsage {
                        peak_memory_bytes: 5 * 1024 * 1024,
                        avg_cpu_percent: 1.0,
                        total_network_bytes: 0,
                        disk_io_operations: 1,
                    },
                };
                
                Some((context, event))
            },
            
            // ë‹¤ë¥¸ ì´ë²¤íŠ¸ë“¤ì€ Task ë ˆë²¨ì´ ì•„ë‹ˆë¯€ë¡œ None ë°˜í™˜
            _ => None,
        }
    }
}

/// ì„œë¹„ìŠ¤ ê¸°ë°˜ ë°°ì¹˜ í¬ë¡¤ë§ ì—”ì§„
pub struct ServiceBasedBatchCrawlingEngine {
    // ì„œë¹„ìŠ¤ ë ˆì´ì–´ë“¤
    status_checker: Arc<dyn StatusChecker>,
    database_analyzer: Arc<dyn DatabaseAnalyzer>,
    product_list_collector: Arc<dyn ProductListCollector>,
    product_detail_collector: Arc<dyn ProductDetailCollector>,
    
    // ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚°ê¸° - Phase 3 Integration
    range_calculator: Arc<CrawlingRangeCalculator>,
    
    // ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ë“¤
    product_repo: Arc<IntegratedProductRepository>,
    product_detail_repo: Arc<IntegratedProductRepository>,
    event_emitter: Arc<Option<EventEmitter>>,
    
    // Live Production Line ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤í„°
    broadcaster: Option<SystemStateBroadcaster>,
    
    // ì¬ì‹œë„ ê´€ë¦¬ì - INTEGRATED_PHASE2_PLAN Week 1 Day 3-4
    retry_manager: Arc<RetryManager>,
    
    // ì„¤ì • ë° ì„¸ì…˜ ì •ë³´
    config: BatchCrawlingConfig,
    session_id: String,
}

#[allow(dead_code)] // Phase2: legacy engine retained for reference; prune in Phase3
impl ServiceBasedBatchCrawlingEngine {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        product_repo: Arc<IntegratedProductRepository>,
        event_emitter: Arc<Option<EventEmitter>>,
        config: BatchCrawlingConfig,
        session_id: String,
        app_config: AppConfig,
    ) -> Self {
        // ì„œë¹„ìŠ¤ë³„ ì„¤ì • ìƒì„±
        let list_collector_config = CollectorConfig {
            max_concurrent: config.list_page_concurrency,
            concurrency: config.list_page_concurrency,
            delay_between_requests: Duration::from_millis(config.delay_ms),
            delay_ms: config.delay_ms,
            batch_size: config.batch_size,
            retry_attempts: config.retry_max,
            retry_max: config.retry_max,
        };
        
        let detail_collector_config = CollectorConfig {
            max_concurrent: config.product_detail_concurrency,
            concurrency: config.product_detail_concurrency,
            delay_between_requests: Duration::from_millis(config.delay_ms),
            delay_ms: config.delay_ms,
            batch_size: config.batch_size,
            retry_attempts: config.retry_max,
            retry_max: config.retry_max,
        };

        // ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        let status_checker: Arc<dyn StatusChecker> = Arc::new(StatusCheckerImpl::new(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
        ));

        // DatabaseAnalyzerëŠ” StatusCheckerImplì„ ì¬ì‚¬ìš© (trait êµ¬í˜„ ì¶”ê°€ë¨)
        let database_analyzer: Arc<dyn DatabaseAnalyzer> = Arc::new(StatusCheckerImpl::new(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
        ));

        // status_checkerë¥¼ ProductListCollectorImplì— ì „ë‹¬í•˜ê¸° ìœ„í•´ concrete typeìœ¼ë¡œ ë‹¤ì‹œ ìƒì„±
        let status_checker_impl = Arc::new(StatusCheckerImpl::new(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
        ));

        let product_list_collector: Arc<dyn ProductListCollector> = Arc::new(ProductListCollectorImpl::new(
            Arc::new(http_client.clone()),  // ğŸ”¥ Mutex ì œê±° - í˜ì´ì§€ ìˆ˜ì§‘ë„ ì§„ì •í•œ ë™ì‹œì„±
            Arc::new(data_extractor.clone()),
            list_collector_config,
            status_checker_impl.clone(),
        ));

        // ProductDetailCollectorëŠ” ì‹¤ì œ ProductDetailCollectorImplì„ ì‚¬ìš© - Mutex ì œê±°ë¡œ ì§„ì •í•œ ë™ì‹œì„±
        let product_detail_collector: Arc<dyn ProductDetailCollector> = Arc::new(ProductDetailCollectorImpl::new(
            Arc::new(http_client.clone()),  // ğŸ”¥ Mutex ì œê±°
            Arc::new(data_extractor.clone()),
            detail_collector_config,
        ));

        // ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚°ê¸° ì´ˆê¸°í™” - Phase 3 Integration
        let range_calculator = Arc::new(CrawlingRangeCalculator::new(
            Arc::clone(&product_repo),
            app_config.clone(),
        ));

        Self {
            status_checker,
            database_analyzer,
            product_list_collector,
            product_detail_collector,
            range_calculator,
            product_repo: product_repo.clone(),
            product_detail_repo: product_repo,
            event_emitter,
            broadcaster: None, // ë‚˜ì¤‘ì— ì„¤ì •ë¨
            retry_manager: Arc::new(RetryManager::new(config.retry_max)),
            config,
            session_id,
        }
    }

    /// SystemStateBroadcaster ì„¤ì • (í¬ë¡¤ë§ ì‹œì‘ ì „ì— í˜¸ì¶œ)
    pub fn set_broadcaster(&mut self, broadcaster: SystemStateBroadcaster) {
        self.broadcaster = Some(broadcaster);
    }

    /// SystemStateBroadcasterì— ëŒ€í•œ mutable ì°¸ì¡°ë¥¼ ë°˜í™˜
    pub fn get_broadcaster_mut(&mut self) -> Option<&mut SystemStateBroadcaster> {
        self.broadcaster.as_mut()
    }

    /// ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚° ê²°ê³¼ë¥¼ ì—”ì§„ì— ì ìš©
    pub fn update_range_from_calculation(&mut self, optimal_range: Option<(u32, u32)>) {
        self.config.update_range_from_calculation(optimal_range);
    }

    /// 4ë‹¨ê³„ ì„œë¹„ìŠ¤ ê¸°ë°˜ í¬ë¡¤ë§ ì‹¤í–‰
    pub async fn execute(&mut self) -> Result<()> {
        let start_time = Instant::now();
        info!("Starting service-based 4-stage batch crawling for session: {}", self.session_id);

        // ğŸ”¥ 1. í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘ ì´ë²¤íŠ¸ (ì‚¬ìš©ìê°€ í¬ë¡¤ë§ ë²„íŠ¼ í´ë¦­í•œ ì‹œì )
        info!("ğŸš€ í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘: {}", self.session_id);
        if let Some(broadcaster) = &self.broadcaster {
            let _ = broadcaster.emit_session_event(
                self.session_id.clone(),
                crate::domain::events::SessionEventType::Started,
                format!("í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘ (í˜ì´ì§€ {}-{})", self.config.start_page, self.config.end_page),
            );
        }

        // ğŸ”¥ 2. ìƒì„¸ ì„¸ì…˜ ì‹œì‘ ì´ë²¤íŠ¸ (ê¸°ì¡´ ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ í˜¸í™˜ì„±)
        self.emit_detailed_event(DetailedCrawlingEvent::SessionStarted {
            session_id: self.session_id.clone(),
            config: self.config.clone(),
        }).await?;

        // ğŸ”¥ 3. ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì‹œì‘ ì´ë²¤íŠ¸ (ìºì‹œ ìš°ì„  í™•ì¸)
        info!("ğŸ” ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì‹œì‘ (ìºì‹œ ìš°ì„ )");
        if let Some(broadcaster) = &self.broadcaster {
            let _ = broadcaster.emit_session_event(
                self.session_id.clone(),
                crate::domain::events::SessionEventType::SiteStatusCheck,
                "ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ë° ìºì‹œ ê²€ì¦ ì‹œì‘".to_string(),
            );
        }

        // ğŸ”¥ 4. StageEvent ë°œìƒ - ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì‹œì‘ (í–¥í›„ êµ¬í˜„)
        // if let Some(ref emitter) = self.event_emitter.as_ref() {
        //     // ConcurrencyEvent ë°œìƒ ì½”ë“œëŠ” í–¥í›„ êµ¬í˜„
        // }

        // ì‹œì‘ ì „ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled before starting");
                return Err(anyhow!("Crawling session cancelled before starting"));
            }
        }

        // Stage 0: ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ (ìºì‹œ ìš°ì„ , í•„ìš”ì‹œ ì‹¤ì œ í™•ì¸)
        let site_status = self.stage0_check_site_status().await?;
        
        // ğŸ”¥ 5. ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ ì´ë²¤íŠ¸ (í–¥í›„ êµ¬í˜„)
        info!("âœ… ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ: ì´ {}í˜ì´ì§€, ì ‘ê·¼ ê°€ëŠ¥", site_status.total_pages);
        // if let Some(ref emitter) = self.event_emitter.as_ref() {
        //     // ConcurrencyEvent ë°œìƒ ì½”ë“œëŠ” í–¥í›„ êµ¬í˜„
        // }
        
        // Stage 0 ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled after Stage 0");
                return Err(anyhow!("Crawling session cancelled after site status check"));
            }
        }
        
        // Stage 0.5: ì§€ëŠ¥í˜• ë²”ìœ„ ì¬ê³„ì‚° ë° ì‹¤ì œ ì ìš© - Phase 4 Implementation
        let (actual_start_page, actual_end_page) = if self.config.disable_intelligent_range {
            // ğŸ­ Actor ì‹œìŠ¤í…œ ëª¨ë“œ: ì‚¬ìš©ìê°€ ì§€ì •í•œ ì •í™•í•œ ë²”ìœ„ ì‚¬ìš©
            info!("ğŸ­ Actor mode: Using exact user-specified range {} to {} (intelligent range disabled)", 
                  self.config.start_page, self.config.end_page);
            (self.config.start_page, self.config.end_page)
        } else {
            // ê¸°ì¡´ ì§€ëŠ¥í˜• ë²”ìœ„ ì¬ê³„ì‚° ë¡œì§
            info!("ğŸ§  Stage 0.5: Performing intelligent range recalculation");
            info!("ğŸ“Š Site analysis: total_pages={}, products_on_last_page={}", 
                  site_status.total_pages, site_status.products_on_last_page);
            
            let optimal_range = self.range_calculator.calculate_next_crawling_range(
                site_status.total_pages,
                site_status.products_on_last_page, // âœ… ì‹¤ì œ ê°’ ì‚¬ìš© (ì´ì „: í•˜ë“œì½”ë”© 10)
            ).await?;
            
            // ê³„ì‚°ëœ ë²”ìœ„ë¥¼ ì‹¤ì œë¡œ ì ìš©í•˜ì—¬ ìµœì¢… ë²”ìœ„ ê²°ì •
            if let Some((optimal_start, optimal_end)) = optimal_range {
                if optimal_start != self.config.start_page || optimal_end != self.config.end_page {
                    info!("ğŸ’¡ Applying intelligent range recommendation: pages {} to {} (original: {} to {})", 
                          optimal_start, optimal_end, self.config.start_page, self.config.end_page);
                    
                    // ë²”ìœ„ ì ìš© ì´ë²¤íŠ¸ ë°œì†¡
                    self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
                        stage: "Range Optimization Applied".to_string(),
                        message: format!("Applied optimal range: {} to {} (was: {} to {})", 
                                       optimal_start, optimal_end, self.config.start_page, self.config.end_page),
                    }).await?;
                    
                    (optimal_start, optimal_end)
                } else {
                    info!("âœ… Current range already optimal: {} to {}", self.config.start_page, self.config.end_page);
                    (self.config.start_page, self.config.end_page)
                }
            } else {
                info!("âœ… All products appear to be crawled - using current range for verification: {} to {}", 
                      self.config.start_page, self.config.end_page);
                (self.config.start_page, self.config.end_page)
            }
        };
        
        info!("ğŸ¯ Final crawling range determined: {} to {}", actual_start_page, actual_end_page);
        
        // Stage 1: ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let _db_analysis = self.stage1_analyze_database().await?;
        
        // Stage 1 ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled after Stage 1");
                return Err(anyhow!("Crawling session cancelled after database analysis"));
            }
        }
        
        // ğŸ”¥ 6. ë°°ì¹˜ ê³„íš ìˆ˜ë¦½ ì´ë²¤íŠ¸ (Stage 1 ì™„ë£Œ í›„)
        let total_pages = if actual_start_page > actual_end_page {
            actual_start_page - actual_end_page + 1
        } else {
            actual_end_page - actual_start_page + 1
        };
        
        let estimated_batches = (total_pages as f32 / 3.0).ceil() as u32; // 3í˜ì´ì§€ì”© ë°°ì¹˜
        info!("ğŸ“‹ ë°°ì¹˜ ê³„íš ìˆ˜ë¦½: ì´ {}í˜ì´ì§€ë¥¼ {}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ", total_pages, estimated_batches);
        
        if let Some(broadcaster) = &self.broadcaster {
            let _ = broadcaster.emit_session_event(
                self.session_id.clone(),
                crate::domain::events::SessionEventType::BatchPlanning,
                format!("ë°°ì¹˜ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {}í˜ì´ì§€ â†’ {}ê°œ ë°°ì¹˜", total_pages, estimated_batches),
            );
        }

        // ğŸ”¥ 7. ê° ë°°ì¹˜ë³„ ìƒì„± ì´ë²¤íŠ¸ ë°œìƒ
        for batch_num in 1..=estimated_batches {
            let start_page_for_batch = actual_start_page + (batch_num - 1) * 3;
            let end_page_for_batch = std::cmp::min(start_page_for_batch + 2, actual_end_page);
            
            self.emit_detailed_event(DetailedCrawlingEvent::BatchCreated {
                batch_id: batch_num,
                total_batches: estimated_batches,
                start_page: start_page_for_batch,
                end_page: end_page_for_batch,
                description: format!("ë°°ì¹˜ {} (í˜ì´ì§€ {}-{})", batch_num, start_page_for_batch, end_page_for_batch),
            }).await?;
        }
        
        // ğŸ”¥ 8. ë°°ì¹˜ ì‹œì‘ ì´ë²¤íŠ¸ (ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘)
        self.emit_detailed_event(DetailedCrawlingEvent::BatchStarted {
            batch_id: 1,
            total_batches: estimated_batches,
            pages_in_batch: total_pages,
        }).await?;
        
        // Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ - ê³„ì‚°ëœ ìµœì  ë²”ìœ„ ì‚¬ìš©
        let product_urls = self.stage2_collect_product_list_optimized(
            actual_start_page, 
            actual_end_page, 
            site_status.total_pages, 
            site_status.products_on_last_page
        ).await?;
        
        // ğŸ”¥ Stage 2 ê²°ê³¼ ê²€ì¦ ë° ë¡œê¹…
        info!("ğŸ“Š Stage 2 completed: {} product URLs collected", product_urls.len());
        if product_urls.is_empty() {
            warn!("âš ï¸  No product URLs collected from Stage 2! This will prevent Stage 3 from running.");
            warn!("   - Start page: {}", actual_start_page);
            warn!("   - End page: {}", actual_end_page);
            warn!("   - This might indicate:");
            warn!("     1. Network issues during product list collection");
            warn!("     2. Website structure changes");
            warn!("     3. Anti-bot measures blocking requests");
            warn!("     4. Pagination calculation errors");
        } else {
            info!("âœ… Stage 2 successful: {} URLs ready for Stage 3", product_urls.len());
            // ìƒ˜í”Œ URL ë¡œê¹… (ë””ë²„ê¹…ìš©)
            if !product_urls.is_empty() {
                info!("ğŸ“ Sample URL: {}", product_urls[0]);
            }
        }
        
        // Stage 2 ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled after Stage 2");
                return Err(anyhow!("Crawling session cancelled after product list collection"));
            }
        }
        
        // ğŸ”¥ Stage 3 ì§„í–‰ ì „ ì¡°ê±´ ê²€ì‚¬
        if product_urls.is_empty() {
            let error_msg = "Cannot proceed to Stage 3: No product URLs collected in Stage 2";
            error!("ğŸš« {}", error_msg);
            return Err(anyhow!(error_msg));
        }
        
        info!("ğŸš€ Proceeding to Stage 3 with {} product URLs", product_urls.len());
        
        // Stage 3: ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘
        let products = self.stage3_collect_product_details(&product_urls).await?;
        let total_products = products.len() as u32;
        
        // Stage 3 ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled after Stage 3");
                return Err(anyhow!("Crawling session cancelled after product details collection"));
            }
        }
        
        // Stage 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        let (processed_count, _new_items, _updated_items, errors) = self.stage4_save_to_database(products).await?;
        
        // ğŸ”¥ 4. ë°°ì¹˜ ì™„ë£Œ ì´ë²¤íŠ¸ (ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í›„ ë°œì†¡)
        self.emit_detailed_event(DetailedCrawlingEvent::BatchCompleted {
            batch: 1,
            total: processed_count as u32,
        }).await?;
        
        // ì„±ê³µë¥  ê³„ì‚°
        let success_rate = if processed_count > 0 {
            (processed_count - errors) as f64 / processed_count as f64
        } else {
            0.0
        };

        // ğŸ”¥ ë°°ì¹˜ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡ (UI ì—°ê²°)
        if let Some(broadcaster) = &mut self.broadcaster {
            let pages_processed = if actual_start_page >= actual_end_page {
                actual_start_page - actual_end_page + 1
            } else {
                actual_end_page - actual_start_page + 1
            };
            
            if let Err(e) = broadcaster.emit_batch_completed(pages_processed, total_products, success_rate) {
                warn!("Failed to emit batch-completed event: {}", e);
            }
        }

        let duration = start_time.elapsed();
        info!("Service-based batch crawling completed in {:?}: {} products collected, {:.2}% success rate", 
            duration, total_products, success_rate * 100.0);
        
        // ğŸ”¥ í¬ë¡¤ë§ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡ (UI ì—°ê²°)
        if let Some(broadcaster) = &mut self.broadcaster {
            if let Err(e) = broadcaster.emit_crawling_completed() {
                warn!("Failed to emit crawling-completed event: {}", e);
            }
        }
        
        // ğŸ”¥ 5. ì„¸ì…˜ ì™„ë£Œ ì´ë²¤íŠ¸ (ëª¨ë“  ì‘ì—… ì™„ë£Œ í›„ ë°œì†¡)
        self.emit_detailed_event(DetailedCrawlingEvent::SessionCompleted {
            session_id: self.session_id.clone(),
            duration,
            total_products,
            success_rate,
        }).await?;
        
        Ok(())
    }

    /// Stage 0: ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ (ìƒˆë¡œìš´ ë‹¨ê³„)
    async fn stage0_check_site_status(&self) -> Result<SiteStatus> {
        info!("Stage 0: Checking site status");
        
        // ğŸ”¥ í¬ë¡¤ë§ ì„¸ì…˜ ë‚´ ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ì‹œì‘ ì´ë²¤íŠ¸
        if let Some(broadcaster) = &self.broadcaster {
            let session_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
                is_standalone: false,  // í¬ë¡¤ë§ ì„¸ì…˜ ë‚´ ì²´í¬
                status: crate::domain::events::SiteCheckStatus::Started,
                message: "í¬ë¡¤ë§ ì„¸ì…˜ ë‚´ ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì‹œì‘".to_string(),
                timestamp: chrono::Utc::now(),
            };
            let _ = broadcaster.emit_site_status_check(&session_event);
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "SiteStatus".to_string(),
            message: "ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ì¤‘...".to_string(),
        }).await?;

        let site_status = self.status_checker.check_site_status().await?;
        
        if !site_status.is_accessible || site_status.total_pages == 0 {
            let error_msg = format!("Site is not accessible or has no pages (pages: {})", site_status.total_pages);
            self.emit_detailed_event(DetailedCrawlingEvent::ErrorOccurred {
                stage: "SiteStatus".to_string(),
                error: error_msg.clone(),
                recoverable: true,
            }).await?;
            return Err(anyhow!(error_msg));
        }

        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "SiteStatus".to_string(),
            items_processed: 1,
        }).await?;

        // ğŸ”¥ í¬ë¡¤ë§ ì„¸ì…˜ ë‚´ ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ì„±ê³µ ì´ë²¤íŠ¸
        if let Some(broadcaster) = &self.broadcaster {
            let success_event = crate::domain::events::CrawlingEvent::SiteStatusCheck {
                is_standalone: false,  // í¬ë¡¤ë§ ì„¸ì…˜ ë‚´ ì²´í¬
                status: crate::domain::events::SiteCheckStatus::Success,
                message: format!("í¬ë¡¤ë§ ì„¸ì…˜ ë‚´ ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ: {}ê°œ í˜ì´ì§€", site_status.total_pages),
                timestamp: chrono::Utc::now(),
            };
            let _ = broadcaster.emit_site_status_check(&success_event);
        }

        info!("Stage 0 completed: Site is healthy (score: {})", site_status.health_score);
        Ok(site_status)
    }

    /// Stage 1: ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ (ìƒˆë¡œìš´ ë‹¨ê³„)
    async fn stage1_analyze_database(&self) -> Result<DatabaseAnalysis> {
        info!("Stage 1: Analyzing database state");
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "DatabaseAnalysis".to_string(),
            message: "ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘...".to_string(),
        }).await?;

        let analysis = self.database_analyzer.analyze_current_state().await?;
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "DatabaseAnalysis".to_string(),
            items_processed: analysis.total_products as usize,
        }).await?;

        info!("Stage 1 completed: {} total products, quality score: {}", 
              analysis.total_products, analysis.data_quality_score);
        Ok(analysis)
    }

    /// Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ (ì„œë¹„ìŠ¤ ê¸°ë°˜)
    // REMOVE_CANDIDATE(Phase3): Currently unused â€“ legacy batch workflow
    async fn stage2_collect_product_list(&self, total_pages: u32, products_on_last_page: u32) -> Result<Vec<ProductUrl>> {
        info!("Stage 2: Collecting product list using ProductListCollector service");
        
        // ì·¨ì†Œ í™•ì¸ - ë‹¨ê³„ ì‹œì‘ ì „
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Stage 2 (ProductList) cancelled before starting");
                return Err(anyhow!("Product list collection cancelled"));
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "ProductList".to_string(),
            message: format!("{}í˜ì´ì§€ì—ì„œ ì œí’ˆ ëª©ë¡ì„ ìˆ˜ì§‘í•˜ëŠ” ì¤‘...", total_pages),
        }).await?;

        let effective_end = total_pages.min(self.config.end_page);
        
        // ì·¨ì†Œ ê°€ëŠ¥í•œ ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì‹¤í–‰ - í•­ìƒ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
        let product_urls = if let Some(cancellation_token) = &self.config.cancellation_token {
            info!("ğŸ›‘ Using cancellation token for product list collection");
            
            // ì·¨ì†Œ í† í°ê³¼ í•¨ê»˜ ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ - ê°œì„ ëœ ProductListCollector ì‚¬ìš©
            self.product_list_collector.collect_page_range_with_cancellation(
                self.config.start_page, 
                effective_end,
                total_pages,
                products_on_last_page,
                cancellation_token.clone()
            ).await?
        } else {
            warn!("âš ï¸  No cancellation token - using parallel collection without cancellation");
            // ì·¨ì†Œ í† í°ì´ ì—†ì–´ë„ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
            self.product_list_collector.collect_page_range(
                self.config.start_page, 
                effective_end,
                total_pages,
                products_on_last_page
            ).await?
        };
        
        // ì·¨ì†Œ í™•ì¸ - ë‹¨ê³„ ì™„ë£Œ í›„
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Stage 2 (ProductList) cancelled after collection");
                return Err(anyhow!("Product list collection cancelled after completion"));
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "ProductList".to_string(),
            items_processed: product_urls.len(),
        }).await?;

        info!("Stage 2 completed: {} product URLs collected", product_urls.len());
        Ok(product_urls)
    }

    /// Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ (ìµœì í™”ëœ ë²”ìœ„ ì‚¬ìš©) - Phase 4 Implementation
    async fn stage2_collect_product_list_optimized(&mut self, start_page: u32, end_page: u32, _total_pages: u32, _products_on_last_page: u32) -> Result<Vec<ProductUrl>> {
        info!("ğŸ”— Stage 2: ProductList ìˆ˜ì§‘ ì‹œì‘ - í˜ì´ì§€ë³„ ë³‘ë ¬ ì‹¤í–‰ ({}~{})", start_page, end_page);
        
        // ğŸ”¥ Stage 2 ë°°ì¹˜ ìƒì„± ì´ë²¤íŠ¸
        let total_pages = if start_page > end_page {
            start_page - end_page + 1
        } else {
            end_page - start_page + 1
        };

        let batch_id = format!("productlist-{}-{}", start_page, end_page);
        
        // ğŸ”¥ ProductList ë°°ì¹˜ ìƒì„± ì´ë²¤íŠ¸
        info!("ğŸ“¦ ProductList ë°°ì¹˜ ìƒì„±: {} ({}í˜ì´ì§€)", batch_id, total_pages);
        if let Some(broadcaster) = &self.broadcaster {
            let metadata = crate::domain::events::BatchMetadata {
                total_items: total_pages,
                processed_items: 0,
                successful_items: 0,
                failed_items: 0,
                start_time: chrono::Utc::now(),
                estimated_completion: None,
            };
            
            let _ = broadcaster.emit_batch_event(
                self.session_id.clone(),
                batch_id.to_string(),
                crate::domain::events::CrawlingStage::ProductList,
                crate::domain::events::BatchEventType::Created,
                format!("ProductList ë°°ì¹˜ ìƒì„±: í˜ì´ì§€ {}~{} ({}ê°œ í˜ì´ì§€)", start_page, end_page, total_pages),
                Some(metadata),
            );
        }

        // ğŸ”¥ ë°°ì¹˜ ì‹œì‘ ì´ë²¤íŠ¸
        if let Some(broadcaster) = &self.broadcaster {
            let _ = broadcaster.emit_batch_event(
                self.session_id.clone(),
                batch_id.to_string(),
                crate::domain::events::CrawlingStage::ProductList,
                crate::domain::events::BatchEventType::Started,
                format!("ProductList ë°°ì¹˜ ì‹œì‘: í˜ì´ì§€ {}~{} ìˆ˜ì§‘ ì¤‘", start_page, end_page),
                None,
            );
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::BatchCreated {
            batch_id: 1,
            total_batches: 1, // í˜„ì¬ëŠ” ë‹¨ì¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            start_page,
            end_page,
            description: format!("í˜ì´ì§€ {}~{} ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ({}ê°œ í˜ì´ì§€)", start_page, end_page, total_pages),
        }).await?;

        self.emit_detailed_event(DetailedCrawlingEvent::BatchStarted {
            batch_id: 1,
            total_batches: 1,
            pages_in_batch: total_pages,
        }).await?;
        
        // ğŸ”¥ ê¸°ì¡´ ë°°ì¹˜ ìƒì„± ì´ë²¤íŠ¸ë„ ìœ ì§€ (í˜¸í™˜ì„±)
        if let Some(broadcaster) = &mut self.broadcaster {
            if let Err(e) = broadcaster.emit_batch_created(start_page, end_page) {
                warn!("Failed to emit batch-created event: {}", e);
            }
        }
        
        // ì·¨ì†Œ í™•ì¸ - ë‹¨ê³„ ì‹œì‘ ì „
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Stage 2 (ProductList) cancelled before starting");
                return Err(anyhow!("Product list collection cancelled"));
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "ProductList (Optimized)".to_string(),
            message: format!("í˜ì´ì§€ {} ~ {}ì—ì„œ ì œí’ˆ ëª©ë¡ì„ ìˆ˜ì§‘í•˜ëŠ” ì¤‘... (ë™ì‹œì„± ì‹¤í–‰)", start_page, end_page),
        }).await?;

        // ğŸ”¥ ë™ì‹œì„± í¬ë¡¤ë§ ì‹¤í–‰ - ì´ë²¤íŠ¸ ë°œì†¡ì„ ìœ„í•œ ì±„ë„ ìƒì„±
        let (event_tx, mut event_rx) = tokio::sync::mpsc::channel::<(String, serde_json::Value)>(100);
        
        // ì´ë²¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ìƒì„±
        let broadcaster_opt = self.broadcaster.take(); // ì†Œìœ ê¶Œ ì´ë™
        let event_handler = tokio::spawn(async move {
            let mut broadcaster = broadcaster_opt;
            while let Some((event_type, payload)) = event_rx.recv().await {
                // ğŸ”¥ ì™„ë£Œ ì‹ í˜¸ ê°ì§€ - concurrent ì‘ì—…ì´ ì™„ë£Œë˜ë©´ ì¦‰ì‹œ ì¢…ë£Œ
                if event_type == "concurrent_phase_completed" {
                    debug!("Concurrent phase completed - terminating event handler");
                    break;
                }
                
                if let Some(ref mut b) = broadcaster {
                    match event_type.as_str() {
                        "page-collection-started" => {
                            if let Ok(detailed_event) = serde_json::from_value::<DetailedCrawlingEvent>(payload) {
                                // ìƒˆë¡œìš´ page-collection-started ì´ë²¤íŠ¸ ì²˜ë¦¬
                                debug!("Page collection started event received: {:?}", detailed_event);
                                // ê¸°ì¡´ ë¸Œë¡œë“œìºìŠ¤í„°ë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡
                                match &detailed_event {
                                    DetailedCrawlingEvent::PageCollectionStarted { page, url, .. } => {
                                        // emit_page_started ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ë¡œê·¸ë¡œë§Œ ì²˜ë¦¬
                                        debug!("Page {} collection started for URL: {}", page, url);
                                    },
                                    _ => {}
                                }
                            }
                        }
                        "page-collection-completed" => {
                            if let Ok(detailed_event) = serde_json::from_value::<DetailedCrawlingEvent>(payload) {
                                // ìƒˆë¡œìš´ page-collection-completed ì´ë²¤íŠ¸ ì²˜ë¦¬
                                debug!("Page collection completed event received: {:?}", detailed_event);
                                // ê¸°ì¡´ ë¸Œë¡œë“œìºìŠ¤í„°ë¡œ ë³€í™˜í•˜ì—¬ ì „ì†¡
                                match &detailed_event {
                                    DetailedCrawlingEvent::PageCollectionCompleted { page, url, products_found, .. } => {
                                        if let Err(e) = b.emit_page_crawled(*page, url.clone(), *products_found, true) {
                                            warn!("Failed to emit page-collection-completed as page-crawled event: {}", e);
                                        }
                                    },
                                    _ => {}
                                }
                            }
                        }
                        "page-started" => {
                            if let Ok(detailed_event) = serde_json::from_value::<DetailedCrawlingEvent>(payload) {
                                // PageStarted ì´ë²¤íŠ¸ëŠ” ë³„ë„ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ ë‚¨ê¹€
                                debug!("Page started event received: {:?}", detailed_event);
                            }
                        }
                        "page-retry-attempt" => {
                            if let Ok(detailed_event) = serde_json::from_value::<DetailedCrawlingEvent>(payload) {
                                // PageRetryAttempt ì´ë²¤íŠ¸ëŠ” ë³„ë„ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ë¡œê·¸ë§Œ ë‚¨ê¹€
                                debug!("Page retry attempt event received: {:?}", detailed_event);
                            }
                        }
                        "page-completed" => {
                            // DetailedCrawlingEventë¥¼ ì§ì ‘ ì²˜ë¦¬ - ê¸°ì¡´ ë¸Œë¡œë“œìºìŠ¤í„° ë©”ì„œë“œ ì‚¬ìš©
                            if let Ok(detailed_event) = serde_json::from_value::<DetailedCrawlingEvent>(payload) {
                                match &detailed_event {
                                    DetailedCrawlingEvent::PageCompleted { page, products_found } => {
                                        if let Err(e) = b.emit_page_crawled(*page, format!("page-{}", page), *products_found, true) {
                                            warn!("Failed to emit page-completed as page-crawled event: {}", e);
                                        }
                                    },
                                    _ => {}
                                }
                            }
                        }
                        "page-crawled" => {
                            if let Ok(data) = serde_json::from_value::<(u32, String, u32, bool)>(payload) {
                                if let Err(e) = b.emit_page_crawled(data.0, data.1, data.2, data.3) {
                                    warn!("Failed to emit page-crawled event: {}", e);
                                }
                            }
                        }
                        "retry-attempt" => {
                            if let Ok(data) = serde_json::from_value::<(String, String, String, u32, u32, String)>(payload) {
                                if let Err(e) = b.emit_retry_attempt(data.0, data.1, data.2, data.3, data.4, data.5) {
                                    warn!("Failed to emit retry-attempt event: {}", e);
                                }
                            }
                        }
                        "retry-success" => {
                            if let Ok(data) = serde_json::from_value::<(String, String, String, u32)>(payload) {
                                if let Err(e) = b.emit_retry_success(data.0, data.1, data.2, data.3) {
                                    warn!("Failed to emit retry-success event: {}", e);
                                }
                            }
                        }
                        "retry-failed" => {
                            if let Ok(data) = serde_json::from_value::<(String, String, String, u32, String)>(payload) {
                                if let Err(e) = b.emit_retry_failed(data.0, data.1, data.2, data.3, data.4) {
                                    warn!("Failed to emit retry-failed event: {}", e);
                                }
                            }
                        }
                        _ => {}
                    }
                }
            }
            broadcaster // ì†Œìœ ê¶Œ ë°˜í™˜
        });

        // ğŸ”¥ ì´ë²¤íŠ¸ ì½œë°± í•¨ìˆ˜ ì •ì˜ - ë” ìƒì„¸í•œ ì´ë²¤íŠ¸ë“¤ ì¶”ê°€
        let _engine_clone = self.session_id.clone();
        let batch_id = 1u32;
        
    let event_tx_clone = event_tx.clone();
    let _page_callback = move |page_id: u32, url: String, product_count: u32, success: bool| -> Result<()> {
            let start_time = std::time::Instant::now();
            
            // ï¿½ ìƒˆë¡œìš´ ì„¸ë¶„í™”ëœ í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘ ì´ë²¤íŠ¸
            let page_start_event = DetailedCrawlingEvent::PageCollectionStarted {
                page: page_id,
                batch_id,
                url: url.clone(),
                estimated_products: Some(25), // í˜ì´ì§€ë‹¹ í‰ê·  ì˜ˆìƒ ì œí’ˆ ìˆ˜
            };
            let start_payload = serde_json::to_value(page_start_event)?;
            if let Err(e) = event_tx_clone.try_send(("page-collection-started".to_string(), start_payload)) {
                warn!("Failed to send page-collection-started event: {}", e);
            }
            
            // ï¿½ ìƒˆë¡œìš´ ì„¸ë¶„í™”ëœ í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ ì´ë²¤íŠ¸
            if success {
                let duration_ms = start_time.elapsed().as_millis() as u64;
                let page_event = DetailedCrawlingEvent::PageCollectionCompleted {
                    page: page_id,
                    batch_id,
                    url: url.clone(),
                    products_found: product_count,
                    duration_ms,
                };
                let payload = serde_json::to_value(page_event)?;
                if let Err(e) = event_tx_clone.try_send(("page-collection-completed".to_string(), payload)) {
                    warn!("Failed to send page-collection-completed event: {}", e);
                }
            }
            
            // ê¸°ì¡´ page-crawled ì´ë²¤íŠ¸ë„ ìœ ì§€
            let legacy_payload = serde_json::to_value((page_id, url, product_count, success))?;
            if let Err(e) = event_tx_clone.try_send(("page-crawled".to_string(), legacy_payload)) {
                warn!("Failed to send page-crawled event: {}", e);
            }
            Ok(())
        };

        let event_tx_clone2 = event_tx.clone();
    let _retry_callback = move |item_id: String, item_type: String, url: String, attempt: u32, max_attempts: u32, reason: String| -> Result<()> {
            // ğŸ”¥ í˜ì´ì§€ ì¬ì‹œë„ ì‹œë„ ì´ë²¤íŠ¸
            if item_type == "page" {
                let page_num = url.split("page=").nth(1)
                    .and_then(|s| s.split('&').next())
                    .and_then(|s| s.parse::<u32>().ok())
                    .unwrap_or(0);
                
                let retry_event = DetailedCrawlingEvent::PageRetryAttempt {
                    page: page_num,
                    batch_id,
                    url: url.clone(),
                    attempt,
                    max_attempts,
                    reason: reason.clone(),
                };
                let retry_payload = serde_json::to_value(retry_event)?;
                if let Err(e) = event_tx_clone2.try_send(("page-retry-attempt".to_string(), retry_payload)) {
                    warn!("Failed to send page-retry-attempt event: {}", e);
                }
            }
            
            // ê¸°ì¡´ ì¬ì‹œë„ ì´ë²¤íŠ¸
            let payload = serde_json::to_value((item_id, item_type, url, attempt, max_attempts, reason))?;
            if let Err(e) = event_tx_clone2.try_send(("retry-attempt".to_string(), payload)) {
                warn!("Failed to send retry-attempt event: {}", e);
            }
            Ok(())
        };

        // ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰ (ì§„ì •í•œ ë™ì‹œì„± ë³´ì¥)
        let product_urls = if let Some(cancellation_token) = &self.config.cancellation_token {
            // ğŸ”¥ ìƒˆë¡œìš´ ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë©”ì„œë“œ ì‚¬ìš© (ë™ì‹œì„± ë³´ì¥)
            let collector = self.product_list_collector.clone();
            let collector_impl = collector.as_ref()
                .as_any()
                .downcast_ref::<ProductListCollectorImpl>()
                .ok_or_else(|| anyhow!("Failed to downcast ProductListCollector"))?;
            
            collector_impl.collect_page_range_with_async_events(
                start_page,
                end_page,
                Some(cancellation_token.clone()),
                self.session_id.clone(),
                batch_id.to_string(),
            ).await?
        } else {
            // ğŸ”¥ í† í°ì´ ì—†ì–´ë„ ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë©”ì„œë“œ ì‚¬ìš©
            let collector = self.product_list_collector.clone();
            let collector_impl = collector.as_ref()
                .as_any()
                .downcast_ref::<ProductListCollectorImpl>()
                .ok_or_else(|| anyhow!("Failed to downcast ProductListCollector"))?;
            
            collector_impl.collect_page_range_with_async_events(
                start_page,
                end_page,
                None,
                self.session_id.clone(),
                batch_id.to_string(),
            ).await.map_err(|e| anyhow!("Product list collection failed: {}", e))?
        };

        // ğŸ”¥ ë°°ì¹˜ ì™„ë£Œ ì´ë²¤íŠ¸
        if let Some(broadcaster) = &self.broadcaster {
            let metadata = crate::domain::events::BatchMetadata {
                total_items: total_pages,
                processed_items: total_pages,
                successful_items: product_urls.len() as u32,
                failed_items: total_pages.saturating_sub(product_urls.len() as u32),
                start_time: chrono::Utc::now(), // ì‹¤ì œë¡œëŠ” ì‹œì‘ ì‹œê°„ì„ ì €ì¥í•´ì•¼ í•¨
                estimated_completion: Some(chrono::Utc::now()),
            };
            
            let _ = broadcaster.emit_batch_event(
                self.session_id.clone(),
                batch_id.to_string(),
                crate::domain::events::CrawlingStage::ProductList,
                crate::domain::events::BatchEventType::Completed,
                format!("ProductList ë°°ì¹˜ ì™„ë£Œ: {}ê°œ ì œí’ˆ URL ìˆ˜ì§‘", product_urls.len()),
                Some(metadata),
            );
        }

        // ì´ë²¤íŠ¸ ì±„ë„ ì¢…ë£Œ
        drop(event_tx);
        
        // ğŸ”¥ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸° ë° ë¸Œë¡œë“œìºìŠ¤í„° ë³µêµ¬ (ì¦‰ì‹œ ì™„ë£Œ íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•)
        match tokio::time::timeout(std::time::Duration::from_millis(100), event_handler).await {
            Ok(Ok(broadcaster_opt)) => {
                debug!("Event handler completed successfully");
                self.broadcaster = broadcaster_opt;
            },
            Ok(Err(e)) => {
                warn!("Event handler task failed: {}", e);
                // ë¸Œë¡œë“œìºìŠ¤í„°ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
                self.broadcaster = None;
            },
            Err(_) => {
                debug!("Event handler processing - force shutdown after concurrent jobs completed");
                // ğŸ”¥ concurrent ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ ê°•ì œ ì¢…ë£Œ
                // ë¸Œë¡œë“œìºìŠ¤í„°ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
                self.broadcaster = None;
            }
        }

        info!("âœ… Stage 2 completed: {} product URLs collected from optimized range with TRUE concurrent execution", product_urls.len());
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "ProductList (Optimized)".to_string(),
            items_processed: product_urls.len(),
        }).await?;

        Ok(product_urls)
    }

    /// Stage 3: ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ (ì„œë¹„ìŠ¤ ê¸°ë°˜ + ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜)
    async fn stage3_collect_product_details(&mut self, product_urls: &[ProductUrl]) -> Result<Vec<(Product, ProductDetail)>> {
        info!("Stage 3: Collecting product details using ProductDetailCollector service with retry mechanism");
        
        // ì·¨ì†Œ í™•ì¸ - ë‹¨ê³„ ì‹œì‘ ì „
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Stage 3 (ProductDetails) cancelled before starting");
                return Err(anyhow!("Product details collection cancelled"));
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "ProductDetails".to_string(),
            message: format!("{}ê°œ ì œí’ˆì˜ ìƒì„¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘... (ì¬ì‹œë„ ì§€ì›)", product_urls.len()),
        }).await?;

    // ì´ˆê¸° ì‹œë„ - cancellation token ì‚¬ìš©
    let mut successful_products = Vec::new();
    // Explicit type to satisfy compiler; kept underscore as it's only for debug scaffolding
    let mut _failed_urls: Vec<ProductUrl> = Vec::new();

        // ï¿½ ì œí’ˆë³„ ì²˜ë¦¬ ì „ì— ìƒˆë¡œìš´ ì„¸ë¶„í™”ëœ ì´ë²¤íŠ¸ë“¤ì„ ë°œìƒì‹œí‚¤ê¸° ìœ„í•œ ë¡œì§ ì¶”ê°€
        for (index, product_url) in product_urls.iter().enumerate() {
            // ï¿½ ì œí’ˆ ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘ ì´ë²¤íŠ¸ (ìƒˆë¡œìš´ êµ¬ì¡°)
            self.emit_detailed_event(DetailedCrawlingEvent::ProductDetailCollectionStarted {
                url: product_url.to_string(),
                product_index: (index + 1) as u32,
                total_products: product_urls.len() as u32,
                batch_id: 1,
            }).await?;
            
            // ğŸš€ ì œí’ˆ ìƒì„¸ ì²˜ë¦¬ ì‹œì‘ ì´ë²¤íŠ¸ (ìƒˆë¡œìš´ êµ¬ì¡°)
            self.emit_detailed_event(DetailedCrawlingEvent::ProductDetailProcessingStarted {
                url: product_url.to_string(),
                product_index: (index + 1) as u32,
                parsing_stage: "HTML_PARSING".to_string(),
            }).await?;
        }

        // í•­ìƒ ì·¨ì†Œ í† í°ì„ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œ - ì—†ìœ¼ë©´ ê¸°ë³¸ í† í° ìƒì„±
        let result = if let Some(cancellation_token) = &self.config.cancellation_token {
            info!("ğŸ›‘ USING PROVIDED CANCELLATION TOKEN for product detail collection");
            info!("ğŸ›‘ Cancellation token is_cancelled: {}", cancellation_token.is_cancelled());
            
            // ğŸ”¥ ì´ë²¤íŠ¸ ê¸°ë°˜ ìˆ˜ì§‘ ë©”ì„œë“œ ì‚¬ìš© (ìƒˆë¡œìš´ êµ¬í˜„)
            if let Some(collector_impl) = self.product_detail_collector.as_any().downcast_ref::<ProductDetailCollectorImpl>() {
                collector_impl.collect_details_with_async_events(
                    product_urls,
                    Some(cancellation_token.clone()),
                    self.session_id.clone(),
                    self.session_id.clone(), // session_idë¥¼ batch_idë¡œë„ ì‚¬ìš©
                ).await
            } else {
                // Fallback to original method
                self.product_detail_collector.collect_details_with_cancellation(product_urls, cancellation_token.clone()).await
            }
        } else {
            warn!("âš ï¸  NO CANCELLATION TOKEN - creating default token for consistent behavior");
            let default_token = CancellationToken::new();
            
            // ğŸ”¥ ì´ë²¤íŠ¸ ê¸°ë°˜ ìˆ˜ì§‘ ë©”ì„œë“œ ì‚¬ìš© (ìƒˆë¡œìš´ êµ¬í˜„)
            if let Some(collector_impl) = self.product_detail_collector.as_any().downcast_ref::<ProductDetailCollectorImpl>() {
                collector_impl.collect_details_with_async_events(
                    product_urls,
                    Some(default_token.clone()),
                    self.session_id.clone(),
                    self.session_id.clone(), // session_idë¥¼ batch_idë¡œë„ ì‚¬ìš©
                ).await
            } else {
                // Fallback to original method
                self.product_detail_collector.collect_details_with_cancellation(product_urls, default_token).await
            }
        };

        match result {
            Ok(product_details) => {
                // ì·¨ì†Œ í™•ì¸ - ë°ì´í„° ë³€í™˜ ì „
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    if cancellation_token.is_cancelled() {
                        warn!("ğŸ›‘ Product details collection cancelled before processing results");
                        return Err(anyhow!("Product details collection cancelled"));
                    }
                }
                
                // ProductDetailì„ Productë¡œ ë³€í™˜í•˜ê³  ì›ë³¸ ProductDetailê³¼ í•¨ê»˜ ì €ì¥
                let product_count = product_details.len();
                successful_products = product_details.into_iter()
                    .enumerate()
                    .map(|(index, detail)| {
                        let product = product_detail_to_product(detail.clone());
                        
                        // ï¿½ ìƒˆë¡œìš´ ì œí’ˆ ìƒì„¸ ìˆ˜ì§‘ ì™„ë£Œ ì´ë²¤íŠ¸ (ë¹„ë™ê¸° ì²˜ë¦¬)
                        if let Some(product_url) = product_urls.get(index) {
                            // ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ìˆ˜ì§‘ ì‹œì‘ë¶€í„° ì¸¡ì •í•´ì•¼ í•¨)
                            let duration_ms = 500 + (index as u64 * 50); // ì‹œë®¬ë ˆì´ì…˜ëœ ì²˜ë¦¬ ì‹œê°„
                            
                            let _completion_event = DetailedCrawlingEvent::ProductDetailCollectionCompleted {
                                url: product_url.to_string(),
                                product_index: (index + 1) as u32,
                                success: true,
                                duration_ms,
                                data_extracted: detail.model.is_some() && detail.manufacturer.is_some(),
                            };
                            
                            // ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë°œì†¡ì„ ìœ„í•œ ë…¼ë¦¬ (í–¥í›„ ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Futureë¡œ ì²˜ë¦¬)
                            // í˜„ì¬ëŠ” ê¸°ì¡´ broadcasterë¥¼ í†µí•´ í˜¸í™˜ì„± ìœ ì§€
                            if let Some(broadcaster) = &mut self.broadcaster {
                                if let Err(e) = broadcaster.emit_product_collected(
                                    product.page_id.map(|id| id as u32).unwrap_or(0),
                                    product.model.clone().unwrap_or_else(|| format!("product-{}", index)),
                                    product_url.to_string(),
                                    true
                                ) {
                                    warn!("Failed to emit product-collected event: {}", e);
                                }
                            }
                        }
                        
                        // ğŸ”¥ ë°°ì¹˜ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ (10ê°œë§ˆë‹¤)
                        if index % 10 == 0 || index == product_count - 1 {
                            let progress = (index + 1) as f64 / product_urls.len() as f64;
                            
                            if let Some(broadcaster) = &mut self.broadcaster {
                                if let Err(e) = broadcaster.emit_batch_progress(
                                    "ProductDetails".to_string(),
                                    progress,
                                    product_urls.len() as u32,
                                    (index + 1) as u32,
                                    0, // items_active
                                    0  // items_failed (ì•„ì§ ì‹¤íŒ¨í•œ í•­ëª© ì—†ìŒ)
                                ) {
                                    warn!("Failed to emit batch-progress event: {}", e);
                                }
                            }
                        }
                        
                        (product, detail)
                    })
                    .collect();
                
                info!("âœ… Initial collection successful: {} products", successful_products.len());
            }
            Err(e) => {
                // cancellation ì²´í¬
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    if cancellation_token.is_cancelled() {
                        warn!("ğŸ›‘ Collection cancelled by user");
                        return Ok(successful_products); // ì´ë¯¸ ìˆ˜ì§‘ëœ ì œí’ˆë“¤ ë°˜í™˜
                    }
                }
                
                warn!("âŒ Initial collection failed: {}", e);
                let failed_urls = product_urls.to_vec();
                
                // ğŸ”¥ ì‹¤íŒ¨í•œ ì œí’ˆë“¤ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡ (UI ì—°ê²°)
                for (index, url) in failed_urls.iter().enumerate() {
                    if let Some(broadcaster) = &mut self.broadcaster {
                        if let Err(emit_err) = broadcaster.emit_product_collected(
                            0, // í˜ì´ì§€ ID ë¯¸ìƒ
                            format!("failed-{}", index),
                            url.to_string(),
                            false
                        ) {
                            warn!("Failed to emit product-collected failure event: {}", emit_err);
                        }
                    }
                }
                
                // ì‹¤íŒ¨í•œ URLë“¤ì„ ì¬ì‹œë„ íì— ì¶”ê°€
                for (index, url) in failed_urls.iter().enumerate() {
                    let item_id = format!("product_detail_{}_{}", self.session_id, index);
                    let mut metadata = std::collections::HashMap::new();
                    metadata.insert("url".to_string(), url.to_string());
                    metadata.insert("stage".to_string(), "product_details".to_string());
                    
                    if let Err(retry_err) = self.retry_manager.add_failed_item(
                        item_id,
                        CrawlingStage::ProductDetails,
                        e.to_string(),
                        url.to_string(),
                        metadata,
                    ).await {
                        warn!("Failed to add item to retry queue: {}", retry_err);
                    }
                }
            }
        }

        // ì¬ì‹œë„ ì²˜ë¦¬ (cancellation token í™•ì¸ í›„)
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Skipping retries due to cancellation");
                self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
                    stage: "ProductDetails".to_string(),
                    items_processed: successful_products.len(),
                }).await?;
                return Ok(successful_products);
            }
        }

        let retry_products = self.process_retries_for_product_details().await?;
        successful_products.extend(retry_products);
        
        // ğŸ”¥ ê° ì œí’ˆë³„ ìˆ˜ì§‘ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡ (ëª¨ë“  ìˆ˜ì§‘ì´ ì™„ë£Œëœ í›„)
    for (index, (_product, detail)) in successful_products.iter().enumerate() {
            if let Some(product_url) = product_urls.get(index) {
                let duration_ms = 500 + (index as u64 * 50); // ì‹œë®¬ë ˆì´ì…˜ëœ ì²˜ë¦¬ ì‹œê°„
                
                // ğŸ”¥ ì²˜ë¦¬ ì‹œì‘ ì´ë²¤íŠ¸
                self.emit_detailed_event(DetailedCrawlingEvent::ProductDetailProcessingStarted {
                    url: product_url.to_string(),
                    product_index: (index + 1) as u32,
                    parsing_stage: "COMPLETED".to_string(),
                }).await?;
                
                // ğŸ”¥ ìˆ˜ì§‘ ì™„ë£Œ ì´ë²¤íŠ¸
                self.emit_detailed_event(DetailedCrawlingEvent::ProductDetailCollectionCompleted {
                    url: product_url.to_string(),
                    product_index: (index + 1) as u32,
                    success: true,
                    duration_ms,
                    data_extracted: detail.model.is_some() && detail.manufacturer.is_some(),
                }).await?;
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "ProductDetails".to_string(),
            items_processed: successful_products.len(),
        }).await?;

        info!("Stage 3 completed: {} products collected (including retries)", successful_products.len());
        Ok(successful_products)
    }
    
    /// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì¬ì‹œë„ ì²˜ë¦¬
    async fn process_retries_for_product_details(&mut self) -> Result<Vec<(Product, ProductDetail)>> {
        info!("ğŸ”„ Processing retries for product details collection");
        let mut retry_products = Vec::new();
        
        // ìµœëŒ€ 3ë²ˆì˜ ì¬ì‹œë„ ì‚¬ì´í´
        for cycle in 1..=3 {
            // ì¬ì‹œë„ ì‚¬ì´í´ ì‹œì‘ ì „ ì·¨ì†Œ í™•ì¸
            if let Some(cancellation_token) = &self.config.cancellation_token {
                if cancellation_token.is_cancelled() {
                    warn!("ğŸ›‘ Retry processing cancelled at cycle {}", cycle);
                    return Ok(retry_products);
                }
            }
            
            let ready_items = self.retry_manager.get_ready_items().await?;
            if ready_items.is_empty() {
                debug!("No items ready for retry in cycle {}", cycle);
                break;
            }
            
            info!("ğŸ”„ Retry cycle {}: Processing {} items", cycle, ready_items.len());
            
            for retry_item in ready_items {
                // ê° ì¬ì‹œë„ í•­ëª© ì²˜ë¦¬ ì „ ì·¨ì†Œ í™•ì¸
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    if cancellation_token.is_cancelled() {
                        warn!("ğŸ›‘ Retry processing cancelled during item processing");
                        return Ok(retry_products);
                    }
                }
                
                if retry_item.stage == CrawlingStage::ProductDetails {
                    let url = retry_item.original_url;
                    let item_id = retry_item.item_id.clone();
                    
                    info!("ğŸ”„ Retrying product detail collection for: {}", url);
                    
                    // ğŸ”¥ ProductRetryAttempt ì´ë²¤íŠ¸ ë°œì†¡
                    self.emit_detailed_event(DetailedCrawlingEvent::ProductRetryAttempt {
                        url: url.clone(),
                        batch_id: 1,
                        attempt: cycle,
                        max_attempts: 3,
                        reason: "Product detail collection failed".to_string(),
                    }).await.unwrap_or_else(|e| warn!("Failed to emit ProductRetryAttempt event: {}", e));
                    
                    // ğŸ”¥ ì¬ì‹œë„ ì‹œë„ ì´ë²¤íŠ¸ ë°œì†¡ (ê¸°ì¡´ broadcaster)
                    if let Some(broadcaster) = &mut self.broadcaster {
                        if let Err(e) = broadcaster.emit_retry_attempt(
                            item_id.clone(),
                            "product".to_string(),
                            url.clone(),
                            cycle,
                            3,
                            "Product detail collection failed".to_string()
                        ) {
                            warn!("Failed to emit retry-attempt event: {}", e);
                        }
                    }
                    
                    // Convert String URL to ProductUrl for the new API
                    let product_url = ProductUrl::new(url.clone(), -1, -1); // Use -1 for retry URLs
                    
                    match self.product_detail_collector.collect_details(&[product_url]).await {
                        Ok(mut product_details) => {
                            if let Some(detail) = product_details.pop() {
                                let product = product_detail_to_product(detail.clone());
                                info!("âœ… Retry successful for: {}", url);
                                retry_products.push((product, detail));
                                
                                // ğŸ”¥ ProductRetrySuccess ì´ë²¤íŠ¸ ë°œì†¡
                                self.emit_detailed_event(DetailedCrawlingEvent::ProductRetrySuccess {
                                    url: url.clone(),
                                    batch_id: 1,
                                    final_attempt: cycle,
                                }).await.unwrap_or_else(|e| warn!("Failed to emit ProductRetrySuccess event: {}", e));
                                
                                // ğŸ”¥ ì¬ì‹œë„ ì„±ê³µ ì´ë²¤íŠ¸ ë°œì†¡ (ê¸°ì¡´ broadcaster)
                                if let Some(broadcaster) = &mut self.broadcaster {
                                    if let Err(e) = broadcaster.emit_retry_success(
                                        item_id.clone(),
                                        "product".to_string(),
                                        url.clone(),
                                        cycle
                                    ) {
                                        warn!("Failed to emit retry-success event: {}", e);
                                    }
                                }
                                
                                // ì„±ê³µ ê¸°ë¡
                                if let Err(e) = self.retry_manager.mark_retry_success(&item_id).await {
                                    warn!("Failed to mark retry success: {}", e);
                                }
                                
                                self.emit_detailed_event(DetailedCrawlingEvent::ProductProcessed {
                                    url: url.clone(),
                                    success: true,
                                }).await?;
                            }
                        }
                        Err(e) => {
                            warn!("âŒ Retry failed for {}: {}", url, e);
                            
                            // ì¬ì‹œë„ íì— ë‹¤ì‹œ ì¶”ê°€ (ì¬ì‹œë„ í•œë„ ë‚´ì—ì„œ)
                            let mut metadata = std::collections::HashMap::new();
                            metadata.insert("url".to_string(), url.clone());
                            metadata.insert("retry_cycle".to_string(), cycle.to_string());
                            
                            if let Err(retry_err) = self.retry_manager.add_failed_item(
                                item_id.clone(),
                                CrawlingStage::ProductDetails,
                                e.to_string(),
                                url.clone(),
                                metadata,
                            ).await {
                                debug!("Item exceeded retry limit or not retryable: {}", retry_err);
                                
                                // ğŸ”¥ ProductRetryFailed ì´ë²¤íŠ¸ ë°œì†¡
                                self.emit_detailed_event(DetailedCrawlingEvent::ProductRetryFailed {
                                    url: url.clone(),
                                    batch_id: 1,
                                    total_attempts: cycle,
                                    final_error: e.to_string(),
                                }).await.unwrap_or_else(|e| warn!("Failed to emit ProductRetryFailed event: {}", e));
                                
                                // ğŸ”¥ ì¬ì‹œë„ ìµœì¢… ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡ (ê¸°ì¡´ broadcaster)
                                if let Some(broadcaster) = &mut self.broadcaster {
                                    if let Err(emit_err) = broadcaster.emit_retry_failed(
                                        item_id.clone(),
                                        "product".to_string(),
                                        url.clone(),
                                        cycle,
                                        e.to_string()
                                    ) {
                                        warn!("Failed to emit retry-failed event: {}", emit_err);
                                    }
                                }
                            }
                            
                            self.emit_detailed_event(DetailedCrawlingEvent::ProductProcessed {
                                url: url.clone(),
                                success: false,
                            }).await?;
                        }
                    }
                    
                    // ì¬ì‹œë„ ê°„ ì§€ì—° (ì·¨ì†Œ í™•ì¸ í¬í•¨)
                    let delay = Duration::from_millis(self.config.delay_ms);
                    if let Some(cancellation_token) = &self.config.cancellation_token {
                        tokio::select! {
                            _ = tokio::time::sleep(delay) => {},
                            _ = cancellation_token.cancelled() => {
                                warn!("ğŸ›‘ Retry processing cancelled during item delay");
                                return Ok(retry_products);
                            }
                        }
                    } else {
                        tokio::time::sleep(delay).await;
                    }
                }
            }
            
            // ì‚¬ì´í´ ê°„ ì§€ì—° (ì·¨ì†Œ í™•ì¸ í¬í•¨)
            if cycle < 3 {
                let cycle_delay = Duration::from_secs(5);
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    tokio::select! {
                        _ = tokio::time::sleep(cycle_delay) => {},
                        _ = cancellation_token.cancelled() => {
                            warn!("ğŸ›‘ Retry processing cancelled during cycle delay");
                            return Ok(retry_products);
                        }
                    }
                } else {
                    tokio::time::sleep(cycle_delay).await;
                }
            }
        }
        
        info!("ğŸ”„ Retry processing completed: {} additional products collected", retry_products.len());
        Ok(retry_products)
    }

    /// Stage 4: ë°ì´í„°ë² ì´ìŠ¤ ë°°ì¹˜ ì €ì¥ (íš¨ìœ¨ì„± ê°œì„ )
    async fn stage4_save_to_database(&mut self, products: Vec<(Product, ProductDetail)>) -> Result<(usize, usize, usize, usize)> {
        info!("Stage 4: Batch saving {} products to database", products.len());
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "DatabaseSave".to_string(),
            message: format!("{}ê°œ ì œí’ˆì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì¤‘...", products.len()),
        }).await?;

        let total_products = products.len();
        let batch_size = 50; // ë°°ì¹˜ í¬ê¸° (50ê°œì”© ì²˜ë¦¬)
        let mut total_new_items = 0;
        let mut total_updated_items = 0;
        let mut total_errors = 0;
        let mut total_processed = 0;

        // ì œí’ˆë“¤ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¶„í• 
        let product_batches: Vec<Vec<(Product, ProductDetail)>> = products
            .chunks(batch_size)
            .map(|chunk| chunk.to_vec())
            .collect();

        let total_batches = product_batches.len();

        info!("ğŸ“¦ Processing {} products in {} batches of {} items each", 
              total_products, total_batches, batch_size);

        for (batch_index, batch) in product_batches.into_iter().enumerate() {
            let batch_id = (batch_index + 1) as u32;
            let batch_start_time = std::time::Instant::now();
            
            // ì·¨ì†Œ í™•ì¸
            if let Some(cancellation_token) = &self.config.cancellation_token {
                if cancellation_token.is_cancelled() {
                    warn!("ğŸ›‘ Database batch save cancelled after {} batches", batch_index);
                    break;
                }
            }

            // ğŸš€ ë°°ì¹˜ ì €ì¥ ì‹œì‘ ì´ë²¤íŠ¸
            self.emit_detailed_event(DetailedCrawlingEvent::DatabaseBatchSaveStarted {
                batch_id,
                products_count: batch.len() as u32,
                batch_size: batch_size as u32,
            }).await?;

            // ë°°ì¹˜ ì²˜ë¦¬
            let mut batch_new_items = 0;
            let mut batch_updated_items = 0;
            let mut batch_errors = 0;

            // ğŸš€ ì‹¤ì œ ë°°ì¹˜ ì €ì¥ ë¡œì§ (íŠ¸ëœì­ì…˜ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì„± ê·¹ëŒ€í™”)
            for (product, product_detail) in batch.iter() {
                // ê°œë³„ ì €ì¥ (í–¥í›„ ì‹¤ì œ ë°°ì¹˜ INSERT/UPDATEë¡œ ê°œì„  ê°€ëŠ¥)
                let product_save_result = self.product_repo.create_or_update_product(product).await;
                let product_detail_save_result = self.product_detail_repo.create_or_update_product_detail(product_detail).await;
                
                match (product_save_result, product_detail_save_result) {
                    (Ok((product_was_updated, product_was_created)), Ok((detail_was_updated, detail_was_created))) => {
                        if product_was_created || detail_was_created {
                            batch_new_items += 1;
                        } else if product_was_updated || detail_was_updated {
                            batch_updated_items += 1;
                        }
                        total_processed += 1;
                    },
                    (Err(e), _) | (_, Err(e)) => {
                        batch_errors += 1;
                        warn!("ë°°ì¹˜ {} ì €ì¥ ì‹¤íŒ¨: {}", batch_id, e);
                    }
                }
            }

            let batch_duration_ms = batch_start_time.elapsed().as_millis() as u64;

            // ğŸš€ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ ì´ë²¤íŠ¸
            self.emit_detailed_event(DetailedCrawlingEvent::DatabaseBatchSaveCompleted {
                batch_id,
                products_saved: (batch.len() - batch_errors) as u32,
                new_items: batch_new_items as u32,
                updated_items: batch_updated_items as u32,
                errors: batch_errors as u32,
                duration_ms: batch_duration_ms,
            }).await?;

            // ë°°ì¹˜ í†µê³„ ëˆ„ì 
            total_new_items += batch_new_items;
            total_updated_items += batch_updated_items;
            total_errors += batch_errors;

            info!("âœ… ë°°ì¹˜ {}/{} ì™„ë£Œ: {}ê°œ ì €ì¥ (ì‹ ê·œ: {}, ì—…ë°ì´íŠ¸: {}, ì˜¤ë¥˜: {}) in {}ms", 
                  batch_id, total_batches, batch.len() - batch_errors, 
                  batch_new_items, batch_updated_items, batch_errors, batch_duration_ms);

            // ë°°ì¹˜ ê°„ ì§§ì€ ì§€ì—° (ì‹œìŠ¤í…œ ë¶€í•˜ ë¶„ì‚°)
            if batch_index < total_batches - 1 {
                tokio::time::sleep(Duration::from_millis(100)).await;
            }
        }

        // ğŸš€ Stage ì™„ë£Œ ì´ë²¤íŠ¸
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "DatabaseSave".to_string(),
            items_processed: total_processed,
        }).await?;

        info!("ğŸ¯ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: ì´ {}ê°œ ì²˜ë¦¬ (ì‹ ê·œ: {}, ì—…ë°ì´íŠ¸: {}, ì˜¤ë¥˜: {})", 
              total_processed, total_new_items, total_updated_items, total_errors);
        
        Ok((total_processed, total_new_items, total_updated_items, total_errors))
    }

    /// ì„¸ë¶„í™”ëœ ì´ë²¤íŠ¸ ë°©ì¶œ
    async fn emit_detailed_event(&self, event: DetailedCrawlingEvent) -> Result<()> {
        // ğŸš€ ìƒˆë¡œìš´ ConcurrencyEvent ë°œí–‰ (TaskLifecycle, Session, Batch í†µí•©)
        if let Some(emitter) = self.event_emitter.as_ref() {
            let concurrency_event = match &event {
                // ì„¸ì…˜ ì´ë²¤íŠ¸ë“¤
                DetailedCrawlingEvent::SessionStarted { session_id, .. } => {
                    Some(ConcurrencyEvent::SessionEvent {
                        session_id: session_id.clone(),
                        event_type: crate::new_architecture::events::task_lifecycle::SessionEventType::Started,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("stage".to_string(), "session_initialization".to_string()),
                        ]),
                    })
                },
                
                DetailedCrawlingEvent::SessionCompleted { session_id, duration, total_products, success_rate } => {
                    Some(ConcurrencyEvent::SessionEvent {
                        session_id: session_id.clone(),
                        event_type: crate::new_architecture::events::task_lifecycle::SessionEventType::Completed,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("duration_seconds".to_string(), duration.as_secs().to_string()),
                            ("total_products".to_string(), total_products.to_string()),
                            ("success_rate".to_string(), success_rate.to_string()),
                        ]),
                    })
                },
                
                // ë°°ì¹˜ ì´ë²¤íŠ¸ë“¤
                DetailedCrawlingEvent::BatchCreated { batch_id, total_batches, start_page, end_page, description } => {
                    Some(ConcurrencyEvent::BatchEvent {
                        session_id: self.session_id.clone(),
                        batch_id: format!("batch_{}", batch_id),
                        event_type: crate::new_architecture::events::task_lifecycle::BatchEventType::Created,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("total_batches".to_string(), total_batches.to_string()),
                            ("start_page".to_string(), start_page.to_string()),
                            ("end_page".to_string(), end_page.to_string()),
                            ("description".to_string(), description.clone()),
                        ]),
                    })
                },
                
                DetailedCrawlingEvent::BatchStarted { batch_id, total_batches, pages_in_batch } => {
                    Some(ConcurrencyEvent::BatchEvent {
                        session_id: self.session_id.clone(),
                        batch_id: format!("batch_{}", batch_id),
                        event_type: crate::new_architecture::events::task_lifecycle::BatchEventType::Started,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("total_batches".to_string(), total_batches.to_string()),
                            ("pages_in_batch".to_string(), pages_in_batch.to_string()),
                        ]),
                    })
                },
                
                DetailedCrawlingEvent::BatchCompleted { batch, total } => {
                    Some(ConcurrencyEvent::BatchEvent {
                        session_id: self.session_id.clone(),
                        batch_id: format!("batch_{}", batch),
                        event_type: crate::new_architecture::events::task_lifecycle::BatchEventType::Completed,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("batch_number".to_string(), batch.to_string()),
                            ("total_batches".to_string(), total.to_string()),
                        ]),
                    })
                },
                
                // Stage ë ˆë²¨ ì´ë²¤íŠ¸ë“¤ - ìƒˆë¡œ ì¶”ê°€
                DetailedCrawlingEvent::StageStarted { stage, message } => {
                    Some(ConcurrencyEvent::SessionEvent {
                        session_id: self.session_id.clone(),
                        event_type: crate::new_architecture::events::task_lifecycle::SessionEventType::Started,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("stage".to_string(), stage.clone()),
                            ("stage_message".to_string(), message.clone()),
                            ("event_category".to_string(), "stage_started".to_string()),
                        ]),
                    })
                },
                
                DetailedCrawlingEvent::StageCompleted { stage, items_processed } => {
                    Some(ConcurrencyEvent::SessionEvent {
                        session_id: self.session_id.clone(),
                        event_type: crate::new_architecture::events::task_lifecycle::SessionEventType::Completed,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("stage".to_string(), stage.clone()),
                            ("items_processed".to_string(), items_processed.to_string()),
                            ("event_category".to_string(), "stage_completed".to_string()),
                        ]),
                    })
                },
                
                // Page ë ˆë²¨ ì´ë²¤íŠ¸ë“¤ - ì¶”ê°€
                DetailedCrawlingEvent::PageStarted { page, batch_id, url } => {
                    Some(ConcurrencyEvent::BatchEvent {
                        session_id: self.session_id.clone(),
                        batch_id: format!("batch_{}", batch_id),
                        event_type: crate::new_architecture::events::task_lifecycle::BatchEventType::Started,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("page_number".to_string(), page.to_string()),
                            ("page_url".to_string(), url.clone()),
                            ("event_category".to_string(), "page_started".to_string()),
                        ]),
                    })
                },
                
                DetailedCrawlingEvent::PageCompleted { page, products_found } => {
                    Some(ConcurrencyEvent::BatchEvent {
                        session_id: self.session_id.clone(),
                        batch_id: "page_batch".to_string(),
                        event_type: crate::new_architecture::events::task_lifecycle::BatchEventType::Completed,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("page_number".to_string(), page.to_string()),
                            ("products_found".to_string(), products_found.to_string()),
                            ("event_category".to_string(), "page_completed".to_string()),
                        ]),
                    })
                },
                
                // Product ë ˆë²¨ ì´ë²¤íŠ¸ë“¤ - ì¶”ê°€
                DetailedCrawlingEvent::ProductStarted { url, batch_id, product_index, total_products } => {
                    Some(ConcurrencyEvent::BatchEvent {
                        session_id: self.session_id.clone(),
                        batch_id: format!("batch_{}", batch_id),
                        event_type: crate::new_architecture::events::task_lifecycle::BatchEventType::Started,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("product_url".to_string(), url.clone()),
                            ("product_index".to_string(), product_index.to_string()),
                            ("total_products".to_string(), total_products.to_string()),
                            ("event_category".to_string(), "product_started".to_string()),
                        ]),
                    })
                },
                
                DetailedCrawlingEvent::ProductProcessed { url, success } => {
                    Some(ConcurrencyEvent::BatchEvent {
                        session_id: self.session_id.clone(),
                        batch_id: "product_batch".to_string(),
                        event_type: if *success { 
                            crate::new_architecture::events::task_lifecycle::BatchEventType::Completed
                        } else { 
                            crate::new_architecture::events::task_lifecycle::BatchEventType::Failed
                        },
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("product_url".to_string(), url.clone()),
                            ("success".to_string(), success.to_string()),
                            ("event_category".to_string(), "product_processed".to_string()),
                        ]),
                    })
                },
                
                // Error ì´ë²¤íŠ¸ë“¤ - ì¶”ê°€
                DetailedCrawlingEvent::ErrorOccurred { stage, error, recoverable } => {
                    Some(ConcurrencyEvent::SessionEvent {
                        session_id: self.session_id.clone(),
                        event_type: crate::new_architecture::events::task_lifecycle::SessionEventType::Failed,
                        timestamp: Utc::now(),
                        metadata: HashMap::from([
                            ("timestamp".to_string(), Utc::now().to_rfc3339()),
                            ("stage".to_string(), stage.clone()),
                            ("error_message".to_string(), error.clone()),
                            ("recoverable".to_string(), recoverable.to_string()),
                            ("event_category".to_string(), "error_occurred".to_string()),
                        ]),
                    })
                },
                
                // Task ë ˆë²¨ ì´ë²¤íŠ¸ë“¤
                _ => {
                    if let Some((context, task_event)) = event.to_task_lifecycle_event(&self.session_id) {
                        Some(ConcurrencyEvent::TaskLifecycle {
                            context,
                            event: task_event,
                        })
                    } else {
                        None
                    }
                }
            };
            
            // ConcurrencyEventë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ì—¬ ë°œí–‰
            if let Some(concurrency_event) = concurrency_event {
                if let Ok(json_value) = serde_json::to_value(&concurrency_event) {
                    emitter.emit_detailed_crawling_event_json(json_value).await?;
                }
            }
        }
        
        if let Some(emitter) = self.event_emitter.as_ref() {
            // DetailedCrawlingEventë¥¼ ê¸°ì¡´ ì´ë²¤íŠ¸ ì‹œìŠ¤í…œê³¼ ì—°ë™
            let progress = match &event {
                DetailedCrawlingEvent::StageStarted { stage, message } => {
                    CrawlingProgress {
                        current: 0,
                        total: if self.config.start_page > self.config.end_page {
                            self.config.start_page - self.config.end_page + 1
                        } else {
                            self.config.end_page - self.config.start_page + 1
                        },
                        percentage: 0.0,
                        current_stage: match stage.as_str() {
                            "SiteStatus" => CrawlingStage::StatusCheck,
                            "DatabaseAnalysis" => CrawlingStage::DatabaseAnalysis,
                            "ProductList" => CrawlingStage::ProductList,
                            "ProductDetails" => CrawlingStage::ProductDetails,
                            "DatabaseSave" => CrawlingStage::DatabaseSave,
                            _ => CrawlingStage::TotalPages,
                        },
                        current_step: message.clone(),
                        status: CrawlingStatus::Running,
                        message: format!("Stage started: {}", stage),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(if self.config.start_page > self.config.end_page {
                            self.config.start_page - self.config.end_page + 1
                        } else {
                            self.config.end_page - self.config.start_page + 1
                        }),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::PageCompleted { page, products_found } => {
                    CrawlingProgress {
                        current: *page,
                        total: if self.config.start_page > self.config.end_page {
                            self.config.start_page - self.config.end_page + 1
                        } else {
                            self.config.end_page - self.config.start_page + 1
                        },
                        percentage: (*page as f64 / (if self.config.start_page > self.config.end_page {
                            self.config.start_page - self.config.end_page + 1
                        } else {
                            self.config.end_page - self.config.start_page + 1
                        }) as f64) * 100.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("í˜ì´ì§€ {}ì—ì„œ {}ê°œ ì œí’ˆ ë°œê²¬", page, products_found),
                        status: CrawlingStatus::Running,
                        message: format!("Page {} processed: {} products found", page, products_found),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: *products_found,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(if self.config.start_page > self.config.end_page {
                            self.config.start_page - self.config.end_page + 1
                        } else {
                            self.config.end_page - self.config.start_page + 1
                        }),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::ProductProcessed { url, success } => {
                    CrawlingProgress {
                        current: 1,
                        total: 1,
                        percentage: if *success { 100.0 } else { 0.0 },
                        current_stage: CrawlingStage::ProductDetails,
                        current_step: if *success { 
                            format!("ì œí’ˆ '{}' ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ", url) 
                        } else { 
                            format!("ì œí’ˆ '{}' ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨", url) 
                        },
                        status: if *success { CrawlingStatus::Running } else { CrawlingStatus::Error },
                        message: format!("Product {}: {}", url, if *success { "success" } else { "failed" }),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: if *success { 1 } else { 0 },
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(1),
                        errors: if *success { 0 } else { 1 },
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::BatchCompleted { batch, total } => {
                    CrawlingProgress {
                        current: *batch,
                        total: *total,
                        percentage: (*batch as f64 / *total as f64) * 100.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("ë°°ì¹˜ {}/{} ì™„ë£Œ", batch, total),
                        status: CrawlingStatus::Running,
                        message: format!("Batch {} of {} completed", batch, total),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 1,
                        updated_items: 0,
                        current_batch: Some(*batch),
                        total_batches: Some(*total),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::ErrorOccurred { stage, error, recoverable } => {
                    CrawlingProgress {
                        current: 0,
                        total: 1,
                        percentage: 0.0,
                        current_stage: match stage.as_str() {
                            "SiteStatus" => CrawlingStage::StatusCheck,
                            "DatabaseAnalysis" => CrawlingStage::DatabaseAnalysis,
                            "ProductList" => CrawlingStage::ProductList,
                            "ProductDetails" => CrawlingStage::ProductDetails,
                            "DatabaseSave" => CrawlingStage::DatabaseSave,
                            _ => CrawlingStage::TotalPages,
                        },
                        current_step: format!("ì˜¤ë¥˜ ë°œìƒ: {}", error),
                        status: if *recoverable { CrawlingStatus::Running } else { CrawlingStatus::Error },
                        message: format!("Error in {}: {} (recoverable: {})", stage, error, recoverable),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(1),
                        errors: 1,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::SessionStarted { session_id, config: _ } => {
                    CrawlingProgress {
                        current: 0,
                        total: if self.config.start_page > self.config.end_page {
                            self.config.start_page - self.config.end_page + 1
                        } else {
                            self.config.end_page - self.config.start_page + 1
                        },
                        percentage: 0.0,
                        current_stage: CrawlingStage::StatusCheck,
                        current_step: format!("í¬ë¡¤ë§ ì„¸ì…˜ {} ì‹œì‘", session_id),
                        status: CrawlingStatus::Running,
                        message: format!("Session {} started", session_id),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(if self.config.start_page > self.config.end_page {
                            self.config.start_page - self.config.end_page + 1
                        } else {
                            self.config.end_page - self.config.start_page + 1
                        }),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::SessionCompleted { session_id, duration, total_products, success_rate } => {
                    CrawlingProgress {
                        current: *total_products,
                        total: *total_products,
                        percentage: 100.0,
                        current_stage: CrawlingStage::DatabaseSave,
                        current_step: format!("ì„¸ì…˜ {} ì™„ë£Œ ({}ì´ˆ, ì„±ê³µë¥ : {:.1}%)", session_id, duration.as_secs(), success_rate * 100.0),
                        status: CrawlingStatus::Completed,
                        message: format!("Session {} completed: {} products, {:.1}% success rate", session_id, total_products, success_rate * 100.0),
                        remaining_time: None,
                        elapsed_time: duration.as_secs(),
                        new_items: *total_products,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                
                // ğŸ”¥ ìƒˆë¡œìš´ ë°°ì¹˜ ê´€ë ¨ ì´ë²¤íŠ¸ë“¤
                DetailedCrawlingEvent::BatchCreated { batch_id, total_batches, start_page, end_page, description } => {
                    CrawlingProgress {
                        current: *batch_id,
                        total: *total_batches,
                        percentage: 0.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("ë°°ì¹˜ {}/{} ìƒì„±: {}", batch_id, total_batches, description),
                        status: CrawlingStatus::Running,
                        message: format!("Batch {}/{} created: pages {} to {}", batch_id, total_batches, start_page, end_page),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(*total_batches),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::BatchStarted { batch_id, total_batches, pages_in_batch } => {
                    CrawlingProgress {
                        current: *batch_id,
                        total: *total_batches,
                        percentage: ((*batch_id - 1) as f64 / *total_batches as f64) * 100.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("ë°°ì¹˜ {}/{} ì‹œì‘ ({}ê°œ í˜ì´ì§€)", batch_id, total_batches, pages_in_batch),
                        status: CrawlingStatus::Running,
                        message: format!("Batch {}/{} started: {} pages", batch_id, total_batches, pages_in_batch),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(*total_batches),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                
                // ğŸ”¥ ìƒˆë¡œìš´ í˜ì´ì§€ ê´€ë ¨ ì´ë²¤íŠ¸ë“¤ (ê¸°ë³¸ ì²˜ë¦¬ë§Œ ì œê³µ)
                DetailedCrawlingEvent::PageStarted { page, batch_id, url: _ } => {
                    CrawlingProgress {
                        current: *page,
                        total: if self.config.start_page > self.config.end_page {
                            self.config.start_page - self.config.end_page + 1
                        } else {
                            self.config.end_page - self.config.start_page + 1
                        },
                        percentage: 0.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("í˜ì´ì§€ {} ì‹œì‘ (ë°°ì¹˜ {})", page, batch_id),
                        status: CrawlingStatus::Running,
                        message: format!("Page {} started in batch {}", page, batch_id),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::PageRetryAttempt { page, batch_id, url: _, attempt, max_attempts, reason } => {
                    CrawlingProgress {
                        current: *attempt,
                        total: *max_attempts,
                        percentage: (*attempt as f64 / *max_attempts as f64) * 100.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("í˜ì´ì§€ {} ì¬ì‹œë„ {}/{} (ë°°ì¹˜ {}) - {}", page, attempt, max_attempts, batch_id, reason),
                        status: CrawlingStatus::Running,
                        message: format!("Page {} attempt {}/{} in batch {} - {}", page, attempt, max_attempts, batch_id, reason),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::PageRetrySuccess { page, batch_id, url: _, final_attempt, products_found } => {
                    CrawlingProgress {
                        current: *final_attempt,
                        total: *final_attempt,
                        percentage: 100.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("í˜ì´ì§€ {} ì¬ì‹œë„ ì„±ê³µ ({}ë²ˆì§¸ ì‹œë„, {}ê°œ ì œí’ˆ, ë°°ì¹˜ {})", page, final_attempt, products_found, batch_id),
                        status: CrawlingStatus::Running,
                        message: format!("Page attempt succeeded on attempt {} with {} products (batch {})", final_attempt, products_found, batch_id),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: *products_found,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::PageRetryFailed { page, batch_id, url: _, total_attempts, final_error } => {
                    CrawlingProgress {
                        current: *total_attempts,
                        total: *total_attempts,
                        percentage: 100.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("í˜ì´ì§€ {} ìµœì¢… ì‹¤íŒ¨ ({}ë²ˆ ì¬ì‹œë„ í›„, ë°°ì¹˜ {}) - {}", page, total_attempts, batch_id, final_error),
                        status: CrawlingStatus::Error,
                        message: format!("Page {} finally failed after {} attempts (batch {}) - {}", page, total_attempts, batch_id, final_error),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 1,
                        timestamp: chrono::Utc::now(),
                    }
                },
                
                // ğŸ”¥ ìƒˆë¡œìš´ ì œí’ˆ ê´€ë ¨ ì´ë²¤íŠ¸ë“¤ (ê¸°ë³¸ ì²˜ë¦¬ë§Œ ì œê³µ)
                DetailedCrawlingEvent::ProductStarted { url: _, batch_id, product_index, total_products } => {
                    CrawlingProgress {
                        current: *product_index,
                        total: *total_products,
                        percentage: (*product_index as f64 / *total_products as f64) * 100.0,
                        current_stage: CrawlingStage::ProductDetails,
                        current_step: format!("ì œí’ˆ {}/{} ì‹œì‘ (ë°°ì¹˜ {})", product_index, total_products, batch_id),
                        status: CrawlingStatus::Running,
                        message: format!("Product {}/{} started in batch {}", product_index, total_products, batch_id),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::ProductRetryAttempt { url: _, batch_id, attempt, max_attempts, reason } => {
                    CrawlingProgress {
                        current: *attempt,
                        total: *max_attempts,
                        percentage: (*attempt as f64 / *max_attempts as f64) * 100.0,
                        current_stage: CrawlingStage::ProductDetails,
                        current_step: format!("ì œí’ˆ ì¬ì‹œë„ {}/{} (ë°°ì¹˜ {}) - {}", attempt, max_attempts, batch_id, reason),
                        status: CrawlingStatus::Running,
                        message: format!("Product attempt {}/{} in batch {} - {}", attempt, max_attempts, batch_id, reason),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::ProductRetrySuccess { url: _, batch_id, final_attempt } => {
                    CrawlingProgress {
                        current: *final_attempt,
                        total: *final_attempt,
                        percentage: 100.0,
                        current_stage: CrawlingStage::ProductDetails,
                        current_step: format!("ì œí’ˆ ì¬ì‹œë„ ì„±ê³µ ({}ë²ˆì§¸ ì‹œë„, ë°°ì¹˜ {})", final_attempt, batch_id),
                        status: CrawlingStatus::Running,
                        message: format!("Product attempt succeeded on attempt {} (batch {})", final_attempt, batch_id),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 1,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::ProductRetryFailed { url: _, batch_id, total_attempts, final_error } => {
                    CrawlingProgress {
                        current: *total_attempts,
                        total: *total_attempts,
                        percentage: 100.0,
                        current_stage: CrawlingStage::ProductDetails,
                        current_step: format!("ì œí’ˆ ìµœì¢… ì‹¤íŒ¨ ({}ë²ˆ ì¬ì‹œë„ í›„, ë°°ì¹˜ {}) - {}", total_attempts, batch_id, final_error),
                        status: CrawlingStatus::Error,
                        message: format!("Product finally failed after {} attempts (batch {}) - {}", total_attempts, batch_id, final_error),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 1,
                        timestamp: chrono::Utc::now(),
                    }
                },
                
                DetailedCrawlingEvent::StageCompleted { stage, items_processed } => {
                    CrawlingProgress {
                        current: *items_processed as u32,
                        total: *items_processed as u32,
                        percentage: 100.0,
                        current_stage: match stage.as_str() {
                            "SiteStatus" => CrawlingStage::StatusCheck,
                            "DatabaseAnalysis" => CrawlingStage::DatabaseAnalysis,
                            "ProductList" => CrawlingStage::ProductList,
                            "ProductDetails" => CrawlingStage::ProductDetails,
                            "DatabaseSave" => CrawlingStage::DatabaseSave,
                            _ => CrawlingStage::TotalPages,
                        },
                        current_step: format!("{} ìŠ¤í…Œì´ì§€ ì™„ë£Œ ({}ê°œ í•­ëª© ì²˜ë¦¬)", stage, items_processed),
                        status: CrawlingStatus::Completed,
                        message: format!("Stage {} completed: {} items processed", stage, items_processed),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: *items_processed as u32,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                // ğŸ”¥ ìƒˆë¡œìš´ í˜ì´ì§€ ìˆ˜ì§‘ ì´ë²¤íŠ¸ë“¤
                DetailedCrawlingEvent::PageCollectionStarted { page, batch_id, url: _, estimated_products } => {
                    CrawlingProgress {
                        current: 0,
                        total: estimated_products.unwrap_or(0),
                        percentage: 0.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("í˜ì´ì§€ {} ìˆ˜ì§‘ ì‹œì‘ (ë°°ì¹˜ {}, ì˜ˆìƒ ì œí’ˆ: {}ê°œ)", page, batch_id, estimated_products.unwrap_or(0)),
                        status: CrawlingStatus::Running,
                        message: format!("Page {} collection started (batch {}, estimated: {})", page, batch_id, estimated_products.unwrap_or(0)),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::PageCollectionCompleted { page, batch_id, url: _, products_found, duration_ms } => {
                    CrawlingProgress {
                        current: *page,
                        total: *page,
                        percentage: 100.0,
                        current_stage: CrawlingStage::ProductList,
                        current_step: format!("í˜ì´ì§€ {} ìˆ˜ì§‘ ì™„ë£Œ: {}ê°œ ì œí’ˆ ë°œê²¬ ({}ms, ë°°ì¹˜ {})", page, products_found, duration_ms, batch_id),
                        status: CrawlingStatus::Running,
                        message: format!("Page {} collection completed: {} products found in {}ms (batch {})", page, products_found, duration_ms, batch_id),
                        remaining_time: None,
                        elapsed_time: *duration_ms,
                        new_items: *products_found,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                // ğŸ”¥ ìƒˆë¡œìš´ ì œí’ˆ ìƒì„¸ ìˆ˜ì§‘ ì´ë²¤íŠ¸ë“¤
                DetailedCrawlingEvent::ProductDetailCollectionStarted { url: _, product_index, total_products, batch_id } => {
                    CrawlingProgress {
                        current: *product_index,
                        total: *total_products,
                        percentage: (*product_index as f64 / *total_products as f64) * 100.0,
                        current_stage: CrawlingStage::ProductDetails,
                        current_step: format!("ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹œì‘: {}/{} (ë°°ì¹˜ {})", product_index, total_products, batch_id),
                        status: CrawlingStatus::Running,
                        message: format!("Product detail collection started: {}/{} (batch {})", product_index, total_products, batch_id),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::ProductDetailProcessingStarted { url: _, product_index, parsing_stage } => {
                    CrawlingProgress {
                        current: *product_index,
                        total: *product_index + 1,
                        percentage: 0.0,
                        current_stage: CrawlingStage::ProductDetails,
                        current_step: format!("ì œí’ˆ {} ìƒì„¸ì •ë³´ ì²˜ë¦¬ ì‹œì‘: {}", product_index, parsing_stage),
                        status: CrawlingStatus::Running,
                        message: format!("Product {} detail processing started: {}", product_index, parsing_stage),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::ProductDetailCollectionCompleted { url: _, product_index, success, duration_ms, data_extracted } => {
                    CrawlingProgress {
                        current: *product_index,
                        total: *product_index,
                        percentage: 100.0,
                        current_stage: CrawlingStage::ProductDetails,
                        current_step: format!("ì œí’ˆ {} ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {} (ë°ì´í„° ì¶”ì¶œ: {}) in {}ms", product_index, if *success { "ì„±ê³µ" } else { "ì‹¤íŒ¨" }, data_extracted, duration_ms),
                        status: if *success { CrawlingStatus::Running } else { CrawlingStatus::Error },
                        message: format!("Product {} detail collection completed: {} (data extracted: {}) in {}ms", product_index, if *success { "success" } else { "failure" }, data_extracted, duration_ms),
                        remaining_time: None,
                        elapsed_time: *duration_ms,
                        new_items: if *success { 1 } else { 0 },
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(1),
                        errors: if *success { 0 } else { 1 },
                        timestamp: chrono::Utc::now(),
                    }
                },
                // ğŸ”¥ ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ ë°°ì¹˜ ì €ì¥ ì´ë²¤íŠ¸ë“¤
                DetailedCrawlingEvent::DatabaseBatchSaveStarted { batch_id, products_count, batch_size } => {
                    CrawlingProgress {
                        current: 0,
                        total: *products_count,
                        percentage: 0.0,
                        current_stage: CrawlingStage::DatabaseSave,
                        current_step: format!("ë°ì´í„°ë² ì´ìŠ¤ ë°°ì¹˜ {} ì €ì¥ ì‹œì‘ ({}ê°œ ì œí’ˆ, ë°°ì¹˜ í¬ê¸°: {})", batch_id, products_count, batch_size),
                        status: CrawlingStatus::Running,
                        message: format!("Database batch {} save started: {} products (batch size: {})", batch_id, products_count, batch_size),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::DatabaseBatchSaveCompleted { batch_id, products_saved, new_items, updated_items, errors, duration_ms } => {
                    CrawlingProgress {
                        current: *products_saved,
                        total: *products_saved,
                        percentage: 100.0,
                        current_stage: CrawlingStage::DatabaseSave,
                        current_step: format!("ë°ì´í„°ë² ì´ìŠ¤ ë°°ì¹˜ {} ì €ì¥ ì™„ë£Œ: {}ê°œ ì €ì¥ (ì‹ ê·œ: {}, ì—…ë°ì´íŠ¸: {}, ì˜¤ë¥˜: {}) in {}ms", batch_id, products_saved, new_items, updated_items, errors, duration_ms),
                        status: CrawlingStatus::Running,
                        message: format!("Database batch {} save completed: {} saved (new: {}, updated: {}, errors: {}) in {}ms", batch_id, products_saved, new_items, updated_items, errors, duration_ms),
                        remaining_time: None,
                        elapsed_time: *duration_ms,
                        new_items: *new_items,
                        updated_items: *updated_items,
                        current_batch: Some(*batch_id),
                        total_batches: Some(1),
                        errors: *errors,
                        timestamp: chrono::Utc::now(),
                    }
                },
            };

            emitter.emit_progress(progress).await?;
            
            // ë˜í•œ DetailedCrawlingEventë¥¼ ì§ì ‘ ì „ì†¡ (ê³„ì¸µì  ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ìš©)
            emitter.emit_detailed_crawling_event(event.clone()).await?;
        }
        
        debug!("Emitted detailed event: {:?}", event);
        Ok(())
    }

    /// Update cancellation token for the current session
    pub fn update_cancellation_token(&mut self, cancellation_token: Option<CancellationToken>) {
        self.config.cancellation_token = cancellation_token;
        info!("ğŸ”„ Updated cancellation token in ServiceBasedBatchCrawlingEngine: {}", 
              self.config.cancellation_token.is_some());
    }

    /// Stop the crawling engine by cancelling the cancellation token
    pub async fn stop(&self) -> Result<(), String> {
        if let Some(cancellation_token) = &self.config.cancellation_token {
            tracing::info!("ğŸ›‘ Stopping ServiceBasedBatchCrawlingEngine by cancelling token");
            cancellation_token.cancel();
            Ok(())
        } else {
            let error_msg = "Cannot stop: No cancellation token available";
            tracing::warn!("âš ï¸ {}", error_msg);
            Err(error_msg.to_string())
        }
    }

    /// AtomicTaskEvent ë°œì†¡ (Live Production Line UIìš©)
    // REMOVE_CANDIDATE(Phase3): Legacy granular event emission
    fn emit_atomic_task_event(&self, task_id: &str, stage_name: &str, status: TaskStatus, progress: f64, message: Option<String>) {
        if let Some(broadcaster) = &self.broadcaster {
            let batch_id = 1; // í˜„ì¬ëŠ” ë‹¨ì¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            let event = AtomicTaskEvent {
                task_id: task_id.to_string(),
                batch_id,
                stage_name: stage_name.to_string(),
                status,
                progress,
                message,
                timestamp: Utc::now(),
            };
            
            if let Err(e) = broadcaster.emit_atomic_task_event(event) {
                warn!("Failed to emit atomic task event: {}", e);
            }
        }
    }
}

impl std::fmt::Debug for ServiceBasedBatchCrawlingEngine {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        f.debug_struct("ServiceBasedBatchCrawlingEngine")
            .field("status_checker", &"Arc<dyn StatusChecker>")
            .field("database_analyzer", &"Arc<dyn DatabaseAnalyzer>") 
            .field("product_list_collector", &"Arc<dyn ProductListCollector>")
            .field("product_detail_collector", &"Arc<dyn ProductDetailCollector>")
            .field("data_processor", &"Arc<dyn DataProcessor>")
            .field("storage_service", &"Arc<dyn StorageService>")
            .field("config", &self.config)
            .finish()
    }
}
