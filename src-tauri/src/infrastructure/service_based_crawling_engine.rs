//! ê°œì„ ëœ ë°°ì¹˜ í¬ë¡¤ë§ ì—”ì§„ - ì„œë¹„ìŠ¤ ë ˆì´ì–´ ë¶„ë¦¬ ë²„ì „
//! 
//! ì´ ëª¨ë“ˆì€ guide/crawling ë¬¸ì„œì˜ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ê° ë‹¨ê³„ë¥¼ 
//! ë…ë¦½ì ì¸ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì—¬ êµ¬í˜„í•œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í¬ë¡¤ë§ ì—”ì§„ì…ë‹ˆë‹¤.

use std::sync::Arc;
use std::time::{Duration, Instant};
use anyhow::{Result, anyhow};
use tracing::{info, warn, debug};
use tokio_util::sync::CancellationToken;
use chrono::Utc;

use crate::domain::services::{
    StatusChecker, DatabaseAnalyzer, ProductListCollector, ProductDetailCollector,
    SiteStatus, DatabaseAnalysis
};
use crate::domain::events::{CrawlingProgress, CrawlingStage, CrawlingStatus};
use crate::domain::product::{Product, ProductDetail};
use crate::domain::product_url::ProductUrl;
use crate::application::EventEmitter;
use crate::infrastructure::{HttpClient, MatterDataExtractor, IntegratedProductRepository, RetryManager};
use crate::infrastructure::crawling_service_impls::*;
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::system_broadcaster::SystemStateBroadcaster;
use crate::events::{AtomicTaskEvent, TaskStatus};

/// ë°°ì¹˜ í¬ë¡¤ë§ ì„¤ì •
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct BatchCrawlingConfig {
    pub start_page: u32,
    pub end_page: u32,
    pub concurrency: u32,
    pub list_page_concurrency: u32,
    pub product_detail_concurrency: u32,
    pub delay_ms: u64,
    pub batch_size: u32,
    pub retry_max: u32,
    pub timeout_ms: u64,
    #[serde(skip)]
    pub cancellation_token: Option<CancellationToken>,
}

impl BatchCrawlingConfig {
    /// Create BatchCrawlingConfig from ValidatedCrawlingConfig for Modern Rust 2024 compliance
    #[must_use]
    pub fn from_validated(validated_config: &crate::application::validated_crawling_config::ValidatedCrawlingConfig) -> Self {
        Self {
            start_page: 1,
            end_page: 1, // Will be set by range calculator
            concurrency: validated_config.max_concurrent(),
            list_page_concurrency: validated_config.list_page_max_concurrent,
            product_detail_concurrency: validated_config.product_detail_max_concurrent,
            delay_ms: validated_config.request_delay_ms,
            batch_size: validated_config.batch_size(),
            retry_max: validated_config.max_retries(),
            timeout_ms: (validated_config.request_timeout_ms as u64),
            cancellation_token: None,
        }
    }
}

impl Default for BatchCrawlingConfig {
    fn default() -> Self {
        // Use ValidatedCrawlingConfig for all defaults instead of hardcoded values
        let validated_config = crate::application::validated_crawling_config::ValidatedCrawlingConfig::default();
        
        Self {
            start_page: 1,
            end_page: 1, // âœ… ê¸°ë³¸ê°’ì„ 1ë¡œ ì„¤ì • (ì‹¤ì œ ê³„ì‚°ëœ ë²”ìœ„ ì‚¬ìš©)
            concurrency: validated_config.max_concurrent(),
            list_page_concurrency: validated_config.list_page_max_concurrent,
            product_detail_concurrency: validated_config.product_detail_max_concurrent,
            delay_ms: validated_config.request_delay_ms,
            batch_size: validated_config.batch_size(),
            retry_max: validated_config.max_retries(),
            timeout_ms: (validated_config.request_timeout_ms as u64),
            cancellation_token: None,
        }
    }
}

impl BatchCrawlingConfig {
    /// Phase 4: ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚° ê²°ê³¼ë¥¼ configì— ì ìš©
    pub fn update_range_from_calculation(&mut self, optimal_range: Option<(u32, u32)>) {
        if let Some((start_page, end_page)) = optimal_range {
            info!("ğŸ”„ Updating crawling range from {}..{} to {}..{}", 
                  self.start_page, self.end_page, start_page, end_page);
            self.start_page = start_page;
            self.end_page = end_page;
        } else {
            info!("ğŸ”„ No optimal range available, keeping current range {}..{}", 
                  self.start_page, self.end_page);
        }
    }
    
    /// í˜„ì¬ ì„¤ì •ëœ ë²”ìœ„ ì •ë³´ ë°˜í™˜
    pub fn get_page_range(&self) -> (u32, u32) {
        (self.start_page, self.end_page)
    }
}

/// ì„¸ë¶„í™”ëœ í¬ë¡¤ë§ ì´ë²¤íŠ¸ íƒ€ì…
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub enum DetailedCrawlingEvent {
    SessionStarted { 
        session_id: String, 
        config: BatchCrawlingConfig 
    },
    StageStarted { 
        stage: String, 
        message: String 
    },
    StageCompleted { 
        stage: String, 
        items_processed: usize 
    },
    PageCompleted { 
        page: u32, 
        products_found: u32 
    },
    ProductProcessed { 
        url: String, 
        success: bool 
    },
    BatchCompleted { 
        batch: u32, 
        total: u32 
    },
    ErrorOccurred { 
        stage: String, 
        error: String, 
        recoverable: bool 
    },
    SessionCompleted {
        session_id: String,
        duration: Duration,
        total_products: u32,
        success_rate: f64,
    },
}

/// ì„œë¹„ìŠ¤ ê¸°ë°˜ ë°°ì¹˜ í¬ë¡¤ë§ ì—”ì§„
pub struct ServiceBasedBatchCrawlingEngine {
    // ì„œë¹„ìŠ¤ ë ˆì´ì–´ë“¤
    status_checker: Arc<dyn StatusChecker>,
    database_analyzer: Arc<dyn DatabaseAnalyzer>,
    product_list_collector: Arc<dyn ProductListCollector>,
    product_detail_collector: Arc<dyn ProductDetailCollector>,
    
    // ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚°ê¸° - Phase 3 Integration
    range_calculator: Arc<CrawlingRangeCalculator>,
    
    // ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ë“¤
    product_repo: Arc<IntegratedProductRepository>,
    product_detail_repo: Arc<IntegratedProductRepository>,
    event_emitter: Arc<Option<EventEmitter>>,
    
    // Live Production Line ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤í„°
    broadcaster: Option<SystemStateBroadcaster>,
    
    // ì¬ì‹œë„ ê´€ë¦¬ì - INTEGRATED_PHASE2_PLAN Week 1 Day 3-4
    retry_manager: Arc<RetryManager>,
    
    // ì„¤ì • ë° ì„¸ì…˜ ì •ë³´
    config: BatchCrawlingConfig,
    session_id: String,
}

impl ServiceBasedBatchCrawlingEngine {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        product_repo: Arc<IntegratedProductRepository>,
        event_emitter: Arc<Option<EventEmitter>>,
        config: BatchCrawlingConfig,
        session_id: String,
        app_config: AppConfig,
    ) -> Self {
        // ì„œë¹„ìŠ¤ë³„ ì„¤ì • ìƒì„±
        let list_collector_config = CollectorConfig {
            max_concurrent: config.list_page_concurrency,
            concurrency: config.list_page_concurrency,
            delay_between_requests: Duration::from_millis(config.delay_ms),
            delay_ms: config.delay_ms,
            batch_size: config.batch_size,
            retry_attempts: config.retry_max,
            retry_max: config.retry_max,
        };
        
        let detail_collector_config = CollectorConfig {
            max_concurrent: config.product_detail_concurrency,
            concurrency: config.product_detail_concurrency,
            delay_between_requests: Duration::from_millis(config.delay_ms),
            delay_ms: config.delay_ms,
            batch_size: config.batch_size,
            retry_attempts: config.retry_max,
            retry_max: config.retry_max,
        };

        // ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        let status_checker = Arc::new(StatusCheckerImpl::new(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
        )) as Arc<dyn StatusChecker>;

        let database_analyzer = Arc::new(DatabaseAnalyzerImpl::new(
            Arc::clone(&product_repo),
        )) as Arc<dyn DatabaseAnalyzer>;

        // status_checkerë¥¼ ProductListCollectorImplì— ì „ë‹¬í•˜ê¸° ìœ„í•´ concrete typeìœ¼ë¡œ ë‹¤ì‹œ ìƒì„±
        let status_checker_impl = Arc::new(StatusCheckerImpl::new(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
        ));

        let product_list_collector = Arc::new(ProductListCollectorImpl::new(
            Arc::new(tokio::sync::Mutex::new(http_client.clone())),
            Arc::new(data_extractor.clone()),
            list_collector_config,
            status_checker_impl,
        )) as Arc<dyn ProductListCollector>;

        let product_detail_collector = Arc::new(ProductDetailCollectorImpl::new(
            Arc::new(tokio::sync::Mutex::new(http_client)),
            Arc::new(data_extractor),
            detail_collector_config,
        )) as Arc<dyn ProductDetailCollector>;

        // ì§€ëŠ¥í˜• ë²”ìœ„ ê³„ì‚°ê¸° ì´ˆê¸°í™” - Phase 3 Integration
        let range_calculator = Arc::new(CrawlingRangeCalculator::new(
            Arc::clone(&product_repo),
            app_config.clone(),
        ));

        Self {
            status_checker,
            database_analyzer,
            product_list_collector,
            product_detail_collector,
            range_calculator,
            product_repo: product_repo.clone(),
            product_detail_repo: product_repo,
            event_emitter,
            broadcaster: None, // ë‚˜ì¤‘ì— ì„¤ì •ë¨
            retry_manager: Arc::new(RetryManager::new(config.retry_max)),
            config,
            session_id,
        }
    }

    /// SystemStateBroadcaster ì„¤ì • (í¬ë¡¤ë§ ì‹œì‘ ì „ì— í˜¸ì¶œ)
    pub fn set_broadcaster(&mut self, broadcaster: SystemStateBroadcaster) {
        self.broadcaster = Some(broadcaster);
    }

    /// 4ë‹¨ê³„ ì„œë¹„ìŠ¤ ê¸°ë°˜ í¬ë¡¤ë§ ì‹¤í–‰
    pub async fn execute(&self) -> Result<()> {
        let start_time = Instant::now();
        info!("Starting service-based 4-stage batch crawling for session: {}", self.session_id);

        // ì„¸ì…˜ ì‹œì‘ ì´ë²¤íŠ¸
        self.emit_detailed_event(DetailedCrawlingEvent::SessionStarted {
            session_id: self.session_id.clone(),
            config: self.config.clone(),
        }).await?;

        // ì‹œì‘ ì „ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled before starting");
                return Err(anyhow!("Crawling session cancelled before starting"));
            }
        }

        // Stage 0: ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸
        let site_status = self.stage0_check_site_status().await?;
        
        // Stage 0 ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled after Stage 0");
                return Err(anyhow!("Crawling session cancelled after site status check"));
            }
        }
        
        // Stage 0.5: ì§€ëŠ¥í˜• ë²”ìœ„ ì¬ê³„ì‚° ë° ì‹¤ì œ ì ìš© - Phase 4 Implementation
        info!("ğŸ§  Stage 0.5: Performing intelligent range recalculation");
        info!("ğŸ“Š Site analysis: total_pages={}, products_on_last_page={}", 
              site_status.total_pages, site_status.products_on_last_page);
        
        let optimal_range = self.range_calculator.calculate_next_crawling_range(
            site_status.total_pages,
            site_status.products_on_last_page, // âœ… ì‹¤ì œ ê°’ ì‚¬ìš© (ì´ì „: í•˜ë“œì½”ë”© 10)
        ).await?;
        
        // ê³„ì‚°ëœ ë²”ìœ„ë¥¼ ì‹¤ì œë¡œ ì ìš©í•˜ì—¬ ìµœì¢… ë²”ìœ„ ê²°ì •
        let (actual_start_page, actual_end_page) = if let Some((optimal_start, optimal_end)) = optimal_range {
            if optimal_start != self.config.start_page || optimal_end != self.config.end_page {
                info!("ğŸ’¡ Applying intelligent range recommendation: pages {} to {} (original: {} to {})", 
                      optimal_start, optimal_end, self.config.start_page, self.config.end_page);
                
                // ë²”ìœ„ ì ìš© ì´ë²¤íŠ¸ ë°œì†¡
                self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
                    stage: "Range Optimization Applied".to_string(),
                    message: format!("Applied optimal range: {} to {} (was: {} to {})", 
                                   optimal_start, optimal_end, self.config.start_page, self.config.end_page),
                }).await?;
                
                (optimal_start, optimal_end)
            } else {
                info!("âœ… Current range already optimal: {} to {}", self.config.start_page, self.config.end_page);
                (self.config.start_page, self.config.end_page)
            }
        } else {
            info!("âœ… All products appear to be crawled - using current range for verification: {} to {}", 
                  self.config.start_page, self.config.end_page);
            (self.config.start_page, self.config.end_page)
        };
        
        info!("ğŸ¯ Final crawling range determined: {} to {}", actual_start_page, actual_end_page);
        
        // Stage 1: ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let _db_analysis = self.stage1_analyze_database().await?;
        
        // Stage 1 ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled after Stage 1");
                return Err(anyhow!("Crawling session cancelled after database analysis"));
            }
        }
        
        // Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ - ê³„ì‚°ëœ ìµœì  ë²”ìœ„ ì‚¬ìš©
        let product_urls = self.stage2_collect_product_list_optimized(actual_start_page, actual_end_page).await?;
        
        // Stage 2 ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled after Stage 2");
                return Err(anyhow!("Crawling session cancelled after product list collection"));
            }
        }
        
        // Stage 3: ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘
        let products = self.stage3_collect_product_details(&product_urls).await?;
        let total_products = products.len() as u32;
        
        // Stage 3 ì™„ë£Œ í›„ ì·¨ì†Œ í™•ì¸
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Crawling session cancelled after Stage 3");
                return Err(anyhow!("Crawling session cancelled after product details collection"));
            }
        }
        
        // Stage 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        let (processed_count, _new_items, _updated_items, errors) = self.stage4_save_to_database(products).await?;
        
        // ì„±ê³µë¥  ê³„ì‚°
        let success_rate = if processed_count > 0 {
            (processed_count - errors) as f64 / processed_count as f64
        } else {
            0.0
        };

        let duration = start_time.elapsed();
        info!("Service-based batch crawling completed in {:?}: {} products collected, {:.2}% success rate", 
            duration, total_products, success_rate * 100.0);
        
        // ì„¸ì…˜ ì™„ë£Œ ì´ë²¤íŠ¸
        self.emit_detailed_event(DetailedCrawlingEvent::SessionCompleted {
            session_id: self.session_id.clone(),
            duration,
            total_products,
            success_rate,
        }).await?;
        
        Ok(())
    }

    /// Stage 0: ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ (ìƒˆë¡œìš´ ë‹¨ê³„)
    async fn stage0_check_site_status(&self) -> Result<SiteStatus> {
        info!("Stage 0: Checking site status");
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "SiteStatus".to_string(),
            message: "ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ì¤‘...".to_string(),
        }).await?;

        let site_status = self.status_checker.check_site_status().await?;
        
        if !site_status.is_accessible || site_status.total_pages == 0 {
            let error_msg = format!("Site is not accessible or has no pages (pages: {})", site_status.total_pages);
            self.emit_detailed_event(DetailedCrawlingEvent::ErrorOccurred {
                stage: "SiteStatus".to_string(),
                error: error_msg.clone(),
                recoverable: true,
            }).await?;
            return Err(anyhow!(error_msg));
        }

        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "SiteStatus".to_string(),
            items_processed: 1,
        }).await?;

        info!("Stage 0 completed: Site is healthy (score: {})", site_status.health_score);
        Ok(site_status)
    }

    /// Stage 1: ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ (ìƒˆë¡œìš´ ë‹¨ê³„)
    async fn stage1_analyze_database(&self) -> Result<DatabaseAnalysis> {
        info!("Stage 1: Analyzing database state");
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "DatabaseAnalysis".to_string(),
            message: "ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘...".to_string(),
        }).await?;

        let analysis = self.database_analyzer.analyze_current_state().await?;
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "DatabaseAnalysis".to_string(),
            items_processed: analysis.total_products as usize,
        }).await?;

        info!("Stage 1 completed: {} total products, quality score: {}", 
              analysis.total_products, analysis.data_quality_score);
        Ok(analysis)
    }

    /// Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ (ì„œë¹„ìŠ¤ ê¸°ë°˜)
    async fn stage2_collect_product_list(&self, total_pages: u32) -> Result<Vec<ProductUrl>> {
        info!("Stage 2: Collecting product list using ProductListCollector service");
        
        // ì·¨ì†Œ í™•ì¸ - ë‹¨ê³„ ì‹œì‘ ì „
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Stage 2 (ProductList) cancelled before starting");
                return Err(anyhow!("Product list collection cancelled"));
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "ProductList".to_string(),
            message: format!("{}í˜ì´ì§€ì—ì„œ ì œí’ˆ ëª©ë¡ì„ ìˆ˜ì§‘í•˜ëŠ” ì¤‘...", total_pages),
        }).await?;

        let effective_end = total_pages.min(self.config.end_page);
        
        // ì·¨ì†Œ ê°€ëŠ¥í•œ ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì‹¤í–‰ - í•­ìƒ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
        let product_urls = if let Some(cancellation_token) = &self.config.cancellation_token {
            info!("ğŸ›‘ Using cancellation token for product list collection");
            
            // ì·¨ì†Œ í† í°ê³¼ í•¨ê»˜ ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ - ê°œì„ ëœ ProductListCollector ì‚¬ìš©
            self.product_list_collector.collect_page_range_with_cancellation(
                self.config.start_page, 
                effective_end, 
                cancellation_token.clone()
            ).await?
        } else {
            warn!("âš ï¸  No cancellation token - using parallel collection without cancellation");
            // ì·¨ì†Œ í† í°ì´ ì—†ì–´ë„ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©
            self.product_list_collector.collect_page_range(self.config.start_page, effective_end).await?
        };
        
        // ì·¨ì†Œ í™•ì¸ - ë‹¨ê³„ ì™„ë£Œ í›„
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Stage 2 (ProductList) cancelled after collection");
                return Err(anyhow!("Product list collection cancelled after completion"));
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "ProductList".to_string(),
            items_processed: product_urls.len(),
        }).await?;

        info!("Stage 2 completed: {} product URLs collected", product_urls.len());
        Ok(product_urls)
    }

    /// Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ (ìµœì í™”ëœ ë²”ìœ„ ì‚¬ìš©) - Phase 4 Implementation
    async fn stage2_collect_product_list_optimized(&self, start_page: u32, end_page: u32) -> Result<Vec<ProductUrl>> {
        info!("Stage 2: Collecting product list using optimized range {} to {}", start_page, end_page);
        
        // ì·¨ì†Œ í™•ì¸ - ë‹¨ê³„ ì‹œì‘ ì „
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Stage 2 (ProductList) cancelled before starting");
                return Err(anyhow!("Product list collection cancelled"));
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "ProductList (Optimized)".to_string(),
            message: format!("í˜ì´ì§€ {} ~ {}ì—ì„œ ì œí’ˆ ëª©ë¡ì„ ìˆ˜ì§‘í•˜ëŠ” ì¤‘...", start_page, end_page),
        }).await?;

        // í˜ì´ì§€ë³„ AtomicTaskEvent ë°œì†¡ì„ ìœ„í•œ í˜ì´ì§€ ë²”ìœ„ ìƒì„±
        let total_pages = if start_page >= end_page {
            start_page.saturating_sub(end_page).saturating_add(1)
        } else {
            end_page.saturating_sub(start_page).saturating_add(1)
        };

        // ê° í˜ì´ì§€ ì‘ì—… ì‹œì‘ ì´ë²¤íŠ¸ ë°œì†¡
        for page_num in if start_page >= end_page {
            (end_page..=start_page).rev().collect::<Vec<_>>()
        } else {
            (start_page..=end_page).collect::<Vec<_>>()
        } {
            self.emit_atomic_task_event(
                &format!("page-{}", page_num),
                "ListPageCollection",
                TaskStatus::Pending,
                0.0,
                Some(format!("Preparing to collect product list from page {}", page_num))
            );
        }

        // ìµœì í™”ëœ ë²”ìœ„ë¡œ ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ (cancellation ì§€ì›)
        let product_urls = if let Some(cancellation_token) = &self.config.cancellation_token {
            self.product_list_collector.collect_page_range_with_cancellation(
                start_page,
                end_page,
                cancellation_token.clone(),
            ).await.map_err(|e| anyhow!("Product list collection failed: {}", e))?
        } else {
            self.product_list_collector.collect_page_range(
                start_page,
                end_page,
            ).await.map_err(|e| anyhow!("Product list collection failed: {}", e))?
        };

        // ê° í˜ì´ì§€ ì‘ì—… ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
        for page_num in if start_page >= end_page {
            (end_page..=start_page).rev().collect::<Vec<_>>()
        } else {
            (start_page..=end_page).collect::<Vec<_>>()
        } {
            self.emit_atomic_task_event(
                &format!("page-{}", page_num),
                "ListPageCollection",
                TaskStatus::Success,
                1.0,
                Some(format!("Completed product list collection from page {}", page_num))
            );
        }

        info!("âœ… Stage 2 completed: {} product URLs collected from optimized range", product_urls.len());
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "ProductList (Optimized)".to_string(),
            items_processed: product_urls.len(),
        }).await?;

        Ok(product_urls)
    }

    /// Stage 3: ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ (ì„œë¹„ìŠ¤ ê¸°ë°˜ + ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜)
    async fn stage3_collect_product_details(&self, product_urls: &[ProductUrl]) -> Result<Vec<(Product, ProductDetail)>> {
        info!("Stage 3: Collecting product details using ProductDetailCollector service with retry mechanism");
        
        // ì·¨ì†Œ í™•ì¸ - ë‹¨ê³„ ì‹œì‘ ì „
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Stage 3 (ProductDetails) cancelled before starting");
                return Err(anyhow!("Product details collection cancelled"));
            }
        }
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "ProductDetails".to_string(),
            message: format!("{}ê°œ ì œí’ˆì˜ ìƒì„¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘... (ì¬ì‹œë„ ì§€ì›)", product_urls.len()),
        }).await?;

        // ê° ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‘ì—… ì‹œì‘ ì´ë²¤íŠ¸ ë°œì†¡
        for (index, url) in product_urls.iter().enumerate() {
            self.emit_atomic_task_event(
                &format!("product-detail-{}", index),
                "ProductDetailCollection",
                TaskStatus::Pending,
                0.0,
                Some(format!("Preparing to collect product detail from {}", url))
            );
        }

        // ì´ˆê¸° ì‹œë„ - cancellation token ì‚¬ìš©
        let mut successful_products = Vec::new();
        let mut failed_urls = Vec::new();

        // í•­ìƒ ì·¨ì†Œ í† í°ì„ ì‚¬ìš©í•˜ë„ë¡ ê°•ì œ - ì—†ìœ¼ë©´ ê¸°ë³¸ í† í° ìƒì„±
        let result = if let Some(cancellation_token) = &self.config.cancellation_token {
            info!("ğŸ›‘ USING PROVIDED CANCELLATION TOKEN for product detail collection");
            info!("ğŸ›‘ Cancellation token is_cancelled: {}", cancellation_token.is_cancelled());
            self.product_detail_collector.collect_details_with_cancellation(product_urls, cancellation_token.clone()).await
        } else {
            warn!("âš ï¸  NO CANCELLATION TOKEN - creating default token for consistent behavior");
            let default_token = CancellationToken::new();
            self.product_detail_collector.collect_details_with_cancellation(product_urls, default_token).await
        };

        match result {
            Ok(product_details) => {
                // ì·¨ì†Œ í™•ì¸ - ë°ì´í„° ë³€í™˜ ì „
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    if cancellation_token.is_cancelled() {
                        warn!("ğŸ›‘ Product details collection cancelled before processing results");
                        return Err(anyhow!("Product details collection cancelled"));
                    }
                }
                
                // ProductDetailì„ Productë¡œ ë³€í™˜í•˜ê³  ì›ë³¸ ProductDetailê³¼ í•¨ê»˜ ì €ì¥
                successful_products = product_details.into_iter()
                    .map(|detail| {
                        let product = crate::infrastructure::crawling_service_impls::product_detail_to_product(detail.clone());
                        (product, detail)
                    })
                    .collect();
                
                // ì„±ê³µí•œ ì œí’ˆë“¤ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œì†¡
                for (index, _) in successful_products.iter().enumerate() {
                    self.emit_atomic_task_event(
                        &format!("product-detail-{}", index),
                        "ProductDetailCollection",
                        TaskStatus::Success,
                        1.0,
                        Some(format!("Successfully collected product detail"))
                    );
                }
                
                info!("âœ… Initial collection successful: {} products", successful_products.len());
            }
            Err(e) => {
                // cancellation ì²´í¬
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    if cancellation_token.is_cancelled() {
                        warn!("ğŸ›‘ Collection cancelled by user");
                        return Ok(successful_products); // ì´ë¯¸ ìˆ˜ì§‘ëœ ì œí’ˆë“¤ ë°˜í™˜
                    }
                }
                
                warn!("âŒ Initial collection failed: {}", e);
                failed_urls = product_urls.to_vec();
                
                // ì‹¤íŒ¨í•œ ì œí’ˆë“¤ ì‹¤íŒ¨ ì´ë²¤íŠ¸ ë°œì†¡
                for (index, _url) in failed_urls.iter().enumerate() {
                    self.emit_atomic_task_event(
                        &format!("product-detail-{}", index),
                        "ProductDetailCollection",
                        TaskStatus::Error,
                        0.0,
                        Some(format!("Failed to collect product detail: {}", e))
                    );
                }
                
                // ì‹¤íŒ¨í•œ URLë“¤ì„ ì¬ì‹œë„ íì— ì¶”ê°€
                for (index, url) in failed_urls.iter().enumerate() {
                    let item_id = format!("product_detail_{}_{}", self.session_id, index);
                    let mut metadata = std::collections::HashMap::new();
                    metadata.insert("url".to_string(), url.to_string());
                    metadata.insert("stage".to_string(), "product_details".to_string());
                    
                    if let Err(retry_err) = self.retry_manager.add_failed_item(
                        item_id,
                        CrawlingStage::ProductDetails,
                        e.to_string(),
                        url.to_string(),
                        metadata,
                    ).await {
                        warn!("Failed to add item to retry queue: {}", retry_err);
                    }
                }
            }
        }

        // ì¬ì‹œë„ ì²˜ë¦¬ (cancellation token í™•ì¸ í›„)
        if let Some(cancellation_token) = &self.config.cancellation_token {
            if cancellation_token.is_cancelled() {
                warn!("ğŸ›‘ Skipping retries due to cancellation");
                self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
                    stage: "ProductDetails".to_string(),
                    items_processed: successful_products.len(),
                }).await?;
                return Ok(successful_products);
            }
        }

        let retry_products = self.process_retries_for_product_details().await?;
        successful_products.extend(retry_products);
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "ProductDetails".to_string(),
            items_processed: successful_products.len(),
        }).await?;

        info!("Stage 3 completed: {} products collected (including retries)", successful_products.len());
        Ok(successful_products)
    }
    
    /// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì¬ì‹œë„ ì²˜ë¦¬
    async fn process_retries_for_product_details(&self) -> Result<Vec<(Product, ProductDetail)>> {
        info!("ğŸ”„ Processing retries for product details collection");
        let mut retry_products = Vec::new();
        
        // ìµœëŒ€ 3ë²ˆì˜ ì¬ì‹œë„ ì‚¬ì´í´
        for cycle in 1..=3 {
            // ì¬ì‹œë„ ì‚¬ì´í´ ì‹œì‘ ì „ ì·¨ì†Œ í™•ì¸
            if let Some(cancellation_token) = &self.config.cancellation_token {
                if cancellation_token.is_cancelled() {
                    warn!("ğŸ›‘ Retry processing cancelled at cycle {}", cycle);
                    return Ok(retry_products);
                }
            }
            
            let ready_items = self.retry_manager.get_ready_items().await?;
            if ready_items.is_empty() {
                debug!("No items ready for retry in cycle {}", cycle);
                break;
            }
            
            info!("ğŸ”„ Retry cycle {}: Processing {} items", cycle, ready_items.len());
            
            for retry_item in ready_items {
                // ê° ì¬ì‹œë„ í•­ëª© ì²˜ë¦¬ ì „ ì·¨ì†Œ í™•ì¸
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    if cancellation_token.is_cancelled() {
                        warn!("ğŸ›‘ Retry processing cancelled during item processing");
                        return Ok(retry_products);
                    }
                }
                
                if retry_item.stage == CrawlingStage::ProductDetails {
                    let url = retry_item.original_url;
                    let item_id = retry_item.item_id.clone();
                    
                    info!("ğŸ”„ Retrying product detail collection for: {}", url);
                    
                    // Convert String URL to ProductUrl for the new API
                    let product_url = ProductUrl::new(url.clone(), -1, -1); // Use -1 for retry URLs
                    
                    match self.product_detail_collector.collect_details(&[product_url]).await {
                        Ok(mut product_details) => {
                            if let Some(detail) = product_details.pop() {
                                let product = crate::infrastructure::crawling_service_impls::product_detail_to_product(detail.clone());
                                info!("âœ… Retry successful for: {}", url);
                                retry_products.push((product, detail));
                                
                                // ì„±ê³µ ê¸°ë¡
                                if let Err(e) = self.retry_manager.mark_retry_success(&item_id).await {
                                    warn!("Failed to mark retry success: {}", e);
                                }
                                
                                self.emit_detailed_event(DetailedCrawlingEvent::ProductProcessed {
                                    url: url.clone(),
                                    success: true,
                                }).await?;
                            }
                        }
                        Err(e) => {
                            warn!("âŒ Retry failed for {}: {}", url, e);
                            
                            // ì¬ì‹œë„ íì— ë‹¤ì‹œ ì¶”ê°€ (ì¬ì‹œë„ í•œë„ ë‚´ì—ì„œ)
                            let mut metadata = std::collections::HashMap::new();
                            metadata.insert("url".to_string(), url.clone());
                            metadata.insert("retry_cycle".to_string(), cycle.to_string());
                            
                            if let Err(retry_err) = self.retry_manager.add_failed_item(
                                item_id,
                                CrawlingStage::ProductDetails,
                                e.to_string(),
                                url.clone(),
                                metadata,
                            ).await {
                                debug!("Item exceeded retry limit or not retryable: {}", retry_err);
                            }
                            
                            self.emit_detailed_event(DetailedCrawlingEvent::ProductProcessed {
                                url: url.clone(),
                                success: false,
                            }).await?;
                        }
                    }
                    
                    // ì¬ì‹œë„ ê°„ ì§€ì—° (ì·¨ì†Œ í™•ì¸ í¬í•¨)
                    let delay = Duration::from_millis(self.config.delay_ms);
                    if let Some(cancellation_token) = &self.config.cancellation_token {
                        tokio::select! {
                            _ = tokio::time::sleep(delay) => {},
                            _ = cancellation_token.cancelled() => {
                                warn!("ğŸ›‘ Retry processing cancelled during item delay");
                                return Ok(retry_products);
                            }
                        }
                    } else {
                        tokio::time::sleep(delay).await;
                    }
                }
            }
            
            // ì‚¬ì´í´ ê°„ ì§€ì—° (ì·¨ì†Œ í™•ì¸ í¬í•¨)
            if cycle < 3 {
                let cycle_delay = Duration::from_secs(5);
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    tokio::select! {
                        _ = tokio::time::sleep(cycle_delay) => {},
                        _ = cancellation_token.cancelled() => {
                            warn!("ğŸ›‘ Retry processing cancelled during cycle delay");
                            return Ok(retry_products);
                        }
                    }
                } else {
                    tokio::time::sleep(cycle_delay).await;
                }
            }
        }
        
        info!("ğŸ”„ Retry processing completed: {} additional products collected", retry_products.len());
        Ok(retry_products)
    }

    /// Stage 4: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    async fn stage4_save_to_database(&self, products: Vec<(Product, ProductDetail)>) -> Result<(usize, usize, usize, usize)> {
        info!("Stage 4: Saving {} products to database", products.len());
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "DatabaseSave".to_string(),
            message: format!("{}ê°œ ì œí’ˆì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì¤‘...", products.len()),
        }).await?;

        let mut new_items = 0;
        let mut updated_items = 0;
        let mut errors = 0;

        for (index, (product, product_detail)) in products.into_iter().enumerate() {
            // ì£¼ê¸°ì ìœ¼ë¡œ ì·¨ì†Œ í™•ì¸ (100ê°œë§ˆë‹¤)
            if index % 100 == 0 {
                if let Some(cancellation_token) = &self.config.cancellation_token {
                    if cancellation_token.is_cancelled() {
                        warn!("ğŸ›‘ Database save cancelled after saving {} products", index);
                        break;
                    }
                }
            }
            
            // Productì™€ ProductDetailì„ ëª¨ë‘ ì €ì¥
            let product_save_result = self.product_repo.create_or_update_product(&product).await;
            let product_detail_save_result = self.product_detail_repo.create_or_update_product_detail(&product_detail).await;
            
            match (product_save_result, product_detail_save_result) {
                (Ok(_), Ok(_)) => {
                    // ì œí’ˆì´ ìƒˆë¡œ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´
                    // ê¸°ì¡´ ì œí’ˆì„ ì¡°íšŒí•´ë³´ê² ìŠµë‹ˆë‹¤
                    match self.product_repo.get_product_by_url(&product.url).await? {
                        Some(_existing) => updated_items += 1,
                        None => new_items += 1,
                    }
                    
                    self.emit_detailed_event(DetailedCrawlingEvent::ProductProcessed {
                        url: product.url.clone(),
                        success: true,
                    }).await?;
                },
                (Err(e), _) | (_, Err(e)) => {
                    errors += 1;
                    warn!("Failed to save product {:?}: {}", product.model, e);
                    
                    self.emit_detailed_event(DetailedCrawlingEvent::ProductProcessed {
                        url: product.url.clone(),
                        success: false,
                    }).await?;
                }
            }
        }

        let total_processed = new_items + updated_items + errors;

        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "DatabaseSave".to_string(),
            items_processed: total_processed,
        }).await?;

        info!("Stage 4 completed: {} new, {} updated, {} errors", new_items, updated_items, errors);
        Ok((total_processed, new_items, updated_items, errors))
    }

    /// ì„¸ë¶„í™”ëœ ì´ë²¤íŠ¸ ë°©ì¶œ
    async fn emit_detailed_event(&self, event: DetailedCrawlingEvent) -> Result<()> {
        if let Some(emitter) = self.event_emitter.as_ref() {
            // DetailedCrawlingEventë¥¼ ê¸°ì¡´ ì´ë²¤íŠ¸ ì‹œìŠ¤í…œê³¼ ì—°ë™
            let progress = match &event {
                DetailedCrawlingEvent::StageStarted { stage, message } => {
                    CrawlingProgress {
                        current: 0,
                        total: self.config.end_page - self.config.start_page + 1,
                        percentage: 0.0,
                        current_stage: match stage.as_str() {
                            "SiteStatus" => CrawlingStage::StatusCheck,
                            "DatabaseAnalysis" => CrawlingStage::DatabaseAnalysis,
                            "ProductList" => CrawlingStage::ProductList,
                            "ProductDetails" => CrawlingStage::ProductDetails,
                            "DatabaseSave" => CrawlingStage::DatabaseSave,
                            _ => CrawlingStage::TotalPages,
                        },
                        current_step: message.clone(),
                        status: CrawlingStatus::Running,
                        message: format!("Stage started: {}", stage),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(self.config.end_page - self.config.start_page + 1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                _ => return Ok(()), // ë‹¤ë¥¸ ì´ë²¤íŠ¸ë“¤ì€ ê¸°ë³¸ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            };

            emitter.emit_progress(progress).await?;
        }
        
        debug!("Emitted detailed event: {:?}", event);
        Ok(())
    }

    /// Update cancellation token for the current session
    pub fn update_cancellation_token(&mut self, cancellation_token: Option<CancellationToken>) {
        self.config.cancellation_token = cancellation_token;
        info!("ğŸ”„ Updated cancellation token in ServiceBasedBatchCrawlingEngine: {}", 
              self.config.cancellation_token.is_some());
    }

    /// Stop the crawling engine by cancelling the cancellation token
    pub async fn stop(&self) -> Result<(), String> {
        if let Some(cancellation_token) = &self.config.cancellation_token {
            tracing::info!("ğŸ›‘ Stopping ServiceBasedBatchCrawlingEngine by cancelling token");
            cancellation_token.cancel();
            Ok(())
        } else {
            let error_msg = "Cannot stop: No cancellation token available";
            tracing::warn!("âš ï¸ {}", error_msg);
            Err(error_msg.to_string())
        }
    }

    /// AtomicTaskEvent ë°œì†¡ (Live Production Line UIìš©)
    fn emit_atomic_task_event(&self, task_id: &str, stage_name: &str, status: TaskStatus, progress: f64, message: Option<String>) {
        if let Some(broadcaster) = &self.broadcaster {
            let batch_id = 1; // í˜„ì¬ëŠ” ë‹¨ì¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬
            let event = AtomicTaskEvent {
                task_id: task_id.to_string(),
                batch_id,
                stage_name: stage_name.to_string(),
                status,
                progress,
                message,
                timestamp: Utc::now(),
            };
            
            if let Err(e) = broadcaster.emit_atomic_task_event(event) {
                warn!("Failed to emit atomic task event: {}", e);
            }
        }
    }
}
