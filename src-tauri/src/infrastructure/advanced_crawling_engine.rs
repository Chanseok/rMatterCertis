//! ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í¬í•¨í•œ í¬ë¡¤ë§ ì—”ì§„
//! 
//! ì´ ëª¨ë“ˆì€ Phase 2ì˜ ëª©í‘œì¸ ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬ ê¸°ëŠ¥ì„ í¬í•¨í•œ
//! ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í¬ë¡¤ë§ ì—”ì§„ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

use std::sync::Arc;
use std::time::{Instant, Duration};
use anyhow::{Result, anyhow};
use tracing::{info, warn, debug};

use crate::domain::services::{
    StatusChecker, DatabaseAnalyzer, ProductListCollector, ProductDetailCollector,
    DeduplicationService, ValidationService, ConflictResolver,
    BatchProgressTracker, BatchRecoveryService, ErrorClassifier
};
use crate::domain::events::{CrawlingProgress, CrawlingStage, CrawlingStatus};
use crate::domain::product::Product;
use crate::domain::product_url::ProductUrl;
use crate::application::EventEmitter;
use crate::infrastructure::{
    HttpClient, MatterDataExtractor, IntegratedProductRepository,
    StatusCheckerImpl, ProductListCollectorImpl,
    CollectorConfig,
    DeduplicationServiceImpl, ValidationServiceImpl, ConflictResolverImpl,
    config::AppConfig
};
use crate::infrastructure::crawling_service_impls::ProductDetailCollectorImpl;
use crate::infrastructure::data_processing_service_impls::{
    BatchProgressTrackerImpl, BatchRecoveryServiceImpl, RetryManagerImpl, ErrorClassifierImpl
};
use crate::infrastructure::service_based_crawling_engine::{BatchCrawlingConfig, DetailedCrawlingEvent};
use crate::domain::services::data_processing_services::ResolutionStrategy;

/// Phase 2 ê³ ê¸‰ í¬ë¡¤ë§ ì—”ì§„ - ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í¬í•¨
#[allow(dead_code)] // Phase 2.2ì—ì„œ ëª¨ë“  í•„ë“œê°€ ì‚¬ìš©ë  ì˜ˆì •
pub struct AdvancedBatchCrawlingEngine {
    // ê¸°ì¡´ ì„œë¹„ìŠ¤ ë ˆì´ì–´ë“¤
    status_checker: Arc<dyn StatusChecker>,
    database_analyzer: Arc<dyn DatabaseAnalyzer>,
    product_list_collector: Arc<dyn ProductListCollector>,
    product_detail_collector: Arc<dyn ProductDetailCollector>,
    
    // ìƒˆë¡œìš´ ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤ë“¤
    deduplication_service: Arc<dyn DeduplicationService>,
    validation_service: Arc<dyn ValidationService>,
    conflict_resolver: Arc<dyn ConflictResolver>,
    
    // ê³ ê¸‰ ê´€ë¦¬ ì„œë¹„ìŠ¤ë“¤
    progress_tracker: Arc<dyn BatchProgressTracker>,
    recovery_service: Arc<dyn BatchRecoveryService>,
    retry_manager: Arc<RetryManagerImpl>, // êµ¬ì²´ì ì¸ íƒ€ì… ì‚¬ìš© (dyn-compatibility ë¬¸ì œ í•´ê²°)
    error_classifier: Arc<dyn ErrorClassifier>,
    
    // ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ë“¤
    product_repo: Arc<IntegratedProductRepository>,
    event_emitter: Arc<Option<EventEmitter>>,
    
    // ì„¤ì • ë° ì„¸ì…˜ ì •ë³´
    config: BatchCrawlingConfig,
    session_id: String,
}

impl AdvancedBatchCrawlingEngine {
    pub fn new(
        http_client: HttpClient,
        data_extractor: MatterDataExtractor,
        product_repo: Arc<IntegratedProductRepository>,
        event_emitter: Arc<Option<EventEmitter>>,
        config: BatchCrawlingConfig,
        session_id: String,
    ) -> Self {
        // ì„œë¹„ìŠ¤ ì„¤ì •
        let collector_config = CollectorConfig {
            max_concurrent: config.concurrency,
            concurrency: config.concurrency,
            delay_between_requests: Duration::from_millis(config.delay_ms),
            delay_ms: config.delay_ms,
            batch_size: config.batch_size,
            retry_attempts: config.retry_max,
            retry_max: config.retry_max,
        };

        // ê¸°ë³¸ ì•± ì„¤ì • ë¡œë“œ
        let app_config = AppConfig::default();

        // ê¸°ì¡´ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        let status_checker: Arc<dyn StatusChecker> = Arc::new(StatusCheckerImpl::new(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
        ));

        // DatabaseAnalyzer trait êµ¬í˜„ì„ ìœ„í•œ ê°„ë‹¨í•œ ë˜í¼ ì‚¬ìš© (trait êµ¬í˜„ ì¶”ê°€ë¨)
        let database_analyzer: Arc<dyn DatabaseAnalyzer> = Arc::new(StatusCheckerImpl::new(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
        ));

        // status_checkerë¥¼ ProductListCollectorImplì— ì „ë‹¬í•˜ê¸° ìœ„í•´ concrete typeìœ¼ë¡œ ë‹¤ì‹œ ìƒì„±
        let status_checker_impl = Arc::new(StatusCheckerImpl::new(
            http_client.clone(),
            data_extractor.clone(),
            app_config.clone(),
        ));

        let product_list_collector: Arc<dyn ProductListCollector> = Arc::new(ProductListCollectorImpl::new(
            Arc::new(tokio::sync::Mutex::new(http_client.clone())),
            Arc::new(data_extractor.clone()),
            collector_config.clone(),
            status_checker_impl.clone(),
        ));

        // ProductDetailCollector ì „ìš© êµ¬í˜„ì²´ ì‚¬ìš©
        let product_detail_collector: Arc<dyn ProductDetailCollector> = Arc::new(ProductDetailCollectorImpl::new(
            Arc::new(tokio::sync::Mutex::new(http_client.clone())),
            Arc::new(data_extractor.clone()),
            collector_config.clone(),
        ));

        // ìƒˆë¡œìš´ ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        let deduplication_service: Arc<dyn DeduplicationService> = Arc::new(DeduplicationServiceImpl::new(0.85));
        let validation_service: Arc<dyn ValidationService> = Arc::new(ValidationServiceImpl::new());
        let conflict_resolver: Arc<dyn ConflictResolver> = Arc::new(ConflictResolverImpl::new(ResolutionStrategy::KeepMostComplete));

        // ê³ ê¸‰ ê´€ë¦¬ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        let progress_tracker: Arc<dyn BatchProgressTracker> = Arc::new(BatchProgressTrackerImpl::new());
        let recovery_service: Arc<dyn BatchRecoveryService> = Arc::new(BatchRecoveryServiceImpl::new());
        let retry_manager = Arc::new(RetryManagerImpl::new(3, 1000)); // êµ¬ì²´ì ì¸ íƒ€ì…
        let error_classifier: Arc<dyn ErrorClassifier> = Arc::new(ErrorClassifierImpl::new());

        Self {
            status_checker,
            database_analyzer,
            product_list_collector,
            product_detail_collector,
            deduplication_service,
            validation_service,
            conflict_resolver,
            progress_tracker,
            recovery_service,
            retry_manager,
            error_classifier,
            product_repo,
            event_emitter,
            config,
            session_id,
        }
    }

    /// ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í¬í•¨í•œ í¬ë¡¤ë§ ì‹¤í–‰
    pub async fn execute(&self) -> Result<()> {
        let start_time = Instant::now();
        info!("ğŸš€ Starting advanced batch crawling with STRICT CONFIG LIMITS for session: {}", self.session_id);
        info!("ğŸ“Š Config limits: start_page={}, end_page={}, batch_size={}, concurrency={}", 
              self.config.start_page, self.config.end_page, self.config.batch_size, self.config.concurrency);

        // ë°°ì¹˜ ì§„í–‰ ì¶”ì  ì‹œì‘
        let batch_id = format!("batch_{}", self.session_id);
        self.progress_tracker.update_progress(&batch_id, crate::domain::services::data_processing_services::BatchProgress {
            batch_id: batch_id.clone(),
            total_items: 100,
            processed_items: 0,
            successful_items: 0,
            failed_items: 0,
            progress_percentage: 0.0,
            estimated_remaining_time: Some(360), // 6ë¶„ ì˜ˆìƒ
            current_stage: "ì´ˆê¸°í™”".to_string(),
        }).await?;

        // ì„¸ì…˜ ì‹œì‘ ì´ë²¤íŠ¸
        self.emit_detailed_event(DetailedCrawlingEvent::SessionStarted {
            session_id: self.session_id.clone(),
            config: self.config.clone(),
        }).await?;

        let mut total_products = 0;
        let mut success_rate = 0.0;

        // ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ë¥¼ ìœ„í•œ ë³€ìˆ˜ë“¤
        let mut execution_result = Ok(());

        // ì „ì²´ ì‹¤í–‰ì„ try-catchë¡œ ê°ì‹¸ì„œ ì˜¤ë¥˜ ì²˜ë¦¬
        match self.execute_with_error_handling(&batch_id).await {
            Ok((products_count, calculated_success_rate)) => {
                total_products = products_count;
                success_rate = calculated_success_rate;
                
                // ë°°ì¹˜ ì™„ë£Œ
                let batch_result = crate::domain::services::data_processing_services::BatchResult {
                    batch_id: batch_id.clone(),
                    total_processed: total_products,
                    successful: (total_products as f64 * success_rate) as u32,
                    failed: total_products - (total_products as f64 * success_rate) as u32,
                    duration_ms: start_time.elapsed().as_millis() as u64,
                    errors: vec![],
                };
                self.progress_tracker.complete_batch(&batch_id, batch_result).await?;
            }
            Err(e) => {
                warn!("Batch execution failed: {}", e);
                
                // ì—ëŸ¬ ë¶„ë¥˜ ë° ë³µêµ¬ ì‹œë„
                let _error_type = self.error_classifier.classify(&e.to_string()).await?;
                let _severity = self.error_classifier.assess_severity(&e.to_string()).await?;
                let is_recoverable = self.error_classifier.assess_recoverability(&e.to_string()).await?;
                
                if is_recoverable {
                    info!("Attempting error recovery for batch {}", batch_id);
                    match self.recovery_service.recover_parsing_error(&e.to_string()).await {
                        Ok(recovery_action) => {
                            info!("Recovery action determined: {:?}", recovery_action);
                            // ë³µêµ¬ ì•¡ì…˜ì— ë”°ë¥¸ ì²˜ë¦¬ëŠ” í–¥í›„ í™•ì¥
                        }
                        Err(recovery_err) => {
                            warn!("Recovery failed: {}", recovery_err);
                        }
                    }
                }
                
                execution_result = Err(e);
            }
        }

        let duration = start_time.elapsed();
        info!("Advanced batch crawling completed in {:?}", duration);
        
        // ì„¸ì…˜ ì™„ë£Œ ì´ë²¤íŠ¸
        self.emit_detailed_event(DetailedCrawlingEvent::SessionCompleted {
            session_id: self.session_id.clone(),
            duration,
            total_products,
            success_rate,
        }).await?;
        
        execution_result
    }    /// ì—ëŸ¬ ì²˜ë¦¬ê°€ í¬í•¨ëœ ì‹¤ì œ ì‹¤í–‰ ë¡œì§
    async fn execute_with_error_handling(&self, batch_id: &str) -> Result<(u32, f64)> {

        // Stage 0: ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸
        let site_status = self.stage0_check_site_status().await?;
        
        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (10%)
        self.progress_tracker.update_progress(batch_id, crate::domain::services::data_processing_services::BatchProgress {
            batch_id: batch_id.to_string(),
            total_items: 100, // ì˜ˆìƒ ì´ ì‘ì—… ìˆ˜
            processed_items: 10,
            successful_items: 10,
            failed_items: 0,
            progress_percentage: 10.0,
            estimated_remaining_time: Some(300), // 5ë¶„ ì˜ˆìƒ
            current_stage: "ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ".to_string(),
        }).await?;

        // Stage 1: ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let _db_analysis = self.stage1_analyze_database().await?;
        
        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (20%)
        self.progress_tracker.update_progress(batch_id, crate::domain::services::data_processing_services::BatchProgress {
            batch_id: batch_id.to_string(),
            total_items: 100,
            processed_items: 20,
            successful_items: 20,
            failed_items: 0,
            progress_percentage: 20.0,
            estimated_remaining_time: Some(240),
            current_stage: "ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì™„ë£Œ".to_string(),
        }).await?;

        // Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘
        let product_urls = self.stage2_collect_product_list(site_status.total_pages).await?;
        
        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (50%)
        self.progress_tracker.update_progress(batch_id, crate::domain::services::data_processing_services::BatchProgress {
            batch_id: batch_id.to_string(),
            total_items: 100,
            processed_items: 50,
            successful_items: 50,
            failed_items: 0,
            progress_percentage: 50.0,
            estimated_remaining_time: Some(150),
            current_stage: format!("ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ ({} URLs)", product_urls.len()),
        }).await?;
        
        // Stage 3: ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘
        let raw_products = self.stage3_collect_product_details(&product_urls).await?;
        
        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (75%)
        self.progress_tracker.update_progress(batch_id, crate::domain::services::data_processing_services::BatchProgress {
            batch_id: batch_id.to_string(),
            total_items: 100,
            processed_items: 75,
            successful_items: 75,
            failed_items: 0,
            progress_percentage: 75.0,
            estimated_remaining_time: Some(60),
            current_stage: format!("ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ ({} products)", raw_products.len()),
        }).await?;
        
        // Stage 4: ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        let processed_products = self.stage4_process_data_pipeline(raw_products).await?;
        let total_products = processed_products.len() as u32;
        
        // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (90%)
        self.progress_tracker.update_progress(batch_id, crate::domain::services::data_processing_services::BatchProgress {
            batch_id: batch_id.to_string(),
            total_items: 100,
            processed_items: 90,
            successful_items: 90,
            failed_items: 0,
            progress_percentage: 90.0,
            estimated_remaining_time: Some(30),
            current_stage: format!("ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ ({} processed)", total_products),
        }).await?;
        
        // Stage 5: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        let (processed_count, _new_items, _updated_items, errors) = self.stage5_save_to_database(processed_products).await?;
        
        // ì„±ê³µë¥  ê³„ì‚°
        let success_rate = if processed_count > 0 {
            (processed_count - errors) as f64 / processed_count as f64
        } else {
            0.0
        };

        // ìµœì¢… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (100%)
        self.progress_tracker.update_progress(batch_id, crate::domain::services::data_processing_services::BatchProgress {
            batch_id: batch_id.to_string(),
            total_items: 100,
            processed_items: 100,
            successful_items: (processed_count - errors) as u32,
            failed_items: errors as u32,
            progress_percentage: 100.0,
            estimated_remaining_time: Some(0),
            current_stage: format!("ì™„ë£Œ - {} ì²˜ë¦¬ë¨, {} ì„±ê³µ, {} ì‹¤íŒ¨", processed_count, processed_count - errors, errors),
        }).await?;

        Ok((total_products, success_rate))
    }

    /// Stage 0: ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ (Public method for direct access)
    pub async fn stage0_check_site_status(&self) -> Result<crate::domain::services::SiteStatus> {
        info!("Stage 0: Checking site status");
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "SiteStatus".to_string(),
            message: "ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ì¤‘...".to_string(),
        }).await?;

        let site_status = self.status_checker.check_site_status().await?;
        
        if !site_status.is_accessible || site_status.total_pages == 0 {
            let error_msg = format!("Site is not accessible or has no pages (pages: {})", site_status.total_pages);
            self.emit_detailed_event(DetailedCrawlingEvent::ErrorOccurred {
                stage: "SiteStatus".to_string(),
                error: error_msg.clone(),
                recoverable: true,
            }).await?;
            return Err(anyhow!(error_msg));
        }

        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "SiteStatus".to_string(),
            items_processed: 1,
        }).await?;

        info!("Stage 0 completed: Site is healthy (score: {})", site_status.health_score);
        Ok(site_status)
    }

    /// Stage 1: ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
    async fn stage1_analyze_database(&self) -> Result<crate::domain::services::DatabaseAnalysis> {
        info!("Stage 1: Analyzing database state");
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "DatabaseAnalysis".to_string(),
            message: "ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘...".to_string(),
        }).await?;

        let analysis = self.database_analyzer.analyze_current_state().await?;
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "DatabaseAnalysis".to_string(),
            items_processed: analysis.total_products as usize,
        }).await?;

        info!("Stage 1 completed: {} total products, quality score: {}", 
              analysis.total_products, analysis.data_quality_score);
        Ok(analysis)
    }

    /// Stage 2: ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘
    async fn stage2_collect_product_list(&self, total_pages: u32) -> Result<Vec<ProductUrl>> {
        info!("ğŸ”„ Stage 2: Collecting product list with STRICT CONFIG LIMITS");
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "ProductList".to_string(),
            message: format!("ì„¤ì • ë²”ìœ„ ë‚´ì—ì„œ ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘ ì¤‘..."),
        }).await?;

        // ì—„ê²©í•œ ì„¤ì • ì œí•œ ì ìš©: start_pageì™€ end_page ë²”ìœ„ ì‚¬ìš©
        let start_page = self.config.start_page;
        let end_page = self.config.end_page;
        let actual_pages_to_process = if start_page > end_page {
            start_page - end_page + 1
        } else {
            end_page - start_page + 1
        };
        
        info!("ğŸ“Š STRICT LIMITS APPLIED:");
        info!("   - Configuration: start_page={}, end_page={}", start_page, end_page);
        info!("   - Site total_pages={}", total_pages);
        info!("   - Pages to process: {} (from {} to {})", actual_pages_to_process, start_page, end_page);
        info!("   - Collection order: {}", if start_page > end_page { "oldest first (descending)" } else { "newest first (ascending)" });
        
        // Use page range collection instead of collect_all_pages
        let product_urls = self.product_list_collector.collect_page_range(start_page, end_page).await?;
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "ProductList".to_string(),
            items_processed: product_urls.len(),
        }).await?;

        info!("âœ… Stage 2 completed: {} product URLs collected from pages {}-{} (range enforced)", 
              product_urls.len(), start_page, end_page);
        Ok(product_urls)
    }

    /// Stage 3: ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘
    async fn stage3_collect_product_details(&self, product_urls: &[ProductUrl]) -> Result<Vec<Product>> {
        info!("Stage 3: Collecting product details");
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "ProductDetails".to_string(),
            message: format!("{}ê°œ ì œí’ˆì˜ ìƒì„¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘...", product_urls.len()),
        }).await?;

        let product_details = self.product_detail_collector.collect_details(product_urls).await?;
        
        // ProductDetailì„ Productë¡œ ë³€í™˜
        let products: Vec<Product> = product_details.into_iter()
            .map(|detail| crate::infrastructure::crawling_service_impls::product_detail_to_product(detail))
            .collect();
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "ProductDetails".to_string(),
            items_processed: products.len(),
        }).await?;

        info!("Stage 3 completed: {} products collected", products.len());
        Ok(products)
    }

    /// Stage 4: ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ìƒˆë¡œìš´ ë‹¨ê³„)
    async fn stage4_process_data_pipeline(&self, raw_products: Vec<Product>) -> Result<Vec<Product>> {
        info!("Stage 4: Processing data through advanced pipeline");
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "DataProcessing".to_string(),
            message: format!("{}ê°œ ì œí’ˆì— ëŒ€í•œ ê³ ê¸‰ ë°ì´í„° ì²˜ë¦¬ ì§„í–‰ ì¤‘...", raw_products.len()),
        }).await?;

        // 4.1: ì¤‘ë³µ ì œê±°
        info!("Step 4.1: Removing duplicates");
        let deduplication_analysis = self.deduplication_service.analyze_duplicates(&raw_products).await?;
        info!("Duplicate analysis: {:.2}% duplicates found", deduplication_analysis.duplicate_rate * 100.0);
        
        let deduplicated_products = self.deduplication_service.remove_duplicates(raw_products).await?;
        info!("Deduplication completed: {} products remaining", deduplicated_products.len());

        // 4.2: ìœ íš¨ì„± ê²€ì‚¬
        info!("Step 4.2: Validating products");
        let validation_result = self.validation_service.validate_all(deduplicated_products).await?;
        info!("Validation completed: {} valid, {} invalid products", 
              validation_result.valid_products.len(), validation_result.invalid_products.len());
        
        if !validation_result.validation_summary.common_errors.is_empty() {
            info!("Common validation errors: {:?}", validation_result.validation_summary.common_errors);
        }

        // 4.3: ì¶©ëŒ í•´ê²°
        info!("Step 4.3: Resolving conflicts");
        let resolved_products = self.conflict_resolver.resolve_conflicts(validation_result.valid_products).await?;
        info!("Conflict resolution completed: {} final products", resolved_products.len());

        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "DataProcessing".to_string(),
            items_processed: resolved_products.len(),
        }).await?;

        info!("Stage 4 completed: Data processing pipeline finished with {} high-quality products", resolved_products.len());
        Ok(resolved_products)
    }

    /// Stage 5: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    async fn stage5_save_to_database(&self, products: Vec<Product>) -> Result<(usize, usize, usize, usize)> {
        info!("Stage 5: Saving {} processed products to database", products.len());
        
        self.emit_detailed_event(DetailedCrawlingEvent::StageStarted {
            stage: "DatabaseSave".to_string(),
            message: format!("{}ê°œ ì œí’ˆì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì¤‘...", products.len()),
        }).await?;

        let mut new_items = 0;
        let updated_items = 0;
        let mut errors = 0;

        for (index, product) in products.iter().enumerate() {
            match self.product_repo.create_or_update_product(product).await {
                Ok(_) => {
                    // ì„ì‹œë¡œ ëª¨ë“  ì œí’ˆì„ new_itemsë¡œ ê³„ì‚°
                    new_items += 1;
                    
                    if (index + 1) % 50 == 0 {
                        self.emit_detailed_event(DetailedCrawlingEvent::BatchCompleted {
                            batch: (index + 1) as u32 / 50,
                            total: ((products.len() + 49) / 50) as u32,
                        }).await?;
                    }
                    
                    self.emit_detailed_event(DetailedCrawlingEvent::ProductProcessed {
                        url: product.url.clone(),
                        success: true,
                    }).await?;
                },
                Err(e) => {
                    errors += 1;
                    warn!("Failed to save product from {}: {}", product.url, e);
                    
                    self.emit_detailed_event(DetailedCrawlingEvent::ProductProcessed {
                        url: product.url.clone(),
                        success: false,
                    }).await?;
                }
            }
        }

        let total_processed = new_items + updated_items + errors;

        self.emit_detailed_event(DetailedCrawlingEvent::StageCompleted {
            stage: "DatabaseSave".to_string(),
            items_processed: total_processed,
        }).await?;

        info!("Stage 5 completed: {} new, {} updated, {} errors", new_items, updated_items, errors);
        Ok((total_processed, new_items, updated_items, errors))
    }

    /// ì„¸ë¶„í™”ëœ ì´ë²¤íŠ¸ ë°©ì¶œ
    async fn emit_detailed_event(&self, event: DetailedCrawlingEvent) -> Result<()> {
        if let Some(emitter) = self.event_emitter.as_ref() {
            // DetailedCrawlingEventë¥¼ ê¸°ì¡´ ì´ë²¤íŠ¸ ì‹œìŠ¤í…œê³¼ ì—°ë™
            let progress = match &event {
                DetailedCrawlingEvent::StageStarted { stage, message } => {
                    CrawlingProgress {
                        current: 0,
                        total: self.config.end_page - self.config.start_page + 1,
                        percentage: 0.0,
                        current_stage: match stage.as_str() {
                            "SiteStatus" => CrawlingStage::StatusCheck,
                            "DatabaseAnalysis" => CrawlingStage::DatabaseAnalysis,
                            "ProductList" => CrawlingStage::ProductList,
                            "ProductDetails" => CrawlingStage::ProductDetails,
                            "DataProcessing" => CrawlingStage::ProductDetails, // ë°ì´í„° ì²˜ë¦¬ë„ ProductDetailsë¡œ ë¶„ë¥˜
                            "DatabaseSave" => CrawlingStage::DatabaseSave,
                            _ => CrawlingStage::TotalPages,
                        },
                        current_step: message.clone(),
                        status: CrawlingStatus::Running,
                        message: format!("Stage started: {}", stage),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(1),
                        total_batches: Some(self.config.end_page - self.config.start_page + 1),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                DetailedCrawlingEvent::BatchCompleted { batch, total } => {
                    CrawlingProgress {
                        current: *batch,
                        total: *total,
                        percentage: (*batch as f64 / *total as f64) * 100.0,
                        current_stage: CrawlingStage::DatabaseSave,
                        current_step: format!("ë°°ì¹˜ {}/{} ì™„ë£Œ", batch, total),
                        status: CrawlingStatus::Running,
                        message: format!("Batch {} of {} completed", batch, total),
                        remaining_time: None,
                        elapsed_time: 0,
                        new_items: 0,
                        updated_items: 0,
                        current_batch: Some(*batch),
                        total_batches: Some(*total),
                        errors: 0,
                        timestamp: chrono::Utc::now(),
                    }
                },
                _ => return Ok(()), // ë‹¤ë¥¸ ì´ë²¤íŠ¸ë“¤ì€ ê¸°ë³¸ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
            };

            emitter.emit_progress(progress).await?;
        }
        
        debug!("Emitted detailed event: {:?}", event);
        Ok(())
    }
}
