//! CrawlingPlanner - ì§€ëŠ¥í˜• í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½ ì‹œìŠ¤í…œ
//! 
//! Actor ê¸°ë°˜ ì•„í‚¤í…ì²˜ì—ì„œ í¬ë¡¤ë§ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  
//! ìµœì í™”ëœ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

use std::sync::Arc;
use serde::{Serialize, Deserialize};
use ts_rs::TS;
use tracing::{info, warn};

use crate::domain::services::{StatusChecker, DatabaseAnalyzer};
use crate::domain::services::crawling_services::{
    DatabaseAnalysis, ProcessingStrategy, CrawlingRangeRecommendation
};
use super::super::{
    SystemConfig,
    actors::types::{CrawlingConfig, BatchConfig, ActorError}
};
use crate::domain::services::SiteStatus;
// Removed lazy_static cache (unused) to reduce warnings
// use lazy_static::lazy_static;
// use std::sync::Mutex;

// (Legacy cache removed)

/// ì§€ëŠ¥í˜• í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½ì
/// 
/// ì‚¬ì´íŠ¸ ìƒíƒœì™€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ 
/// ìµœì í™”ëœ í¬ë¡¤ë§ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
pub struct CrawlingPlanner {
    /// ìƒíƒœ í™•ì¸ê¸°
    status_checker: Arc<dyn StatusChecker>,
    
    /// ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ê¸°
    database_analyzer: Arc<dyn DatabaseAnalyzer>,
    
    /// ì‹œìŠ¤í…œ ì„¤ì •
    config: Arc<SystemConfig>,

    /// (ì„ íƒ) í†µí•© ì œí’ˆ ì €ì¥ì†Œ - ContinueFromDb ì „ëµ ì •ë°€ ê³„ì‚°ì— ì‚¬ìš©
    product_repo: Option<Arc<crate::infrastructure::IntegratedProductRepository>>,
}

impl CrawlingPlanner {
    /// ìƒˆë¡œìš´ CrawlingPlanner ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `status_checker` - ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ê¸°
    /// * `database_analyzer` - ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ê¸°
    /// * `config` - ì‹œìŠ¤í…œ ì„¤ì •
    #[must_use]
    pub fn new(
        status_checker: Arc<dyn StatusChecker>,
        database_analyzer: Arc<dyn DatabaseAnalyzer>,
        config: Arc<SystemConfig>,
    ) -> Self {
        Self {
            status_checker,
            database_analyzer,
            config,
            product_repo: None,
        }
    }

    /// í†µí•© ì œí’ˆ ì €ì¥ì†Œë¥¼ ì¶”ê°€ë¡œ ì—°ê²° (builder íŒ¨í„´)
    #[must_use]
    pub fn with_repository(mut self, repo: Arc<crate::infrastructure::IntegratedProductRepository>) -> Self {
        self.product_repo = Some(repo);
        self
    }
    
    /// í¬ë¡¤ë§ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `crawling_config` - ê¸°ë³¸ í¬ë¡¤ë§ ì„¤ì •
    /// 
    /// # Returns
    /// * `Ok(CrawlingPlan)` - ìˆ˜ë¦½ëœ í¬ë¡¤ë§ ê³„íš
    /// * `Err(ActorError)` - ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨
    pub async fn create_crawling_plan(
        &self,
        crawling_config: &CrawlingConfig,
    ) -> Result<CrawlingPlan, ActorError> {
        // 1. ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸
        let site_status = self.status_checker
            .check_site_status()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Site status check failed: {e}")))?;
        
        // 2. ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let db_analysis = self.database_analyzer
            .analyze_current_state()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Database analysis failed: {e}")))?;
        
        // 3. ìµœì í™”ëœ ê³„íš ìˆ˜ë¦½
        let plan = self.optimize_crawling_strategy(
            crawling_config,
            Box::new(site_status),
            Box::new(db_analysis),
        ).await?;
        
        Ok(plan)
    }

    /// ìºì‹œëœ SiteStatusë¥¼ í™œìš©í•´ í¬ë¡¤ë§ ê³„íšì„ ìˆ˜ë¦½í•˜ê³ , ì‚¬ìš©ëœ SiteStatusë„ í•¨ê»˜ ë°˜í™˜í•©ë‹ˆë‹¤.
    pub async fn create_crawling_plan_with_cache(
        &self,
        crawling_config: &CrawlingConfig,
        cached_site_status: Option<SiteStatus>,
    ) -> Result<(CrawlingPlan, SiteStatus), ActorError> {
        // 1. ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ (ìºì‹œ ìš°ì„ )
        let site_status = if let Some(cached) = cached_site_status {
            cached
        } else {
            self.status_checker
                .check_site_status()
                .await
                .map_err(|e| ActorError::CommandProcessingFailed(format!("Site status check failed: {e}")))?
        };

        // 2. ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ (SharedStateCache ì¬ì‚¬ìš© ì‹œë„)
        let db_analysis = self.database_analyzer
            .analyze_current_state()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Database analysis failed: {e}")))?;

        // 3. ìµœì í™”ëœ ê³„íš ìˆ˜ë¦½
        let plan = self.optimize_crawling_strategy(
            crawling_config,
            Box::new(site_status.clone()),
            Box::new(db_analysis),
        ).await?;

        Ok((plan, site_status))
    }

    /// ìºì‹œëœ SiteStatus ë° DatabaseAnalysisë¥¼ í™œìš©í•´ í¬ë¡¤ë§ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ëª¨ë‘ ë°˜í™˜í•©ë‹ˆë‹¤.
    /// ê¸°ì¡´ create_crawling_plan_with_cache ì™€ ë‹¬ë¦¬ DB ë¶„ì„ë„ ìºì‹œë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    pub async fn create_crawling_plan_with_caches(
        &self,
        crawling_config: &CrawlingConfig,
        cached_site_status: Option<SiteStatus>,
        cached_db_analysis: Option<DatabaseAnalysis>,
    ) -> Result<(CrawlingPlan, SiteStatus, DatabaseAnalysis), ActorError> {
        // 1. ì‚¬ì´íŠ¸ ìƒíƒœ (ìºì‹œ ìš°ì„ )
        let site_status = if let Some(cached) = cached_site_status {
            cached
        } else {
            self.status_checker
                .check_site_status()
                .await
                .map_err(|e| ActorError::CommandProcessingFailed(format!("Site status check failed: {e}")))?
        };

        // 2. DB ë¶„ì„ (ìºì‹œ ìš°ì„ )
        let db_analysis = if let Some(cached) = cached_db_analysis {
            cached
        } else {
            self.database_analyzer
                .analyze_current_state()
                .await
                .map_err(|e| ActorError::CommandProcessingFailed(format!("Database analysis failed: {e}")))?
        };

        // 3. ìµœì í™”ëœ ê³„íš ìˆ˜ë¦½
        let plan = self.optimize_crawling_strategy(
            crawling_config,
            Box::new(site_status.clone()),
            Box::new(db_analysis.clone()),
        ).await?;

        Ok((plan, site_status, db_analysis))
    }
    
    /// ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    /// 
    /// # Returns
    /// * `Ok((SiteStatus, DatabaseAnalysis))` - ë¶„ì„ëœ ì‹œìŠ¤í…œ ìƒíƒœ
    /// * `Err(ActorError)` - ë¶„ì„ ì‹¤íŒ¨
    pub async fn analyze_system_state(&self) -> Result<(crate::domain::services::SiteStatus, DatabaseAnalysis), ActorError> {
        // ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸
        let site_status = self.status_checker
            .check_site_status()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Site status check failed: {e}")))?;
        
        // ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let db_analysis = self.database_analyzer
            .analyze_current_state()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Database analysis failed: {e}")))?;
        
        Ok((site_status, db_analysis))
    }
    
    /// ìºì‹œëœ ì‚¬ì´íŠ¸ ìƒíƒœë¡œ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `cached_site_status` - ìºì‹œëœ ì‚¬ì´íŠ¸ ìƒíƒœ
    /// 
    /// # Returns
    /// * `Ok((SiteStatus, DatabaseAnalysis))` - ë¶„ì„ëœ ì‹œìŠ¤í…œ ìƒíƒœ
    /// * `Err(ActorError)` - ë¶„ì„ ì‹¤íŒ¨
    pub async fn analyze_system_state_with_cache(&self, cached_site_status: Option<crate::domain::services::SiteStatus>) -> Result<(crate::domain::services::SiteStatus, DatabaseAnalysis), ActorError> {
        // ìºì‹œëœ ìƒíƒœê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ í™•ì¸
        let site_status = if let Some(cached) = cached_site_status {
            cached
        } else {
            self.status_checker
                .check_site_status()
                .await
                .map_err(|e| ActorError::CommandProcessingFailed(format!("Site status check failed: {e}")))?
        };
        
        // ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let db_analysis = self.database_analyzer
            .analyze_current_state()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Database analysis failed: {e}")))?;
        
        Ok((site_status, db_analysis))
    }
    
    /// í¬ë¡¤ë§ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `site_status` - ì‚¬ì´íŠ¸ ìƒíƒœ
    /// * `db_analysis` - ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼
    /// 
    /// # Returns
    /// * `Ok((CrawlingRangeRecommendation, ProcessingStrategy))` - ê²°ì •ëœ ì „ëµ
    /// * `Err(ActorError)` - ì „ëµ ê²°ì • ì‹¤íŒ¨
    pub async fn determine_crawling_strategy(
        &self,
        site_status: &crate::domain::services::SiteStatus,
        db_analysis: &DatabaseAnalysis,
    ) -> Result<(CrawlingRangeRecommendation, ProcessingStrategy), ActorError> {
        // ì‚¬ì´íŠ¸ ìƒíƒœì™€ DB ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í¬ë¡¤ë§ ë²”ìœ„ ì¶”ì²œ
        let is_site_healthy = site_status.is_accessible && site_status.health_score > 0.7;
        let range_recommendation = if is_site_healthy {
            if db_analysis.total_products > 5000 {
                CrawlingRangeRecommendation::Partial(50) // ë¶€ë¶„ í¬ë¡¤ë§
            } else {
                CrawlingRangeRecommendation::Full // ì „ì²´ í¬ë¡¤ë§
            }
        } else {
            CrawlingRangeRecommendation::Partial(20) // ì‚¬ì´íŠ¸ ìƒíƒœê°€ ì¢‹ì§€ ì•Šìœ¼ë©´ ìµœì†Œí•œì˜ í¬ë¡¤ë§
        };
        
        // ì²˜ë¦¬ ì „ëµ ê²°ì •
        let processing_strategy = ProcessingStrategy {
            recommended_batch_size: self.calculate_optimal_batch_size(100),
            recommended_concurrency: self.calculate_optimal_concurrency(),
            should_skip_duplicates: db_analysis.missing_products_count > 100,
            should_update_existing: db_analysis.data_quality_score < 0.8,
            priority_urls: vec![],
        };
        
        Ok((range_recommendation, processing_strategy))
    }
    
    /// ë°°ì¹˜ ì„¤ì •ì„ ìµœì í™”í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `base_config` - ê¸°ë³¸ ë°°ì¹˜ ì„¤ì •
    /// * `total_pages` - ì´ í˜ì´ì§€ ìˆ˜
    /// 
    /// # Returns
    /// * `BatchConfig` - ìµœì í™”ëœ ë°°ì¹˜ ì„¤ì •
    #[must_use]
    pub fn optimize_batch_config(
        &self,
        base_config: &BatchConfig,
        total_pages: u32,
    ) -> BatchConfig {
        let optimal_batch_size = self.calculate_optimal_batch_size(total_pages);
        let optimal_concurrency = self.calculate_optimal_concurrency();
        
        BatchConfig {
            batch_size: optimal_batch_size.min(base_config.batch_size),
            concurrency_limit: optimal_concurrency.min(base_config.concurrency_limit),
            batch_delay_ms: self.calculate_optimal_delay(),
            retry_on_failure: base_config.retry_on_failure,
            start_page: base_config.start_page,
            end_page: base_config.end_page,
        }
    }
    
    /// í¬ë¡¤ë§ ì „ëµì„ ìµœì í™”í•©ë‹ˆë‹¤.
    async fn optimize_crawling_strategy(
        &self,
        config: &CrawlingConfig,
        site_status_any: Box<dyn std::any::Any + Send>,
        db_analysis_any: Box<dyn std::any::Any + Send>,
    ) -> Result<CrawlingPlan, ActorError> {
        // ì‹¤ì œ ìµœì í™”: SiteStatus + DatabaseAnalysis ê¸°ë°˜ìœ¼ë¡œ ìµœì‹  í˜ì´ì§€ë¶€í„° Nê°œë¥¼ ì„ íƒ
        // 1) ì „ë‹¬ëœ Anyë¥¼ ë‹¤ìš´ìºìŠ¤íŠ¸
        let site_status = match site_status_any.downcast::<SiteStatus>() {
            Ok(b) => *b,
            Err(_) => return Err(ActorError::CommandProcessingFailed("Failed to downcast SiteStatus".to_string())),
        };
        let db_analysis = match db_analysis_any.downcast::<DatabaseAnalysis>() {
            Ok(b) => *b,
            Err(_) => return Err(ActorError::CommandProcessingFailed("Failed to downcast DatabaseAnalysis".to_string())),
        };

        // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        // Range REUSE GUARD (simple in-memory, per process)
        // ìµœê·¼ 60ì´ˆ ì´ë‚´ ë™ì¼ total_pages + requested_countë©´ ì¬ì‚¬ìš©
        // (ì‹¤ì œ ì˜ì†í™”ëŠ” ì¶”í›„ ConfigManager í†µí•© ì‹œ í™•ì¥)
    // (ì „ì—­ lazy_static ìºì‹œ ì‚¬ìš©)

    let now = std::time::Instant::now();

        // 2) ìš”ì²­í•œ í˜ì´ì§€ ìˆ˜ ê³„ì‚° (UI ì…ë ¥ì˜ start/endëŠ” 'ê°œìˆ˜'ë§Œ ì‚¬ìš©)
        let requested_count = if config.start_page >= config.end_page { config.start_page - config.end_page + 1 } else { config.end_page - config.start_page + 1 };

        let total_pages_on_site = site_status.total_pages.max(1);
        let count = requested_count.max(1).min(total_pages_on_site);

        // ì „ëµ ë¶„ê¸°
    // Track DB position if fetched during strategy-specific branch (to avoid duplicate query later)
    let mut db_position_for_reuse: Option<(Option<i32>, Option<i32>)> = None;
    let mut page_range: Vec<u32> = match config.strategy {
            crate::new_architecture::actors::types::CrawlingStrategy::NewestFirst => {
                // Enhanced newest-first: prioritize pages containing products lacking details if repository available
                if let Some(repo) = &self.product_repo {
                    let fetch_limit: i32 = ((count * 20).max(20)).min(2000) as i32;
                    match repo.get_products_without_details(fetch_limit).await {
                        Ok(missing) if !missing.is_empty() => {
                            let mut pages: Vec<u32> = missing.iter().filter_map(|p| p.page_id.map(|pid| pid as u32)).collect();
                            pages.sort_unstable();
                            pages.dedup();
                            pages.sort_by(|a,b| b.cmp(a));
                            let mut selected: Vec<u32> = pages.iter().take(count as usize).copied().collect();
                            if let (Some(min_sel), Some(max_sel)) = (selected.iter().min().copied(), selected.iter().max().copied()) {
                                let mut boundary = vec![];
                                if min_sel > 1 { boundary.push(min_sel - 1); }
                                if max_sel < total_pages_on_site { boundary.push(max_sel + 1); }
                                for b in boundary { if !selected.contains(&b) { selected.push(b); } }
                            }
                            if selected.len() < count as usize {
                                let mut candidate = total_pages_on_site;
                                while selected.len() < count as usize && candidate >= 1 {
                                    if !selected.contains(&candidate) { selected.push(candidate); }
                                    if candidate == 1 { break; }
                                    candidate -= 1;
                                }
                            }
                            selected.sort_by(|a,b| b.cmp(a));
                            info!("ğŸ§­ Missing-detail aware page range computed: base_missing_pages={} requested={} final_selected={} pages={:?}", pages.len(), count, selected.len(), selected);
                            if !selected.is_empty() { selected } else {
                                let start = total_pages_on_site; let end = start.saturating_sub(count - 1).max(1); (end..=start).rev().collect()
                            }
                        }
                        Ok(_) => {
                            let force_recrawl = std::env::var("MC_FORCE_RECRAWL_ON_COMPLETE")
                                .map(|v| { let t = v.trim(); !(t.eq("0") || t.eq_ignore_ascii_case("false")) })
                                .unwrap_or(false);
                            if force_recrawl {
                                let start = total_pages_on_site; let end = start.saturating_sub(count - 1).max(1); let pages: Vec<u32> = (end..=start).rev().collect();
                                info!("â™»ï¸ Force recrawl enabled (MC_FORCE_RECRAWL_ON_COMPLETE) -> selecting newest pages again count={} pages={:?}", pages.len(), pages);
                                pages
                            } else {
                                info!("âœ… All products already have details (no missing detail rows) -> skipping list page crawling (use MC_FORCE_RECRAWL_ON_COMPLETE=1 to override)");
                                Vec::new()
                            }
                        }
                        Err(e) => {
                            let start = total_pages_on_site; let end = start.saturating_sub(count - 1).max(1); let pages: Vec<u32> = (end..=start).rev().collect();
                            warn!("âš ï¸ Missing-detail fetch failed ({}), fallback newest-first pages count={}", e, pages.len());
                            pages
                        }
                    }
                } else {
                    let start = total_pages_on_site; let end = start.saturating_sub(count - 1).max(1); let pages: Vec<u32> = (end..=start).rev().collect();
                    info!("ğŸ”§ Computed newest-first page range (no repo): total_pages_on_site={} requested_count={} actual_count={} pages={:?}", total_pages_on_site, requested_count, pages.len(), pages);
                    pages
                }
            }
            crate::new_architecture::actors::types::CrawlingStrategy::ContinueFromDb => {
                // Reintroduce lightweight cache + fallback helper removed during cleanup
                #[derive(Clone)]
                struct DbCachedRange { pages: Vec<u32>, total_pages: u32, requested: u32, last_db_page_id: Option<i32>, last_db_index: Option<i32>, ts: std::time::Instant }
                lazy_static::lazy_static! {
                    static ref LAST_DB_RANGE: std::sync::Mutex<Option<DbCachedRange>> = std::sync::Mutex::new(None);
                }
                let newest_fallback_pages = || {
                    let start = total_pages_on_site; let end = start.saturating_sub(count - 1).max(1); (end..=start).rev().collect::<Vec<u32>>()
                };
                if self.product_repo.is_none() {
                    warn!("ğŸ§ª ContinueFromDb requested but product_repo not attached -> fallback newest-first");
                    return Ok(CrawlingPlan { session_id: uuid::Uuid::new_v4().to_string(), phases: vec![], total_estimated_duration_secs: 0, optimization_strategy: OptimizationStrategy::Balanced, created_at: chrono::Utc::now(), db_total_products: None, db_unique_products: None, db_last_update: None }); // reuse placeholder
                }
                let repo = self.product_repo.as_ref().unwrap().clone();

                // 2) DB ìƒíƒœ ì¡°íšŒ
                let (max_page_id, max_index_in_page) = match repo.get_max_page_id_and_index().await {
                    Ok(v) => v,
                    Err(e) => {
                        warn!("âš ï¸ Failed to read DB state ({e}); using newest-first fallback");
                        return Ok(CrawlingPlan { session_id: uuid::Uuid::new_v4().to_string(), phases: vec![], total_estimated_duration_secs: 0, optimization_strategy: OptimizationStrategy::Balanced, created_at: chrono::Utc::now(), db_total_products: None, db_unique_products: None, db_last_update: None });
                    }
                };
                db_position_for_reuse = Some((max_page_id, max_index_in_page));

                // 3) ìºì‹œ ì¬ì‚¬ìš© íŒë‹¨
                if let Some(cached) = LAST_DB_RANGE.lock().unwrap().as_ref() {
                    if cached.total_pages == total_pages_on_site && cached.requested == count && cached.last_db_page_id == max_page_id && cached.last_db_index == max_index_in_page && now.duration_since(cached.ts).as_secs() < 60 {
                        info!("â™»ï¸ Reusing cached ContinueFromDb range: {:?}", cached.pages);
                        return Ok(CrawlingPlan { session_id: uuid::Uuid::new_v4().to_string(), phases: vec![], total_estimated_duration_secs: 0, optimization_strategy: OptimizationStrategy::Balanced, created_at: chrono::Utc::now(), db_total_products: None, db_unique_products: None, db_last_update: None });
                    }
                }

                // 4) ì •ë°€ ë²”ìœ„ ê³„ì‚°
                let products_on_last_page = site_status.products_on_last_page;
                let precise = match repo.calculate_next_crawling_range(total_pages_on_site, products_on_last_page, count).await {
                    Ok(opt) => opt,
                    Err(e) => {
                        warn!("âš ï¸ Failed calculate_next_crawling_range ({e}); fallback to newest-first pages");
                        None
                    }
                };
                let mut pages: Vec<u32> = if let Some((start_page, end_page)) = precise {
                    if start_page >= end_page { (end_page..=start_page).rev().collect() } else { (start_page..=end_page).rev().collect() }
                } else { newest_fallback_pages() };
                // Drop pages that are already fully detailed to reduce no-op batches
                if let Some(repo_ref) = self.product_repo.as_ref() {
                    let repo = repo_ref.clone();
                    let total_pages = site_status.total_pages;
                    let products_on_last = site_status.products_on_last_page;
                    let mut filtered: Vec<u32> = Vec::with_capacity(pages.len());
                    for sp in pages.iter().copied() {
                        match repo.is_site_page_fully_detailed(sp, total_pages, products_on_last).await {
                            Ok(true) => { tracing::info!("ğŸ§¹ Skipping fully detailed page {} from plan", sp); },
                            Ok(false) | Err(_) => { filtered.push(sp); }
                        }
                    }
                    if filtered.len() != pages.len() {
                        tracing::info!("ğŸ§® Planner filtered pages: before={} after={}", pages.len(), filtered.len());
                        pages = filtered;
                    }
                }
                if let (Some(mp), Some(mi)) = (max_page_id, max_index_in_page) {
                    if mi < 11 { // partial page
                        let partial_site_page = total_pages_on_site - mp as u32;
                        if !pages.contains(&partial_site_page) {
                            pages.insert(0, partial_site_page);
                            info!("ğŸ” Partial page re-included (planner): {} (db_page_id={}, index_in_page={})", partial_site_page, mp, mi);
                        }
                    }
                }
                *LAST_DB_RANGE.lock().unwrap() = Some(DbCachedRange { pages: pages.clone(), total_pages: total_pages_on_site, requested: count, last_db_page_id: max_page_id, last_db_index: max_index_in_page, ts: now });
                info!("ğŸ”§ Computed ContinueFromDb range: db_last=({:?},{:?}) pages={:?}", max_page_id, max_index_in_page, pages);
                pages
            }
        };

        // 4) Partial page reinclusion (unified)
        if !page_range.is_empty() {
            // Prefer already fetched DB position (ContinueFromDb) to avoid an extra query.
            let position = if let Some(pos) = &db_position_for_reuse {
                Some(pos.clone())
            } else if let Some(repo) = &self.product_repo {
                repo.get_max_page_id_and_index().await.ok()
            } else { None };
            if let Some((max_page_id, max_index_in_page)) = position {
                if let (Some(mp), Some(mi)) = (max_page_id, max_index_in_page) {
                    if mi < 11 { // partial page needs refresh
                        let total_pages_on_site = site_status.total_pages.max(1);
                        let partial_site_page = total_pages_on_site - mp as u32;
                        if !page_range.contains(&partial_site_page) {
                            if db_position_for_reuse.is_some() {
                                info!("ğŸ” Partial page re-included (planner unified, reuse db_position): {} (db_page_id={}, index_in_page={})", partial_site_page, mp, mi);
                            } else {
                                info!("ğŸ” Partial page re-included (planner unified, fresh db_position): {} (db_page_id={}, index_in_page={})", partial_site_page, mp, mi);
                            }
                            page_range.insert(0, partial_site_page);
                            page_range.sort_by(|a,b| b.cmp(a));
                            page_range.dedup();
                        }
                    }
                }
            }
        }

        // 5) ë°°ì¹˜ í¬ê¸°ì— ë”°ë¼ ë¶„í• 
        let batch_size = config.batch_size.max(1) as usize;
        // í˜ì´ì§€ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ì¬í¬ë¡¤ë§ì´ í•„ìš” ì—†ëŠ” ìƒíƒœì´ë¯€ë¡œ ë°°ì¹˜ ìƒì„± ìƒëµ
        let batched_pages: Vec<Vec<u32>> = if page_range.is_empty() {
            Vec::new()
        } else if page_range.len() > batch_size {
            page_range.chunks(batch_size).map(|c| c.to_vec()).collect()
        } else {
            vec![page_range.clone()]
        };

        if page_range.is_empty() {
            info!("ğŸ“‹ ë°°ì¹˜ ê³„íš ìˆ˜ë¦½: ìˆ˜ì§‘í•  ì‹ ê·œ í˜ì´ì§€ ì—†ìŒ (ëª¨ë“  detail ì´ë¯¸ ì¡´ì¬) batches=0");
        } else {
            info!(
                "ğŸ“‹ ë°°ì¹˜ ê³„íš ìˆ˜ë¦½: ì´ {}í˜ì´ì§€ë¥¼ {}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  (batch_size={})",
                page_range.len(),
                batched_pages.len(),
                batch_size
            );
        }

    // 5) ë‹¨ê³„ êµ¬ì„±: StatusCheck â†’ (List batches) â†’ DataValidation
        let mut phases = vec![CrawlingPhase {
            phase_type: PhaseType::StatusCheck,
            estimated_duration_secs: 30,
            priority: 1,
            pages: vec![],
        }];

        for (batch_idx, batch_pages) in batched_pages.iter().enumerate() {
            if batch_pages.is_empty() { continue; }
            phases.push(CrawlingPhase {
                phase_type: PhaseType::ListPageCrawling,
                estimated_duration_secs: (batch_pages.len() * 2) as u64,
                priority: 2 + batch_idx as u32,
                pages: batch_pages.clone(),
            });
        }

        phases.extend(vec![
            CrawlingPhase {
                phase_type: PhaseType::DataValidation,
                estimated_duration_secs: (count / 2).max(1) as u64,
                priority: 101,
                pages: vec![],
            },
        ]);

        let total_estimated_duration_secs = phases
            .iter()
            .map(|p| p.estimated_duration_secs)
            .sum();

        Ok(CrawlingPlan {
            session_id: format!("crawling_{}", uuid::Uuid::new_v4()),
            phases,
            total_estimated_duration_secs,
            optimization_strategy: OptimizationStrategy::Balanced,
            created_at: chrono::Utc::now(),
            db_total_products: Some(db_analysis.total_products),
            db_unique_products: Some(db_analysis.unique_products),
            db_last_update: db_analysis.last_update,
        })
    }
    
    /// ìµœì  ë°°ì¹˜ í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    fn calculate_optimal_batch_size(&self, total_pages: u32) -> u32 {
        // ì´ í˜ì´ì§€ ìˆ˜ì— ë”°ë¥¸ ì ì‘ì  ë°°ì¹˜ í¬ê¸°
        match total_pages {
            1..=50 => 10,
            51..=200 => 20,
            201..=1000 => 50,
            _ => 100,
        }
    }
    
    /// ìµœì  ë™ì‹œì„± ìˆ˜ì¤€ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    fn calculate_optimal_concurrency(&self) -> u32 {
        // ì‹œìŠ¤í…œ ì„¤ì • ê¸°ë°˜ ë™ì‹œì„± ê³„ì‚°
        self.config.crawling
            .as_ref()
            .and_then(|c| c.default_concurrency_limit)
            .unwrap_or(5)
            .min(10)
    }
    
    /// ìµœì  ì§€ì—° ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    fn calculate_optimal_delay(&self) -> u64 {
        // ì„¤ì •ëœ ì§€ì—° ì‹œê°„ ì‚¬ìš©
        self.config.crawling
            .as_ref()
            .and_then(|c| c.request_delay_ms)
            .unwrap_or(1000)
    }
}

/// í¬ë¡¤ë§ ê³„íš
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct CrawlingPlan {
    /// ì„¸ì…˜ ID
    pub session_id: String,
    
    /// í¬ë¡¤ë§ ë‹¨ê³„ë“¤
    pub phases: Vec<CrawlingPhase>,
    
    /// ì´ ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    pub total_estimated_duration_secs: u64,
    
    /// ìµœì í™” ì „ëµ
    pub optimization_strategy: OptimizationStrategy,
    
    /// ê³„íš ìƒì„± ì‹œê°„
    pub created_at: chrono::DateTime<chrono::Utc>,
    // â¬‡ï¸ Database snapshot (ì„ íƒì  - ExecutionPlan ìŠ¤ëƒ…ìƒ· í•´ì‹œ ì•ˆì •í™”ì— í™œìš©)
    pub db_total_products: Option<u32>,
    pub db_unique_products: Option<u32>,
    pub db_last_update: Option<chrono::DateTime<chrono::Utc>>,
}

/// í¬ë¡¤ë§ ë‹¨ê³„
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct CrawlingPhase {
    /// ë‹¨ê³„ íƒ€ì…
    pub phase_type: PhaseType,
    
    /// ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    pub estimated_duration_secs: u64,
    
    /// ìš°ì„ ìˆœìœ„ (ë‚®ì„ìˆ˜ë¡ ë¨¼ì € ì‹¤í–‰)
    pub priority: u32,
    
    /// ì²˜ë¦¬í•  í˜ì´ì§€ ëª©ë¡
    pub pages: Vec<u32>,
}

/// ë‹¨ê³„ íƒ€ì…
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub enum PhaseType {
    /// ìƒíƒœ í™•ì¸
    StatusCheck,
    
    /// ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§
    ListPageCrawling,
    
    /// ìƒí’ˆ ìƒì„¸ í¬ë¡¤ë§
    ProductDetailCrawling,
    
    /// ë°ì´í„° ê²€ì¦
    DataValidation,
    
    /// ë°ì´í„° ì €ì¥
    DataSaving,
}

/// ìµœì í™” ì „ëµ
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub enum OptimizationStrategy {
    /// ì†ë„ ìš°ì„ 
    Speed,
    
    /// ì•ˆì •ì„± ìš°ì„ 
    Stability,
    
    /// ê· í˜•
    Balanced,
    
    /// ë¦¬ì†ŒìŠ¤ ì ˆì•½
    ResourceEfficient,
}

