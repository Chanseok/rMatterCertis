use std::sync::Arc;
use anyhow::{Result, anyhow};
use tracing::{info, warn};

use crate::domain::services::{
    StatusChecker, DatabaseAnalyzer, ProcessingStrategy
};
use crate::domain::services::crawling_services::{SiteStatus, DatabaseAnalysis, CrawlingRangeRecommendation};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::crawling_service_impls::CrawlingRangeCalculator;
use crate::infrastructure::IntegratedProductRepository;

/// í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½ ì„œë¹„ìŠ¤
///
/// ì—¬ëŸ¬ ì„œë¹„ìŠ¤(StatusChecker, DatabaseAnalyzer)ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬
/// ìµœì ì˜ í¬ë¡¤ë§ ì „ëµê³¼ ì²˜ë¦¬ ë°©ì‹ì„ ê²°ì •í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
/// 
/// **í•µì‹¬ ì›ì¹™**: ServiceBasedBatchCrawlingEngineì˜ ê²€ì¦ëœ í¬ë¡¤ë§ ë²”ìœ„ ê³„ì‚° ë¡œì§ì„ ì¬ì‚¬ìš©
pub struct CrawlingPlanner {
    status_checker: Arc<dyn StatusChecker>,
    database_analyzer: Arc<dyn DatabaseAnalyzer>,
    config: Arc<AppConfig>,
    range_calculator: CrawlingRangeCalculator,
}

impl CrawlingPlanner {
    /// ìƒˆë¡œìš´ CrawlingPlanner ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    /// 
    /// **ì¤‘ìš”**: ServiceBasedBatchCrawlingEngineì˜ ê²€ì¦ëœ CrawlingRangeCalculatorë¥¼ í†µí•©
    pub fn new(
        status_checker: Arc<dyn StatusChecker>,
        database_analyzer: Arc<dyn DatabaseAnalyzer>,
        product_repo: Arc<IntegratedProductRepository>,
        config: Arc<AppConfig>,
    ) -> Self {
        let range_calculator = CrawlingRangeCalculator::new(
            product_repo,
            (*config).clone(),
        );

        Self {
            status_checker,
            database_analyzer,
            config,
            range_calculator,
        }
    }

    /// ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í¬ë¡¤ë§ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
    /// 
    /// ë°˜í™˜ê°’:
    /// - SiteStatus: ì‚¬ì´íŠ¸ ê¸°ë³¸ ì •ë³´
    /// - DatabaseAnalysis: DB ë¶„ì„ ê²°ê³¼  
    /// - ProcessingStrategy: ì²˜ë¦¬ ì „ëµ
    pub async fn create_crawling_plan(&self) -> Result<(SiteStatus, DatabaseAnalysis, ProcessingStrategy)> {
        info!("ğŸ¯ [CrawlingPlanner] Creating comprehensive crawling plan...");

        // 1. ê¸°ë³¸ ë¶„ì„
        let site_status = self.status_checker.check_site_status().await?;
        let db_analysis = self.database_analyzer.analyze_current_state().await?;

        info!("ğŸ“Š [CrawlingPlanner] Site analysis: {} total pages, {} products in DB", 
              site_status.total_pages, db_analysis.total_products);

        // 2. í¬ë¡¤ë§ ì „ëµ ê²°ì •
        let (crawling_recommendation, processing_strategy) = self.determine_crawling_strategy(
            &site_status,
            &db_analysis,
        ).await?;

        info!("âœ… [CrawlingPlanner] Plan created successfully");

        Ok((site_status, db_analysis, processing_strategy))
    }

    /// ì‚¬ì´íŠ¸ ìƒíƒœì™€ DB ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ í¬ë¡¤ë§ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
    /// 
    /// **í•µì‹¬**: ServiceBasedBatchCrawlingEngineì˜ ê²€ì¦ëœ ë²”ìœ„ ê³„ì‚° ë¡œì§ì„ ì¬ì‚¬ìš©
    /// 
    /// ë°˜í™˜ê°’:
    /// (CrawlingRangeRecommendation, ProcessingStrategy) íŠœí”Œ
    async fn determine_crawling_strategy(
        &self,
        site_status: &SiteStatus,
        db_analysis: &DatabaseAnalysis,
    ) -> Result<(CrawlingRangeRecommendation, ProcessingStrategy)> {
        info!("ğŸ§® [CrawlingPlanner] Using ServiceBasedBatchCrawlingEngine's proven range calculation logic...");

        // 1. ServiceBasedBatchCrawlingEngineì˜ ê²€ì¦ëœ ë²”ìœ„ ê³„ì‚° ë¡œì§ ì‚¬ìš©
        let optimal_range = self.range_calculator.calculate_next_crawling_range(
            site_status.total_pages,
            site_status.products_on_last_page,
        ).await?;

        // 2. ë²”ìœ„ ì¶”ì²œ ê²°ê³¼ë¥¼ CrawlingRangeRecommendation í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        let range_recommendation = match optimal_range {
            Some((start_page, end_page)) => {
                info!("ğŸ¯ [PROPER ACTOR] CrawlingPlanner range: {} to {} (reverse crawling)", 
                      start_page, end_page);
                
                // ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ì¸ì§€ í™•ì¸
                if start_page == site_status.total_pages && end_page == 1 {
                    CrawlingRangeRecommendation::Full
                } else {
                    // ì—­ìˆœ í¬ë¡¤ë§ì´ë¯€ë¡œ ì‹¤ì œ í˜ì´ì§€ ìˆ˜ëŠ” start_page - end_page + 1
                    let pages_to_crawl = start_page - end_page + 1;
                    CrawlingRangeRecommendation::Partial(pages_to_crawl)
                }
            },
            None => {
                warn!("âš ï¸ [CrawlingPlanner] No optimal range calculated, using fallback");
                CrawlingRangeRecommendation::None
            }
        };

        // 3. ProcessingStrategy ê²°ì •
        let processing_strategy = self.determine_processing_strategy_from_config(
            site_status,
            db_analysis,
        ).await?;

        // 4. ë²”ìœ„ ì •ë³´ ë¡œê¹… (range_recommendationì— ë”°ë¼ ë‹¤ë¥´ê²Œ í‘œì‹œ)
        match &range_recommendation {
            CrawlingRangeRecommendation::Full => {
                info!(
                    "ğŸ“‹ [CrawlingPlanner] Strategy determined: FULL crawling (1â†’{}), batch_size: {}, concurrency: {}",
                    site_status.total_pages,
                    processing_strategy.recommended_batch_size,
                    processing_strategy.recommended_concurrency
                );
            },
            CrawlingRangeRecommendation::Partial(pages) => {
                if let Some((start_page, end_page)) = optimal_range {
                    info!(
                        "ğŸ“‹ [CrawlingPlanner] Strategy determined: PARTIAL crawling ({}â†’{}, {} pages), batch_size: {}, concurrency: {}",
                        start_page, end_page, pages,
                        processing_strategy.recommended_batch_size,
                        processing_strategy.recommended_concurrency
                    );
                }
            },
            CrawlingRangeRecommendation::None => {
                info!(
                    "ğŸ“‹ [CrawlingPlanner] Strategy determined: NO crawling needed, batch_size: {}, concurrency: {}",
                    processing_strategy.recommended_batch_size,
                    processing_strategy.recommended_concurrency
                );
            }
        }

        Ok((range_recommendation, processing_strategy))
    }

    /// ì‚¬ì´íŠ¸ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ í¬ë¡¤ë§ ë²”ìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    /// CrawlingRangeCalculatorë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë²”ìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    pub async fn calculate_actual_crawling_range(&self, site_status: &SiteStatus) -> Result<Option<(u32, u32)>> {
        self.range_calculator.calculate_next_crawling_range(
            site_status.total_pages,
            site_status.products_on_last_page,
        ).await
    }

    /// ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ì²˜ë¦¬ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
    /// 
    /// **í•µì‹¬**: ëª¨ë“  ê°’ì„ ì„¤ì •ì—ì„œ ì½ë˜, í˜„ì¬ ìƒí™©ì— ë§ê²Œ ì§€ëŠ¥í˜• ì¡°ì •
    async fn determine_processing_strategy_from_config(
        &self,
        _site_status: &SiteStatus,
        _db_analysis: &DatabaseAnalysis,
    ) -> Result<ProcessingStrategy> {
        info!("âš™ï¸ [CrawlingPlanner] Using user configuration values directly...");
        
        // âœ… ì‚¬ìš©ì ì„¤ì •ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì„ì˜ ë³€ê²½ ê¸ˆì§€)
        let batch_size = self.config.user.batch.batch_size;
        let concurrency = self.config.user.max_concurrent_requests;
        
        info!("ğŸ“Š [CrawlingPlanner] Using user settings: batch_size={}, concurrency={}", 
              batch_size, concurrency);
        
        // í¬ë¡¤ë§ ì „ëµ ì„¤ì • (ì¤‘ë³µ ì²˜ë¦¬ëŠ” DB ë ˆë²¨ì—ì„œ ìë™ ì²˜ë¦¬ë¨)
        let should_skip_duplicates = false; // URLì´ Primary Keyì´ë¯€ë¡œ DBê°€ ìë™ìœ¼ë¡œ ì¤‘ë³µ ì²˜ë¦¬
        
        Ok(ProcessingStrategy {
            recommended_batch_size: batch_size,
            recommended_concurrency: concurrency,
            should_skip_duplicates,
            should_update_existing: true, // ê¸°ì¡´ ì œí’ˆ ì •ë³´ ì—…ë°ì´íŠ¸ í—ˆìš©
            priority_urls: vec![], // í–¥í›„ ìš°ì„ ìˆœìœ„ URL ê¸°ëŠ¥ êµ¬í˜„ ì‹œ ì‚¬ìš©
        })
    }
}
