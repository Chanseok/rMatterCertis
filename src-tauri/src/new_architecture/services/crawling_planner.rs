//! CrawlingPlanner - ì§€ëŠ¥í˜• í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½ ì‹œìŠ¤í…œ
//! 
//! Actor ê¸°ë°˜ ì•„í‚¤í…ì²˜ì—ì„œ í¬ë¡¤ë§ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  
//! ìµœì í™”ëœ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

use std::sync::Arc;
use serde::{Serialize, Deserialize};
use ts_rs::TS;
use tracing::info;

use crate::domain::services::{StatusChecker, DatabaseAnalyzer};
use crate::domain::services::crawling_services::{
    DatabaseAnalysis, ProcessingStrategy, DuplicateAnalysis, 
    FieldAnalysis, CrawlingRangeRecommendation
};
use super::super::{
    SystemConfig,
    actors::types::{CrawlingConfig, BatchConfig, ActorError}
};

/// ì§€ëŠ¥í˜• í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½ì
/// 
/// ì‚¬ì´íŠ¸ ìƒíƒœì™€ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ 
/// ìµœì í™”ëœ í¬ë¡¤ë§ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
pub struct CrawlingPlanner {
    /// ìƒíƒœ í™•ì¸ê¸°
    status_checker: Arc<dyn StatusChecker>,
    
    /// ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ê¸°
    database_analyzer: Arc<dyn DatabaseAnalyzer>,
    
    /// ì‹œìŠ¤í…œ ì„¤ì •
    config: Arc<SystemConfig>,
}

impl CrawlingPlanner {
    /// ìƒˆë¡œìš´ CrawlingPlanner ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `status_checker` - ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ê¸°
    /// * `database_analyzer` - ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ê¸°
    /// * `config` - ì‹œìŠ¤í…œ ì„¤ì •
    #[must_use]
    pub fn new(
        status_checker: Arc<dyn StatusChecker>,
        database_analyzer: Arc<dyn DatabaseAnalyzer>,
        config: Arc<SystemConfig>,
    ) -> Self {
        Self {
            status_checker,
            database_analyzer,
            config,
        }
    }
    
    /// í¬ë¡¤ë§ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `crawling_config` - ê¸°ë³¸ í¬ë¡¤ë§ ì„¤ì •
    /// 
    /// # Returns
    /// * `Ok(CrawlingPlan)` - ìˆ˜ë¦½ëœ í¬ë¡¤ë§ ê³„íš
    /// * `Err(ActorError)` - ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨
    pub async fn create_crawling_plan(
        &self,
        crawling_config: &CrawlingConfig,
    ) -> Result<CrawlingPlan, ActorError> {
        // 1. ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸
        let site_status = self.status_checker
            .check_site_status()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Site status check failed: {e}")))?;
        
        // 2. ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let db_analysis = self.database_analyzer
            .analyze_current_state()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Database analysis failed: {e}")))?;
        
        // 3. ìµœì í™”ëœ ê³„íš ìˆ˜ë¦½
        let plan = self.optimize_crawling_strategy(
            crawling_config,
            Box::new(site_status),
            Box::new(db_analysis),
        ).await?;
        
        Ok(plan)
    }
    
    /// ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    /// 
    /// # Returns
    /// * `Ok((SiteStatus, DatabaseAnalysis))` - ë¶„ì„ëœ ì‹œìŠ¤í…œ ìƒíƒœ
    /// * `Err(ActorError)` - ë¶„ì„ ì‹¤íŒ¨
    pub async fn analyze_system_state(&self) -> Result<(crate::domain::services::SiteStatus, DatabaseAnalysis), ActorError> {
        // ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸
        let site_status = self.status_checker
            .check_site_status()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Site status check failed: {e}")))?;
        
        // ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let db_analysis = self.database_analyzer
            .analyze_current_state()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Database analysis failed: {e}")))?;
        
        Ok((site_status, db_analysis))
    }
    
    /// ìºì‹œëœ ì‚¬ì´íŠ¸ ìƒíƒœë¡œ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `cached_site_status` - ìºì‹œëœ ì‚¬ì´íŠ¸ ìƒíƒœ
    /// 
    /// # Returns
    /// * `Ok((SiteStatus, DatabaseAnalysis))` - ë¶„ì„ëœ ì‹œìŠ¤í…œ ìƒíƒœ
    /// * `Err(ActorError)` - ë¶„ì„ ì‹¤íŒ¨
    pub async fn analyze_system_state_with_cache(&self, cached_site_status: Option<crate::domain::services::SiteStatus>) -> Result<(crate::domain::services::SiteStatus, DatabaseAnalysis), ActorError> {
        // ìºì‹œëœ ìƒíƒœê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ í™•ì¸
        let site_status = if let Some(cached) = cached_site_status {
            cached
        } else {
            self.status_checker
                .check_site_status()
                .await
                .map_err(|e| ActorError::CommandProcessingFailed(format!("Site status check failed: {e}")))?
        };
        
        // ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
        let db_analysis = self.database_analyzer
            .analyze_current_state()
            .await
            .map_err(|e| ActorError::CommandProcessingFailed(format!("Database analysis failed: {e}")))?;
        
        Ok((site_status, db_analysis))
    }
    
    /// í¬ë¡¤ë§ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `site_status` - ì‚¬ì´íŠ¸ ìƒíƒœ
    /// * `db_analysis` - ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼
    /// 
    /// # Returns
    /// * `Ok((CrawlingRangeRecommendation, ProcessingStrategy))` - ê²°ì •ëœ ì „ëµ
    /// * `Err(ActorError)` - ì „ëµ ê²°ì • ì‹¤íŒ¨
    pub async fn determine_crawling_strategy(
        &self,
        site_status: &crate::domain::services::SiteStatus,
        db_analysis: &DatabaseAnalysis,
    ) -> Result<(CrawlingRangeRecommendation, ProcessingStrategy), ActorError> {
        // ì‚¬ì´íŠ¸ ìƒíƒœì™€ DB ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í¬ë¡¤ë§ ë²”ìœ„ ì¶”ì²œ
        let is_site_healthy = site_status.is_accessible && site_status.health_score > 0.7;
        let range_recommendation = if is_site_healthy {
            if db_analysis.total_products > 5000 {
                CrawlingRangeRecommendation::Partial(50) // ë¶€ë¶„ í¬ë¡¤ë§
            } else {
                CrawlingRangeRecommendation::Full // ì „ì²´ í¬ë¡¤ë§
            }
        } else {
            CrawlingRangeRecommendation::Partial(20) // ì‚¬ì´íŠ¸ ìƒíƒœê°€ ì¢‹ì§€ ì•Šìœ¼ë©´ ìµœì†Œí•œì˜ í¬ë¡¤ë§
        };
        
        // ì²˜ë¦¬ ì „ëµ ê²°ì •
        let processing_strategy = ProcessingStrategy {
            recommended_batch_size: self.calculate_optimal_batch_size(100),
            recommended_concurrency: self.calculate_optimal_concurrency(),
            should_skip_duplicates: db_analysis.missing_products_count > 100,
            should_update_existing: db_analysis.data_quality_score < 0.8,
            priority_urls: vec![],
        };
        
        Ok((range_recommendation, processing_strategy))
    }
    
    /// ë°°ì¹˜ ì„¤ì •ì„ ìµœì í™”í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `base_config` - ê¸°ë³¸ ë°°ì¹˜ ì„¤ì •
    /// * `total_pages` - ì´ í˜ì´ì§€ ìˆ˜
    /// 
    /// # Returns
    /// * `BatchConfig` - ìµœì í™”ëœ ë°°ì¹˜ ì„¤ì •
    #[must_use]
    pub fn optimize_batch_config(
        &self,
        base_config: &BatchConfig,
        total_pages: u32,
    ) -> BatchConfig {
        let optimal_batch_size = self.calculate_optimal_batch_size(total_pages);
        let optimal_concurrency = self.calculate_optimal_concurrency();
        
        BatchConfig {
            batch_size: optimal_batch_size.min(base_config.batch_size),
            concurrency_limit: optimal_concurrency.min(base_config.concurrency_limit),
            batch_delay_ms: self.calculate_optimal_delay(),
            retry_on_failure: base_config.retry_on_failure,
        }
    }
    
    /// í¬ë¡¤ë§ ì „ëµì„ ìµœì í™”í•©ë‹ˆë‹¤.
    async fn optimize_crawling_strategy(
        &self,
        config: &CrawlingConfig,
        _site_status: Box<dyn std::any::Any + Send>, // SiteStatus trait object workaround
        _db_analysis: Box<dyn std::any::Any + Send>, // DatabaseAnalysis trait object workaround
    ) -> Result<CrawlingPlan, ActorError> {
        // Mock êµ¬í˜„ - ì‹¤ì œë¡œëŠ” site_statusì™€ db_analysisë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™”
        let total_pages = if config.start_page >= config.end_page {
            config.start_page - config.end_page + 1  // ì—­ìˆœ í¬ë¡¤ë§
        } else {
            config.end_page - config.start_page + 1  // ì •ìˆœ í¬ë¡¤ë§
        };
        
        // ğŸ”§ ì—­ìˆœ í¬ë¡¤ë§ ì§€ì›: start_page >= end_pageì¸ ê²½ìš° ë²”ìœ„ë¥¼ ë’¤ì§‘ì–´ì„œ ìƒì„±
        let page_range: Vec<u32> = if config.start_page >= config.end_page {
            // ì—­ìˆœ: 299, 298, 297, 296, 295
            (config.end_page..=config.start_page).rev().collect()
        } else {
            // ì •ìˆœ: start_page..=end_page
            (config.start_page..=config.end_page).collect()
        };
        
        info!("ğŸ”§ CrawlingPlanner page range generation: start={}, end={}, reverse={}, pages={:?}", 
              config.start_page, config.end_page, config.start_page >= config.end_page, page_range);
        
        // ğŸ”§ batch_sizeì— ë”°ë¥¸ ë°°ì¹˜ ë¶„í•  ë¡œì§ êµ¬í˜„
        let batch_size = config.batch_size as usize;
        let batched_pages = if batch_size > 0 && page_range.len() > batch_size {
            page_range.chunks(batch_size).map(|chunk| chunk.to_vec()).collect::<Vec<_>>()
        } else {
            vec![page_range.clone()] // ì‘ì€ ë²”ìœ„ëŠ” í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ
        };
        
        info!("ğŸ“‹ ë°°ì¹˜ ê³„íš ìˆ˜ë¦½: ì´ {}í˜ì´ì§€ë¥¼ {}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  (batch_size={})", 
              page_range.len(), batched_pages.len(), batch_size);
        
        // ğŸ¯ ê° ë°°ì¹˜ë³„ë¡œ ListPageCrawling phase ìƒì„±
        let mut phases = vec![
            CrawlingPhase {
                phase_type: PhaseType::StatusCheck,
                estimated_duration_secs: 30,
                priority: 1,
                pages: vec![], // ìƒíƒœ í™•ì¸ì€ í˜ì´ì§€ë³„ ì²˜ë¦¬ ì—†ìŒ
            },
        ];
        
        // ë°°ì¹˜ë³„ ListPageCrawling phases ì¶”ê°€
        for (batch_idx, batch_pages) in batched_pages.iter().enumerate() {
            phases.push(CrawlingPhase {
                phase_type: PhaseType::ListPageCrawling,
                estimated_duration_secs: (batch_pages.len() * 2) as u64, // í˜ì´ì§€ë‹¹ 2ì´ˆ ì¶”ì •
                priority: 2 + batch_idx as u32, // ë°°ì¹˜ë³„ ìš°ì„ ìˆœìœ„
                pages: batch_pages.clone(),
            });
        }
        
        // ë‚˜ë¨¸ì§€ phases ì¶”ê°€
        phases.extend(vec![
            CrawlingPhase {
                phase_type: PhaseType::ProductDetailCrawling,
                estimated_duration_secs: (total_pages * 10) as u64, // í˜ì´ì§€ë‹¹ 10ì´ˆ ì¶”ì • (ìƒí’ˆ ìƒì„¸)
                priority: 100, // ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ë§ˆì§€ë§‰ì— ì‹¤í–‰
                pages: page_range.clone(),
            },
            CrawlingPhase {
                phase_type: PhaseType::DataValidation,
                estimated_duration_secs: (total_pages / 2) as u64, // ê²€ì¦ì€ ë¹ ë¦„
                priority: 101,
                pages: vec![],
            },
        ]);
        
        let total_estimated_duration_secs = phases.iter().map(|p| p.estimated_duration_secs).sum();
        
        Ok(CrawlingPlan {
            session_id: format!("crawling_{}", uuid::Uuid::new_v4()),
            phases,
            total_estimated_duration_secs,
            optimization_strategy: OptimizationStrategy::Balanced,
            created_at: chrono::Utc::now(),
        })
    }
    
    /// ìµœì  ë°°ì¹˜ í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    fn calculate_optimal_batch_size(&self, total_pages: u32) -> u32 {
        // ì´ í˜ì´ì§€ ìˆ˜ì— ë”°ë¥¸ ì ì‘ì  ë°°ì¹˜ í¬ê¸°
        match total_pages {
            1..=50 => 10,
            51..=200 => 20,
            201..=1000 => 50,
            _ => 100,
        }
    }
    
    /// ìµœì  ë™ì‹œì„± ìˆ˜ì¤€ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    fn calculate_optimal_concurrency(&self) -> u32 {
        // ì‹œìŠ¤í…œ ì„¤ì • ê¸°ë°˜ ë™ì‹œì„± ê³„ì‚°
        self.config.crawling
            .as_ref()
            .and_then(|c| c.default_concurrency_limit)
            .unwrap_or(5)
            .min(10)
    }
    
    /// ìµœì  ì§€ì—° ì‹œê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    fn calculate_optimal_delay(&self) -> u64 {
        // ì„¤ì •ëœ ì§€ì—° ì‹œê°„ ì‚¬ìš©
        self.config.crawling
            .as_ref()
            .and_then(|c| c.request_delay_ms)
            .unwrap_or(1000)
    }
}

/// í¬ë¡¤ë§ ê³„íš
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct CrawlingPlan {
    /// ì„¸ì…˜ ID
    pub session_id: String,
    
    /// í¬ë¡¤ë§ ë‹¨ê³„ë“¤
    pub phases: Vec<CrawlingPhase>,
    
    /// ì´ ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    pub total_estimated_duration_secs: u64,
    
    /// ìµœì í™” ì „ëµ
    pub optimization_strategy: OptimizationStrategy,
    
    /// ê³„íš ìƒì„± ì‹œê°„
    pub created_at: chrono::DateTime<chrono::Utc>,
}

/// í¬ë¡¤ë§ ë‹¨ê³„
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct CrawlingPhase {
    /// ë‹¨ê³„ íƒ€ì…
    pub phase_type: PhaseType,
    
    /// ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
    pub estimated_duration_secs: u64,
    
    /// ìš°ì„ ìˆœìœ„ (ë‚®ì„ìˆ˜ë¡ ë¨¼ì € ì‹¤í–‰)
    pub priority: u32,
    
    /// ì²˜ë¦¬í•  í˜ì´ì§€ ëª©ë¡
    pub pages: Vec<u32>,
}

/// ë‹¨ê³„ íƒ€ì…
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub enum PhaseType {
    /// ìƒíƒœ í™•ì¸
    StatusCheck,
    
    /// ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ í¬ë¡¤ë§
    ListPageCrawling,
    
    /// ìƒí’ˆ ìƒì„¸ í¬ë¡¤ë§
    ProductDetailCrawling,
    
    /// ë°ì´í„° ê²€ì¦
    DataValidation,
    
    /// ë°ì´í„° ì €ì¥
    DataSaving,
}

/// ìµœì í™” ì „ëµ
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub enum OptimizationStrategy {
    /// ì†ë„ ìš°ì„ 
    Speed,
    
    /// ì•ˆì •ì„± ìš°ì„ 
    Stability,
    
    /// ê· í˜•
    Balanced,
    
    /// ë¦¬ì†ŒìŠ¤ ì ˆì•½
    ResourceEfficient,
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    
    // Mock implementations for testing
    struct MockStatusChecker;
    struct MockDatabaseAnalyzer;
    
    #[async_trait::async_trait]
    impl StatusChecker for MockStatusChecker {
        async fn check_site_status(&self, _url: &str) -> Result<Box<dyn std::any::Any>, Box<dyn std::error::Error + Send + Sync>> {
            Ok(Box::new("mock_status"))
        }
    }
    
    #[async_trait::async_trait]
    impl DatabaseAnalyzer for MockDatabaseAnalyzer {
        async fn analyze_current_state(&self) -> anyhow::Result<DatabaseAnalysis> {
            Ok(DatabaseAnalysis {
                total_products: 0,
                unique_products: 0,
                missing_products_count: 0,
                last_update: Some(chrono::Utc::now()),
                missing_fields_analysis: FieldAnalysis {
                    missing_company: 0,
                    missing_model: 0,
                    missing_matter_version: 0,
                    missing_connectivity: 0,
                    missing_certification_date: 0,
                },
                data_quality_score: 1.0,
            })
        }
        
        async fn recommend_processing_strategy(&self) -> anyhow::Result<ProcessingStrategy> {
            Ok(ProcessingStrategy::default())
        }
        
        async fn analyze_duplicates(&self) -> anyhow::Result<DuplicateAnalysis> {
            Ok(DuplicateAnalysis {
                duplicate_pairs: vec![],
                total_duplicates: 0,
                confidence_scores: vec![],
            })
        }
    }
    
    #[tokio::test]
    async fn test_crawling_planner_creation() {
        let status_checker = Arc::new(MockStatusChecker) as Arc<dyn StatusChecker>;
        let database_analyzer = Arc::new(MockDatabaseAnalyzer) as Arc<dyn DatabaseAnalyzer>;
        let config = Arc::new(SystemConfig::default());
        
        let planner = CrawlingPlanner::new(status_checker, database_analyzer, config);
        
        // í”Œë˜ë„ˆê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert_eq!(planner.config.crawling.default_concurrency_limit, 10);
    }
    
    #[test]
    fn test_batch_config_optimization() {
        let status_checker = Arc::new(MockStatusChecker) as Arc<dyn StatusChecker>;
        let database_analyzer = Arc::new(MockDatabaseAnalyzer) as Arc<dyn DatabaseAnalyzer>;
        let config = Arc::new(SystemConfig::default());
        
        let planner = CrawlingPlanner::new(status_checker, database_analyzer, config);
        
        let base_config = BatchConfig {
            batch_size: 100,
            concurrency_limit: 20,
            batch_delay_ms: 1000,
            retry_on_failure: true,
        };
        
        let optimized = planner.optimize_batch_config(&base_config, 150);
        
        // ìµœì í™”ëœ ì„¤ì •ì´ ê¸°ë³¸ê°’ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì€ì§€ í™•ì¸
        assert!(optimized.batch_size <= base_config.batch_size);
        assert!(optimized.concurrency_limit <= base_config.concurrency_limit);
    }
    
    #[test]
    fn test_optimal_batch_size_calculation() {
        let status_checker = Arc::new(MockStatusChecker) as Arc<dyn StatusChecker>;
        let database_analyzer = Arc::new(MockDatabaseAnalyzer) as Arc<dyn DatabaseAnalyzer>;
        let config = Arc::new(SystemConfig::default());
        
        let planner = CrawlingPlanner::new(status_checker, database_analyzer, config);
        
        assert_eq!(planner.calculate_optimal_batch_size(30), 10);
        assert_eq!(planner.calculate_optimal_batch_size(100), 20);
        assert_eq!(planner.calculate_optimal_batch_size(500), 50);
        assert_eq!(planner.calculate_optimal_batch_size(2000), 100);
    }
}
