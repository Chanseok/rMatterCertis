use std::sync::Arc;
use anyhow::{Result, anyhow};
use tracing::{info, warn};

use crate::domain::services::{
    StatusChecker, DatabaseAnalyzer, SiteStatus, DatabaseAnalysis, ProcessingStrategy
};
use crate::domain::services::crawling_services::CrawlingRangeRecommendation;
use crate::new_architecture::config::SystemConfig;

/// í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½ ì„œë¹„ìŠ¤
///
/// ì—¬ëŸ¬ ì„œë¹„ìŠ¤(StatusChecker, DatabaseAnalyzer)ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬
/// ìµœì ì˜ í¬ë¡¤ë§ ì‹¤í–‰ ê³„íš(ì–´ë–¤ í˜ì´ì§€ë¥¼, ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ)ì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤.
/// 
/// **ì„¤ê³„ ì›ì¹™**: ëª¨ë“  í•˜ë“œì½”ë”© ê°’ì„ ì œê±°í•˜ê³  ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ëŠ¥í˜• ìµœì í™”
pub struct CrawlingPlanner {
    status_checker: Arc<dyn StatusChecker>,
    db_analyzer: Arc<dyn DatabaseAnalyzer>,
    config: Arc<SystemConfig>,
}

impl CrawlingPlanner {
    /// CrawlingPlannerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    /// 
    /// # Arguments
    /// * `status_checker` - ì‚¬ì´íŠ¸ ìƒíƒœ ë¶„ì„ ì„œë¹„ìŠ¤
    /// * `db_analyzer` - ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì„œë¹„ìŠ¤  
    /// * `config` - ì‹œìŠ¤í…œ ì„¤ì • (ëª¨ë“  í•˜ë“œì½”ë”© ê°’ì„ ëŒ€ì²´)
    pub fn new(
        status_checker: Arc<dyn StatusChecker>,
        db_analyzer: Arc<dyn DatabaseAnalyzer>,
        config: Arc<SystemConfig>,
    ) -> Self {
        info!("ğŸ—ï¸ [CrawlingPlanner] Initializing with config-based intelligent system");
        Self {
            status_checker,
            db_analyzer,
            config,
        }
    }

    /// ì‹œìŠ¤í…œì˜ í˜„ì¬ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    ///
    /// # Arguments
    /// * `cached_site_status` - ì´ë¯¸ í™•ì¸ëœ ì‚¬ì´íŠ¸ ìƒíƒœ (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
    ///
    /// # Returns
    /// (SiteStatus, DatabaseAnalysis) íŠœí”Œ
    pub async fn analyze_system_state_with_cache(&self, cached_site_status: SiteStatus) -> Result<(SiteStatus, DatabaseAnalysis)> {
        info!("ğŸ§  [CrawlingPlanner] Starting intelligent system state analysis with cached site status...");

        // 1. ìºì‹œëœ ì‚¬ì´íŠ¸ ìƒíƒœ ì‚¬ìš© (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
        info!("âœ… [CrawlingPlanner] Using cached site status: {} pages found", cached_site_status.total_pages);

        // 2. ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ë¶„ì„
        let db_analysis = self.db_analyzer.analyze_current_state().await?;
        info!("âœ… [CrawlingPlanner] Database analysis complete: {} products in DB", db_analysis.total_products);

        Ok((cached_site_status, db_analysis))
    }

    /// ì‹œìŠ¤í…œì˜ í˜„ì¬ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤. (ë ˆê±°ì‹œ í˜¸í™˜ìš©)
    ///
    /// # Returns
    /// (SiteStatus, DatabaseAnalysis) íŠœí”Œ
    pub async fn analyze_system_state(&self) -> Result<(SiteStatus, DatabaseAnalysis)> {
        warn!("âš ï¸ [CrawlingPlanner] Using legacy analyze_system_state - consider using analyze_system_state_with_cache for better performance");

        // 1. ì‚¬ì´íŠ¸ ìƒíƒœ ë¶„ì„
        let site_status = self.status_checker.check_site_status().await?;
        info!("âœ… [CrawlingPlanner] Site status analysis complete: {} pages found", site_status.total_pages);

        // 2. ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ë¶„ì„
        let db_analysis = self.db_analyzer.analyze_current_state().await?;
        info!("âœ… [CrawlingPlanner] Database analysis complete: {} products in DB", db_analysis.total_products);

        Ok((site_status, db_analysis))
    }

    /// ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í¬ë¡¤ë§ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
    /// 
    /// **ì„¤ê³„ ì›ì¹™**: í•˜ë“œì½”ë”© ê°’ ì™„ì „ ì œê±°, ì„¤ì • íŒŒì¼ ê¸°ë°˜ ì§€ëŠ¥í˜• ìµœì í™”
    ///
    /// # Arguments
    /// * `site_status` - ì‚¬ì´íŠ¸ í˜„ì¬ ìƒíƒœ
    /// * `db_analysis` - ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼
    ///
    /// # Returns
    /// (CrawlingRangeRecommendation, ProcessingStrategy) íŠœí”Œ
    pub async fn determine_crawling_strategy(
        &self,
        site_status: &SiteStatus,
        db_analysis: &DatabaseAnalysis,
    ) -> Result<(CrawlingRangeRecommendation, ProcessingStrategy)> {
        info!("ğŸ¯ [CrawlingPlanner] Determining config-based intelligent crawling strategy...");

        // 1. í¬ë¡¤ë§ ë²”ìœ„ ì¶”ì²œ ê°€ì ¸ì˜¤ê¸° (ì§€ëŠ¥í˜• ë¶„ì„)
        let range_recommendation = self.status_checker
            .calculate_crawling_range_recommendation(site_status, db_analysis)
            .await?;

        // 2. ì„¤ì • ê¸°ë°˜ ì²˜ë¦¬ ì „ëµ ê²°ì • (í•˜ë“œì½”ë”© ê°’ ì™„ì „ ì œê±°)
        let processing_strategy = self.determine_processing_strategy_from_config(
            site_status,
            db_analysis,
        ).await?;

        info!(
            "ğŸ“‹ [CrawlingPlanner] Strategy determined: {:?}, batch_size: {}, concurrency: {}",
            range_recommendation, 
            processing_strategy.recommended_batch_size,
            processing_strategy.recommended_concurrency
        );

        Ok((range_recommendation, processing_strategy))
    }

    /// ì„¤ì • íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì ì˜ ì²˜ë¦¬ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤.
    /// 
    /// **í•µì‹¬**: ëª¨ë“  ê°’ì„ ì„¤ì •ì—ì„œ ì½ë˜, í˜„ì¬ ìƒí™©ì— ë§ê²Œ ì§€ëŠ¥í˜• ì¡°ì •
    async fn determine_processing_strategy_from_config(
        &self,
        site_status: &SiteStatus,
        db_analysis: &DatabaseAnalysis,
    ) -> Result<ProcessingStrategy> {
        info!("âš™ï¸ [CrawlingPlanner] Determining processing strategy from config...");
        
        // ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ì½ê¸°
        let base_batch_size = self.config.performance.batch_sizes.initial_size;
        let base_concurrency = self.config.performance.concurrency.max_concurrent_batches;
        
        info!("ğŸ“Š [CrawlingPlanner] Base config values: batch_size={}, concurrency={}", 
              base_batch_size, base_concurrency);
        
        // í˜„ì¬ ìƒí™©ì— ë§ëŠ” ì§€ëŠ¥í˜• ì¡°ì •
        let adjusted_batch_size = self.calculate_optimal_batch_size(
            base_batch_size,
            db_analysis.total_products.into(),
            site_status.total_pages,
        );
        
        let adjusted_concurrency = self.calculate_optimal_concurrency(
            base_concurrency,
            site_status.total_pages,
            db_analysis.total_products.into(),
        );
        
        info!("ğŸ¯ [CrawlingPlanner] Intelligent adjustments: batch_size: {} -> {}, concurrency: {} -> {}", 
              base_batch_size, adjusted_batch_size, base_concurrency, adjusted_concurrency);
        
        // ì¤‘ë³µ ì²˜ë¦¬ ì „ëµ (ì„¤ì • + ë¶„ì„ ê¸°ë°˜)
        let should_skip_duplicates = self.should_skip_duplicates_based_on_config(db_analysis);
        
        Ok(ProcessingStrategy {
            recommended_batch_size: adjusted_batch_size,
            recommended_concurrency: adjusted_concurrency,
            should_skip_duplicates,
            should_update_existing: self.config.system.update_existing_items,
            priority_urls: vec![], // í–¥í›„ ì„¤ì •ì—ì„œ ìš°ì„ ìˆœìœ„ URL ëª©ë¡ ì½ê¸°
        })
    }

    /// ì„¤ì • ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
    fn calculate_optimal_batch_size(
        &self,
        base_batch_size: u32,
        total_products_in_db: u64,
        total_pages: u32,
    ) -> u32 {
        // ì„¤ì •ì—ì„œ ìµœì†Œ/ìµœëŒ€ê°’ ì½ê¸°
        let min_batch = self.config.performance.batch_sizes.min_size;
        let max_batch = self.config.performance.batch_sizes.max_size;
        
        // ì§€ëŠ¥í˜• ì¡°ì •: DB í¬ê¸°ì™€ í˜ì´ì§€ ìˆ˜ë¥¼ ê³ ë ¤í•œ ìµœì í™”
        let adjusted_size = if total_products_in_db < 1000 {
            // ì‘ì€ DB: ë” í° ë°°ì¹˜ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€
            (base_batch_size as f32 * self.config.performance.batch_sizes.small_db_multiplier) as u32
        } else if total_pages > 100 {
            // ë§ì€ í˜ì´ì§€: ì‘ì€ ë°°ì¹˜ë¡œ ì•ˆì •ì„± í™•ë³´
            (base_batch_size as f32 * self.config.performance.batch_sizes.large_site_multiplier) as u32
        } else {
            base_batch_size
        };
        
        // ì„¤ì •ëœ ë²”ìœ„ ë‚´ë¡œ ì œí•œ
        adjusted_size.clamp(min_batch, max_batch)
    }

    /// ì„¤ì • ê¸°ë°˜ ìµœì  ë™ì‹œì„± ê³„ì‚°
    fn calculate_optimal_concurrency(
        &self,
        base_concurrency: u32,
        total_pages: u32,
        total_products_in_db: u64,
    ) -> u32 {
        let min_concurrency = self.config.performance.concurrency.min_concurrent_batches;
        let max_concurrency = self.config.performance.concurrency.max_concurrent_batches;
        
        // ì§€ëŠ¥í˜• ì¡°ì •: ì‚¬ì´íŠ¸ í¬ê¸°ì™€ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ê³ ë ¤
        let adjusted_concurrency = if total_pages > 100 && total_products_in_db < 10000 {
            // í° ì‚¬ì´íŠ¸ + ì‘ì€ DB: ë†’ì€ ë™ì‹œì„± ê°€ëŠ¥
            (base_concurrency as f32 * self.config.performance.concurrency.high_load_multiplier) as u32
        } else if total_products_in_db > 50000 {
            // í° DB: ë™ì‹œì„± ì œí•œìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
            (base_concurrency as f32 * self.config.performance.concurrency.stable_load_multiplier) as u32
        } else {
            base_concurrency
        };
        
        adjusted_concurrency.clamp(min_concurrency, max_concurrency)
    }

    /// ì„¤ì •ê³¼ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ìŠ¤í‚µ ì—¬ë¶€ ê²°ì •
    fn should_skip_duplicates_based_on_config(&self, db_analysis: &DatabaseAnalysis) -> bool {
        // ì„¤ì •ì—ì„œ ì¤‘ë³µ ì²˜ë¦¬ ì„ê³„ê°’ ì½ê¸°
        let duplicate_threshold = self.config.performance.deduplication.skip_threshold_percentage;
        
        // í˜„ì¬ ì¤‘ë³µë¥  ê³„ì‚°
        let current_duplicate_rate = if db_analysis.total_products > 0 {
            (db_analysis.duplicate_count as f64 / db_analysis.total_products as f64) * 100.0
        } else {
            0.0
        };
        
        info!("ğŸ” [CrawlingPlanner] Duplicate analysis: rate={:.2}%, threshold={:.2}%, total={}, duplicates={}", 
              current_duplicate_rate, duplicate_threshold, db_analysis.total_products, db_analysis.duplicate_count);
        
        // ì„¤ì •ëœ ì„ê³„ê°’ê³¼ ë¹„êµí•˜ì—¬ ê²°ì •
        let should_skip = current_duplicate_rate > duplicate_threshold;
        info!("ğŸ¯ [CrawlingPlanner] Skip duplicates decision: {}", should_skip);
        
        should_skip
    }
}