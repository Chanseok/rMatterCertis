//! ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ì™€ OneShot Actor ì‹œìŠ¤í…œ í†µí•©
//! Modern Rust 2024 ì¤€ìˆ˜: ê¸°ì¡´ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ë¥¼ OneShot Actor íŒ¨í„´ìœ¼ë¡œ ì—°ë™

#![warn(clippy::all, clippy::pedantic, clippy::nursery)]
#![deny(clippy::unwrap_used, clippy::expect_used, clippy::panic)]

use std::sync::Arc;
use std::time::{Duration, Instant};

use tokio_util::sync::CancellationToken;
use tracing::{info, warn, error, debug};
use anyhow::Result;

use crate::domain::services::crawling_services::{
    StatusChecker, ProductListCollector, ProductDetailCollector, DatabaseAnalyzer,
    SiteStatus, CrawlingRangeRecommendation
};
use crate::domain::product_url::ProductUrl;
use crate::domain::product::ProductDetail;
use crate::infrastructure::crawling_service_impls::{StatusCheckerImpl, ProductListCollectorImpl, CollectorConfig};
use crate::infrastructure::config::AppConfig;
use crate::infrastructure::{HttpClient, MatterDataExtractor, IntegratedProductRepository};
use crate::new_architecture::actor_system::{
    StageResult, StageSuccessResult, StageError, CollectionMetrics, ProcessingMetrics,
    FailedItem
};
use crate::new_architecture::channel_types::{StageType, StageItem};
use crate::new_architecture::system_config::SystemConfig;

/// ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ì™€ OneShot Actor ì‹œìŠ¤í…œì„ ì—°ê²°í•˜ëŠ” í†µí•© ì„œë¹„ìŠ¤
pub struct CrawlingIntegrationService {
    status_checker: Arc<dyn StatusChecker>,
    list_collector: Arc<dyn ProductListCollector>,
    detail_collector: Arc<dyn ProductDetailCollector>,
    database_analyzer: Arc<dyn DatabaseAnalyzer>,
    product_repository: Arc<IntegratedProductRepository>,
    config: Arc<SystemConfig>,
    app_config: AppConfig,
}

impl CrawlingIntegrationService {
    /// ê¸°ì¡´ ì¸í”„ë¼ë¥¼ ì‚¬ìš©í•˜ì—¬ í†µí•© ì„œë¹„ìŠ¤ ìƒì„±
    pub async fn new(
        config: Arc<SystemConfig>,
        app_config: AppConfig,
    ) -> Result<Self> {
        // ê¸°ì¡´ ì¸í”„ë¼ ì„œë¹„ìŠ¤ë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ íŒ¨í„´ ì¬ì‚¬ìš©)
        let http_client = Arc::new(tokio::sync::Mutex::new(
            HttpClient::create_from_global_config()?
        ));
        
        let data_extractor = Arc::new(
            MatterDataExtractor::new()?
        );
        
        // ì‹¤ì œ DB ì—°ê²° URL ê°€ì ¸ì˜¤ê¸°
        let database_url = crate::commands::crawling_v4::get_database_url_v4()
            .map_err(|e| anyhow::anyhow!("Failed to get database URL: {}", e))?;
        
        // DB ì—°ê²° í’€ ìƒì„±
        let db_pool = sqlx::SqlitePool::connect(&database_url).await
            .map_err(|e| anyhow::anyhow!("Failed to connect to database: {}", e))?;
        
        // ProductRepository ì´ˆê¸°í™” (DB ì—°ê²° í¬í•¨)
        let product_repository = Arc::new(
            IntegratedProductRepository::new(db_pool)
        );
        
        // StatusChecker ìƒì„± (ProductRepository í¬í•¨)
        let http_client_for_checker = http_client.lock().await.clone();
        let data_extractor_for_checker = (*data_extractor).clone();
        let status_checker_impl = Arc::new(StatusCheckerImpl::with_product_repo(
            http_client_for_checker,
            data_extractor_for_checker,
            app_config.clone(),
            product_repository.clone(),
        ));
        let status_checker: Arc<dyn StatusChecker> = status_checker_impl.clone();
        
        // DatabaseAnalyzerëŠ” StatusCheckerImpl ì¬ì‚¬ìš©  
        let database_analyzer: Arc<dyn DatabaseAnalyzer> = status_checker_impl.clone();
        
        // ProductListCollector ìƒì„±
        let collector_config = CollectorConfig {
            batch_size: app_config.user.batch.batch_size,
            max_concurrent: app_config.user.max_concurrent_requests,
            concurrency: app_config.user.max_concurrent_requests,
            delay_between_requests: Duration::from_millis(app_config.user.request_delay_ms),
            delay_ms: app_config.user.request_delay_ms,
            retry_attempts: 3,
            retry_max: 3,
        };
        
        let list_collector: Arc<dyn ProductListCollector> = Arc::new(ProductListCollectorImpl::new(
            Arc::new(HttpClient::create_from_global_config()?),  // ğŸ”¥ Mutex ì œê±°
            data_extractor.clone(),
            collector_config.clone(),
            status_checker_impl.clone(),
        ));
        
        // ProductDetailCollectorëŠ” ProductListCollectorImpl ì¬ì‚¬ìš© (ê¸°ì¡´ íŒ¨í„´)
        let detail_collector: Arc<dyn ProductDetailCollector> = Arc::new(ProductListCollectorImpl::new(
            Arc::new(HttpClient::create_from_global_config()?),  // ğŸ”¥ Mutex ì œê±°
            data_extractor.clone(),
            collector_config.clone(),
            status_checker_impl.clone(),
        ));
        
        Ok(Self {
            status_checker,
            list_collector,
            detail_collector,
            database_analyzer,
            product_repository,
            config,
            app_config,
        })
    }
    
    /// ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ë‹¨ê³„ ì‹¤í–‰ (OneShot ê²°ê³¼ ë°˜í™˜)
    pub async fn execute_list_collection_stage(
        &self,
        pages: Vec<u32>,
        concurrency_limit: u32,
        cancellation_token: CancellationToken,
    ) -> StageResult {
        let start_time = Instant::now();
        
        info!(
            pages_count = pages.len(),
            concurrency_limit = concurrency_limit,
            "Starting real list collection stage"
        );
        
        // ì„¤ì •ì—ì„œ ë°°ì¹˜ í¬ê¸° ë¡œë“œ
        let batch_size = self.config.performance.batch_sizes.initial_size.min(50) as usize;
        let mut all_collected_urls = Vec::new();
        let mut successful_pages = Vec::new();
        let mut failed_pages = Vec::new();
        
        // í˜ì´ì§€ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for chunk in pages.chunks(batch_size) {
            // ì·¨ì†Œ í™•ì¸
            if cancellation_token.is_cancelled() {
                return StageResult::FatalError {
                    error: StageError::ValidationError {
                        message: "Collection cancelled by user".to_string(),
                    },
                    stage_id: "list-collection".to_string(),
                    context: "User cancellation".to_string(),
                };
            }
            
            match self.collect_page_batch_with_retry(chunk, cancellation_token.clone()).await {
                Ok(batch_result) => {
                    for (page, urls) in batch_result {
                        if urls.is_empty() {
                            failed_pages.push(page);
                        } else {
                            all_collected_urls.extend(urls);
                            successful_pages.push(page);
                        }
                    }
                }
                Err(e) => {
                    error!(error = %e, "Batch collection failed");
                    failed_pages.extend(chunk.iter());
                }
            }
        }
        
        let elapsed = start_time.elapsed();
        let total_pages = pages.len() as u32;
        let successful_count = successful_pages.len() as u32;
        let failed_count = failed_pages.len() as u32;
        
        // ê²°ê³¼ ë¶„ë¥˜
        if successful_count == 0 {
            StageResult::FatalError {
                error: StageError::NetworkError {
                    message: format!("All {} pages failed to collect", total_pages),
                },
                stage_id: "list-collection".to_string(),
                context: "Complete collection failure".to_string(),
            }
        } else if failed_count == 0 {
            StageResult::Success(StageSuccessResult {
                processed_items: total_pages,
                stage_duration_ms: elapsed.as_millis() as u64,
                collection_metrics: Some(CollectionMetrics {
                    total_items: total_pages,
                    successful_items: successful_count,
                    failed_items: failed_count,
                    duration_ms: elapsed.as_millis() as u64,
                    avg_response_time_ms: if successful_count > 0 {
                        elapsed.as_millis() as u64 / successful_count as u64
                    } else { 0 },
                    success_rate: 100.0,
                }),
                processing_metrics: None,
            })
        } else {
            StageResult::PartialSuccess {
                success_items: StageSuccessResult {
                    processed_items: successful_count,
                    stage_duration_ms: elapsed.as_millis() as u64,
                    collection_metrics: Some(CollectionMetrics {
                        total_items: total_pages,
                        successful_items: successful_count,
                        failed_items: failed_count,
                        duration_ms: elapsed.as_millis() as u64,
                        avg_response_time_ms: if successful_count > 0 {
                            elapsed.as_millis() as u64 / successful_count as u64
                        } else { 0 },
                        success_rate: (successful_count as f64 / total_pages as f64) * 100.0,
                    }),
                    processing_metrics: None,
                },
                failed_items: failed_pages.into_iter().map(|page| FailedItem {
                    item_id: page.to_string(),
                    error: StageError::NetworkError {
                        message: "Page collection failed".to_string(),
                    },
                    retry_count: 0,
                    last_attempt_ms: std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap_or_default()
                        .as_millis() as u64,
                }).collect(),
                stage_id: "list-collection".to_string(),
            }
        }
    }
    
    /// ì‹¤ì œ ìƒì„¸ ìˆ˜ì§‘ ë‹¨ê³„ ì‹¤í–‰ (OneShot ê²°ê³¼ ë°˜í™˜)
    pub async fn execute_detail_collection_stage(
        &self,
        product_urls: Vec<ProductUrl>,
        concurrency_limit: u32,
        cancellation_token: CancellationToken,
    ) -> StageResult {
        let start_time = Instant::now();
        
        info!(
            urls_count = product_urls.len(),
            concurrency_limit = concurrency_limit,
            "Starting real detail collection stage"
        );
        
        // ì„¤ì •ì—ì„œ ë°°ì¹˜ í¬ê¸° ë¡œë“œ
        let batch_size = self.config.performance.batch_sizes.initial_size.min(20) as usize;
        let mut all_collected_details = Vec::new();
        let mut successful_urls = Vec::new();
        let mut failed_urls = Vec::new();
        
        // URLì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for chunk in product_urls.chunks(batch_size) {
            // ì·¨ì†Œ í™•ì¸
            if cancellation_token.is_cancelled() {
                return StageResult::FatalError {
                    error: StageError::ValidationError {
                        message: "Detail collection cancelled by user".to_string(),
                    },
                    stage_id: "detail-collection".to_string(),
                    context: "User cancellation".to_string(),
                };
            }
            
            match self.collect_detail_batch_with_retry(chunk, cancellation_token.clone()).await {
                Ok(batch_details) => {
                    for detail in batch_details {
                        all_collected_details.push(detail.clone());
                        successful_urls.push(detail.url.clone());
                    }
                }
                Err(e) => {
                    error!(error = %e, "Detail batch collection failed");
                    failed_urls.extend(chunk.iter().map(|url| url.url.clone()));
                }
            }
        }
        
        let elapsed = start_time.elapsed();
        let total_urls = product_urls.len() as u32;
        let successful_count = successful_urls.len() as u32;
        let failed_count = failed_urls.len() as u32;
        
        // ê²°ê³¼ ë¶„ë¥˜
        if successful_count == 0 {
            StageResult::FatalError {
                error: StageError::NetworkError {
                    message: format!("All {} product details failed to collect", total_urls),
                },
                stage_id: "detail-collection".to_string(),
                context: "Complete detail collection failure".to_string(),
            }
        } else if failed_count == 0 {
            StageResult::Success(StageSuccessResult {
                processed_items: total_urls,
                stage_duration_ms: elapsed.as_millis() as u64,
                collection_metrics: None,
                processing_metrics: Some(ProcessingMetrics {
                    total_processed: total_urls,
                    successful_saves: successful_count,
                    failed_saves: failed_count,
                    duration_ms: elapsed.as_millis() as u64,
                    avg_processing_time_ms: if successful_count > 0 {
                        elapsed.as_millis() as u64 / successful_count as u64
                    } else { 0 },
                    success_rate: 100.0,
                }),
            })
        } else {
            StageResult::PartialSuccess {
                success_items: StageSuccessResult {
                    processed_items: successful_count,
                    stage_duration_ms: elapsed.as_millis() as u64,
                    collection_metrics: None,
                    processing_metrics: Some(ProcessingMetrics {
                        total_processed: total_urls,
                        successful_saves: successful_count,
                        failed_saves: failed_count,
                        duration_ms: elapsed.as_millis() as u64,
                        avg_processing_time_ms: if successful_count > 0 {
                            elapsed.as_millis() as u64 / successful_count as u64
                        } else { 0 },
                        success_rate: (successful_count as f64 / total_urls as f64) * 100.0,
                    }),
                },
                failed_items: failed_urls.into_iter().map(|url| FailedItem {
                    item_id: url,
                    error: StageError::NetworkError {
                        message: "Product detail collection failed".to_string(),
                    },
                    retry_count: 0,
                    last_attempt_ms: std::time::SystemTime::now()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap_or_default()
                        .as_millis() as u64,
                }).collect(),
                stage_id: "detail-collection".to_string(),
            }
        }
    }
    
    /// ì‚¬ì´íŠ¸ ìƒíƒœ ë¶„ì„ ì‹¤í–‰
    pub async fn execute_site_analysis(&self) -> Result<SiteStatus> {
        info!("Starting real site status analysis");
        
        self.status_checker.check_site_status().await
    }
    
    /// í¬ë¡¤ë§ ë²”ìœ„ ê¶Œì¥ì‚¬í•­ ê³„ì‚°
    pub async fn calculate_crawling_recommendation(&self) -> Result<CrawlingRangeRecommendation> {
        info!("Calculating real crawling range recommendation with actual DB data");
        
        let site_status = self.status_checker.check_site_status().await?;
        
        // ì‹¤ì œ DB ìƒíƒœ í™•ì¸
        let db_stats = self.product_repository.get_database_statistics().await?;
        info!(
            total_products = db_stats.total_products,
            active_products = db_stats.active_products,
            "Real DB stats retrieved"
        );
        
        // DB ë¶„ì„ì„ ìœ„í•œ ë¶„ì„ ê²°ê³¼ ìƒì„± (ì‹¤ì œ DB ë°ì´í„° ê¸°ë°˜)
        let db_analysis = crate::domain::services::crawling_services::DatabaseAnalysis {
            total_products: db_stats.total_products as u32,
            unique_products: db_stats.active_products as u32,
            duplicate_count: 0,
            last_update: db_stats.last_crawled,
            missing_fields_analysis: crate::domain::services::crawling_services::FieldAnalysis {
                missing_company: 0,
                missing_model: 0,
                missing_matter_version: 0,
                missing_connectivity: 0,
                missing_certification_date: 0,
            },
            data_quality_score: 0.8,
        };
        
        self.status_checker.calculate_crawling_range_recommendation(&site_status, &db_analysis).await
    }
    
    /// ë°°ì¹˜ í˜ì´ì§€ ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)
    async fn collect_page_batch_with_retry(
        &self,
        pages: &[u32],
        cancellation_token: CancellationToken,
    ) -> Result<Vec<(u32, Vec<ProductUrl>)>> {
        let mut results = Vec::new();
        
        // ê°œë³„ í˜ì´ì§€ ìˆ˜ì§‘
        for &page in pages {
            if cancellation_token.is_cancelled() {
                break;
            }
            
            match self.collect_single_page_with_retry(page, 3).await {
                Ok(urls) => {
                    results.push((page, urls));
                }
                Err(e) => {
                    warn!(page = page, error = %e, "Failed to collect page after retries");
                    results.push((page, Vec::new()));
                }
            }
        }
        
        Ok(results)
    }
    
    /// ë‹¨ì¼ í˜ì´ì§€ ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)
    async fn collect_single_page_with_retry(
        &self,
        page: u32,
        max_retries: u32,
    ) -> Result<Vec<ProductUrl>> {
        // ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸í•˜ì—¬ ì‹¤ì œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        let site_status = self.status_checker.check_site_status().await?;
        let mut last_error = None;
        
        for attempt in 0..=max_retries {
            match self.list_collector.collect_single_page(
                page,
                site_status.total_pages,
                site_status.products_on_last_page
            ).await {
                Ok(urls) => {
                    if attempt > 0 {
                        info!(page = page, attempt = attempt, "Page collection succeeded after retry");
                    }
                    return Ok(urls);
                }
                Err(e) => {
                    last_error = Some(e);
                    if attempt < max_retries {
                        let delay = Duration::from_millis(1000 * (2_u64.pow(attempt)));
                        debug!(page = page, attempt = attempt, delay_ms = delay.as_millis(), "Retrying page collection");
                        tokio::time::sleep(delay).await;
                    }
                }
            }
        }
        
        Err(last_error.unwrap_or_else(|| anyhow::anyhow!("Unknown error")))
    }
    
    /// ë°°ì¹˜ ìƒì„¸ ìˆ˜ì§‘ (ì¬ì‹œë„ í¬í•¨)
    async fn collect_detail_batch_with_retry(
        &self,
        urls: &[ProductUrl],
        cancellation_token: CancellationToken,
    ) -> Result<Vec<ProductDetail>> {
        // ì·¨ì†Œ í† í°ê³¼ í•¨ê»˜ ì‹¤ì œ ìƒì„¸ ìˆ˜ì§‘ í˜¸ì¶œ
        self.detail_collector.collect_details_with_cancellation(urls, cancellation_token).await
    }
}

/// StageActorì—ì„œ ì‹¤ì œ í¬ë¡¤ë§ ì„œë¹„ìŠ¤ ì‚¬ìš©ì„ ìœ„í•œ ë„ìš°ë¯¸ êµ¬ì¡°ì²´
pub struct RealCrawlingStageExecutor {
    integration_service: Arc<CrawlingIntegrationService>,
}

impl RealCrawlingStageExecutor {
    pub fn new(integration_service: Arc<CrawlingIntegrationService>) -> Self {
        Self {
            integration_service,
        }
    }
    
    /// StageActorì—ì„œ í˜¸ì¶œí•  ì‹¤ì œ ë‹¨ê³„ ì‹¤í–‰ ë©”ì„œë“œ
    pub async fn execute_stage(
        &self,
        stage_type: StageType,
        items: Vec<StageItem>,
        concurrency_limit: u32,
        cancellation_token: CancellationToken,
    ) -> StageResult {
        match stage_type {
            StageType::ListCollection | StageType::Collection => {
                let pages: Vec<u32> = items.into_iter()
                    .filter_map(|item| match item {
                        StageItem::Page(page) => Some(page),
                        _ => None,
                    })
                    .collect();
                
                self.integration_service.execute_list_collection_stage(
                    pages,
                    concurrency_limit,
                    cancellation_token,
                ).await
            }
            
            StageType::DetailCollection | StageType::Processing => {
                // í˜„ì¬ëŠ” URL ì•„ì´í…œì´ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ì²˜ë¦¬
                // ì‹¤ì œë¡œëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ìˆ˜ì§‘ëœ URLì„ ë°›ì•„ì•¼ í•¨
                let urls = Vec::new(); // TODO: ì‹¤ì œ URL ì „ë‹¬ êµ¬í˜„
                
                self.integration_service.execute_detail_collection_stage(
                    urls,
                    concurrency_limit,
                    cancellation_token,
                ).await
            }
            
            StageType::DataValidation => {
                // ë°ì´í„° ê²€ì¦ ë¡œì§ (í˜„ì¬ëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬)
                StageResult::Success(StageSuccessResult {
                    processed_items: items.len() as u32,
                    stage_duration_ms: 100,
                    collection_metrics: None,
                    processing_metrics: None,
                })
            }
            
            StageType::DatabaseSave => {
                // ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë¡œì§ (í˜„ì¬ëŠ” ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬)
                StageResult::Success(StageSuccessResult {
                    processed_items: items.len() as u32,
                    stage_duration_ms: 200,
                    collection_metrics: None,
                    processing_metrics: None,
                })
            }
        }
    }
}
