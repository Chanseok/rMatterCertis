//! Event types for real-time communication between backend and frontend
//! 
//! This module defines all event types that will be emitted from the Rust backend
//! to the SolidJS frontend for real-time updates during crawling operations.

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use chrono::{DateTime, Utc};

/// Represents the current stage of the crawling process
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum CrawlingStage {
    /// System is idle, no crawling in progress
    Idle,
    /// Checking site status and accessibility
    StatusCheck,
    /// Analyzing current database state
    DatabaseAnalysis,
    /// Discovering total number of pages to crawl
    TotalPages,
    /// Collecting product list from pages
    ProductList,
    /// Collecting detailed product information
    ProductDetails,
    /// Saving data to database
    DatabaseSave,
    /// Legacy database stage (keeping for backward compatibility)
    Database,
}

impl std::fmt::Display for CrawlingStage {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            CrawlingStage::Idle => write!(f, "ëŒ€ê¸°"),
            CrawlingStage::StatusCheck => write!(f, "ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸"),
            CrawlingStage::DatabaseAnalysis => write!(f, "ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„"),
            CrawlingStage::TotalPages => write!(f, "ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸"),
            CrawlingStage::ProductList => write!(f, "ì œí’ˆ ëª©ë¡ ìˆ˜ì§‘"),
            CrawlingStage::ProductDetails => write!(f, "ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘"),
            CrawlingStage::DatabaseSave => write!(f, "ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"),
            CrawlingStage::Database => write!(f, "ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥"), // ë ˆê±°ì‹œ í˜¸í™˜ì„±
        }
    }
}

/// Overall status of the crawling operation
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum CrawlingStatus {
    /// No crawling operation is running
    Idle,
    /// Crawling is actively running
    Running,
    /// Crawling is temporarily paused
    Paused,
    /// Crawling completed successfully
    Completed,
    /// Crawling stopped due to error
    Error,
    /// Crawling was cancelled by user
    Cancelled,
}

/// Detailed progress information for the entire crawling operation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingProgress {
    /// Current progress count
    pub current: u32,
    /// Total expected items to process
    pub total: u32,
    /// Progress percentage (0.0 to 100.0)
    pub percentage: f64,
    /// Current stage of crawling
    pub current_stage: CrawlingStage,
    /// Human-readable description of current step
    pub current_step: String,
    /// Overall status
    pub status: CrawlingStatus,
    /// Status message for display
    pub message: String,
    /// Estimated remaining time in seconds
    pub remaining_time: Option<u64>,
    /// Elapsed time in seconds since start
    pub elapsed_time: u64,
    /// Number of new items discovered
    pub new_items: u32,
    /// Number of items updated
    pub updated_items: u32,
    /// Current batch being processed
    pub current_batch: Option<u32>,
    /// Total number of batches
    pub total_batches: Option<u32>,
    /// Number of errors encountered
    pub errors: u32,
    /// Timestamp of this progress update
    pub timestamp: DateTime<Utc>,
}

impl Default for CrawlingProgress {
    fn default() -> Self {
        Self {
            current: 0,
            total: 0,
            percentage: 0.0,
            current_stage: CrawlingStage::Idle,
            current_step: "ëŒ€ê¸° ì¤‘".to_string(),
            status: CrawlingStatus::Idle,
            message: "í¬ë¡¤ë§ì´ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤".to_string(),
            remaining_time: None,
            elapsed_time: 0,
            new_items: 0,
            updated_items: 0,
            current_batch: None,
            total_batches: None,
            errors: 0,
            timestamp: Utc::now(),
        }
    }
}

impl CrawlingProgress {
    /// Calculate and update derived fields based on current progress
    pub fn calculate_derived_fields(&mut self, start_time: DateTime<Utc>) {
        // Calculate percentage
        if self.total > 0 {
            self.percentage = (self.current as f64 / self.total as f64) * 100.0;
        } else {
            self.percentage = 0.0;
        }
        
        // Calculate elapsed time
        let now = Utc::now();
        self.elapsed_time = (now - start_time).num_seconds().max(0) as u64;
        
        // Estimate remaining time based on current progress
        if self.current > 0 && self.elapsed_time > 0 {
            let items_per_second = self.current as f64 / self.elapsed_time as f64;
            if items_per_second > 0.0 {
                let remaining_items = self.total.saturating_sub(self.current) as f64;
                self.remaining_time = Some((remaining_items / items_per_second) as u64);
            }
        }
        
        // Update timestamp
        self.timestamp = now;
    }
    
    /// Create a new progress instance with calculated fields
    pub fn new_with_calculation(
        current: u32,
        total: u32,
        stage: CrawlingStage,
        step: String,
        status: CrawlingStatus,
        message: String,
        start_time: DateTime<Utc>,
        new_items: u32,
        updated_items: u32,
        errors: u32,
    ) -> Self {
        let mut progress = Self {
            current,
            total,
            percentage: 0.0,
            current_stage: stage,
            current_step: step,
            status,
            message,
            remaining_time: None,
            elapsed_time: 0,
            new_items,
            updated_items,
            current_batch: None,
            total_batches: None,
            errors,
            timestamp: Utc::now(),
        };
        
        progress.calculate_derived_fields(start_time);
        progress
    }
}

/// Individual task status within a parallel crawling operation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingTaskStatus {
    /// Unique identifier for the task
    pub task_id: String,
    /// URL being processed by this task
    pub url: String,
    /// Current status of the task
    pub status: TaskStatus,
    /// Status message
    pub message: String,
    /// Timestamp when status was updated
    pub timestamp: DateTime<Utc>,
    /// Current stage this task is in
    pub stage: CrawlingStage,
    /// Additional details specific to the task
    pub details: Option<HashMap<String, serde_json::Value>>,
}

/// Status of an individual crawling task
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum TaskStatus {
    /// Task is waiting to be processed
    Pending,
    /// Task is currently being processed
    Running,
    /// Task completed successfully
    Completed,
    /// Task failed with error
    Failed,
    /// Task was cancelled
    Cancelled,
}

/// Database statistics for monitoring
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DatabaseStats {
    /// Total number of products in database
    pub total_products: u64,
    /// Total number of devices in database  
    pub total_devices: u64,
    /// Last update timestamp
    pub last_updated: DateTime<Utc>,
    /// Estimated storage size
    pub storage_size: String,
    /// Number of incomplete records
    pub incomplete_records: u64,
    /// Database health status
    pub health_status: DatabaseHealth,
}

/// Database health indicators
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum DatabaseHealth {
    /// Database is operating normally
    Healthy,
    /// Database has minor issues
    Warning,
    /// Database has serious issues
    Critical,
}

/// Summary of crawling results after completion
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CrawlingResult {
    /// Total number of items processed
    pub total_processed: u32,
    /// Number of new items added
    pub new_items: u32,
    /// Number of existing items updated
    pub updated_items: u32,
    /// Number of errors encountered
    pub errors: u32,
    /// Duration of crawling operation in milliseconds
    pub duration_ms: u64,
    /// Stages that were completed
    pub stages_completed: Vec<CrawlingStage>,
    /// Start time of the operation
    pub start_time: DateTime<Utc>,
    /// End time of the operation
    pub end_time: DateTime<Utc>,
    /// Performance metrics
    pub performance_metrics: PerformanceMetrics,
}

/// Performance metrics for crawling operations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMetrics {
    /// Average processing time per item in milliseconds
    pub avg_processing_time_ms: f64,
    /// Items processed per second
    pub items_per_second: f64,
    /// Memory usage in MB
    pub memory_usage_mb: f64,
    /// Network requests made
    pub network_requests: u64,
    /// Cache hit rate percentage
    pub cache_hit_rate: f64,
}

/// Event types that can be emitted to the frontend
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", content = "data")]
pub enum CrawlingEvent {
    /// Progress update event
    ProgressUpdate(CrawlingProgress),
    /// Individual task status update
    TaskUpdate(CrawlingTaskStatus),
    /// Stage change notification
    StageChange {
        from: CrawlingStage,
        to: CrawlingStage,
        message: String,
    },
    /// Error notification
    Error {
        error_id: String,
        message: String,
        stage: CrawlingStage,
        recoverable: bool,
    },
    /// Database statistics update
    DatabaseUpdate(DatabaseStats),
    /// Final results notification
    Completed(CrawlingResult),
    /// ğŸ”¥ ë…ë¦½ì ì¸ ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ì´ë²¤íŠ¸ (í¬ë¡¤ë§ ì„¸ì…˜ê³¼ ë¬´ê´€)
    SiteStatusCheck {
        is_standalone: bool,  // trueë©´ ë…ë¦½ì ì¸ ì²´í¬, falseë©´ í¬ë¡¤ë§ ì„¸ì…˜ ë‚´ ì²´í¬
        status: SiteCheckStatus,
        message: String,
        timestamp: DateTime<Utc>,
    },
    /// ğŸ”¥ í¬ë¡¤ë§ ì„¸ì…˜ ì´ë²¤íŠ¸
    SessionEvent {
        session_id: String,
        event_type: SessionEventType,
        message: String,
        timestamp: DateTime<Utc>,
    },
    /// ğŸ”¥ ì„¸ì…˜ ë¼ì´í”„ì‚¬ì´í´ ì´ë²¤íŠ¸ (UI í‘œì‹œìš©)
    SessionLifecycle {
        session_id: String,
        event_type: SessionEventType,
        message: String,
        timestamp: DateTime<Utc>,
    },
    /// ğŸ”¥ ë°°ì¹˜ ì´ë²¤íŠ¸ (ê° ìŠ¤í…Œì´ì§€ë³„ ë°°ì¹˜)
    BatchEvent {
        session_id: String,
        batch_id: String,
        stage: CrawlingStage,
        event_type: BatchEventType,
        message: String,
        timestamp: DateTime<Utc>,
        metadata: Option<BatchMetadata>,
    },
    /// ğŸ”¥ ProductList í˜ì´ì§€ë³„ ì´ë²¤íŠ¸
    ProductListPageEvent {
        session_id: String,
        batch_id: String,
        page_number: u32,
        event_type: PageEventType,
        message: String,
        timestamp: DateTime<Utc>,
        metadata: Option<PageMetadata>,
    },
    /// ğŸ”¥ ì œí’ˆë³„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì´ë²¤íŠ¸ (ê¸°ì¡´ TaskUpdate ë³´ì™„)
    ProductDetailEvent {
        session_id: String,
        batch_id: String,
        product_id: String,
        product_url: String,
        event_type: ProductEventType,
        message: String,
        timestamp: DateTime<Utc>,
        metadata: Option<ProductMetadata>,
    },
}

/// ì‚¬ì´íŠ¸ ìƒíƒœ ì²´í¬ ê²°ê³¼
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SiteCheckStatus {
    /// ì²´í¬ ì‹œì‘
    Started,
    /// ì²´í¬ ì¤‘
    InProgress,
    /// ì²´í¬ ì„±ê³µ
    Success,
    /// ì²´í¬ ì‹¤íŒ¨
    Failed,
}

/// ğŸ”¥ ì„¸ì…˜ ì´ë²¤íŠ¸ íƒ€ì…
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SessionEventType {
    /// í¬ë¡¤ë§ ì„¸ì…˜ ì‹œì‘ (ì‚¬ìš©ìê°€ í¬ë¡¤ë§ ë²„íŠ¼ í´ë¦­)
    Started,
    /// ì‚¬ì´íŠ¸ ìƒíƒœ í™•ì¸ ë° ìºì‹œ ê²€ì¦
    SiteStatusCheck,
    /// ë°°ì¹˜ ê³„íš ìˆ˜ë¦½ (ì´ í˜ì´ì§€ ìˆ˜, ë°°ì¹˜ ë¶„í•  ê³„íš)
    BatchPlanning,
    /// í¬ë¡¤ë§ ì„¸ì…˜ ì™„ë£Œ
    Completed,
    /// í¬ë¡¤ë§ ì„¸ì…˜ ì‹¤íŒ¨
    Failed,
    /// í¬ë¡¤ë§ ì„¸ì…˜ ì·¨ì†Œ
    Cancelled,
    /// í¬ë¡¤ë§ ì„¸ì…˜ ì¼ì‹œì •ì§€
    Paused,
    /// í¬ë¡¤ë§ ì„¸ì…˜ ì¬ê°œ
    Resumed,
}

/// ğŸ”¥ ë°°ì¹˜ ì´ë²¤íŠ¸ íƒ€ì…
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum BatchEventType {
    /// ë°°ì¹˜ ìƒì„±ë¨
    Created,
    /// ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
    Started,
    /// ë°°ì¹˜ ì§„í–‰ ì¤‘
    Progress,
    /// ë°°ì¹˜ ì™„ë£Œ
    Completed,
    /// ë°°ì¹˜ ì‹¤íŒ¨
    Failed,
    /// ë°°ì¹˜ ì¬ì‹œë„
    Retrying,
}

/// ğŸ”¥ í˜ì´ì§€ ì´ë²¤íŠ¸ íƒ€ì…
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PageEventType {
    /// í˜ì´ì§€ ì²˜ë¦¬ ì‹œì‘
    Started,
    /// í˜ì´ì§€ ì²˜ë¦¬ ì¤‘
    Progress,
    /// í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ
    Completed,
    /// í˜ì´ì§€ ì²˜ë¦¬ ì‹¤íŒ¨
    Failed,
    /// í˜ì´ì§€ ì¬ì‹œë„
    Retrying,
}

/// ğŸ”¥ ì œí’ˆ ì´ë²¤íŠ¸ íƒ€ì…
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ProductEventType {
    /// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹œì‘
    Started,
    /// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì¤‘
    Progress,
    /// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ
    Completed,
    /// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨
    Failed,
    /// ì œí’ˆ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì¬ì‹œë„
    Retrying,
}

/// ğŸ”¥ ë°°ì¹˜ ë©”íƒ€ë°ì´í„°
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchMetadata {
    /// ë°°ì¹˜ì— í¬í•¨ëœ ì•„ì´í…œ ìˆ˜
    pub total_items: u32,
    /// ì²˜ë¦¬ëœ ì•„ì´í…œ ìˆ˜
    pub processed_items: u32,
    /// ì„±ê³µí•œ ì•„ì´í…œ ìˆ˜
    pub successful_items: u32,
    /// ì‹¤íŒ¨í•œ ì•„ì´í…œ ìˆ˜
    pub failed_items: u32,
    /// ì²˜ë¦¬ ì‹œì‘ ì‹œê°„
    pub start_time: DateTime<Utc>,
    /// ì˜ˆìƒ ì™„ë£Œ ì‹œê°„
    pub estimated_completion: Option<DateTime<Utc>>,
}

/// ğŸ”¥ í˜ì´ì§€ ë©”íƒ€ë°ì´í„°
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PageMetadata {
    /// í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ì œí’ˆ ìˆ˜
    pub products_found: u32,
    /// ì²˜ë¦¬ëœ ì œí’ˆ ìˆ˜
    pub products_processed: u32,
    /// í˜ì´ì§€ ë¡œë“œ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
    pub load_time_ms: u64,
    /// í˜ì´ì§€ í¬ê¸° (ë°”ì´íŠ¸)
    pub page_size_bytes: u64,
}

/// ğŸ”¥ ì œí’ˆ ë©”íƒ€ë°ì´í„°
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProductMetadata {
    /// ì œí’ˆëª…
    pub product_name: Option<String>,
    /// ì œí’ˆ ì¹´í…Œê³ ë¦¬
    pub category: Option<String>,
    /// ì¸ì¦ ë²ˆí˜¸
    pub certification_number: Option<String>,
    /// ì²˜ë¦¬ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
    pub processing_time_ms: u64,
    /// í˜ì´ì§€ í¬ê¸° (ë°”ì´íŠ¸)
    pub page_size_bytes: u64,
    /// ì¬ì‹œë„ íšŸìˆ˜
    pub retry_count: u32,
}

impl CrawlingEvent {
    /// Get the event type as a string for Tauri event emission
    pub fn event_name(&self) -> &'static str {
        match self {
            CrawlingEvent::ProgressUpdate(_) => "crawling-progress",
            CrawlingEvent::TaskUpdate(_) => "crawling-task-update",
            CrawlingEvent::StageChange { .. } => "crawling-stage-change",
            CrawlingEvent::Error { .. } => "crawling-error",
            CrawlingEvent::DatabaseUpdate(_) => "database-update",
            CrawlingEvent::Completed(_) => "crawling-completed",
            CrawlingEvent::SiteStatusCheck { .. } => "site-status-check",
            CrawlingEvent::SessionEvent { .. } => "session-event",
            CrawlingEvent::BatchEvent { .. } => "batch-event",
            CrawlingEvent::ProductListPageEvent { .. } => "product-list-page-event",
            CrawlingEvent::ProductDetailEvent { .. } => "product-detail-event",
            CrawlingEvent::SessionLifecycle { .. } => "session-lifecycle",
        }
    }
}
