# rMatterCertis ì•„í‚¤í…ì²˜ ì¬êµ¬ì¶• ì‹¤í–‰ ê³„íš

*ë³¸ ë¬¸ì„œëŠ” `re-arch-gem.md`ì˜ Trait ê¸°ë°˜ ì„¤ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¥¼ ë…ë¦½ì ìœ¼ë¡œ êµ¬í˜„í•˜ê³  ê¸°ëŠ¥ ë‹¨ìœ„ë¡œ ê¸°ì¡´ ì‹œìŠ¤í…œì„ ëŒ€ì²´í•˜ëŠ” êµ¬ì²´ì  ì‹¤í–‰ ê³„íšì„ ì œì‹œí•©ë‹ˆë‹¤.*

## 1. ì „ëµ ê°œìš”: Clean Slate + Incremental Replacement

### 1.1 í•µì‹¬ ì „ëµ

```mermaid
graph LR
    subgraph "ê¸°ì¡´ ì‹œìŠ¤í…œ (ìœ ì§€)"
        A1[CrawlingOrchestrator]
        A2[WorkerPool]
        A3[ê¸°ì¡´ UI]
    end
    
    subgraph "ìƒˆ ì‹œìŠ¤í…œ (ë…ë¦½ êµ¬ì¶•)"
        B1[SessionOrchestrator]
        B2[AsyncTask Traits]
        B3[StageRunner Traits]
        B4[ìƒˆ UI Components]
    end
    
    subgraph "ê¸°ëŠ¥ë³„ êµì²´"
        C1[ì¤€ë¹„ ë‹¨ê³„ â†’ ìƒˆ ì‹œìŠ¤í…œ]
        C2[ëª©ë¡ ìˆ˜ì§‘ â†’ ìƒˆ ì‹œìŠ¤í…œ]
        C3[ìƒì„¸ ìˆ˜ì§‘ â†’ ìƒˆ ì‹œìŠ¤í…œ]
        C4[ìµœì¢… ì „í™˜]
    end
    
    B1 --> C1
    B2 --> C1
    B3 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> C4
    
    style B1 fill:#e3f2fd
    style B2 fill:#e3f2fd
    style B3 fill:#e3f2fd
    style C4 fill:#c8e6c9
```

### 1.2 ì¥ì  ë¶„ì„

**âœ… ì†ë„**: ë³µì¡í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì—†ì´ ì§ì§„
**âœ… ì•ˆì •ì„±**: ê¸°ì¡´ ì‹œìŠ¤í…œì€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
**âœ… ê²€ì¦**: ê° ê¸°ëŠ¥ë³„ë¡œ ë…ë¦½ì  í…ŒìŠ¤íŠ¸
**âœ… ë¡¤ë°±**: ì–¸ì œë“  ê¸°ì¡´ ì‹œìŠ¤í…œìœ¼ë¡œ ë³µê·€ ê°€ëŠ¥

## 2. ìƒˆ ì•„í‚¤í…ì²˜ ì„¤ê³„

### 2.1 ì „ì²´ êµ¬ì¡°ë„

```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[New Crawling UI<br/>Trait ê¸°ë°˜ ì‹¤ì‹œê°„ ì¶”ì ]
    end
    
    subgraph "Application Layer"
        SO[SessionOrchestrator<br/>ì›Œí¬í”Œë¡œ ì´ê´„]
        BM[BatchManager<br/>ë°°ì¹˜ ì²˜ë¦¬ ì´ê´„]
        SR[StageRunner Trait<br/>ë‹¨ê³„ë³„ ì‹¤í–‰]
        AT[AsyncTask Trait<br/>ê°œë³„ ì‘ì—…]
    end
    
    subgraph "Event System"
        EH[EventHub<br/>í†µí•© ì´ë²¤íŠ¸ í—ˆë¸Œ]
        SE[SessionEvent]
        ST[StageEvent] 
        TE[TaskEvent]
        BE[BatchEvent]
    end
    
    subgraph "Domain Layer"
        WF[Workflow Definition]
        SS[Session State]
        TS[Task State]
        BS[Batch State]
    end
    
    subgraph "Infrastructure Layer"
        HC[HttpClient]
        DB[Database Access]
        FS[File System]
        LOG[Logging]
        MM[Memory Monitor]
    end
    
    UI --> SO
    SO --> BM
    BM --> SR
    SR --> AT
    
    SO --> EH
    BM --> EH
    SR --> EH
    AT --> EH
    
    EH --> SE
    EH --> ST
    EH --> TE
    EH --> BE
    
    SO --> WF
    BM --> BS
    SR --> SS
    AT --> TS
    
    BM --> MM
    AT --> HC
    AT --> DB
    AT --> FS
    AT --> LOG
    
    style SO fill:#e3f2fd
    style BM fill:#f3e5f5
    style SR fill:#fff3e0
    style AT fill:#e8f5e8
```

### 2.2 Modern Rust 2024 ê¸°ë°˜ í•µì‹¬ ì•„í‚¤í…ì²˜

```mermaid
classDiagram
    class AsyncTask {
        <<trait>>
        +id() &str
        +name() &str
        +estimated_duration() Duration
        +priority() u8
        +validate(context) Future~Result~
        +execute(context, events) Future~TaskResult~
        +cleanup(context) Future~Result~
        +cancel() Future~Result~
    }
    
    class BatchableTask {
        <<trait>>
        +BatchItem: Associated Type
        +split_into_batches(items, size) Vec~Vec~BatchItem~~
        +process_item(item, context, events) Future~Result~
        +process_item_impl(item, context) Future~Result~
    }
    
    class MetricsAware {
        <<trait>>
        +metrics_key() &'static str
        +throughput_unit() &'static str
        +estimated_throughput() f64
    }
    
    class StageRunner {
        <<trait>>
        +Task: Associated Type
        +stage_name() &'static str
        +required_tasks() Vec~Box~Task~~
        +validate_stage(context) Future~Result~
        +run_stage(context, events, token) Future~StageResult~
        +cleanup_stage(context) Future~Result~
    }
    
    class SessionOrchestrator {
        +new(batch_manager, event_hub) Self
        +run_workflow(config) Future~WorkflowResult~
        +pause_workflow() Result~()~
        +resume_workflow() Result~()~
        +cancel_workflow() Result~()~
    }
    
    class BatchManager {
        +new(config) Self
        +execute_batched_task(task, items, context, events, token) Future~BatchResult~
        -execute_single_batch(...) Future~Value~
        -update_eta(...) Future~()~
        -force_gc_and_measure() Future~u64~
    }
    
    class MetricsCollector {
        +new() Self
        +record_batch_completion(task_type, items, duration) Future~()~
        +get_throughput(task_type) Future~Option~f64~~
    }
    
    class EtaCalculator {
        +new(metrics_collector) Self
        +calculate_eta(completed, total) Future~Option~EtaInfo~~
    }
    
    class EventHub {
        +new() Self
        +send(event) Result~()~
        +subscribe() EventReceiver
        +emit_activity_event(event) Future~()~
        +emit_metrics_event(event) Future~()~
    }
    
    class BatchManager {
        +new(config) Self
        +execute_collection_workflow(batch_plan, context, event_hub) Future~CollectionResult~
        +execute_detail_workflow(batch_plan, context, event_hub) Future~DetailResult~
        +plan_collection_batches(total_pages, session_config) Future~BatchPlan~
        +plan_detail_batches(product_links, session_config) Future~BatchPlan~
    }
    
    SessionOrchestrator --> BatchManager
    BatchManager --> MetricsCollector
    BatchManager --> EtaCalculator
    BatchManager --> StageRunner
    StageRunner --> AsyncTask
    
    AsyncTask <|-- BatchableTask
    AsyncTask <|-- MetricsAware
    
    SessionOrchestrator --> EventHub
    BatchManager --> EventHub
    StageRunner --> EventHub
    AsyncTask --> EventHub
    
    note for AsyncTask "Modern async fn in traits"
    note for BatchableTask "re-arch-plan-r-gem: Activity ì¶”ì "
    note for MetricsAware "re-arch-plan-r-gem: ì„±ëŠ¥ ì¸¡ì •"
    note for MetricsCollector "re-arch-plan-r-gem: ETA ê³„ì‚°"
```

### 2.3 ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ êµ¬ì¡° (re-arch-plan-r-gem ì œì•ˆ ë°˜ì˜)

```mermaid
graph TD
    subgraph "UI Layer"
        UI[Crawling Control UI]
    end
    
    subgraph "Facade Layer (ëª…í™•í•œ ì§„ì…ì )"
        CF["<b>CrawlingFacade</b><br/>ì‹œìŠ¤í…œ API"]
    end
    
    subgraph "Application Layer"
        SO[SessionOrchestrator<br/>ì›Œí¬í”Œë¡œ ì´ê´„]
        PCA[PreCrawlingAnalyzer<br/>ë°ì´í„° ìˆ˜ì§‘ ì¡°ì •]
        CP["<b>CrawlingPlanner</b><br/>ë„ë©”ì¸ ì§€ì‹ ì¤‘ì‹¬"]
        BM[BatchManager<br/>ë°°ì¹˜ ì²˜ë¦¬]
    end
    
    subgraph "Analysis Layer"
        SSC[SiteStatusChecker<br/>í†µí•© ì‚¬ì´íŠ¸ ë¶„ì„]
        DBA[DatabaseAnalyzer<br/>DB ìƒíƒœ ë¶„ì„]
    end
    
    subgraph "Configuration Layer"
        SC["<b>SessionConfig</b><br/>(ë¶ˆë³€ ì„¤ì •)"]
        AC[AppContext<br/>ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸]
    end
    
    subgraph "Event System"
        EH[EventHub<br/>ì´ë²¤íŠ¸ í—ˆë¸Œ]
    end
    
    UI --> CF
    CF --> SO
    SO --> PCA
    SO --> CP
    SO --> SC
    
    PCA --> SSC
    PCA --> DBA
    
    CP --> SC
    SC --> AC
    
    SO --> BM
    BM --> AC
    
    CF --> EH
    SO --> EH
    
    style CF fill:#e3f2fd
    style CP fill:#fff3e0
    style SC fill:#e8f5e8
    style SSC fill:#f3e5f5
```

**í•µì‹¬ ê°œì„ ì‚¬í•­ (re-arch-plan-r-gem ë°˜ì˜):**

1. **ëª…í™•í•œ ì§„ì…ì **: `CrawlingFacade`ê°€ UIì™€ ë‚´ë¶€ ì‹œìŠ¤í…œ ê°„ API ì—­í• 
2. **ë„ë©”ì¸ ì§€ì‹ ì¤‘ì‹¬**: `CrawlingPlanner`ê°€ í•µì‹¬ ì˜ì‚¬ê²°ì • ë‹´ë‹¹
3. **ë°ì´í„° íë¦„ ëª…í™•í™”**: [ì‚¬ìš©ì ì„¤ì •] + [ì‚¬ì´íŠ¸ ìƒíƒœ] + [DB ìƒíƒœ] â†’ CrawlingPlan
4. **ì‹¤í–‰ ì¼ê´€ì„±**: ë¶ˆë³€ `SessionConfig`ë¡œ ëª¨ë“  ì‘ì—…ì˜ ì¼ê´€ì„± ë³´ì¥
5. **ì—­í•  ë¶„ë¦¬**: ë°ì´í„° ìˆ˜ì§‘(Analyzers) vs ì˜ì‚¬ê²°ì •(Planner) ëª…í™•íˆ êµ¬ë¶„

## 3. ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš

### 3.1 Phase 0: ê¸°ë°˜ êµ¬ì¡° êµ¬ì¶• (Week 1)

#### ëª©í‘œ: ìƒˆ ì‹œìŠ¤í…œì˜ ê³¨ê²© ì™„ì„±

```mermaid
gantt
    title Phase 0: ê¸°ë°˜ êµ¬ì¡° êµ¬ì¶•
    dateFormat YYYY-MM-DD
    section Core Traits
    AsyncTask Trait ì •ì˜      :trait-1, 2025-07-19, 2d
    StageRunner Trait ì •ì˜    :trait-2, after trait-1, 2d
    EventHub êµ¬í˜„            :event-1, after trait-2, 2d
    Context êµ¬ì¡°ì²´ ì •ì˜       :context-1, 2025-07-19, 1d
```

#### Modern Rust 2024 ë””ë ‰í† ë¦¬ êµ¬ì¡° (re-arch-plan-r-gem ì œì•ˆ ë°˜ì˜)

```
src-tauri/src/new_architecture/
â”œâ”€â”€ constants.rs              # ê¸°ë³¸ ìƒìˆ˜ (í•˜ë“œì½”ë”© ë°©ì§€)
â”œâ”€â”€ config.rs                # ì„¤ì • ì‹œìŠ¤í…œ ë£¨íŠ¸
â”œâ”€â”€ config/                  # ì„¤ì • ì„¸ë¶€ ëª¨ë“ˆë“¤
â”‚   â”œâ”€â”€ user_config.rs       # config.toml ë¡œë“œ
â”‚   â”œâ”€â”€ session_config.rs    # ëŸ°íƒ€ì„ ì„¤ì •
â”‚   â””â”€â”€ app_context.rs       # í†µí•© ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
â”œâ”€â”€ traits.rs                # trait ë£¨íŠ¸ (mod.rs ì œê±°)
â”œâ”€â”€ traits/                  # trait ì„¸ë¶€ êµ¬í˜„ë“¤
â”‚   â”œâ”€â”€ async_task.rs
â”‚   â”œâ”€â”€ stage_runner.rs
â”‚   â”œâ”€â”€ batchable_task.rs
â”‚   â””â”€â”€ metrics_aware.rs
â”œâ”€â”€ orchestrator.rs          # SessionOrchestrator
â”œâ”€â”€ batch.rs                 # ë°°ì¹˜ ì‹œìŠ¤í…œ ë£¨íŠ¸  
â”œâ”€â”€ batch/                   # ë°°ì¹˜ ì„¸ë¶€ ëª¨ë“ˆë“¤
â”‚   â”œâ”€â”€ manager.rs
â”‚   â”œâ”€â”€ memory_monitor.rs
â”‚   â””â”€â”€ metrics.rs
â”œâ”€â”€ events.rs                # ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ ë£¨íŠ¸
â”œâ”€â”€ events/                  # ì´ë²¤íŠ¸ ì„¸ë¶€ ëª¨ë“ˆë“¤
â”‚   â”œâ”€â”€ hub.rs
â”‚   â”œâ”€â”€ types.rs
â”‚   â””â”€â”€ activity.rs
â”œâ”€â”€ domain.rs                # ë„ë©”ì¸ ëª¨ë¸ ë£¨íŠ¸
â”œâ”€â”€ domain/                  # ë„ë©”ì¸ ì„¸ë¶€ ëª¨ë¸ë“¤
â”‚   â”œâ”€â”€ workflow.rs
â”‚   â”œâ”€â”€ session.rs
â”‚   â””â”€â”€ batch_state.rs
â””â”€â”€ lib.rs                   # í†µí•© API
```

#### í•µì‹¬ ëª¨ë“ˆ êµ¬í˜„ (Modern Rust ë°©ì‹)

```rust
// src-tauri/src/new_architecture/traits.rs
//! ëª¨ë“  trait ì •ì˜ ë£¨íŠ¸ (re-arch-plan-r-gem: mod.rs ì™„ì „ ì œê±°)
pub mod async_task;
pub mod stage_runner;
pub mod batchable_task;
pub mod metrics_aware;

// Modern Rust re-export pattern
pub use async_task::AsyncTask;
pub use stage_runner::StageRunner;
pub use batchable_task::BatchableTask;
pub use metrics_aware::MetricsAware;

// src-tauri/src/new_architecture/config.rs
//! ì„¤ì • ì‹œìŠ¤í…œ ë£¨íŠ¸ (ì„¤ì • ì£¼ë„ ì•„í‚¤í…ì²˜)
pub mod user_config;
pub mod session_config;
pub mod app_context;

pub use user_config::UserConfig;
pub use session_config::SessionConfig;
pub use app_context::AppContext;

// src-tauri/src/new_architecture/lib.rs
//! Modern Rust 2024 ê¸°ë°˜ ìƒˆ ì•„í‚¤í…ì²˜ í†µí•© API
//! 
//! re-arch-plan-r-gem ì œì•ˆ ë°˜ì˜:
//! - ì„¤ì • ì£¼ë„ ì•„í‚¤í…ì²˜
//! - mod.rs ì™„ì „ ì œê±°
//! - Zero Hard-Coding

// ìƒìˆ˜ ë° ì„¤ì •
pub mod constants;
pub mod config;

// í•µì‹¬ ì•„í‚¤í…ì²˜
pub mod traits;
pub mod orchestrator;
pub mod batch;
pub mod events;

// ë„ë©”ì¸ ë° êµ¬í˜„
pub mod domain;
pub mod stages;
pub mod tasks;

// í¸ì˜ë¥¼ ìœ„í•œ re-export
pub use traits::{AsyncTask, StageRunner, BatchableTask, MetricsAware};
pub use orchestrator::SessionOrchestrator;
pub use batch::BatchManager;
pub use events::{EventHub, UnifiedEvent};
pub use config::{UserConfig, SessionConfig, AppContext};

/// í¬ë¡¤ë§ ì‹œìŠ¤í…œ Facade (re-arch-plan-r-gem ì œì•ˆ: ëª…í™•í•œ ì§„ì…ì )
/// 
/// ê¸°ì¡´ NewCrawlingSystem ë¬¸ì œì :
/// - ëª¨í˜¸í•œ ì±…ì„: ë‹¨ìˆœ ì»¨í…Œì´ë„ˆ ì—­í• ë§Œ ìˆ˜í–‰
/// - ë¶ˆë¶„ëª…í•œ ë°ì´í„° íë¦„
/// - ì„¤ì • ê´€ë¦¬ í˜¼ë€
/// 
/// CrawlingFacade í•´ê²°ì±…:
/// - UIì™€ ë‚´ë¶€ ì‹œìŠ¤í…œ ê°„ ëª…í™•í•œ API ì—­í• 
/// - ì™¸ë¶€ì—ëŠ” ë‹¨ìˆœí•œ ì¸í„°í˜ì´ìŠ¤, ë‚´ë¶€ ë³µì¡ì„± ì€ë‹‰
/// - ë¶ˆë³€ SessionConfigë¥¼ í†µí•œ ì¼ê´€ì„± í™•ë³´
#[derive(Debug)]
pub struct CrawlingFacade {
    event_hub: std::sync::Arc<EventHub>,
}

impl CrawlingFacade {
    /// ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ Facade ìƒì„±
    pub fn new() -> crate::Result<Self> {
        let event_hub = std::sync::Arc::new(EventHub::new());
        
        Ok(Self {
            event_hub,
            // BatchManagerëŠ” Planning ë‹¨ê³„ì—ì„œ ì ì‘ì ìœ¼ë¡œ ìƒì„±ë¨
        })
    }
    
    /// ì „ì²´ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œ ì‹¤í–‰ (ëª…í™•í•œ ë°ì´í„° íë¦„)
    /// 
    /// re-arch-plan-r-gem ì œì•ˆ íë¦„:
    /// 1. ì‚¬ìš©ì ì„¤ì • ê²€ì¦
    /// 2. ë¶„ì„ ë‹¨ê³„ (SiteStatus + DBReport ìˆ˜ì§‘)
    /// 3. ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê³„íš ìˆ˜ë¦½ (CrawlingPlanner)
    /// 4. ë¶ˆë³€ SessionConfig ìƒì„±
    /// 5. ê³„íš ì‹¤í–‰
    pub async fn start_full_crawl(
        &self, 
        user_config: UserConfig
    ) -> crate::Result<domain::WorkflowResult> {
        // 1ë‹¨ê³„: ì‚¬ìš©ì ì„¤ì • ê²€ì¦
        user_config.validate()?;
        
        // 2ë‹¨ê³„: SessionOrchestrator ìƒì„± (ë¶ˆë³€ ì»´í¬ë„ŒíŠ¸ë“¤ ì£¼ì…)
        let orchestrator = SessionOrchestrator::new(
            self.batch_manager.clone(),
            self.event_hub.clone(),
        );
        
        // 3ë‹¨ê³„: ì „ì²´ ì›Œí¬í”Œë¡œ ì‹¤í–‰ (ë‚´ë¶€ì—ì„œ ë¶„ì„ â†’ ê³„íš â†’ ì‹¤í–‰)
        orchestrator.run_workflow(user_config).await
    }
    
    /// ì¦ë¶„ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œ ì‹¤í–‰
    pub async fn start_incremental_crawl(
        &self,
        user_config: UserConfig
    ) -> crate::Result<domain::WorkflowResult> {
        user_config.validate()?;
        
        let orchestrator = SessionOrchestrator::new(
            self.batch_manager.clone(),
            self.event_hub.clone(),
        );
        
        // ì¦ë¶„ í¬ë¡¤ë§ìš© ì„¤ì •ìœ¼ë¡œ ë³€ê²½
        let mut incremental_config = user_config;
        incremental_config.crawling.crawl_type = domain::CrawlType::Incremental;
        
        orchestrator.run_workflow(incremental_config).await
    }
    
    /// ë³µêµ¬ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œ ì‹¤í–‰
    pub async fn start_recovery_crawl(
        &self,
        user_config: UserConfig
    ) -> crate::Result<domain::WorkflowResult> {
        user_config.validate()?;
        
        let orchestrator = SessionOrchestrator::new(
            self.batch_manager.clone(),
            self.event_hub.clone(),
        );
        
        // ë³µêµ¬ í¬ë¡¤ë§ìš© ì„¤ì •ìœ¼ë¡œ ë³€ê²½
        let mut recovery_config = user_config;
        recovery_config.crawling.crawl_type = domain::CrawlType::Recovery;
        
        orchestrator.run_workflow(recovery_config).await
    }
    
    /// ì´ë²¤íŠ¸ ìˆ˜ì‹ ê¸° ì œê³µ (UI ì—…ë°ì´íŠ¸ìš©)
    pub fn subscribe_to_events(&self) -> events::EventReceiver {
        self.event_hub.subscribe()
    }
}

/// SessionOrchestrator ì›Œí¬í”Œë¡œ êµ¬í˜„ (re-arch-plan-r-gem ë°ì´í„° íë¦„ + UI í”¼ë“œë°± ë°˜ì˜)
impl SessionOrchestrator {
    /// ìƒˆë¡œìš´ ë°ì´í„° íë¦„ ê¸°ë°˜ ì›Œí¬í”Œë¡œ ì‹¤í–‰ (ì´ë²¤íŠ¸ ë°œí–‰ í¬í•¨)
    /// 
    /// re-arch-plan-r-gem ì œì•ˆ ë‹¨ê³„:
    /// 1. ë¶„ì„ ë‹¨ê³„: SiteStatus + DBReport ìˆ˜ì§‘ (+ ì§„í–‰ ì´ë²¤íŠ¸)
    /// 2. ê³„íš ìˆ˜ë¦½ ë‹¨ê³„: CrawlingPlannerë¡œ ë„ë©”ì¸ ì§€ì‹ í™œìš© (+ ê³„íš ì´ë²¤íŠ¸)
    /// 3. ì‹¤í–‰ ë‹¨ê³„: ë¶ˆë³€ SessionConfigë¡œ ì¼ê´€ì„± í™•ë³´ (+ ì‹¤í–‰ ì´ë²¤íŠ¸)
    pub async fn run_workflow(
        &self,
        user_config: UserConfig
    ) -> crate::Result<domain::WorkflowResult> {
        let session_id = uuid::Uuid::new_v4().to_string();
        let start_time = std::time::Instant::now();
        
        // ğŸ¯ ì„¸ì…˜ ì‹œì‘ ì´ë²¤íŠ¸ ë°œí–‰ (UI í”¼ë“œë°±)
        self.emit_session_started(
            session_id.clone(),
            format!("í¬ë¡¤ë§ íƒ€ì…: {:?}, ë°°ì¹˜ í¬ê¸°: {}", 
                user_config.crawling.crawl_type, 
                user_config.crawling.batch_size)
        ).await?;
        
        // ğŸ¯ ë¶„ì„ ë‹¨ê³„ ì‹œì‘ ì´ë²¤íŠ¸
        self.emit_stage_changed(
            WorkflowStage::Initializing,
            WorkflowStage::Analyzing,
            None
        ).await?;
        
        // 1ë‹¨ê³„: ë¶„ì„ ë‹¨ê³„ (ëª…í™•í•œ ë°ì´í„° ìˆ˜ì§‘ + ì´ë²¤íŠ¸ ë°œí–‰)
        let pre_analyzer = PreCrawlingAnalyzer::new(self.event_hub.clone());
        let analysis_result = pre_analyzer.analyze_all().await?;
        
        // ğŸ¯ ê³„íš ìˆ˜ë¦½ ë‹¨ê³„ ì‹œì‘ ì´ë²¤íŠ¸
        self.emit_stage_changed(
            WorkflowStage::Analyzing,
            WorkflowStage::Planning,
            None
        ).await?;
        
        // 2ë‹¨ê³„: ê³„íš ìˆ˜ë¦½ ë‹¨ê³„ (ë„ë©”ì¸ ì§€ì‹ ì¤‘ì‹¬ + ì´ë²¤íŠ¸ ë°œí–‰)
        let planner = CrawlingPlanner::new(self.event_hub.clone());
        let crawling_plan = planner.create_comprehensive_plan(
            user_config.crawling.crawl_type.clone(),
            &analysis_result.site_status,
            &analysis_result.db_report,
        ).await?; // CrawlingPlanner ë‚´ë¶€ì—ì„œ PlanningEvent ë°œí–‰
        
        // í¬ë¡¤ë§ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ì¡°ê¸° ì¢…ë£Œ
        if !crawling_plan.needs_crawling() {
            let result = domain::WorkflowResult::no_action_taken(
                "No crawling needed based on current analysis"
            );
            
            // ğŸ¯ ì™„ë£Œ ì´ë²¤íŠ¸ ë°œí–‰
            self.emit_event(AppEvent::Session(SessionEvent::Completed {
                result: result.clone(),
                total_duration_ms: start_time.elapsed().as_millis() as u64,
            })).await?;
            
            return Ok(result);
        }
        
        // 3ë‹¨ê³„: ë¶ˆë³€ SessionConfig ìƒì„± (ì¼ê´€ì„± í™•ë³´)
        let session_config = SessionConfig::new(
            user_config,
            analysis_result.site_status,
            crawling_plan.clone(),
        );

        // 4ë‹¨ê³„: Planning ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ BatchManager ìƒì„±
        let batch_config = crawling_plan.batch_config.clone();
        let batch_manager = std::sync::Arc::new(BatchManager::new(batch_config));
        
        // ğŸ¯ ì‹¤í–‰ ë‹¨ê³„ ì‹œì‘ ì´ë²¤íŠ¸
        self.emit_stage_changed(
            WorkflowStage::Planning,
            WorkflowStage::Executing,
            Some(session_config.estimated_duration_seconds())
        ).await?;
        
        // ğŸ¯ ì‹¤í–‰ ì‹œì‘ ìƒì„¸ ì´ë²¤íŠ¸
        self.emit_event(AppEvent::Session(SessionEvent::ExecutionStarted {
            plan: crawling_plan,
            estimated_duration_seconds: session_config.estimated_duration_seconds(),
        })).await?;
        
        // 4ë‹¨ê³„: ì‹¤í–‰ ë‹¨ê³„
        self.execute_crawling_plan(session_config).await
    }
    
    /// í¬ë¡¤ë§ ê³„íš ì‹¤í–‰ (ë¶ˆë³€ SessionConfig ê¸°ë°˜)
    async fn execute_crawling_plan(
        &self,
        session_config: SessionConfig
    ) -> crate::Result<domain::WorkflowResult> {
        
        // AppContext ìƒì„± (ëª¨ë“  í•˜ìœ„ ì‘ì—…ì— ì „íŒŒ)
        let app_context = AppContext::new(
            uuid::Uuid::new_v4().to_string(),
            std::sync::Arc::new(session_config),
        );
        
        // ë°°ì¹˜ ê³„íš ìƒì„±
        let batch_plan = self.batch_manager.create_batch_plan(
            &app_context.config.crawling_plan
        ).await?;
        
        // ì‹¤ì œ í¬ë¡¤ë§ ì‹¤í–‰
        match app_context.config.crawling_plan.strategy {
            domain::CrawlingStrategy::Full => {
                self.execute_full_crawling(batch_plan, app_context).await
            }
            domain::CrawlingStrategy::Incremental => {
                self.execute_incremental_crawling(batch_plan, app_context).await
            }
            domain::CrawlingStrategy::Recovery => {
                self.execute_recovery_crawling(batch_plan, app_context).await
            }
            domain::CrawlingStrategy::NoAction => {
                Ok(domain::WorkflowResult::no_action_taken("No action required"))
            }
        }
    }
        new_config.validate()?;
        self.user_config = new_config;
        // BatchManager ë“± ì¬êµ¬ì„± í•„ìš” ì‹œ ì²˜ë¦¬
        Ok(())
    }
}
```

### 3.2 Phase 1: í¬ë¡¤ë§ ì „ì²˜ë¦¬ ë° ë°°ì¹˜ ê³„íš ë‹¨ê³„ (Week 2)

#### ëª©í‘œ: ëª…í™•í•œ ë°ì´í„° íë¦„ê³¼ ë„ë©”ì¸ ì§€ì‹ ì¤‘ì‹¬ ì„¤ê³„ (re-arch-plan-r-gem ë°˜ì˜)

**í•µì‹¬ ê°œë…**: CrawlingFacade â†’ ë¶„ì„ â†’ CrawlingPlanner â†’ ë¶ˆë³€ SessionConfig â†’ ì‹¤í–‰ì˜ ëª…í™•í•œ ë°ì´í„° íë¦„

```mermaid
sequenceDiagram
    participant UI as Crawling Control UI
    participant Facade as CrawlingFacade
    participant SO as SessionOrchestrator
    participant Analyzer as PreCrawlingAnalyzer
    participant SSC as SiteStatusChecker
    participant DBA as DatabaseAnalyzer
    participant Planner as CrawlingPlanner
    participant BM as BatchManager

    UI->>Facade: start_full_crawl(user_config)
    Facade->>SO: run_workflow(user_config)
    
    Note over SO: 1ë‹¨ê³„: ë¶„ì„ (ëª…í™•í•œ ë°ì´í„° ìˆ˜ì§‘)
    SO->>Analyzer: analyze_all()
    Analyzer->>SSC: check_site_status_and_scale()
    SSC-->>Analyzer: SiteStatus
    Analyzer->>DBA: analyze_database_state()
    DBA-->>Analyzer: DBStateReport
    Analyzer-->>SO: AnalysisResult{site_status, db_report}

    Note over SO: 2ë‹¨ê³„: ê³„íš ìˆ˜ë¦½ (ë„ë©”ì¸ ì§€ì‹ í™œìš©)
    SO->>Planner: create_plan(crawl_type, site_status, db_report)
    Note over Planner: ì¦ë¶„/ë³µêµ¬/ì „ì²´ í¬ë¡¤ë§ ë¡œì§ ì ìš©
    Planner-->>SO: CrawlingPlan

    Note over SO: 3ë‹¨ê³„: ë¶ˆë³€ SessionConfig ìƒì„±
    SO->>SO: SessionConfig::new(user_config, site_status, plan)
    
    Note over SO: 4ë‹¨ê³„: ì‹¤í–‰ (ì¼ê´€ëœ ì„¤ì • ê¸°ë°˜)
    SO->>BM: execute_plan(SessionConfig)
    BM-->>SO: WorkflowResult
    SO-->>Facade: WorkflowResult
    Facade-->>UI: í¬ë¡¤ë§ ì™„ë£Œ ê²°ê³¼
        PA-->>PC: page_info (í˜ì´ì§€ êµ¬ì¡°, URL íŒ¨í„´)
        
        PC->>PC: calculate_optimal_strategy()
        PC->>CACHE: save_analysis_result()
        PC-->>SO: comprehensive_analysis_result
    end
    
    SO->>BM: create_batch_plan(analysis_result)
    BM-->>SO: optimized_batch_plan
    SO->>EH: SessionEvent::ReadyToCrawl
    EH->>UI: í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œ + ì˜ˆìƒ ì‹œê°„
```

#### í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ ì„¤ê³„

```mermaid
flowchart TD
    subgraph "Pre-Crawling Analyzer"
        PCA[PreCrawlingAnalyzer<br/>í†µí•© ë¶„ì„ ì¡°ì •ì]
        
        subgraph "ë¶„ì„ ëª¨ë“ˆë“¤"
            SA[SiteAnalyzer<br/>ì‚¬ì´íŠ¸ êµ¬ì¡° ë¶„ì„]
            DA[DatabaseAnalyzer<br/>DB ìƒíƒœ ë¶„ì„]
            PA[PageDiscoveryAnalyzer<br/>í˜ì´ì§€ êµ¬ì¡° ë°œê²¬]
            MA[MemoryAnalyzer<br/>ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë¶„ì„]
        end
        
        subgraph "ìºì‹± ì‹œìŠ¤í…œ"
            AC[AnalysisCache<br/>ë¶„ì„ ê²°ê³¼ ìºì‹œ]
            CV[CacheValidator<br/>ìºì‹œ ìœ íš¨ì„± ê²€ì¦]
        end
    end
    
    subgraph "ë¶„ì„ ê²°ê³¼"
        AR[AnalysisResult<br/>ì¢…í•© ë¶„ì„ ì •ë³´]
        BS[BatchStrategy<br/>ìµœì  ë°°ì¹˜ ì „ëµ]
        CS[CrawlingStrategy<br/>í¬ë¡¤ë§ ì „ëµ]
    end
    
    PCA --> SA
    PCA --> DA
    PCA --> PA
    PCA --> MA
    
    PCA --> AC
    AC --> CV
    
    SA --> AR
    DA --> AR
    PA --> AR
    MA --> AR
    
    AR --> BS
    AR --> CS
    
    style PCA fill:#e3f2fd
    style AC fill:#f3e5f5
    style AR fill:#e8f5e8
```

#### SiteStatusChecker: ë‹¨ì¼ ì±…ì„ ê¸°ë°˜ í†µí•© ë¶„ì„ ëª¨ë“ˆ (re-arch-plan-r-gem ì „ë©´ ì¬ì„¤ê³„)

**âš ï¸ ê¸°ì¡´ ì„¤ê³„ íê¸°**: re-arch-plan-r-gem.md ê²€í†  ê²°ê³¼, ê¸°ì¡´ SiteAnalyzerì™€ PageDiscoveryAnalyzer ë¶„ë¦¬ëŠ” **í˜„ì‹¤ê³¼ ë™ë–¨ì–´ì§„ ì˜ëª»ëœ ì„¤ê³„**ë¡œ íŒëª…ë˜ì–´ **ì „ë©´ íê¸°í•©ë‹ˆë‹¤**.

**âœ… ìƒˆë¡œìš´ ì„¤ê³„**: ëª…í™•í•œ ë‹¨ì¼ ì±…ì„ì„ ê°€ì§„ **SiteStatusChecker** ëª¨ë“ˆë¡œ ì™„ì „ ëŒ€ì²´í•˜ì—¬ íš¨ìœ¨ì„±ê³¼ ëª…í™•ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

```rust
// src-tauri/src/new_architecture/analyzers/site_status_checker.rs
//! ì‚¬ì´íŠ¸ ìƒíƒœ ë° ê·œëª¨ í†µí•© ë¶„ì„ê¸° (ê¸°ì¡´ ì„¤ê³„ ì „ë©´ ëŒ€ì²´)

use crate::constants;
use std::time::{Duration, Instant};

/// ì‚¬ì´íŠ¸ ìƒíƒœ ë° ê·œëª¨ í†µí•© ë¶„ì„ê¸° (SiteAnalyzer + PageDiscoveryAnalyzer í†µí•©)
pub struct SiteStatusChecker {
    site_config: SiteConfig,
    http_client: HttpClient,
}

impl SiteStatusChecker {
    /// ì‚¬ì´íŠ¸ ìƒíƒœ ë° ê·œëª¨ í†µí•© ë¶„ì„ (ë‹¨ì¼ ì±…ì„)
    /// 
    /// í•µì‹¬ ì§ˆë¬¸: "í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ì˜ í˜„ì¬ ìƒíƒœì™€ ì •í™•í•œ ê·œëª¨ëŠ” ì–´ë–»ê²Œ ë˜ëŠ”ê°€?"
    pub async fn check_site_status_and_scale(&self) -> Result<SiteStatus> {
        let start_time = Instant::now();
        
        // 1ë‹¨ê³„: ì²« í˜ì´ì§€ ì ‘ê·¼ + ì—°ê²°ì„± í™•ì¸
        let first_page_response = self.access_first_page().await?;
        if !first_page_response.is_accessible {
            return Ok(SiteStatus::inaccessible(start_time.elapsed()));
        }
        
        // 2ë‹¨ê³„: í˜ì´ì§€ë„¤ì´ì…˜ êµ¬ì¡° ë¶„ì„í•˜ì—¬ ì´ í˜ì´ì§€ ìˆ˜ íŒŒì•…
        let total_pages = self.extract_total_pages(&first_page_response.html).await?;
        
        // 3ë‹¨ê³„: ë§ˆì§€ë§‰ í˜ì´ì§€ë¡œ ì§ì ‘ ì´ë™
        let last_page_response = self.access_last_page(total_pages).await?;
        
        // 4ë‹¨ê³„: ë§ˆì§€ë§‰ í˜ì´ì§€ì— í¬í•¨ëœ ì œí’ˆ ìˆ˜ ê³„ì‚°
        let products_on_last_page = self.count_products_on_page(&last_page_response.html).await?;
        
        // 5ë‹¨ê³„: ì „ì²´ ì œí’ˆ ìˆ˜ ì¶”ì •
        let analysis_duration = start_time.elapsed();
        
        Ok(SiteStatus::new(
            total_pages,
            products_on_last_page,
            first_page_response.server_info,
            first_page_response.avg_response_time,
            analysis_duration,
        ))
    }
    
    /// 1ë‹¨ê³„: ì²« í˜ì´ì§€ ì ‘ê·¼ ë° ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
    async fn access_first_page(&self) -> Result<FirstPageResponse> {
        let start = Instant::now();
        
        let response = self.http_client
            .get(&self.site_config.base_url)
            .timeout(constants::analysis_cache::QUICK_VALIDATION_TIMEOUT)
            .send()
            .await?;
        
        let response_time = start.elapsed();
        let html = response.text().await?;
        
        Ok(FirstPageResponse {
            is_accessible: response.status().is_success(),
            status_code: response.status().as_u16(),
            html,
            avg_response_time: response_time,
            server_info: response.headers().get("server")
                .and_then(|v| v.to_str().ok())
                .map(|s| s.to_string()),
        })
    }
    
    /// 2ë‹¨ê³„: í˜ì´ì§€ë„¤ì´ì…˜ì—ì„œ ì´ í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ
    async fn extract_total_pages(&self, html: &str) -> Result<u32> {
        // HTML íŒŒì‹±í•˜ì—¬ í˜ì´ì§€ë„¤ì´ì…˜ êµ¬ì¡° ë¶„ì„
        let document = Html::parse_document(html);
        
        // ì¼ë°˜ì ì¸ í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ë“¤ì„ ì‹œë„
        let selectors = [
            ".pagination .page-link:last-child",
            ".paging a:last-child",
            ".page-numbers:last-child",
            "a[href*='page=']:last-child",
        ];
        
        for selector_str in &selectors {
            if let Ok(selector) = Selector::parse(selector_str) {
                if let Some(element) = document.select(&selector).next() {
                    if let Ok(page_num) = element.text().collect::<String>().trim().parse::<u32>() {
                        return Ok(page_num);
                    }
                }
            }
        }
        
        // URL íŒ¨í„´ ë¶„ì„ì„ í†µí•œ ì´ í˜ì´ì§€ ìˆ˜ ì¶”ì •
        self.estimate_total_pages_from_url_pattern().await
    }
    
    /// 3ë‹¨ê³„: ë§ˆì§€ë§‰ í˜ì´ì§€ ì§ì ‘ ì ‘ê·¼
    async fn access_last_page(&self, total_pages: u32) -> Result<LastPageResponse> {
        let last_page_url = self.construct_page_url(total_pages);
        
        let response = self.http_client
            .get(&last_page_url)
            .timeout(constants::network::DEFAULT_TIMEOUT)
            .send()
            .await?;
        
        let html = response.text().await?;
        
        Ok(LastPageResponse {
            html,
            is_valid: response.status().is_success(),
        })
    }
    
    /// 4ë‹¨ê³„: í˜ì´ì§€ì˜ ì œí’ˆ ìˆ˜ ê³„ì‚°
    async fn count_products_on_page(&self, html: &str) -> Result<u32> {
        let document = Html::parse_document(html);
        
        // ì œí’ˆ ì•„ì´í…œì„ ë‚˜íƒ€ë‚´ëŠ” ì¼ë°˜ì ì¸ ì…€ë ‰í„°ë“¤
        let product_selectors = [
            ".product-item",
            ".item",
            ".product",
            "[data-product-id]",
            ".list-item",
        ];
        
        for selector_str in &product_selectors {
            if let Ok(selector) = Selector::parse(selector_str) {
                let count = document.select(&selector).count() as u32;
                if count > 0 {
                    return Ok(count);
                }
            }
        }
        
        // ê¸°ë³¸ê°’: ì¼ë°˜ì ì¸ í˜ì´ì§€ë‹¹ ì œí’ˆ ìˆ˜
        Ok(12) // ëŒ€ë¶€ë¶„ ì‚¬ì´íŠ¸ê°€ í˜ì´ì§€ë‹¹ 12ê°œ ë‚´ì™¸
    }
}

/// ì‚¬ì´íŠ¸ ìƒíƒœ ì •ë³´ (í†µí•© ê²°ê³¼)
#[derive(Debug, Clone)]
pub struct SiteStatus {
    /// ì‚¬ì´íŠ¸ì—ì„œ ë°œê²¬ëœ ì´ í˜ì´ì§€ ìˆ˜
    pub total_pages: u32,
    
    /// ë§ˆì§€ë§‰ í˜ì´ì§€ì— ì¡´ì¬í•˜ëŠ” ì œí’ˆì˜ ìˆ˜
    pub products_on_last_page: u32,
    
    /// ìœ„ ë‘ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„ì‚°ëœ ì „ì²´ ì œí’ˆ ìˆ˜
    pub calculated_total_products: u32,
    
    /// ì‚¬ì´íŠ¸ ì ‘ì† ê°€ëŠ¥ ì—¬ë¶€
    pub is_accessible: bool,
    
    /// ì„œë²„ ì •ë³´
    pub server_info: Option<String>,
    
    /// í‰ê·  ì‘ë‹µ ì‹œê°„
    pub avg_response_time: Duration,
    
    /// ë¶„ì„ì— ì†Œìš”ëœ ì‹œê°„
    pub analysis_duration: Duration,
}

impl SiteStatus {
    /// ì •ìƒì ì¸ ì‚¬ì´íŠ¸ ìƒíƒœ ìƒì„±
    pub fn new(
        total_pages: u32,
        products_on_last_page: u32,
        server_info: Option<String>,
        avg_response_time: Duration,
        analysis_duration: Duration,
    ) -> Self {
        // ì „ì²´ ì œí’ˆ ìˆ˜ ê³„ì‚° ë¡œì§
        let calculated_total_products = if total_pages > 0 {
            // ì¼ë°˜ì ìœ¼ë¡œ ë§ˆì§€ë§‰ í˜ì´ì§€ ì œì™¸í•˜ê³ ëŠ” í˜ì´ì§€ë‹¹ 12ê°œ ê°€ì •
            (total_pages - 1) * 12 + products_on_last_page
        } else {
            0
        };
        
        Self {
            total_pages,
            products_on_last_page,
            calculated_total_products,
            is_accessible: true,
            server_info,
            avg_response_time,
            analysis_duration,
        }
    }
    
    /// ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ ìƒíƒœ
    pub fn inaccessible(analysis_duration: Duration) -> Self {
        Self {
            total_pages: 0,
            products_on_last_page: 0,
            calculated_total_products: 0,
            is_accessible: false,
            server_info: None,
            avg_response_time: Duration::from_secs(0),
            analysis_duration,
        }
    }
    
    /// í¬ë¡¤ë§ ê°€ì¹˜ í‰ê°€
    pub fn is_worth_crawling(&self) -> bool {
        self.is_accessible && self.total_pages > 0 && self.calculated_total_products > 0
    }
}

/// ì²« í˜ì´ì§€ ì‘ë‹µ êµ¬ì¡°ì²´
#[derive(Debug)]
struct FirstPageResponse {
    is_accessible: bool,
    status_code: u16,
    html: String,
    avg_response_time: Duration,
    server_info: Option<String>,
}

/// ë§ˆì§€ë§‰ í˜ì´ì§€ ì‘ë‹µ êµ¬ì¡°ì²´
#[derive(Debug)]
struct LastPageResponse {
    html: String,
    is_valid: bool,
}
```

// 3. DatabaseAnalyzer - ë¡œì»¬ DB ìƒíƒœ ë¶„ì„
pub struct DatabaseAnalyzer {
    db_connection: DatabaseConnection,
}

impl DatabaseAnalyzer {
    /// ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ëŠ¥ë ¥ ë° ìƒíƒœ ë¶„ì„
    pub async fn analyze_database_status(&self) -> Result<DatabaseAnalysisResult> {
        let mut result = DatabaseAnalysisResult::new();
        
        // ì—°ê²° ìƒíƒœ í™•ì¸
        result.connectivity = self.test_db_connectivity().await?;
        
        // ì €ì¥ ê³µê°„ ë¶„ì„
        result.storage_info = self.analyze_storage_capacity().await?;
        
        // ê¸°ì¡´ ë°ì´í„° ë¶„ì„ (ì¤‘ë³µ ë°©ì§€ìš©)
        result.existing_data = self.analyze_existing_data().await?;
        
        // ì“°ê¸° ì„±ëŠ¥ ì¸¡ì •
        result.write_performance = self.measure_write_performance().await?;
        
        Ok(result)
    }
    
    /// ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì •ë³´ ì¡°íšŒ (ì¦ë¶„ ì—…ë°ì´íŠ¸ìš©)
    pub async fn get_last_crawling_info(&self, site_name: &str) -> Result<Option<LastCrawlingInfo>> {
        // ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œì , ìˆ˜ì§‘ëœ ë°ì´í„° ë²”ìœ„ ë“± ì¡°íšŒ
    }
}

// 4. í†µí•© ë¶„ì„ ê²°ê³¼
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComprehensiveAnalysisResult {
    pub site_info: SiteAnalysisResult,
    pub page_structure: PageStructureInfo,
    pub database_status: DatabaseAnalysisResult,
    pub memory_capacity: MemoryAnalysisResult,
    pub recommended_strategy: CrawlingStrategy,
    pub optimal_batch_plan: BatchPlan,
    pub estimated_duration: Duration,
    pub analysis_timestamp: SystemTime,
}

// 5. ìºì‹± ì‹œìŠ¤í…œ
pub struct AnalysisCache {
    cache_dir: PathBuf,
    max_cache_age: Duration,
}

impl AnalysisCache {
    /// ë¶„ì„ ê²°ê³¼ ìºì‹œ ì¡°íšŒ
    pub async fn get_cached_analysis(&self, site_name: &str) -> Option<ComprehensiveAnalysisResult> {
        let cache_file = self.cache_dir.join(format!("{}_analysis.json", site_name));
        
        if !cache_file.exists() {
            return None;
        }
        
        // íŒŒì¼ ìƒì„± ì‹œê°„ í™•ì¸
        let metadata = std::fs::metadata(&cache_file).ok()?;
        let age = metadata.modified().ok()?.elapsed().ok()?;
        
        if age > self.max_cache_age {
            // ìºì‹œ ë§Œë£Œ
            let _ = std::fs::remove_file(&cache_file);
            return None;
        }
        
        // ìºì‹œ ë¡œë“œ
        let content = std::fs::read_to_string(&cache_file).ok()?;
        serde_json::from_str(&content).ok()
    }
    
    /// ë¶„ì„ ê²°ê³¼ ìºì‹œ ì €ì¥
    pub async fn save_analysis_result(
        &self, 
        site_name: &str, 
        result: &ComprehensiveAnalysisResult
    ) -> Result<()> {
        let cache_file = self.cache_dir.join(format!("{}_analysis.json", site_name));
        let content = serde_json::to_string_pretty(result)?;
        std::fs::write(&cache_file, content)?;
        Ok(())
    }
}
```

#### êµ¬í˜„ ìš°ì„ ìˆœìœ„ (re-arch-plan-r-gem ì—­í•  ì¬ì„¤ê³„ ë°˜ì˜)

**Week 1 - Facade Layer & UI í”¼ë“œë°± ê¸°ë°˜ êµ¬ì¡° (re-arch-plan-r-gem ë³´ì™„)**
1. **CrawlingFacade**: UIì™€ ë‚´ë¶€ ì‹œìŠ¤í…œ ê°„ ëª…í™•í•œ API (NewCrawlingSystem ëŒ€ì²´)
2. **ì´ë²¤íŠ¸ ì‹œìŠ¤í…œ**: AppEvent, EventHub, EventEmitter íŠ¸ë ˆì´íŠ¸ êµ¬í˜„
3. **SessionConfig**: ë¶ˆë³€ ì„¸ì…˜ ì„¤ì • êµ¬ì¡°ì²´ (ì‹¤í–‰ ì¼ê´€ì„± í™•ë³´)
4. **AppContext**: ëª¨ë“  ì‘ì—…ì— ì „íŒŒë˜ëŠ” ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸
5. **UI Event Listener**: ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìˆ˜ì‹  ë° UI ì—…ë°ì´íŠ¸ ê¸°ë°˜

**Week 2 - ë¶„ì„ Layer & ì´ë²¤íŠ¸ ë°œí–‰ (íˆ¬ëª…ì„± í™•ë³´)**
1. **SiteStatusChecker**: í†µí•© ì‚¬ì´íŠ¸ ìƒíƒœ ë¶„ì„ê¸° (ì§„í–‰ ì´ë²¤íŠ¸ ë°œí–‰)
2. **DatabaseAnalyzer**: DB ìƒíƒœ ë¶„ì„ (ë¶„ì„ ì§„í–‰ ìƒí™© ì‹¤ì‹œê°„ ì „ë‹¬)
3. **PreCrawlingAnalyzer**: ë°ì´í„° ìˆ˜ì§‘ ì¡°ì •ì (AnalysisEvent ë°œí–‰ ì±…ì„)
4. **CrawlingPlanner**: ë„ë©”ì¸ ì§€ì‹ ì¤‘ì‹¬ ê³„íš ìˆ˜ë¦½ (PlanningEvent ë°œí–‰)
5. **AnalysisCache**: ë¶„ì„ ê²°ê³¼ ìºì‹± (ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ ì´ë²¤íŠ¸)

**Week 3 - ì‹¤í–‰ ì›Œí¬í”Œë¡œ & ì‹¤ì‹œê°„ í”¼ë“œë°± (ê¹œê¹œì´ ì‹œìŠ¤í…œ íƒˆí”¼)**
1. **SessionOrchestrator**: ì›Œí¬í”Œë¡œ ì¡°ì •ì (ê° ë‹¨ê³„ë³„ SessionEvent/BatchEvent ë°œí–‰)
2. **ExecutionContext**: ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬ (ì‹¤ì‹œê°„ ìƒíƒœ ì´ë²¤íŠ¸ ë°œí–‰)
3. **CrawlingAction**: ì›ìì  í¬ë¡¤ë§ ì‘ì—… ë‹¨ìœ„ (ì§„í–‰ë¥  ì¶”ì )
4. **ErrorHandler**: í†µí•© ì—ëŸ¬ ì²˜ë¦¬ê¸° (ì—ëŸ¬ ì´ë²¤íŠ¸ ë°œí–‰)
5. **UI Progress Components**: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ (EventListener ê¸°ë°˜)

**Week 4 - ì„±ëŠ¥ ìµœì í™” & ëª¨ë‹ˆí„°ë§ (í”„ë¡œë•ì…˜ ì¤€ë¹„)**
1. **BatchProcessor**: ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬ ìµœì í™” (ë°°ì¹˜ ì§„í–‰ ì´ë²¤íŠ¸)
2. **MemoryManager**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê´€ë¦¬ (ë¦¬ì†ŒìŠ¤ ìƒíƒœ ì´ë²¤íŠ¸)
3. **MetricsCollector**: ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ (ë©”íŠ¸ë¦­ ì´ë²¤íŠ¸)
4. **HealthChecker**: ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ëª¨ë‹ˆí„°ë§
5. **Production Dashboard**: ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ëŒ€ì‹œë³´ë“œ

#### CrawlingPlanner: ê²€ì¦ëœ ë„ë©”ì¸ ì§€ì‹ì˜ ê³„ìŠ¹

**âš ï¸ ë¬¸ì œ ì¸ì‹**: ê¸°ì¡´ ì„¤ê³„ëŠ” ê¸°ìˆ ì  êµ¬ì¡°ë§Œ ì œì‹œí•˜ê³  **í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§(ë„ë©”ì¸ ì§€ì‹)**ì´ ëˆ„ë½ë¨
- ì¦ë¶„ í¬ë¡¤ë§ (Incremental Crawling) ë¡œì§
- ëˆ„ë½ ë°ì´í„° ë³µêµ¬ (Missing Data Recovery) ë¡œì§  
- ì§€ëŠ¥ì ì¸ ë²”ìœ„ ê³„ì‚° (Dynamic Range Calculation) ë¡œì§

**âœ… í•´ê²°ì±…**: ë°ì´í„° ìˆ˜ì§‘(Analyzers)ê³¼ ì˜ì‚¬ê²°ì •(CrawlingPlanner)ì˜ ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬

```mermaid
graph TD
    subgraph "Data Gathering Layer (ë¶„ì„ ëª¨ë“ˆ)"
        A[SiteStatusChecker<br/>"ì‚¬ì´íŠ¸ëŠ” ì´ 1250 í˜ì´ì§€ì´ê³ ,<br/>ë§ˆì§€ë§‰ í˜ì´ì§€ì—” 8ê°œ ìˆì–´."]
        B[DatabaseAnalyzer<br/>"DBì—ëŠ” 1200 í˜ì´ì§€ê¹Œì§€<br/>ì €ì¥ë˜ì–´ ìˆì–´."]
    end

    subgraph "Decision Making Layer (ë„ë©”ì¸ ì„œë¹„ìŠ¤)"
        C["<b>CrawlingPlanner</b><br/>(Domain Knowledge Inside)"]
    end

    subgraph "Final Output"
        D["<b>CrawlingPlan</b><br/>"ì´ë²ˆì—” 1201~1250 í˜ì´ì§€ë§Œ<br/>ìˆ˜ì§‘í•˜ë©´ ë˜ê² ë‹¤!"]
    end

    A --> C
    B --> C
    C --> D
```

#### CrawlingPlanner ìƒì„¸ ì„¤ê³„

```rust
// src-tauri/src/new_architecture/domain/planner.rs
//! í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½ ë„ë©”ì¸ ì„œë¹„ìŠ¤ (ê¸°ì¡´ ê²€ì¦ëœ ë¡œì§ ê³„ìŠ¹)

/// í¬ë¡¤ë§ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ë„ë©”ì¸ ì„œë¹„ìŠ¤
/// 
/// í•µì‹¬ ì±…ì„: ìˆ˜ì§‘ëœ ë¶„ì„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ í¬ë¡¤ë§ ê³„íš ìƒì„±
/// í¬í•¨ëœ ë„ë©”ì¸ ì§€ì‹: ì¦ë¶„ ìˆ˜ì§‘, ë³µêµ¬ ìˆ˜ì§‘, ë²”ìœ„ ìµœì í™”
pub struct CrawlingPlanner;

impl CrawlingPlanner {
    /// ğŸ¯ 3ê°€ì§€ ì£¼ìš” ì •ë³´ë¥¼ ì¢…í•©í•œ í¬ê´„ì  í¬ë¡¤ë§ ê³„íš ìˆ˜ë¦½
    /// 
    /// **ì¢…í•© íŒë‹¨ ìš”ì†Œ**:
    /// 1. ì‚¬ìš©ì ì˜ë„ (CrawlType): ì „ì²´/ì¦ë¶„/ë³µêµ¬
    /// 2. ì‚¬ì´íŠ¸ ìƒíƒœ (SiteStatus): ì´ í˜ì´ì§€ ìˆ˜, ì‘ë‹µ ì†ë„, ë¶€í•˜ ìƒíƒœ
    /// 3. DB ìƒíƒœ (DBStateReport): ê¸°ì¡´ ë°ì´í„°, ëˆ„ë½ í˜ì´ì§€, ì˜¤ë¥˜ íŒ¨í„´
    pub async fn create_comprehensive_plan(
        &self,
        user_intent: CrawlType,           // ì‚¬ìš©ì ì˜ë„ (ì „ì²´/ì¦ë¶„/ë³µêµ¬)
        site_status: &SiteStatus,         // ì‚¬ì´íŠ¸ í˜„ì¬ ìƒíƒœ  
        db_report: &DBStateReport,        // DB ê³¼ê±° ìƒíƒœ
    ) -> Result<CrawlingPlan, PlanningError> {
        
        if !site_status.is_accessible {
            return Err(PlanningError::SiteNotAccessible);
        }

        let (start_page, end_page, strategy) = match user_intent {
            CrawlType::Full => {
                // **ë„ë©”ì¸ ì§€ì‹ 1: ì „ì²´ í¬ë¡¤ë§**
                // ì²˜ìŒë¶€í„° ëê¹Œì§€ ëª¨ë“  í˜ì´ì§€ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•¨
                (1, site_status.total_pages, CrawlingStrategy::Full)
            }
            CrawlType::Incremental => {
                // **ë„ë©”ì¸ ì§€ì‹ 2: ì¦ë¶„ í¬ë¡¤ë§** 
                let last_crawled = db_report.last_crawled_page.unwrap_or(0);
                if last_crawled >= site_status.total_pages {
                    // ì´ë¯¸ ìµœì‹  ìƒíƒœì´ë¯€ë¡œ í•  ì¼ì´ ì—†ìŒ
                    return Ok(CrawlingPlan::no_action_needed());
                }
                // ë§ˆì§€ë§‰ìœ¼ë¡œ ìˆ˜ì§‘í•œ í˜ì´ì§€ ë‹¤ìŒë¶€í„° ëê¹Œì§€
                (last_crawled + 1, site_status.total_pages, CrawlingStrategy::Incremental)
            }
            CrawlType::Recovery => {
                // **ë„ë©”ì¸ ì§€ì‹ 3: ë³µêµ¬ í¬ë¡¤ë§**
                // DB ë¶„ì„ ê²°ê³¼ ëˆ„ë½ëœ í˜ì´ì§€ë§Œì„ ëŒ€ìƒìœ¼ë¡œ í•¨
                return Ok(CrawlingPlan::for_recovery(
                    db_report.missing_pages.clone(),
                    site_status,
                    db_report
                ));
            }
        };

        if start_page > end_page {
            return Ok(CrawlingPlan::no_action_needed());
        }

        // ğŸ¯ 3ê°€ì§€ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì  BatchConfig ê²°ì •
        let batch_config = self.determine_optimal_batch_config(
            &strategy,
            site_status,
            db_report,
            end_page - start_page + 1, // ì´ ì‘ì—… í˜ì´ì§€ ìˆ˜
        );

        Ok(CrawlingPlan {
            target_pages: (start_page..=end_page).collect(),
            strategy,
            estimated_items: self.estimate_items(start_page, end_page, site_status),
            priority: PlanPriority::Normal,
            batch_config,
        })
    }

    /// ğŸ§  ë„ë©”ì¸ ì§€ì‹ ì¤‘ì‹¬: 3ê°€ì§€ ì •ë³´ ì¢…í•©ìœ¼ë¡œ ìµœì  ë°°ì¹˜ ì„¤ì • ê²°ì •
    /// 
    /// **ê²°ì • ì•Œê³ ë¦¬ì¦˜**:
    /// - ì‚¬ì´íŠ¸ ì‘ë‹µ ì†ë„ê°€ ëŠë¦¬ë©´ â†’ ì‘ì€ ë°°ì¹˜ í¬ê¸° + ê¸´ ì§€ì—°ì‹œê°„
    /// - DB ì˜¤ë¥˜ íŒ¨í„´ì´ ë§ìœ¼ë©´ â†’ ë§ì€ ì¬ì‹œë„ + ì—ëŸ¬ ë°±ì˜¤í”„
    /// - ëŒ€ëŸ‰ ì‘ì—…ì´ë©´ â†’ í° ë°°ì¹˜ í¬ê¸°ë¡œ íš¨ìœ¨ì„± í™•ë³´
    /// - ë³µêµ¬ ì‘ì—…ì´ë©´ â†’ ì‹ ì¤‘í•œ ì¬ì‹œë„ ì •ì±…
    fn determine_optimal_batch_config(
        &self,
        strategy: &CrawlingStrategy,
        site_status: &SiteStatus,
        db_report: &DBStateReport,
        total_pages: u32,
    ) -> BatchConfig {
        
        // 1ï¸âƒ£ ì‚¬ì´íŠ¸ ìƒíƒœ ê¸°ë°˜ ê¸°ë³¸ ë°°ì¹˜ í¬ê¸° ê²°ì •
        let base_batch_size = match site_status.average_response_time_ms {
            0..=500 => 50,      // ë¹ ë¥¸ ì‘ë‹µ: í° ë°°ì¹˜
            501..=2000 => 20,   // ë³´í†µ ì‘ë‹µ: ì¤‘ê°„ ë°°ì¹˜  
            _ => 10,            // ëŠë¦° ì‘ë‹µ: ì‘ì€ ë°°ì¹˜
        };

        // 2ï¸âƒ£ DB ì˜¤ë¥˜ íŒ¨í„´ ê¸°ë°˜ ì¬ì‹œë„ ì •ì±… ê²°ì •
        let error_rate = db_report.recent_error_count as f32 / db_report.total_attempts.max(1) as f32;
        let max_retries = match error_rate {
            0.0..=0.05 => 3,      // ë‚®ì€ ì˜¤ë¥˜ìœ¨: ê¸°ë³¸ ì¬ì‹œë„
            0.05..=0.15 => 5,     // ì¤‘ê°„ ì˜¤ë¥˜ìœ¨: ì¦ê°€ëœ ì¬ì‹œë„
            _ => 8,               // ë†’ì€ ì˜¤ë¥˜ìœ¨: ì ê·¹ì  ì¬ì‹œë„
        };

        // 3ï¸âƒ£ í¬ë¡¤ë§ ì „ëµë³„ ì„¸ë¶€ ì¡°ì •
        let (adjusted_batch_size, delay_ms) = match strategy {
            CrawlingStrategy::Full => {
                // ì „ì²´ í¬ë¡¤ë§: íš¨ìœ¨ì„± ìš°ì„ , í° ë°°ì¹˜
                (base_batch_size * 2, 1000)
            }
            CrawlingStrategy::Incremental => {
                // ì¦ë¶„ í¬ë¡¤ë§: ê· í˜• ì¡íŒ ì ‘ê·¼
                (base_batch_size, 1500)
            }
            CrawlingStrategy::Recovery => {
                // ë³µêµ¬ í¬ë¡¤ë§: ì‹ ì¤‘í•¨ ìš°ì„ , ì‘ì€ ë°°ì¹˜ + ê¸´ ì§€ì—°
                (base_batch_size / 2, 3000)
            }
            CrawlingStrategy::NoAction => {
                // ì‘ì—… ì—†ìŒ: ê¸°ë³¸ê°’
                (1, 1000)
            }
        };

        // 4ï¸âƒ£ ì´ ì‘ì—…ëŸ‰ ê¸°ë°˜ ìµœì¢… ì¡°ì •
        let final_batch_size = if total_pages > 1000 {
            adjusted_batch_size * 2  // ëŒ€ëŸ‰ ì‘ì—…: ë°°ì¹˜ í¬ê¸° ì¦ê°€
        } else if total_pages < 50 {
            (adjusted_batch_size / 2).max(1)  // ì†ŒëŸ‰ ì‘ì—…: ë°°ì¹˜ í¬ê¸° ê°ì†Œ
        } else {
            adjusted_batch_size
        };

        BatchConfig {
            batch_size: final_batch_size,
            max_retries,
            delay_between_batches_ms: delay_ms,
            timeout_per_request_ms: site_status.average_response_time_ms * 3 + 5000,
            concurrent_requests: if site_status.server_load_level < 0.7 { 3 } else { 1 },
        }
    }

    /// í˜ì´ì§€ ë²”ìœ„ì™€ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ ì•„ì´í…œ ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë¡œì§
    fn estimate_items(&self, start: u32, end: u32, status: &SiteStatus) -> u32 {
        if start > end { return 0; }
        
        let num_pages = end - start + 1;
        if end == status.total_pages {
            // ë§ˆì§€ë§‰ í˜ì´ì§€ê°€ í¬í•¨ëœ ê²½ìš°: (ì „ì²´ í˜ì´ì§€ - 1) * ê¸°ë³¸ê°’ + ë§ˆì§€ë§‰ í˜ì´ì§€ ì‹¤ì œê°’
            (num_pages - 1) * constants::site::DEFAULT_PRODUCTS_PER_PAGE + status.products_on_last_page
        } else {
            // ë§ˆì§€ë§‰ í˜ì´ì§€ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°: ëª¨ë“  í˜ì´ì§€ * ê¸°ë³¸ê°’
            num_pages * constants::site::DEFAULT_PRODUCTS_PER_PAGE
        }
    }
}

/// í¬ë¡¤ë§ ê³„íš ê²°ê³¼
#[derive(Debug, Clone)]
pub struct CrawlingPlan {
    /// í¬ë¡¤ë§ ëŒ€ìƒ í˜ì´ì§€ ëª©ë¡
    pub target_pages: Vec<u32>,
    
    /// í¬ë¡¤ë§ ì „ëµ
    pub strategy: CrawlingStrategy,
    
    /// ì˜ˆìƒ ìˆ˜ì§‘ ì•„ì´í…œ ìˆ˜
    pub estimated_items: u32,
    
    /// ê³„íš ìš°ì„ ìˆœìœ„
    pub priority: PlanPriority,
    
    /// ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì í™”ëœ ë°°ì¹˜ ì„¤ì •
    pub batch_config: BatchConfig,
}

impl CrawlingPlan {
    /// ì‘ì—…ì´ í•„ìš” ì—†ëŠ” ê²½ìš°ì˜ ê³„íš
    pub fn no_action_needed() -> Self {
        Self {
            target_pages: vec![],
            strategy: CrawlingStrategy::NoAction,
            estimated_items: 0,
            priority: PlanPriority::None,
            batch_config: BatchConfig::minimal(), // ìµœì†Œí•œì˜ ê¸°ë³¸ ì„¤ì •
        }
    }
    
    /// ë³µêµ¬ í¬ë¡¤ë§ì„ ìœ„í•œ ê³„íš (ì—°ì†ë˜ì§€ ì•Šì€ í˜ì´ì§€ë“¤)
    /// 
    /// **ë³µêµ¬ ì‘ì—… íŠ¹ì„±**: ì‹ ì¤‘í•œ ë°°ì¹˜ ì„¤ì • ì ìš©
    /// - ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ì•ˆì •ì„± í™•ë³´
    /// - ë†’ì€ ì¬ì‹œë„ íšŸìˆ˜ë¡œ ë³µêµ¬ ì™„ë£Œìœ¨ í–¥ìƒ
    /// - ê¸´ ì§€ì—° ì‹œê°„ìœ¼ë¡œ ì‚¬ì´íŠ¸ ë¶€í•˜ ìµœì†Œí™”
    pub fn for_recovery(
        missing_pages: Vec<u32>,
        site_status: &SiteStatus,
        db_report: &DBStateReport
    ) -> Self {
        let estimated_items = missing_pages.len() as u32 * constants::site::DEFAULT_PRODUCTS_PER_PAGE;
        
        // ë³µêµ¬ ì‘ì—…ì„ ìœ„í•œ ì‹ ì¤‘í•œ ë°°ì¹˜ ì„¤ì •
        let batch_config = BatchConfig {
            batch_size: 5,  // ë³µêµ¬ëŠ” ì‘ì€ ë°°ì¹˜ë¡œ ì•ˆì „í•˜ê²Œ
            max_retries: 8, // ë³µêµ¬ëŠ” ì¬ì‹œë„ë¥¼ ì ê·¹ì ìœ¼ë¡œ
            delay_between_batches_ms: 5000, // ë³µêµ¬ëŠ” ê¸´ ì§€ì—°ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ
            timeout_per_request_ms: site_status.average_response_time_ms * 5 + 10000,
            concurrent_requests: 1, // ë³µêµ¬ëŠ” ìˆœì°¨ì ìœ¼ë¡œ
        };
        
        Self {
            target_pages: missing_pages,
            strategy: CrawlingStrategy::Recovery,
            estimated_items,
            priority: PlanPriority::High, // ë³µêµ¬ëŠ” ë†’ì€ ìš°ì„ ìˆœìœ„
            batch_config,
        }
    }
    
    /// í¬ë¡¤ë§ì´ í•„ìš”í•œì§€ í™•ì¸
    pub fn needs_crawling(&self) -> bool {
        !self.target_pages.is_empty() && self.strategy != CrawlingStrategy::NoAction
    }
    
    /// ì—°ì†ëœ í˜ì´ì§€ ë²”ìœ„ë“¤ë¡œ ê·¸ë£¹í™” (íš¨ìœ¨ì ì¸ í¬ë¡¤ë§ì„ ìœ„í•´)
    pub fn get_page_ranges(&self) -> Vec<(u32, u32)> {
        if self.target_pages.is_empty() {
            return vec![];
        }
        
        let mut sorted_pages = self.target_pages.clone();
        sorted_pages.sort();
        
        let mut ranges = vec![];
        let mut start = sorted_pages[0];
        let mut end = sorted_pages[0];
        
        for &page in sorted_pages.iter().skip(1) {
            if page == end + 1 {
                end = page; // ì—°ì†ëœ í˜ì´ì§€
            } else {
                ranges.push((start, end)); // ì´ì „ ë²”ìœ„ ì™„ë£Œ
                start = page;
                end = page;
            }
        }
        ranges.push((start, end)); // ë§ˆì§€ë§‰ ë²”ìœ„
        
        ranges
    }
}

/// ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
#[derive(Debug, Clone)]
pub struct BatchConfig {
    /// ë°°ì¹˜ë‹¹ ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜
    pub batch_size: u32,
    
    /// ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    pub max_retries: u32,
    
    /// ë°°ì¹˜ ê°„ ì§€ì—° ì‹œê°„ (ë°€ë¦¬ì´ˆ)
    pub delay_between_batches_ms: u64,
    
    /// ìš”ì²­ë‹¹ íƒ€ì„ì•„ì›ƒ (ë°€ë¦¬ì´ˆ)
    pub timeout_per_request_ms: u64,
    
    /// ë™ì‹œ ìš”ì²­ ìˆ˜
    pub concurrent_requests: u32,
}

impl BatchConfig {
    /// ìµœì†Œí•œì˜ ê¸°ë³¸ ë°°ì¹˜ ì„¤ì • (ì‘ì—…ì´ ì—†ì„ ë•Œ ì‚¬ìš©)
    pub fn minimal() -> Self {
        Self {
            batch_size: 1,
            max_retries: 1,
            delay_between_batches_ms: 1000,
            timeout_per_request_ms: 30000,
            concurrent_requests: 1,
        }
    }
}
```

## 4. ê¸°ëŒ€ íš¨ê³¼

- **ë‹¨ê³„ì  ì „í™˜**: ê¸°ì¡´ ì‹œìŠ¤í…œì— ì˜í–¥ ìµœì†Œí™”
- **ë¦¬ìŠ¤í¬ ê´€ë¦¬**: ê° ë‹¨ê³„ë³„ë¡œ ë¡¤ë°± ê°€ëŠ¥
- **ì„±ëŠ¥ ìµœì í™”**: ë©”íŠ¸ë¦­ ê¸°ë°˜ ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¡°ì •
- **ë„ë©”ì¸ ì§€ì‹ í™œìš©**: CrawlingPlannerë¥¼ í†µí•œ ê²€ì¦ëœ í¬ë¡¤ë§ ì „ëµ ì ìš©

## 5. ê²°ë¡ 

*ì´ë²ˆ ì•„í‚¤í…ì²˜ ì¬êµ¬ì¶•ì€ ë‹¨ìˆœí•œ ê¸°ìˆ ì  ë³€í™”ê°€ ì•„ë‹Œ, í¬ë¡¤ë§ ì‹œìŠ¤í…œì˜ ë¼ˆëŒ€ë¥¼ ìƒˆë¡­ê²Œ í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ê° ë‹¨ê³„ì—ì„œì˜ ë©´ë°€í•œ ê²€í† ì™€ í…ŒìŠ¤íŠ¸ê°€ í•„ìˆ˜ì ì´ë©°, ì´ë¥¼ í†µí•´ ë”ìš± ê²¬ê³ í•˜ê³  ìœ ì—°í•œ í¬ë¡¤ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.*

## ì•„í‚¤í…ì²˜ ë³€í™˜ ìš”ì•½: ê¹œê¹œì´ ì‹œìŠ¤í…œì—ì„œ íˆ¬ëª…í•œ ì‹œìŠ¤í…œìœ¼ë¡œ

### ê¸°ì¡´ ì‹œìŠ¤í…œì˜ ê·¼ë³¸ì ì¸ ë¬¸ì œ (re-arch-plan-r-gem.md ê²€í†  ê²°ê³¼)

**"ê¹œê¹œì´ ì‹œìŠ¤í…œ" í˜„ìƒ**:
- ì‚¬ìš©ìëŠ” ì‹œìŠ¤í…œì´ ë¬´ì—‡ì„ í•˜ê³  ìˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìŒ
- ì§„í–‰ ìƒí™©, ì˜¤ë¥˜, ì™„ë£Œ ìƒíƒœ ë“± ëª¨ë“  ì •ë³´ê°€ ë¸”ë™ë°•ìŠ¤ ìƒíƒœ
- UI í”¼ë“œë°± ë©”ì»¤ë‹ˆì¦˜ì˜ ì™„ì „í•œ ë¶€ì¬

### ë³€í™˜ëœ ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ íŠ¹ì§•

**1. ì´ë²¤íŠ¸ ì¤‘ì‹¬ íˆ¬ëª…ì„± (Event-Driven Transparency)**
```rust
// ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ EventEmitter íŠ¸ë ˆì´íŠ¸ êµ¬í˜„
trait EventEmitter {
    fn emit_event(&self, event: AppEvent);
}

// ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ê³µìœ 
self.emit_event(AppEvent::Session(SessionEvent::StageStarted {
    stage: "ë¶„ì„".to_string(),
    estimated_duration: Duration::from_secs(30),
}));
```

**2. ë‹¨ê³„ë³„ ì´ë²¤íŠ¸ ë°œí–‰ ì²´ê³„**
- **SessionEvent**: ì„¸ì…˜ ì‹œì‘/ì¢…ë£Œ, ë‹¨ê³„ ì „í™˜
- **AnalysisEvent**: ë¶„ì„ ì§„í–‰ë¥ , ë°œê²¬ ì‚¬í•­, ì™„ë£Œ ìƒíƒœ
- **PlanningEvent**: ê³„íš ìˆ˜ë¦½ ì§„í–‰ ìƒí™©, ìµœì¢… ê³„íš
- **BatchEvent**: ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ë¥ , ê°œë³„ ì‘ì—… ìƒíƒœ

**3. ì»´í¬ë„ŒíŠ¸ë³„ ì´ë²¤íŠ¸ ë°œí–‰ ì±…ì„**
- **SessionOrchestrator**: ì›Œí¬í”Œë¡œ ì „ì²´ ì§„í–‰ ìƒí™©
- **SiteStatusChecker**: ì‚¬ì´íŠ¸ ë¶„ì„ ì„¸ë¶€ ì§„í–‰ë¥ 
- **CrawlingPlanner**: ê³„íš ìˆ˜ë¦½ ê³¼ì • íˆ¬ëª…í™”
- **CrawlingAction**: ê°œë³„ ì‘ì—… ì‹¤ì‹œê°„ ìƒíƒœ

### ì‚¬ìš©ì ê²½í—˜ ê°œì„  íš¨ê³¼

**Before (ê¹œê¹ì´ ì‹œìŠ¤í…œ)**:
```
[í¬ë¡¤ë§ ì‹œì‘] â†’ ... ê¸´ ì¹¨ë¬µ ... â†’ [ì™„ë£Œ ë˜ëŠ” ì—ëŸ¬]
```

**After (íˆ¬ëª…í•œ ì‹œìŠ¤í…œ)**:
```
[ì„¸ì…˜ ì‹œì‘] â†’ [ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘ (30%)] â†’ [DB ë¶„ì„ ì™„ë£Œ] 
â†’ [ê³„íš ìˆ˜ë¦½ ì¤‘] â†’ [ê³„íš ì™„ë£Œ: 50í˜ì´ì§€ ì²˜ë¦¬ ì˜ˆì •] 
â†’ [ë°°ì¹˜ 1/5 ì²˜ë¦¬ ì¤‘ (20%)] â†’ [ëª¨ë“  ì‘ì—… ì™„ë£Œ]
```

### êµ¬í˜„ ìš°ì„ ìˆœìœ„ì—ì„œì˜ UI í”¼ë“œë°± ì¤‘ì‹¬ì„±

1. **Week 1**: ì´ë²¤íŠ¸ ì‹œìŠ¤í…œê³¼ EventEmitter íŠ¸ë ˆì´íŠ¸ ìš°ì„  êµ¬í˜„
2. **Week 2**: ëª¨ë“  ë¶„ì„ ì»´í¬ë„ŒíŠ¸ì— ì§„í–‰ ì´ë²¤íŠ¸ ë°œí–‰ í†µí•©
3. **Week 3**: ì‹¤ì‹œê°„ UI ì»´í¬ë„ŒíŠ¸ì™€ SessionOrchestrator ì´ë²¤íŠ¸ ì—°ë™
4. **Week 4**: í”„ë¡œë•ì…˜ ëŒ€ì‹œë³´ë“œë¡œ ì „ì²´ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§

ì´ ì•„í‚¤í…ì²˜ ë³€í™˜ì„ í†µí•´ **ì‚¬ìš©ìëŠ” ì‹œìŠ¤í…œì˜ ëª¨ë“  ë™ì‘ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íŒŒì•…**í•  ìˆ˜ ìˆìœ¼ë©°, ë¬¸ì œ ë°œìƒ ì‹œ **ì •í™•í•œ ìœ„ì¹˜ì™€ ì›ì¸ì„ ì¦‰ì‹œ ì‹ë³„**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## CrawlingPlanner ì¤‘ì‹¬ì˜ ì ì‘ì  ë°°ì¹˜ ì„¤ì • ì•„í‚¤í…ì²˜

### ğŸ¯ í•µì‹¬ ê°œì„ ì‚¬í•­: ë””í´íŠ¸ì—ì„œ ì§€ëŠ¥í˜• ê²°ì •ìœ¼ë¡œ

**Before (ë¬¸ì œ ìƒí™©)**:
```rust
// âŒ CrawlingFacade ìƒì„± ì‹œì ì— ë””í´íŠ¸ BatchConfig
let batch_config = batch::BatchConfig::default();
let batch_manager = std::sync::Arc::new(BatchManager::new(batch_config));
```

**After (ê°œì„ ëœ ì•„í‚¤í…ì²˜)**:
```rust
// âœ… Planning ë‹¨ê³„ì—ì„œ 3ê°€ì§€ ì •ë³´ ì¢…í•© í›„ ì ì‘ì  ê²°ì •
let crawling_plan = planner.create_comprehensive_plan(
    user_config.crawling.crawl_type.clone(),
    &analysis_result.site_status,      // 1ï¸âƒ£ ì‚¬ì´íŠ¸ í˜„ì¬ ìƒíƒœ
    &analysis_result.db_report,        // 2ï¸âƒ£ DB ê³¼ê±° ë°ì´í„°  
).await?;                              // 3ï¸âƒ£ ì‚¬ìš©ì ì˜ë„

let batch_config = crawling_plan.batch_config.clone(); // ğŸ§  ì§€ëŠ¥í˜• ê²°ì •
let batch_manager = std::sync::Arc::new(BatchManager::new(batch_config));
```

### ğŸ§  CrawlingPlannerì˜ ì§€ëŠ¥í˜• ë°°ì¹˜ ê²°ì • ì•Œê³ ë¦¬ì¦˜

**1. ì‚¬ì´íŠ¸ ìƒíƒœ ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ê²°ì •**
- ë¹ ë¥¸ ì‘ë‹µ (0-500ms) â†’ í° ë°°ì¹˜ (50ê°œ)  
- ë³´í†µ ì‘ë‹µ (501-2000ms) â†’ ì¤‘ê°„ ë°°ì¹˜ (20ê°œ)
- ëŠë¦° ì‘ë‹µ (2000ms+) â†’ ì‘ì€ ë°°ì¹˜ (10ê°œ)

**2. DB ì˜¤ë¥˜ íŒ¨í„´ ê¸°ë°˜ ì¬ì‹œë„ ì •ì±…**
- ë‚®ì€ ì˜¤ë¥˜ìœ¨ (0-5%) â†’ ê¸°ë³¸ ì¬ì‹œë„ (3íšŒ)
- ì¤‘ê°„ ì˜¤ë¥˜ìœ¨ (5-15%) â†’ ì¦ê°€ ì¬ì‹œë„ (5íšŒ)  
- ë†’ì€ ì˜¤ë¥˜ìœ¨ (15%+) â†’ ì ê·¹ ì¬ì‹œë„ (8íšŒ)

**3. í¬ë¡¤ë§ ì „ëµë³„ ì„¸ë¶€ ì¡°ì •**
- **ì „ì²´ í¬ë¡¤ë§**: íš¨ìœ¨ì„± ìš°ì„  â†’ í° ë°°ì¹˜ + ì§§ì€ ì§€ì—°
- **ì¦ë¶„ í¬ë¡¤ë§**: ê· í˜• ì¡íŒ ì ‘ê·¼ â†’ ì¤‘ê°„ ë°°ì¹˜ + ì¤‘ê°„ ì§€ì—°
- **ë³µêµ¬ í¬ë¡¤ë§**: ì‹ ì¤‘í•¨ ìš°ì„  â†’ ì‘ì€ ë°°ì¹˜ + ê¸´ ì§€ì—°

**4. ì´ ì‘ì—…ëŸ‰ ê¸°ë°˜ ìµœì¢… ì¡°ì •**
- ëŒ€ëŸ‰ ì‘ì—… (1000+ í˜ì´ì§€) â†’ ë°°ì¹˜ í¬ê¸° 2ë°° ì¦ê°€
- ì†ŒëŸ‰ ì‘ì—… (50 í˜ì´ì§€ ë¯¸ë§Œ) â†’ ë°°ì¹˜ í¬ê¸° ì ˆë°˜ ê°ì†Œ

### ğŸ“Š ì‹¤ì œ ì ìš© ì‚¬ë¡€

```rust
// ğŸ” ì‹œë‚˜ë¦¬ì˜¤: ì¦ë¶„ í¬ë¡¤ë§, ì‚¬ì´íŠ¸ ì‘ë‹µ 1200ms, ì˜¤ë¥˜ìœ¨ 8%
let optimal_config = BatchConfig {
    batch_size: 20,           // ì¤‘ê°„ ì‘ë‹µ ì†ë„ ê¸°ë°˜
    max_retries: 5,           // ì¤‘ê°„ ì˜¤ë¥˜ìœ¨ ê¸°ë°˜  
    delay_between_batches_ms: 1500,  // ì¦ë¶„ ì „ëµ ê¸°ë°˜
    timeout_per_request_ms: 8600,    // ì‘ë‹µì‹œê°„ * 3 + 5ì´ˆ
    concurrent_requests: 3,   // ì„œë²„ ë¶€í•˜ < 0.7 ê¸°ë°˜
};
```

ì´ì œ **CrawlingFacadeëŠ” ë‹¨ìˆœí•œ ì§„ì…ì  ì—­í• **ë§Œ í•˜ê³ , **CrawlingPlannerê°€ ëª¨ë“  ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ì§€ëŠ¥í˜• ê²°ì •**ì„ ë‹´ë‹¹í•˜ëŠ” ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬ê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
